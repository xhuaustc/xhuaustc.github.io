<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LangChain框架入门与实践：组件详解、使用场景与示例 | Michael Blog</title><meta name="author" content="Michael Pan"><meta name="copyright" content="Michael Pan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="背景与目标读者LangChain 是一个面向大型语言模型（Large Language Models, LLM）应用开发的开源框架，由 Harrison Chase 于 2022 年发布，并在 2023 年成立公司后快速发展。它通过统一的抽象与模块化组件，帮助开发者高效构建复杂的 AI 应用，如聊天机器人、文档问答（RAG）、智能代理（Agent）与自动摘要等。 本文面向有一定 Python 基础">
<meta property="og:type" content="article">
<meta property="og:title" content="LangChain框架入门与实践：组件详解、使用场景与示例">
<meta property="og:url" content="https://xhua.eu.org/posts/475a94dfb51a.html">
<meta property="og:site_name" content="Michael Blog">
<meta property="og:description" content="背景与目标读者LangChain 是一个面向大型语言模型（Large Language Models, LLM）应用开发的开源框架，由 Harrison Chase 于 2022 年发布，并在 2023 年成立公司后快速发展。它通过统一的抽象与模块化组件，帮助开发者高效构建复杂的 AI 应用，如聊天机器人、文档问答（RAG）、智能代理（Agent）与自动摘要等。 本文面向有一定 Python 基础">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.xhua.eu.org/9fc1cdf8c8977cbf80c76c219e1b45cd8838a4300c9634013138eac9dedeef93.jpg">
<meta property="article:published_time" content="2025-08-09T08:37:33.000Z">
<meta property="article:modified_time" content="2026-02-24T07:27:54.627Z">
<meta property="article:author" content="Michael Pan">
<meta property="article:tag" content="Agent">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="LangChain">
<meta property="article:tag" content="Prompt">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img.xhua.eu.org/9fc1cdf8c8977cbf80c76c219e1b45cd8838a4300c9634013138eac9dedeef93.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LangChain框架入门与实践：组件详解、使用场景与示例",
  "url": "https://xhua.eu.org/posts/475a94dfb51a.html",
  "image": "https://img.xhua.eu.org/9fc1cdf8c8977cbf80c76c219e1b45cd8838a4300c9634013138eac9dedeef93.jpg",
  "datePublished": "2025-08-09T08:37:33.000Z",
  "dateModified": "2026-02-24T07:27:54.627Z",
  "author": [
    {
      "@type": "Person",
      "name": "Michael Pan",
      "url": "https://xhua.eu.org"
    }
  ]
}</script><link rel="shortcut icon" href="https://img.xhua.eu.org/ee7822a9c1b896de5649988ed5a9dc89c8f46fb54dd442f2d9c74721a05fa708.jpg"><link rel="canonical" href="https://xhua.eu.org/posts/475a94dfb51a.html"><link rel="preconnect"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: '/pluginsSrc/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LangChain框架入门与实践：组件详解、使用场景与示例',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/preloader-frosted-glass.css"><meta name="generator" content="Hexo 8.1.1"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      if ($loadingBox.classList.contains('loaded')) return
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()

  if (document.readyState === 'complete') {
    preloader.endLoading()
  } else {
    window.addEventListener('load', preloader.endLoading)
    document.addEventListener('DOMContentLoaded', preloader.endLoading)
    // Add timeout protection: force end after 7 seconds
    setTimeout(preloader.endLoading, 7000)
  }

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://img.xhua.eu.org/87ab7c10242ff1ab32f46f7c7b335d0581d3885fa40b8e3dc1d97014e67ea56d.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">264</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">121</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(https://img.xhua.eu.org/9fc1cdf8c8977cbf80c76c219e1b45cd8838a4300c9634013138eac9dedeef93.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Michael Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">LangChain框架入门与实践：组件详解、使用场景与示例</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LangChain框架入门与实践：组件详解、使用场景与示例</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-09T08:37:33.000Z" title="发表于 2025-08-09 16:37:33">2025-08-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-24T07:27:54.627Z" title="更新于 2026-02-24 15:27:54">2026-02-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="背景与目标读者"><a href="#背景与目标读者" class="headerlink" title="背景与目标读者"></a>背景与目标读者</h3><p>LangChain 是一个面向大型语言模型（Large Language Models, LLM）应用开发的开源框架，由 Harrison Chase 于 2022 年发布，并在 2023 年成立公司后快速发展。它通过统一的抽象与模块化组件，帮助开发者高效构建复杂的 AI 应用，如聊天机器人、文档问答（RAG）、智能代理（Agent）与自动摘要等。</p>
<p>本文面向有一定 Python 基础、希望系统了解并快速上手 LangChain 的工程师与技术爱好者，覆盖核心组件、常见应用场景与可运行示例代码。</p>
<hr>
<h3 id="LangChain-是什么，为什么需要它？"><a href="#LangChain-是什么，为什么需要它？" class="headerlink" title="LangChain 是什么，为什么需要它？"></a>LangChain 是什么，为什么需要它？</h3><ul>
<li>统一接口：屏蔽不同模型与服务的差异（如 OpenAI、Hugging Face、本地模型等），提供一致的调用方式。</li>
<li>组件化设计：围绕模型、提示（Prompt）、链（Chain）、代理（Agent）、记忆（Memory）、索引（Indexes&#x2F;Retriever）等模块化组合，便于扩展与维护。</li>
<li>工程化能力：提供可观测（Callbacks）、持久化（Checkpointers&#x2F;Message History）、工具接入（Tools）、生态集成（VectorStore&#x2F;Embeddings&#x2F;Loaders）等工程能力。</li>
</ul>
<hr>
<h3 id="核心组件与用法速览"><a href="#核心组件与用法速览" class="headerlink" title="核心组件与用法速览"></a>核心组件与用法速览</h3><h4 id="1-模型（Models）"><a href="#1-模型（Models）" class="headerlink" title="1) 模型（Models）"></a>1) 模型（Models）</h4><p>对接各类聊天&#x2F;补全模型，常见通过 <code>langchain-openai</code>、<code>langchain-community</code>、<code>langchain-anthropic</code> 等包集成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>, temperature=<span class="number">0.2</span>)</span><br><span class="line">resp = model.invoke(<span class="string">&quot;用一句话解释什么是RAG？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(resp.content)</span><br></pre></td></tr></table></figure>

<h4 id="2-提示（Prompts）"><a href="#2-提示（Prompts）" class="headerlink" title="2) 提示（Prompts）"></a>2) 提示（Prompts）</h4><p>将系统指令、示例与用户输入组织为模板，支持变量插值与角色化消息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一名资深AI助教，输出要简洁准确。&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;请用要点列举回答：&#123;question&#125;&quot;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<h4 id="3-链（Chains-LCEL）"><a href="#3-链（Chains-LCEL）" class="headerlink" title="3) 链（Chains &#x2F; LCEL）"></a>3) 链（Chains &#x2F; LCEL）</h4><p>使用 LangChain Expression Language（LCEL）以“管道”方式组合组件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line">chain = prompt | model | StrOutputParser()</span><br><span class="line"><span class="built_in">print</span>(chain.invoke(&#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;LangChain 的核心组件有哪些？&quot;</span>&#125;))</span><br></pre></td></tr></table></figure>

<h4 id="4-代理（Agents）"><a href="#4-代理（Agents）" class="headerlink" title="4) 代理（Agents）"></a>4) 代理（Agents）</h4><p>让 LLM 作为决策引擎，基于工具（Tools）动态选择行动，如搜索、计算或代码执行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> tool</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> create_react_agent, AgentExecutor</span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a: <span class="built_in">float</span>, b: <span class="built_in">float</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a + b&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line">tools = [add]</span><br><span class="line">agent = create_react_agent(model, tools)</span><br><span class="line">executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;请计算 12.5 与 7.5 的和，然后用一句中文描述结果。&quot;</span>&#125;)[<span class="string">&quot;output&quot;</span>])</span><br></pre></td></tr></table></figure>

<h4 id="5-记忆（Memory）"><a href="#5-记忆（Memory）" class="headerlink" title="5) 记忆（Memory）"></a>5) 记忆（Memory）</h4><p>存储会话历史，使多轮对话更连贯。推荐以 <code>RunnableWithMessageHistory</code> 使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.chat_history <span class="keyword">import</span> InMemoryChatMessageHistory</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables.history <span class="keyword">import</span> RunnableWithMessageHistory</span><br><span class="line"></span><br><span class="line">history_store = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_history</span>(<span class="params">session_id: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="keyword">return</span> history_store.setdefault(session_id, InMemoryChatMessageHistory())</span><br><span class="line"></span><br><span class="line">conversational_chain = RunnableWithMessageHistory(</span><br><span class="line">    chain,</span><br><span class="line">    get_history,</span><br><span class="line">    input_messages_key=<span class="string">&quot;question&quot;</span>,</span><br><span class="line">    history_messages_key=<span class="string">&quot;history&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(conversational_chain.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;记住我喜欢Python。&quot;</span>&#125;,</span><br><span class="line">    config=&#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;session_id&quot;</span>: <span class="string">&quot;u1&quot;</span>&#125;&#125;</span><br><span class="line">))</span><br><span class="line"><span class="built_in">print</span>(conversational_chain.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;我刚才说我喜欢哪种语言？&quot;</span>&#125;,</span><br><span class="line">    config=&#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;session_id&quot;</span>: <span class="string">&quot;u1&quot;</span>&#125;&#125;</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<h4 id="6-索引-检索（Indexes-VectorStores-Retrievers）"><a href="#6-索引-检索（Indexes-VectorStores-Retrievers）" class="headerlink" title="6) 索引 &#x2F; 检索（Indexes &#x2F; VectorStores &#x2F; Retrievers）"></a>6) 索引 &#x2F; 检索（Indexes &#x2F; VectorStores &#x2F; Retrievers）</h4><p>将文档切分、向量化并索引，供模型检索以提升事实性（RAG）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">docs = [</span><br><span class="line">    <span class="string">&quot;LangChain 提供统一的模型接口与组件化能力。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;RAG 将检索与生成结合，提升回答的准确性。&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">100</span>, chunk_overlap=<span class="number">20</span>)</span><br><span class="line">splits = splitter.create_documents(docs)</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">vectordb = FAISS.from_documents(splits, embeddings)</span><br><span class="line">retriever = vectordb.as_retriever(k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">context_docs = retriever.invoke(<span class="string">&quot;什么是RAG？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>([d.page_content <span class="keyword">for</span> d <span class="keyword">in</span> context_docs])</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="快速开始：环境与第一个程序"><a href="#快速开始：环境与第一个程序" class="headerlink" title="快速开始：环境与第一个程序"></a>快速开始：环境与第一个程序</h3><h4 id="安装与准备"><a href="#安装与准备" class="headerlink" title="安装与准备"></a>安装与准备</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U <span class="string">&quot;langchain&gt;=0.2&quot;</span> langchain-openai langchain-community langchain-text-splitters faiss-cpu python-dotenv</span><br></pre></td></tr></table></figure>

<p>配置密钥（以 OpenAI 为例），建议使用环境变量或 <code>.env</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setx OPENAI_API_KEY <span class="string">&quot;sk-xxxxxxxx&quot;</span>   <span class="comment"># Windows PowerShell 可使用 $env:OPENAI_API_KEY=&quot;...&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="Hello-LangChain（LCEL-版）"><a href="#Hello-LangChain（LCEL-版）" class="headerlink" title="Hello, LangChain（LCEL 版）"></a>Hello, LangChain（LCEL 版）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&quot;OPENAI_API_KEY&quot;</span>] = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>, <span class="string">&quot;&lt;YOUR_KEY&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>, temperature=<span class="number">0.2</span>)</span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是专业的技术助理，回答务必准确且简洁。&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">chain = prompt | model | StrOutputParser()</span><br><span class="line"><span class="built_in">print</span>(chain.invoke(&#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;一句话介绍LangChain。&quot;</span>&#125;))</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="常见使用场景与示例"><a href="#常见使用场景与示例" class="headerlink" title="常见使用场景与示例"></a>常见使用场景与示例</h3><h4 id="场景一：知识库问答（RAG）"><a href="#场景一：知识库问答（RAG）" class="headerlink" title="场景一：知识库问答（RAG）"></a>场景一：知识库问答（RAG）</h4><p>将企业文档做切分与向量化，检索相关片段并与提示拼接后交给模型生成答案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI, OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line">docs = [</span><br><span class="line">    <span class="string">&quot;LangChain 是用于构建由大型语言模型驱动应用的框架。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;它包含模型、提示、链、代理、记忆与索引等组件。&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">120</span>, chunk_overlap=<span class="number">20</span>)</span><br><span class="line">splits = splitter.create_documents(docs)</span><br><span class="line"></span><br><span class="line">emb = OpenAIEmbeddings()</span><br><span class="line">vectordb = FAISS.from_documents(splits, emb)</span><br><span class="line">retriever = vectordb.as_retriever(k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;请基于给定上下文回答用户问题，若无法从上下文得到答案，请明确说明。上下文：\n&#123;context&#125;&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;问题：&#123;question&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>, temperature=<span class="number">0</span>)</span><br><span class="line">parser = StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rag_answer</span>(<span class="params">question: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    context_docs = retriever.invoke(question)</span><br><span class="line">    context = <span class="string">&quot;\n\n&quot;</span>.join(d.page_content <span class="keyword">for</span> d <span class="keyword">in</span> context_docs)</span><br><span class="line">    chain = prompt | model | parser</span><br><span class="line">    <span class="keyword">return</span> chain.invoke(&#123;<span class="string">&quot;context&quot;</span>: context, <span class="string">&quot;question&quot;</span>: question&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(rag_answer(<span class="string">&quot;LangChain 的核心组件是什么？&quot;</span>))</span><br></pre></td></tr></table></figure>

<h4 id="场景二：多轮聊天机器人（会话记忆）"><a href="#场景二：多轮聊天机器人（会话记忆）" class="headerlink" title="场景二：多轮聊天机器人（会话记忆）"></a>场景二：多轮聊天机器人（会话记忆）</h4><p>使用 <code>RunnableWithMessageHistory</code> 追踪对话上下文，使回答更连贯。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.chat_history <span class="keyword">import</span> InMemoryChatMessageHistory</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables.history <span class="keyword">import</span> RunnableWithMessageHistory</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是友好的中文助理，记住用户偏好。&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;placeholder&quot;</span>, <span class="string">&quot;&#123;history&#125;&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">chain = prompt | ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>, temperature=<span class="number">0.3</span>) | StrOutputParser()</span><br><span class="line">store = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_history</span>(<span class="params">sid</span>):</span><br><span class="line">    <span class="keyword">return</span> store.setdefault(sid, InMemoryChatMessageHistory())</span><br><span class="line"></span><br><span class="line">conv = RunnableWithMessageHistory(chain, get_history, input_messages_key=<span class="string">&quot;question&quot;</span>, history_messages_key=<span class="string">&quot;history&quot;</span>)</span><br><span class="line"></span><br><span class="line">sid = <span class="string">&quot;user-42&quot;</span></span><br><span class="line"><span class="built_in">print</span>(conv.invoke(&#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;我喜欢Go语言，请记住。&quot;</span>&#125;, config=&#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;session_id&quot;</span>: sid&#125;&#125;))</span><br><span class="line"><span class="built_in">print</span>(conv.invoke(&#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;我喜欢哪种语言？&quot;</span>&#125;, config=&#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;session_id&quot;</span>: sid&#125;&#125;))</span><br></pre></td></tr></table></figure>

<h4 id="场景三：长文档摘要"><a href="#场景三：长文档摘要" class="headerlink" title="场景三：长文档摘要"></a>场景三：长文档摘要</h4><p>将长文切分后分别摘要，再做聚合（Map-Reduce 思路）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">LangChain 通过统一抽象与模块化组件，帮助开发者构建LLM应用……（长文略）</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">splits = RecursiveCharacterTextSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">50</span>).split_text(text)</span><br><span class="line">model = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>, temperature=<span class="number">0</span>)</span><br><span class="line">parser = StrOutputParser()</span><br><span class="line"></span><br><span class="line">map_prompt = ChatPromptTemplate.from_template(<span class="string">&quot;请用中文简洁总结：&#123;chunk&#125;&quot;</span>)</span><br><span class="line">map_chain = map_prompt | model | parser</span><br><span class="line"></span><br><span class="line">summaries = [map_chain.invoke(&#123;<span class="string">&quot;chunk&quot;</span>: c&#125;) <span class="keyword">for</span> c <span class="keyword">in</span> splits]</span><br><span class="line"></span><br><span class="line">reduce_prompt = ChatPromptTemplate.from_template(<span class="string">&quot;将以下摘要合并为一段清晰总结：\n&#123;points&#125;&quot;</span>)</span><br><span class="line">reduce_chain = reduce_prompt | model | parser</span><br><span class="line"></span><br><span class="line">final_summary = reduce_chain.invoke(&#123;<span class="string">&quot;points&quot;</span>: <span class="string">&quot;\n&quot;</span>.join(summaries)&#125;)</span><br><span class="line"><span class="built_in">print</span>(final_summary)</span><br></pre></td></tr></table></figure>

<h4 id="场景四：工具调用-数据分析代理"><a href="#场景四：工具调用-数据分析代理" class="headerlink" title="场景四：工具调用&#x2F;数据分析代理"></a>场景四：工具调用&#x2F;数据分析代理</h4><p>通过 Agent 使用工具（如计算、搜索、数据库查询）来完成复杂任务。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> tool</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> create_react_agent, AgentExecutor</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">float</span>, b: <span class="built_in">float</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply two numbers&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line">tools = [multiply]</span><br><span class="line">agent = create_react_agent(ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>), tools)</span><br><span class="line">executor = AgentExecutor(agent=agent, tools=tools)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;请计算 3.2 乘以 8，并给出中文说明。&quot;</span>&#125;)[<span class="string">&quot;output&quot;</span>])</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="进阶与工程化建议"><a href="#进阶与工程化建议" class="headerlink" title="进阶与工程化建议"></a>进阶与工程化建议</h3><ul>
<li>版本与依赖：<ul>
<li>推荐使用 <code>langchain&gt;=0.2</code> 与分包（<code>langchain-openai</code>、<code>langchain-community</code> 等）；不同版本 API 可能有变更。</li>
<li>向量库可选 Lancedb、FAISS、Chroma、Milvus、PGVector 等；生产建议持久化与备份策略。</li>
</ul>
</li>
<li>可观测与调试：<ul>
<li>使用 Callbacks 记录 Token 成本与时延；保留提示与上下文以便复现。</li>
</ul>
</li>
<li>安全与隐私：<ul>
<li>使用环境变量存储 API Key，不要硬编码；</li>
<li>对数据做脱敏，遵守隐私合规；</li>
<li>明确第三方服务的数据保留政策。</li>
</ul>
</li>
<li>部署建议：<ul>
<li>后端服务化（FastAPI 等），前后端分离；</li>
<li>缓存热点向量检索与响应；</li>
<li>结合队列与异步提高吞吐。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="常见问题（FAQ）"><a href="#常见问题（FAQ）" class="headerlink" title="常见问题（FAQ）"></a>常见问题（FAQ）</h3><ul>
<li>输出偶尔不稳定？<ul>
<li>降低 <code>temperature</code>，并在提示中明确约束输出格式；</li>
<li>为 RAG 增强检索质量（更好的切分、合适的 <code>k</code> 值与重排序）。</li>
</ul>
</li>
<li>检索不相关？<ul>
<li>优化切分策略与嵌入模型；尝试添加领域示例与更明确的系统指令。</li>
</ul>
</li>
<li>Agent 行为“走偏”？<ul>
<li>限制可用工具、添加防护指令与超时；在日志中审计推理轨迹。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li>LangChain 官方文档（入门&#x2F;指南&#x2F;组件）：<code>https://python.langchain.com/docs/</code></li>
<li>OpenAI 接入（langchain-openai）：<code>https://python.langchain.com/docs/integrations/chat/openai</code></li>
<li>向量数据库（FAISS&#x2F;Chroma&#x2F;Milvus）：<code>https://python.langchain.com/docs/integrations/vectorstores/</code></li>
</ul>
<hr>
<blockquote>
<p>本文由 AI 辅助生成，如有错误或建议，欢迎指出。</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://xhua.eu.org">Michael Pan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://xhua.eu.org/posts/475a94dfb51a.html">https://xhua.eu.org/posts/475a94dfb51a.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://xhua.eu.org" target="_blank">Michael Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/llm/">LLM</a><a class="post-meta__tags" href="/tags/langchain/">LangChain</a><a class="post-meta__tags" href="/tags/rag/">RAG</a><a class="post-meta__tags" href="/tags/agent/">Agent</a><a class="post-meta__tags" href="/tags/prompt/">Prompt</a></div><div class="post-share"><div class="social-share" data-image="https://img.xhua.eu.org/9fc1cdf8c8977cbf80c76c219e1b45cd8838a4300c9634013138eac9dedeef93.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="/pluginsSrc/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="/pluginsSrc/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/05696bb73c9b.html" title="从零构建RAG文档问答系统：技术栈与实现方案详解"><img class="cover" src="https://img.xhua.eu.org/9c3e0a3e1e6b76eb0b4376ad2609349f0aa2b06b15f0ae7881c4431cc3c253e6.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">从零构建RAG文档问答系统：技术栈与实现方案详解</div></div><div class="info-2"><div class="info-item-1">从零构建RAG文档问答系统：技术栈与实现方案详解引言在人工智能快速发展的今天，如何让AI模型基于特定文档内容进行准确回答，成为了一个重要的技术挑战。传统的问答系统往往存在”幻觉”问题，即模型会生成看似合理但实际不准确的信息。为了解决这个问题，我们构建了一个基于RAG（Retrieval-Augmented Generation）技术的文档问答系统。 本文将详细介绍这个项目的技术栈选择、架构设计、实现方案以及开发过程中的关键决策。 项目概述项目源代码: https://github.com/xhuaustc/rag-qa-system    我们的RAG文档问答系统具有以下核心特性：  🔍 多格式文档支持: PDF、DOCX、Markdown、TXT等 🤖 多LLM后端: Ollama、OpenAI、Azure OpenAI 📝 智能文档分块: 支持中英文混合文本的智能分块 🔗 向量检索: 基于ChromaDB的高效向量检索 💬 智能问答: 基于文档内容的智能问答 ⚙️ 灵活配置: 支持环境变量和代码配置 🛠️ 模块化设计: 清晰的模块分离和扩展性  技术栈选择核心框架...</div></div></div></a><a class="pagination-related" href="/posts/c076d9f3f05d.html" title="vLLM高性能大模型推理引擎使用指南"><img class="cover" src="https://img.xhua.eu.org/daaca0b411fbe28fb31f728aff34d87083919fff6300004fea34ff5fce7ad91b.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">vLLM高性能大模型推理引擎使用指南</div></div><div class="info-2"><div class="info-item-1">在当今AI快速发展的时代，大模型推理性能直接影响着应用的用户体验和成本效益。vLLM作为一个高性能的大模型推理引擎，为开发者提供了快速、高效的模型服务解决方案。本文将详细介绍如何使用vLLM进行离线推理和在线服务部署，特别是如何利用uv工具进行快速环境管理，以及如何部署兼容OpenAI API的模型服务。 什么是vLLMvLLM（Very Large Language Model）是由UC Berkeley开发的高性能大语言模型推理和服务引擎。它具有以下特点：  高吞吐量：通过PagedAttention等技术优化，显著提升推理速度 内存效率：动态内存管理，减少显存占用 易于使用：提供简洁的Python API和OpenAI兼容接口 灵活部署：支持批量推理和在线服务两种模式  环境准备与安装系统要求 操作系统：Linux Python版本：3.9 - 3.12 硬件：NVIDIA GPU（推荐）  使用uv工具快速安装uv是一个超快的Python环境管理器，可以显著加速环境创建和包安装过程。 1. 安装uv工具12345# 在Linux/macOS上安装uvcurl -LsSf ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/05696bb73c9b.html" title="从零构建RAG文档问答系统：技术栈与实现方案详解"><img class="cover" src="https://img.xhua.eu.org/9c3e0a3e1e6b76eb0b4376ad2609349f0aa2b06b15f0ae7881c4431cc3c253e6.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-11</div><div class="info-item-2">从零构建RAG文档问答系统：技术栈与实现方案详解</div></div><div class="info-2"><div class="info-item-1">从零构建RAG文档问答系统：技术栈与实现方案详解引言在人工智能快速发展的今天，如何让AI模型基于特定文档内容进行准确回答，成为了一个重要的技术挑战。传统的问答系统往往存在”幻觉”问题，即模型会生成看似合理但实际不准确的信息。为了解决这个问题，我们构建了一个基于RAG（Retrieval-Augmented Generation）技术的文档问答系统。 本文将详细介绍这个项目的技术栈选择、架构设计、实现方案以及开发过程中的关键决策。 项目概述项目源代码: https://github.com/xhuaustc/rag-qa-system    我们的RAG文档问答系统具有以下核心特性：  🔍 多格式文档支持: PDF、DOCX、Markdown、TXT等 🤖 多LLM后端: Ollama、OpenAI、Azure OpenAI 📝 智能文档分块: 支持中英文混合文本的智能分块 🔗 向量检索: 基于ChromaDB的高效向量检索 💬 智能问答: 基于文档内容的智能问答 ⚙️ 灵活配置: 支持环境变量和代码配置 🛠️ 模块化设计: 清晰的模块分离和扩展性  技术栈选择核心框架...</div></div></div></a><a class="pagination-related" href="/posts/3ce7fb915f3c.html" title="LightRAG：轻量级检索增强生成系统详解"><img class="cover" src="https://img.xhua.eu.org/57deb8c28c2131d70e8ca4a62d8fbdf542a722f639d2faef875a761dc2efe28a.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-13</div><div class="info-item-2">LightRAG：轻量级检索增强生成系统详解</div></div><div class="info-2"><div class="info-item-1">随着大语言模型（LLM）的快速发展，如何让AI系统能够访问和处理大量外部知识成为了一个关键挑战。检索增强生成（Retrieval-Augmented Generation，RAG）技术应运而生，而LightRAG作为一个轻量级且高效的RAG系统，通过结合知识图谱和向量检索技术，为企业级知识管理和智能问答提供了优秀的解决方案。 LightRAG 简介LightRAG是一个现代化的检索增强生成系统，专注于提供高质量的问答和知识管理功能。该系统最大的特点是将传统的向量检索与知识图谱技术相结合，实现了更精准和上下文相关的信息检索。 核心特性 轻量级设计：优化的架构设计，降低资源消耗 多模态支持：同时支持向量检索和图谱检索 多存储后端：兼容Neo4j、PostgreSQL、Faiss等多种存储系统 多模型支持：支持OpenAI、Hugging Face、Ollama等主流LLM 生产就绪：提供完整的API接口和Web UI界面 高并发处理：支持并发索引和查询操作  系统架构设计LightRAG采用分层模块化架构，确保了系统的可扩展性和维护性。 整体架构LightRAG的架构分为索引（Ind...</div></div></div></a><a class="pagination-related" href="/posts/b0a1603977e7.html" title="生产级大语言模型平台系统设计：多期落地方案与实践"><img class="cover" src="https://img.xhua.eu.org/0c48a51774caee38ab8195ab16d9895325b3056f41cb0b06ee3bff5c009bc2d4.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-18</div><div class="info-item-2">生产级大语言模型平台系统设计：多期落地方案与实践</div></div><div class="info-2"><div class="info-item-1">背景与目标随着大语言模型在企业内的应用场景不断扩展，单一模型服务或简单的 API + 网关 架构已经难以满足生产环境下的多租户管理、资源隔离、安全合规、可观测性以及快速迭代等要求。企业需要一套生产级别的大语言模型平台系统，以平台化的方式统一承载模型推理、Agent 编排、MCP 工具生态及 RAG 检索能力。 本文面向有一定 DevOps&#x2F;平台工程基础的读者，设计一套可生产落地的大语言模型平台，从整体架构到关键模块拆解，涵盖：  模型部署与运行时管理 多集群 &#x2F; 多云资源管理与调度 监控、日志、链路追踪与容量管理 安全与访问控制 RAG 平台 Agent 平台 MCP（Model Context Protocol）生态集成 平台运维与发布管理  并按照优先级划分为多期落地路线，便于企业按阶段实施。  本文更偏向平台架构设计与关键实现要点，不绑定某个具体云厂商，可结合 Kubernetes、Service Mesh、向量数据库等基础设施实施。  多期落地规划概览为了降低一次性建设的复杂度，建议将大模型平台拆分为多期，逐步演进：  一期（核心推理与基础运维能力，必...</div></div></div></a><a class="pagination-related" href="/posts/f3be144d72f7.html" title="RAGFlow 使用指南：从深度解析到生产化部署运维全攻略"><img class="cover" src="https://img.xhua.eu.org/9e579dee3c9a77be8fbf6096f6c6e836159ab13adc00214775fd08d50a159fa8.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-30</div><div class="info-item-2">RAGFlow 使用指南：从深度解析到生产化部署运维全攻略</div></div><div class="info-2"><div class="info-item-1">RAGFlow 使用指南：从深度解析到生产化部署运维全攻略1. 引言：为什么选择 RAGFlow？在 RAG（检索增强生成）领域，业界公认的挑战在于：“Garbage in, garbage out”。如果输入的上下文质量低下、版式混乱，LLM 再强也无法给出准确答案。 RAGFlow 的核心优势在于它对高质量数据接入的执着。它不只是简单的“向量化工具”，而是强调两点：  细粒度文档解析（DeepDoc）：针对图片、表格等复杂版式，通过 OCR 和版面分析，确保文档被“吃透”。 可追溯引用：每一个答案都能精准追溯到原始文档片段，有效降低大模型幻觉。  如果你需要处理大量复杂的 PDF、扫描件、金融财报或技术手册，RAGFlow 提供的“数据质量优先”路径将是你的不二之选。  2. 核心功能深度解析2.1 知识库（Datasets）与 DeepDoc 解析知识库是 RAGFlow 的底座。它将非结构化文件转化为可检索的证据库。  深度解析（DeepDoc）：这是 RAGFlow 的杀手锏。它在解析阶段执行 OCR、表格结构识别等重度预处理。 切分策略（Chunking）： 通用文档...</div></div></div></a><a class="pagination-related" href="/posts/0ca62b9873a9.html" title="Fabric：开源AI工作流与Prompt辅助框架详解"><img class="cover" src="https://img.xhua.eu.org/fca19c00172ae18891f6df2829b0a8324a8af645d6f64c4736ed99df3d681c7f.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-08</div><div class="info-item-2">Fabric：开源AI工作流与Prompt辅助框架详解</div></div><div class="info-2"><div class="info-item-1">在 AI 技术爆发的今天，我们拥有了无数强大的大模型和工具，但如何高效地将这些能力集成到日常工作流中，仍然是一个巨大的挑战。通常我们面临的问题不是”AI 能做什么”，而是”如何让 AI 帮我做这件事”。 Fabric 正是为了解决这个问题而诞生的。它是一个旨在通过 AI 增强人类能力的开源框架，核心理念是将 AI 的原子能力封装成标准化的”模式”（Patterns），让我们能够像使用命令行工具一样方便地调用 AI 能力。 什么是 Fabric？Fabric 由安全专家 Daniel Miessler 创建，它不仅仅是一个工具，更是一种使用 AI 的方法论。 核心痛点 Prompt 管理混乱：每个人都在写 Prompt，但很难复用、版本控制和分享。 集成困难：在这个应用里用 ChatGPT，在那个应用里用 Claude，缺乏统一的入口。 上下文切换：为了使用 AI，需要在不同窗口间频繁切换，打断心流。  核心特性 Patterns（模式）：Fabric 将高质量的 Prompt 封装为 Pattern，每个 Pattern 解决一个具体问题（如”提取视频摘要”、”分析代码安全”、”...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%9B%AE%E6%A0%87%E8%AF%BB%E8%80%85"><span class="toc-text">背景与目标读者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LangChain-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%83%EF%BC%9F"><span class="toc-text">LangChain 是什么，为什么需要它？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%B8%8E%E7%94%A8%E6%B3%95%E9%80%9F%E8%A7%88"><span class="toc-text">核心组件与用法速览</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%EF%BC%88Models%EF%BC%89"><span class="toc-text">1) 模型（Models）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%8F%90%E7%A4%BA%EF%BC%88Prompts%EF%BC%89"><span class="toc-text">2) 提示（Prompts）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E9%93%BE%EF%BC%88Chains-LCEL%EF%BC%89"><span class="toc-text">3) 链（Chains &#x2F; LCEL）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%BB%A3%E7%90%86%EF%BC%88Agents%EF%BC%89"><span class="toc-text">4) 代理（Agents）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E8%AE%B0%E5%BF%86%EF%BC%88Memory%EF%BC%89"><span class="toc-text">5) 记忆（Memory）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E7%B4%A2%E5%BC%95-%E6%A3%80%E7%B4%A2%EF%BC%88Indexes-VectorStores-Retrievers%EF%BC%89"><span class="toc-text">6) 索引 &#x2F; 检索（Indexes &#x2F; VectorStores &#x2F; Retrievers）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B%EF%BC%9A%E7%8E%AF%E5%A2%83%E4%B8%8E%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F"><span class="toc-text">快速开始：环境与第一个程序</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E4%B8%8E%E5%87%86%E5%A4%87"><span class="toc-text">安装与准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hello-LangChain%EF%BC%88LCEL-%E7%89%88%EF%BC%89"><span class="toc-text">Hello, LangChain（LCEL 版）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E4%B8%8E%E7%A4%BA%E4%BE%8B"><span class="toc-text">常见使用场景与示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%B8%80%EF%BC%9A%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%EF%BC%88RAG%EF%BC%89"><span class="toc-text">场景一：知识库问答（RAG）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%BA%8C%EF%BC%9A%E5%A4%9A%E8%BD%AE%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%EF%BC%88%E4%BC%9A%E8%AF%9D%E8%AE%B0%E5%BF%86%EF%BC%89"><span class="toc-text">场景二：多轮聊天机器人（会话记忆）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%B8%89%EF%BC%9A%E9%95%BF%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81"><span class="toc-text">场景三：长文档摘要</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E5%9B%9B%EF%BC%9A%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%BB%A3%E7%90%86"><span class="toc-text">场景四：工具调用&#x2F;数据分析代理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E9%98%B6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%8C%96%E5%BB%BA%E8%AE%AE"><span class="toc-text">进阶与工程化建议</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%EF%BC%88FAQ%EF%BC%89"><span class="toc-text">常见问题（FAQ）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-text">参考资料</span></a></li></ol></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2026 By Michael Pan</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>