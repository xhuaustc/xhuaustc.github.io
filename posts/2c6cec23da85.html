<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>使用vLLM部署Qwen3-Next-80B-A3B-Instruct大模型完整指南 | Michael Blog</title><meta name="author" content="Michael Pan"><meta name="copyright" content="Michael Pan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="在大模型时代，如何高效部署和运维一个80B级别的大语言模型服务是许多AI工程师面临的挑战。本文将详细介绍使用vLLM部署Qwen3-Next-80B-A3B-Instruct模型的完整流程，包括模型查找、参数配置、显存估算、下载部署、监控管理、性能压测以及推理追踪等关键环节。通过本文，您将能够快速搭建一个生产级别的大模型推理服务。 目标读者本文适合以下读者：  AI&#x2F;ML工程师，需要部署">
<meta property="og:type" content="article">
<meta property="og:title" content="使用vLLM部署Qwen3-Next-80B-A3B-Instruct大模型完整指南">
<meta property="og:url" content="https://xhua.eu.org/posts/2c6cec23da85.html">
<meta property="og:site_name" content="Michael Blog">
<meta property="og:description" content="在大模型时代，如何高效部署和运维一个80B级别的大语言模型服务是许多AI工程师面临的挑战。本文将详细介绍使用vLLM部署Qwen3-Next-80B-A3B-Instruct模型的完整流程，包括模型查找、参数配置、显存估算、下载部署、监控管理、性能压测以及推理追踪等关键环节。通过本文，您将能够快速搭建一个生产级别的大模型推理服务。 目标读者本文适合以下读者：  AI&#x2F;ML工程师，需要部署">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.xhua.eu.org/daaca0b411fbe28fb31f728aff34d87083919fff6300004fea34ff5fce7ad91b.jpg">
<meta property="article:published_time" content="2025-11-24T13:52:00.000Z">
<meta property="article:modified_time" content="2026-02-24T07:27:54.628Z">
<meta property="article:author" content="Michael Pan">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Qwen">
<meta property="article:tag" content="vLLM">
<meta property="article:tag" content="大模型部署">
<meta property="article:tag" content="性能优化">
<meta property="article:tag" content="模型推理">
<meta property="article:tag" content="模型监控">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img.xhua.eu.org/daaca0b411fbe28fb31f728aff34d87083919fff6300004fea34ff5fce7ad91b.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "使用vLLM部署Qwen3-Next-80B-A3B-Instruct大模型完整指南",
  "url": "https://xhua.eu.org/posts/2c6cec23da85.html",
  "image": "https://img.xhua.eu.org/daaca0b411fbe28fb31f728aff34d87083919fff6300004fea34ff5fce7ad91b.jpg",
  "datePublished": "2025-11-24T13:52:00.000Z",
  "dateModified": "2026-02-24T07:27:54.628Z",
  "author": [
    {
      "@type": "Person",
      "name": "Michael Pan",
      "url": "https://xhua.eu.org"
    }
  ]
}</script><link rel="shortcut icon" href="https://img.xhua.eu.org/ee7822a9c1b896de5649988ed5a9dc89c8f46fb54dd442f2d9c74721a05fa708.jpg"><link rel="canonical" href="https://xhua.eu.org/posts/2c6cec23da85.html"><link rel="preconnect"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: '/pluginsSrc/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '使用vLLM部署Qwen3-Next-80B-A3B-Instruct大模型完整指南',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/preloader-frosted-glass.css"><meta name="generator" content="Hexo 8.1.1"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      if ($loadingBox.classList.contains('loaded')) return
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()

  if (document.readyState === 'complete') {
    preloader.endLoading()
  } else {
    window.addEventListener('load', preloader.endLoading)
    document.addEventListener('DOMContentLoaded', preloader.endLoading)
    // Add timeout protection: force end after 7 seconds
    setTimeout(preloader.endLoading, 7000)
  }

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://img.xhua.eu.org/87ab7c10242ff1ab32f46f7c7b335d0581d3885fa40b8e3dc1d97014e67ea56d.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">264</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">121</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(https://img.xhua.eu.org/daaca0b411fbe28fb31f728aff34d87083919fff6300004fea34ff5fce7ad91b.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Michael Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">使用vLLM部署Qwen3-Next-80B-A3B-Instruct大模型完整指南</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">使用vLLM部署Qwen3-Next-80B-A3B-Instruct大模型完整指南</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-24T13:52:00.000Z" title="发表于 2025-11-24 21:52:00">2025-11-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-24T07:27:54.628Z" title="更新于 2026-02-24 15:27:54">2026-02-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>在大模型时代，如何高效部署和运维一个80B级别的大语言模型服务是许多AI工程师面临的挑战。本文将详细介绍使用vLLM部署Qwen3-Next-80B-A3B-Instruct模型的完整流程，包括模型查找、参数配置、显存估算、下载部署、监控管理、性能压测以及推理追踪等关键环节。通过本文，您将能够快速搭建一个生产级别的大模型推理服务。</p>
<h2 id="目标读者"><a href="#目标读者" class="headerlink" title="目标读者"></a>目标读者</h2><p>本文适合以下读者：</p>
<ul>
<li>AI&#x2F;ML工程师，需要部署大规模语言模型服务</li>
<li>DevOps工程师，负责管理和运维大模型推理平台</li>
<li>技术架构师，评估大模型部署方案</li>
<li>研究人员，需要高性能推理环境</li>
</ul>
<h2 id="一、模型查找与选择"><a href="#一、模型查找与选择" class="headerlink" title="一、模型查找与选择"></a>一、模型查找与选择</h2><h3 id="1-1-Qwen3-Next-80B-A3B-Instruct模型介绍"><a href="#1-1-Qwen3-Next-80B-A3B-Instruct模型介绍" class="headerlink" title="1.1 Qwen3-Next-80B-A3B-Instruct模型介绍"></a>1.1 Qwen3-Next-80B-A3B-Instruct模型介绍</h3><p>Qwen3-Next-80B-A3B-Instruct是阿里云通义千问团队推出的最新一代大语言模型，采用先进的<strong>MoE（Mixture of Experts）架构</strong>，具有以下特点：</p>
<ul>
<li><strong>模型架构</strong>：MoE混合专家模型，总参数80B，激活参数仅3B</li>
<li><strong>性能优势</strong>：以3B的计算成本获得接近80B Dense模型的性能</li>
<li><strong>上下文长度</strong>：支持最长256K tokens的上下文（推理时建议8K-32K以平衡性能）</li>
<li><strong>多语言能力</strong>：中英文双语能力突出</li>
<li><strong>指令跟随</strong>：经过指令微调，适合对话和任务执行</li>
<li><strong>推理效率</strong>：相比传统80B Dense模型，推理速度提升约20-25倍</li>
</ul>
<blockquote>
<p><strong>MoE架构说明</strong>：模型包含多个专家网络，每次推理只激活部分专家（约3B参数），其余专家处于休眠状态。这种设计使得大规模模型可以在相对较小的计算资源上高效运行，同时保持接近全量模型的性能表现。</p>
</blockquote>
<h3 id="1-2-在HuggingFace查找模型"><a href="#1-2-在HuggingFace查找模型" class="headerlink" title="1.2 在HuggingFace查找模型"></a>1.2 在HuggingFace查找模型</h3><p>访问HuggingFace模型库查找模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 浏览器访问</span></span><br><span class="line">https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或在HuggingFace网站搜索</span></span><br><span class="line">https://huggingface.co/models?search=qwen3-next-80b-a3b</span><br></pre></td></tr></table></figure>

<h3 id="1-3-在ModelScope查找模型（国内推荐）"><a href="#1-3-在ModelScope查找模型（国内推荐）" class="headerlink" title="1.3 在ModelScope查找模型（国内推荐）"></a>1.3 在ModelScope查找模型（国内推荐）</h3><p>对于国内用户，建议使用ModelScope：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 浏览器访问</span></span><br><span class="line">https://modelscope.cn/models/qwen/Qwen3-Next-80B-A3B-Instruct</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或使用modelscope命令行工具搜索</span></span><br><span class="line">modelscope download --<span class="built_in">help</span></span><br></pre></td></tr></table></figure>

<h3 id="1-4-模型文件结构说明"><a href="#1-4-模型文件结构说明" class="headerlink" title="1.4 模型文件结构说明"></a>1.4 模型文件结构说明</h3><p>一个标准的HuggingFace模型包含以下关键文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Qwen3-Next-80B-A3B-Instruct/</span><br><span class="line">├── config.json                    # 模型配置文件</span><br><span class="line">├── tokenizer_config.json          # 分词器配置</span><br><span class="line">├── tokenizer.json                 # 分词器文件</span><br><span class="line">├── special_tokens_map.json        # 特殊token映射</span><br><span class="line">├── generation_config.json         # 生成配置</span><br><span class="line">├── pytorch_model.bin.index.json   # 模型权重索引</span><br><span class="line">├── pytorch_model-00001-of-00017.bin  # 模型权重文件（分片）</span><br><span class="line">├── pytorch_model-00002-of-00017.bin</span><br><span class="line">├── ...</span><br><span class="line">└── README.md                      # 模型说明文档</span><br></pre></td></tr></table></figure>

<h2 id="二、关键参数说明"><a href="#二、关键参数说明" class="headerlink" title="二、关键参数说明"></a>二、关键参数说明</h2><h3 id="2-1-模型加载参数"><a href="#2-1-模型加载参数" class="headerlink" title="2.1 模型加载参数"></a>2.1 模型加载参数</h3><p>vLLM提供了丰富的参数来控制模型加载和推理行为：</p>
<h4 id="基础配置参数"><a href="#基础配置参数" class="headerlink" title="基础配置参数"></a>基础配置参数</h4><table>
<thead>
<tr>
<th>参数名称</th>
<th>说明</th>
<th>默认值</th>
<th>推荐值（80B模型）</th>
</tr>
</thead>
<tbody><tr>
<td><code>--model</code></td>
<td>模型名称或路径</td>
<td>必填</td>
<td><code>Qwen/Qwen3-Next-80B-A3B-Instruct</code></td>
</tr>
<tr>
<td><code>--tensor-parallel-size</code></td>
<td>张量并行度（GPU数量）</td>
<td>1</td>
<td>4-8</td>
</tr>
<tr>
<td><code>--dtype</code></td>
<td>数据类型</td>
<td>auto</td>
<td><code>bfloat16</code> or <code>float16</code></td>
</tr>
<tr>
<td><code>--max-model-len</code></td>
<td>最大序列长度</td>
<td>模型默认</td>
<td>8192-32768</td>
</tr>
<tr>
<td><code>--gpu-memory-utilization</code></td>
<td>GPU显存利用率</td>
<td>0.9</td>
<td>0.85-0.9</td>
</tr>
</tbody></table>
<h4 id="性能优化参数"><a href="#性能优化参数" class="headerlink" title="性能优化参数"></a>性能优化参数</h4><table>
<thead>
<tr>
<th>参数名称</th>
<th>说明</th>
<th>默认值</th>
<th>推荐值</th>
</tr>
</thead>
<tbody><tr>
<td><code>--max-num-seqs</code></td>
<td>最大并行序列数</td>
<td>256</td>
<td>64-128</td>
</tr>
<tr>
<td><code>--max-num-batched-tokens</code></td>
<td>批次最大token数</td>
<td>根据显存自动</td>
<td>8192</td>
</tr>
<tr>
<td><code>--enable-chunked-prefill</code></td>
<td>分块预填充</td>
<td>False</td>
<td>True</td>
</tr>
<tr>
<td><code>--enable-prefix-caching</code></td>
<td>前缀缓存</td>
<td>False</td>
<td>True</td>
</tr>
</tbody></table>
<h4 id="量化参数"><a href="#量化参数" class="headerlink" title="量化参数"></a>量化参数</h4><table>
<thead>
<tr>
<th>参数名称</th>
<th>说明</th>
<th>可选值</th>
</tr>
</thead>
<tbody><tr>
<td><code>--quantization</code></td>
<td>量化方法</td>
<td><code>awq</code>, <code>gptq</code>, <code>fp8</code></td>
</tr>
<tr>
<td><code>--kv-cache-dtype</code></td>
<td>KV缓存数据类型</td>
<td><code>auto</code>, <code>fp8</code></td>
</tr>
</tbody></table>
<h3 id="2-2-API服务参数"><a href="#2-2-API服务参数" class="headerlink" title="2.2 API服务参数"></a>2.2 API服务参数</h3><table>
<thead>
<tr>
<th>参数名称</th>
<th>说明</th>
<th>默认值</th>
<th>推荐值</th>
</tr>
</thead>
<tbody><tr>
<td><code>--host</code></td>
<td>服务监听地址</td>
<td>127.0.0.1</td>
<td>0.0.0.0</td>
</tr>
<tr>
<td><code>--port</code></td>
<td>服务端口</td>
<td>8000</td>
<td>8000</td>
</tr>
<tr>
<td><code>--api-key</code></td>
<td>API密钥</td>
<td>None</td>
<td>设置强密钥</td>
</tr>
<tr>
<td><code>--served-model-name</code></td>
<td>服务中的模型名称</td>
<td>与模型名相同</td>
<td>自定义</td>
</tr>
<tr>
<td><code>--max-log-len</code></td>
<td>日志最大长度</td>
<td>None</td>
<td>100</td>
</tr>
</tbody></table>
<h3 id="2-3-推理生成参数"><a href="#2-3-推理生成参数" class="headerlink" title="2.3 推理生成参数"></a>2.3 推理生成参数</h3><p>这些参数在调用API时传递：</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>说明</th>
<th>默认值</th>
<th>范围</th>
</tr>
</thead>
<tbody><tr>
<td><code>temperature</code></td>
<td>生成温度，控制随机性</td>
<td>1.0</td>
<td>0.0-2.0</td>
</tr>
<tr>
<td><code>top_p</code></td>
<td>核采样概率</td>
<td>1.0</td>
<td>0.0-1.0</td>
</tr>
<tr>
<td><code>top_k</code></td>
<td>Top-K采样</td>
<td>-1（禁用）</td>
<td>1-100</td>
</tr>
<tr>
<td><code>max_tokens</code></td>
<td>最大生成长度</td>
<td>16</td>
<td>1-32768</td>
</tr>
<tr>
<td><code>frequency_penalty</code></td>
<td>频率惩罚</td>
<td>0.0</td>
<td>-2.0-2.0</td>
</tr>
<tr>
<td><code>presence_penalty</code></td>
<td>存在惩罚</td>
<td>0.0</td>
<td>-2.0-2.0</td>
</tr>
<tr>
<td><code>repetition_penalty</code></td>
<td>重复惩罚</td>
<td>1.0</td>
<td>1.0-2.0</td>
</tr>
</tbody></table>
<h3 id="2-4-关键性能参数详解-⭐"><a href="#2-4-关键性能参数详解-⭐" class="headerlink" title="2.4 关键性能参数详解 ⭐"></a>2.4 关键性能参数详解 ⭐</h3><p>这三个参数对显存占用和推理性能有重大影响，需要仔细配置：</p>
<h4 id="max-model-len：最大序列长度"><a href="#max-model-len：最大序列长度" class="headerlink" title="--max-model-len：最大序列长度"></a><code>--max-model-len</code>：最大序列长度</h4><p><strong>作用</strong>：控制单个请求的输入+输出的最大token数</p>
<p><strong>显存影响</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># KV缓存显存计算公式</span></span><br><span class="line">KV_cache_memory = (</span><br><span class="line">    max_model_len *           <span class="comment"># 序列长度</span></span><br><span class="line">    num_layers *              <span class="comment"># 层数（Qwen3-80B约80层）</span></span><br><span class="line">    hidden_size *             <span class="comment"># 隐藏层大小（约8192）</span></span><br><span class="line">    <span class="number">2</span> *                       <span class="comment"># K和V两个cache</span></span><br><span class="line">    dtype_bytes *             <span class="comment"># 数据类型（BF16=2字节）</span></span><br><span class="line">    max_num_seqs              <span class="comment"># 并发序列数</span></span><br><span class="line">) / (<span class="number">1024</span>**<span class="number">3</span>) / tensor_parallel_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：计算不同max-model-len的显存占用</span></span><br><span class="line"><span class="comment"># 假设：80层，hidden_size=8192，BF16，64并发，4卡并行</span></span><br><span class="line">max_model_len=<span class="number">4096</span>  -&gt; KV缓存约 8GB/GPU</span><br><span class="line">max_model_len=<span class="number">8192</span>  -&gt; KV缓存约 16GB/GPU  </span><br><span class="line">max_model_len=<span class="number">16384</span> -&gt; KV缓存约 32GB/GPU</span><br><span class="line">max_model_len=<span class="number">32768</span> -&gt; KV缓存约 64GB/GPU</span><br></pre></td></tr></table></figure>

<p><strong>性能影响</strong>：</p>
<ul>
<li>✅ <strong>越大</strong>：支持更长的上下文，但显存占用成线性增长</li>
<li>⚠️ <strong>过大</strong>：显存不足会导致OOM或降低并发能力</li>
<li>💡 <strong>建议</strong>：根据实际业务需求设置，不要盲目追求最大值</li>
</ul>
<p><strong>配置建议</strong>：</p>
<table>
<thead>
<tr>
<th>GPU配置</th>
<th>推荐max-model-len</th>
<th>显存占用</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>4×A100-80GB (BF16)</td>
<td>8192</td>
<td>~16GB KV</td>
<td>通用对话</td>
</tr>
<tr>
<td>4×A100-80GB (BF16)</td>
<td>16384</td>
<td>~32GB KV</td>
<td>长文档分析</td>
</tr>
<tr>
<td>8×A100-80GB (BF16)</td>
<td>32768</td>
<td>~32GB KV</td>
<td>RAG检索</td>
</tr>
<tr>
<td>4×A100-80GB (FP8)</td>
<td>16384</td>
<td>~16GB KV</td>
<td>性价比高</td>
</tr>
</tbody></table>
<h4 id="max-num-seqs：最大并发序列数"><a href="#max-num-seqs：最大并发序列数" class="headerlink" title="--max-num-seqs：最大并发序列数"></a><code>--max-num-seqs</code>：最大并发序列数</h4><p><strong>作用</strong>：同时处理的请求数量上限</p>
<p><strong>显存影响</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显存需求与max-num-seqs成正比</span></span><br><span class="line">total_memory = (</span><br><span class="line">    model_weights +                    <span class="comment"># 模型权重（固定）</span></span><br><span class="line">    KV_cache * max_num_seqs +          <span class="comment"># KV缓存（线性增长）</span></span><br><span class="line">    activation * active_seqs           <span class="comment"># 激活值（动态）</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：4卡A100-80GB，max-model-len=8192，BF16</span></span><br><span class="line">max_num_seqs=<span class="number">32</span>  -&gt; KV缓存约 8GB/GPU  (可用显存充足)</span><br><span class="line">max_num_seqs=<span class="number">64</span>  -&gt; KV缓存约 16GB/GPU (推荐配置)</span><br><span class="line">max_num_seqs=<span class="number">128</span> -&gt; KV缓存约 32GB/GPU (接近上限)</span><br><span class="line">max_num_seqs=<span class="number">256</span> -&gt; KV缓存约 64GB/GPU (可能OOM)</span><br></pre></td></tr></table></figure>

<p><strong>性能影响</strong>：</p>
<ul>
<li>✅ <strong>越大</strong>：支持更高并发，吞吐量提升</li>
<li>⚠️ <strong>过大</strong>：显存不足，或导致batch过大降低响应速度</li>
<li>⚠️ <strong>过小</strong>：并发能力受限，GPU利用率低</li>
<li>💡 <strong>最优值</strong>：使GPU利用率达到80-90%</li>
</ul>
<p><strong>配置建议</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保守配置（稳定优先）</span></span><br><span class="line">--max-num-seqs 32 \</span><br><span class="line">--max-model-len 8192</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平衡配置（推荐）</span></span><br><span class="line">--max-num-seqs 64 \</span><br><span class="line">--max-model-len 8192</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激进配置（吞吐优先）</span></span><br><span class="line">--max-num-seqs 128 \</span><br><span class="line">--max-model-len 8192</span><br><span class="line"></span><br><span class="line"><span class="comment"># 长上下文配置</span></span><br><span class="line">--max-num-seqs 32 \</span><br><span class="line">--max-model-len 32768</span><br></pre></td></tr></table></figure>

<h4 id="max-num-batched-tokens：批次最大token数"><a href="#max-num-batched-tokens：批次最大token数" class="headerlink" title="--max-num-batched-tokens：批次最大token数"></a><code>--max-num-batched-tokens</code>：批次最大token数</h4><p><strong>作用</strong>：单个批次中所有序列的token总数上限</p>
<p><strong>计算关系</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 约束条件</span></span><br><span class="line">max_num_batched_tokens &gt;= max_model_len</span><br><span class="line">max_num_batched_tokens &lt;= max_num_seqs * max_model_len</span><br><span class="line"></span><br><span class="line"><span class="comment"># vLLM动态批处理逻辑</span></span><br><span class="line">actual_batch_tokens = <span class="built_in">sum</span>([</span><br><span class="line">    <span class="built_in">len</span>(seq.prompt) + seq.output_len </span><br><span class="line">    <span class="keyword">for</span> seq <span class="keyword">in</span> current_batch</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果 actual_batch_tokens &gt; max_num_batched_tokens:</span></span><br><span class="line"><span class="comment">#     等待下一个batch</span></span><br></pre></td></tr></table></figure>

<p><strong>显存影响</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 影响激活值显存（对MoE模型影响较小）</span></span><br><span class="line">activation_memory = (</span><br><span class="line">    max_num_batched_tokens * </span><br><span class="line">    hidden_size * </span><br><span class="line">    num_layers * </span><br><span class="line">    dtype_bytes * </span><br><span class="line">    activation_factor  <span class="comment"># MoE约为0.05，Dense约为1.0</span></span><br><span class="line">) / (<span class="number">1024</span>**<span class="number">3</span>) / tensor_parallel_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：MoE模型，4卡，BF16</span></span><br><span class="line">max_num_batched_tokens=<span class="number">8192</span>  -&gt; 激活值约 <span class="number">0.5</span>GB/GPU</span><br><span class="line">max_num_batched_tokens=<span class="number">16384</span> -&gt; 激活值约 <span class="number">1.0</span>GB/GPU</span><br><span class="line">max_num_batched_tokens=<span class="number">32768</span> -&gt; 激活值约 <span class="number">2.0</span>GB/GPU</span><br></pre></td></tr></table></figure>

<p><strong>性能影响</strong>：</p>
<ul>
<li>✅ <strong>合理设置</strong>：优化批处理效率</li>
<li>⚠️ <strong>过小</strong>：限制批次大小，降低GPU利用率</li>
<li>⚠️ <strong>过大</strong>：可能导致单个batch过大，增加延迟</li>
<li>💡 <strong>自动模式</strong>：不设置则vLLM自动计算（推荐）</li>
</ul>
<p><strong>配置建议</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1：自动模式（推荐）</span></span><br><span class="line">vllm serve model-path \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --max-num-seqs 64</span><br><span class="line">    <span class="comment"># 不设置max-num-batched-tokens，让vLLM自动计算</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2：手动设置</span></span><br><span class="line">vllm serve model-path \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --max-num-seqs 64 \</span><br><span class="line">    --max-num-batched-tokens 16384  <span class="comment"># 通常设为 max_model_len * 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式3：高并发短序列</span></span><br><span class="line">vllm serve model-path \</span><br><span class="line">    --max-model-len 2048 \</span><br><span class="line">    --max-num-seqs 128 \</span><br><span class="line">    --max-num-batched-tokens 8192</span><br></pre></td></tr></table></figure>

<h4 id="三个参数的协同配置"><a href="#三个参数的协同配置" class="headerlink" title="三个参数的协同配置"></a>三个参数的协同配置</h4><p><strong>场景1：通用对话服务（推荐）</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vllm serve model-path \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --max-model-len 8192 \          <span class="comment"># 支持8K上下文</span></span><br><span class="line">    --max-num-seqs 64 \             <span class="comment"># 64并发</span></span><br><span class="line">    --gpu-memory-utilization 0.85   <span class="comment"># 自动计算batched-tokens</span></span><br><span class="line"></span><br><span class="line">预期性能：</span><br><span class="line">- QPS: 20-40</span><br><span class="line">- 平均延迟: 1-2秒</span><br><span class="line">- GPU显存: 每卡约55GB</span><br><span class="line">- 吞吐量: 1500-2000 tokens/s</span><br></pre></td></tr></table></figure>

<p><strong>场景2：长文档处理</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vllm serve model-path \</span><br><span class="line">    --tensor-parallel-size 8 \</span><br><span class="line">    --max-model-len 32768 \         <span class="comment"># 支持32K上下文</span></span><br><span class="line">    --max-num-seqs 32 \             <span class="comment"># 降低并发以节省显存</span></span><br><span class="line">    --gpu-memory-utilization 0.90</span><br><span class="line"></span><br><span class="line">预期性能：</span><br><span class="line">- QPS: 5-10</span><br><span class="line">- 平均延迟: 3-5秒</span><br><span class="line">- GPU显存: 每卡约60GB</span><br><span class="line">- 适合: RAG、文档分析</span><br></pre></td></tr></table></figure>

<p><strong>场景3：高吞吐批处理</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vllm serve model-path \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --max-model-len 4096 \          <span class="comment"># 较短上下文</span></span><br><span class="line">    --max-num-seqs 128 \            <span class="comment"># 高并发</span></span><br><span class="line">    --max-num-batched-tokens 16384 \</span><br><span class="line">    --gpu-memory-utilization 0.85</span><br><span class="line"></span><br><span class="line">预期性能：</span><br><span class="line">- QPS: 50-80</span><br><span class="line">- 平均延迟: 0.5-1秒</span><br><span class="line">- GPU显存: 每卡约50GB</span><br><span class="line">- 适合: API服务、批量推理</span><br></pre></td></tr></table></figure>

<p><strong>场景4：低显存环境</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vllm serve model-path \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --max-model-len 4096 \          <span class="comment"># 限制序列长度</span></span><br><span class="line">    --max-num-seqs 16 \             <span class="comment"># 低并发</span></span><br><span class="line">    --gpu-memory-utilization 0.75   <span class="comment"># 保守设置</span></span><br><span class="line"></span><br><span class="line">预期性能：</span><br><span class="line">- QPS: 8-15</span><br><span class="line">- 平均延迟: 1-2秒</span><br><span class="line">- GPU显存: 每卡约45GB</span><br><span class="line">- 适合: 显存受限环境</span><br></pre></td></tr></table></figure>

<h4 id="参数调优工作流"><a href="#参数调优工作流" class="headerlink" title="参数调优工作流"></a>参数调优工作流</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 步骤1：从保守配置开始</span></span><br><span class="line">vllm serve model-path \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --max-num-seqs 32 \</span><br><span class="line">    --gpu-memory-utilization 0.80</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤2：监控GPU显存使用</span></span><br><span class="line">nvidia-smi dmon -s u -d 5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤3：如果显存充足（&lt;70%），逐步提升</span></span><br><span class="line"><span class="comment"># 方式A：增加并发</span></span><br><span class="line">--max-num-seqs 64  <span class="comment"># 提升2倍</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式B：增加上下文</span></span><br><span class="line">--max-model-len 16384  <span class="comment"># 提升2倍</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式C：两者平衡</span></span><br><span class="line">--max-model-len 12288 \</span><br><span class="line">--max-num-seqs 48</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤4：压测验证</span></span><br><span class="line"><span class="comment"># 使用压测工具测试实际QPS和延迟</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤5：根据业务需求微调</span></span><br><span class="line"><span class="comment"># 如果追求低延迟：降低max-num-seqs</span></span><br><span class="line"><span class="comment"># 如果追求高吞吐：提升max-num-seqs</span></span><br></pre></td></tr></table></figure>

<h4 id="显存占用速查表"><a href="#显存占用速查表" class="headerlink" title="显存占用速查表"></a>显存占用速查表</h4><p><strong>4卡A100-80GB，BF16，Qwen3-80B-A3B配置</strong>：</p>
<table>
<thead>
<tr>
<th>max-model-len</th>
<th>max-num-seqs</th>
<th>每卡显存</th>
<th>适用QPS</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>4096</td>
<td>32</td>
<td>~48GB</td>
<td>15-25</td>
<td>低并发</td>
</tr>
<tr>
<td>8192</td>
<td>32</td>
<td>~50GB</td>
<td>12-20</td>
<td>平衡</td>
</tr>
<tr>
<td>8192</td>
<td>64</td>
<td>~55GB</td>
<td>20-40</td>
<td>推荐⭐</td>
</tr>
<tr>
<td>8192</td>
<td>128</td>
<td>~65GB</td>
<td>40-60</td>
<td>高并发</td>
</tr>
<tr>
<td>16384</td>
<td>32</td>
<td>~58GB</td>
<td>10-15</td>
<td>长文档</td>
</tr>
<tr>
<td>16384</td>
<td>64</td>
<td>~68GB</td>
<td>15-25</td>
<td>长文档+并发</td>
</tr>
<tr>
<td>32768</td>
<td>16</td>
<td>~60GB</td>
<td>5-10</td>
<td>超长文档</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>关键原则</strong>：</p>
<ul>
<li>显存占用 ≈ 模型权重 + (max_model_len × max_num_seqs × 系数)</li>
<li>MoE模型的”系数”比Dense模型小很多</li>
<li>优先保证稳定性，再追求性能</li>
<li>定期监控和调整</li>
</ul>
</blockquote>
<h2 id="三、显存预估与资源规划"><a href="#三、显存预估与资源规划" class="headerlink" title="三、显存预估与资源规划"></a>三、显存预估与资源规划</h2><h3 id="3-1-MoE模型架构说明"><a href="#3-1-MoE模型架构说明" class="headerlink" title="3.1 MoE模型架构说明"></a>3.1 MoE模型架构说明</h3><p><strong>重要</strong>：Qwen3-Next-80B-A3B-Instruct 是一个 <strong>MoE（Mixture of Experts）架构</strong>的模型，具有以下特点：</p>
<ul>
<li><strong>总参数量</strong>：80B（800亿参数）</li>
<li><strong>激活参数量</strong>：3B（30亿参数）</li>
<li><strong>显存占用</strong>：需要加载全部80B参数到显存（约160GB for BF16）</li>
<li><strong>计算优势</strong>：推理时只计算3B参数，大幅降低计算量和功耗</li>
</ul>
<blockquote>
<p><strong>MoE工作原理</strong>：模型包含多个”专家”网络，所有专家权重都加载在GPU显存中。推理时，路由网络决定激活哪些专家（约3B参数），其余专家权重保持”休眠”状态（在显存中但不参与计算）。这种设计的关键优势是：</p>
<ul>
<li><strong>权重显存</strong>：与Dense模型相同（需要完整的80B）</li>
<li><strong>计算显存</strong>：远小于Dense模型（只需3B的激活值和中间结果）</li>
<li><strong>推理速度</strong>：快20-25倍（计算量只有3.75%）</li>
</ul>
</blockquote>
<h3 id="3-2-显存占用详解"><a href="#3-2-显存占用详解" class="headerlink" title="3.2 显存占用详解"></a>3.2 显存占用详解</h3><p>理解MoE模型的显存分配很重要：</p>
<table>
<thead>
<tr>
<th>显存类型</th>
<th>Dense 80B</th>
<th>MoE 80B-A3B</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>模型权重</strong></td>
<td>160GB</td>
<td>160GB</td>
<td>相同（都需加载全部参数）</td>
</tr>
<tr>
<td><strong>KV缓存</strong></td>
<td>30GB</td>
<td>30GB</td>
<td>相同（取决于序列长度）</td>
</tr>
<tr>
<td><strong>激活值&#x2F;梯度</strong></td>
<td>~30GB</td>
<td>~2GB</td>
<td><strong>大幅减少</strong>（只计算3B）</td>
</tr>
<tr>
<td><strong>总显存</strong></td>
<td>~220GB</td>
<td>~192GB</td>
<td>节省约13%</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>关键理解</strong>：MoE模型的优势主要体现在<strong>计算效率</strong>而非<strong>权重显存</strong>。虽然显存节省有限，但推理速度提升巨大。</p>
</blockquote>
<h3 id="3-3-模型权重显存计算"><a href="#3-3-模型权重显存计算" class="headerlink" title="3.3 模型权重显存计算"></a>3.3 模型权重显存计算</h3><p><strong>vLLM默认加载方式</strong>：全量加载所有专家到GPU显存</p>
<p>对于MoE模型，显存需求计算公式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">显存需求（GB）= 模型权重显存 + KV缓存显存 + 激活值显存</span><br></pre></td></tr></table></figure>

<h4 id="FP16-BF16-精度（推荐）"><a href="#FP16-BF16-精度（推荐）" class="headerlink" title="FP16&#x2F;BF16 精度（推荐）"></a>FP16&#x2F;BF16 精度（推荐）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 模型权重显存（所有80B参数）</span></span><br><span class="line">80B × 2 bytes = 160GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. KV缓存（取决于batch size和序列长度）</span></span><br><span class="line"><span class="comment"># 假设最大序列8192，batch size 64</span></span><br><span class="line">KV缓存 ≈ 20-30GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 激活值和中间结果（只需3B计算）</span></span><br><span class="line">激活值 ≈ 2-5GB  <span class="comment"># 比Dense模型的30GB小得多</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 总显存需求</span></span><br><span class="line">基础显存 = 160GB + 30GB + 5GB = 195GB</span><br><span class="line">实际使用（含10%系统开销）≈ 195GB × 1.1 = 215GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 多卡分配（张量并行）</span></span><br><span class="line">每卡显存（4卡）= 215GB ÷ 4 ≈ 54GB ✅ A100-80GB可用</span><br><span class="line">每卡显存（2卡）= 215GB ÷ 2 ≈ 108GB ❌ 超出A100-80GB</span><br></pre></td></tr></table></figure>

<h4 id="FP8-INT8-量化"><a href="#FP8-INT8-量化" class="headerlink" title="FP8&#x2F;INT8 量化"></a>FP8&#x2F;INT8 量化</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型权重显存（量化后）</span></span><br><span class="line">80B × 1 byte = 80GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># KV缓存和激活值</span></span><br><span class="line">KV缓存 ≈ 15-20GB</span><br><span class="line">激活值 ≈ 1-3GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># 总显存需求</span></span><br><span class="line">基础显存 = 80GB + 20GB + 3GB = 103GB</span><br><span class="line">实际使用 ≈ 103GB × 1.1 = 113GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多卡分配</span></span><br><span class="line">每卡显存（2卡）= 113GB ÷ 2 ≈ 57GB ✅ A100-80GB可用</span><br><span class="line">每卡显存（单卡）= 113GB ❌ 超出A100-80GB</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>重要提示</strong>：</p>
<ul>
<li>MoE模型的显存需求主要在<strong>模型权重</strong>（160GB）</li>
<li>激活值显存很小（<del>5GB vs Dense的</del>30GB），这是速度快的原因</li>
<li><strong>不要期望</strong>MoE能显著减少显存需求，它的优势在计算速度</li>
<li>vLLM会将所有专家参数分布到所有GPU上</li>
</ul>
</blockquote>
<h3 id="3-4-GPU配置方案（针对MoE模型）"><a href="#3-4-GPU配置方案（针对MoE模型）" class="headerlink" title="3.4 GPU配置方案（针对MoE模型）"></a>3.4 GPU配置方案（针对MoE模型）</h3><blockquote>
<p><strong>配置前提</strong>：基于vLLM全量加载所有专家参数的方式</p>
</blockquote>
<h4 id="方案一：A100-80GB-×-3卡（最小配置）"><a href="#方案一：A100-80GB-×-3卡（最小配置）" class="headerlink" title="方案一：A100-80GB × 3卡（最小配置）"></a>方案一：A100-80GB × 3卡（最小配置）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">总显存: 240GB</span><br><span class="line">可用显存（90%利用率）: 216GB</span><br><span class="line">数据精度: BF16</span><br><span class="line">张量并行度: 3</span><br><span class="line">最大序列长度: 8192</span><br><span class="line">预估并发: 24-48 requests</span><br><span class="line">适用场景: 开发测试、预算受限</span><br><span class="line"></span><br><span class="line">说明: 最小配置，215GB刚好能放下</span><br><span class="line">      每卡约72GB，留有少量余量</span><br></pre></td></tr></table></figure>

<h4 id="方案二：A100-80GB-×-4卡（生产推荐）⭐"><a href="#方案二：A100-80GB-×-4卡（生产推荐）⭐" class="headerlink" title="方案二：A100-80GB × 4卡（生产推荐）⭐"></a>方案二：A100-80GB × 4卡（生产推荐）⭐</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">总显存: 320GB</span><br><span class="line">可用显存（90%利用率）: 288GB</span><br><span class="line">每卡分配: ~54GB</span><br><span class="line">数据精度: BF16</span><br><span class="line">张量并行度: 4</span><br><span class="line">最大序列长度: 16384</span><br><span class="line">预估并发: 64-128 requests</span><br><span class="line">适用场景: 生产环境、高并发服务</span><br><span class="line"></span><br><span class="line">说明: 最佳平衡方案</span><br><span class="line">      每卡仅用54GB，余量充足</span><br><span class="line">      可支持更长上下文和更大batch</span><br></pre></td></tr></table></figure>

<h4 id="方案三：A100-80GB-×-8卡（高性能）"><a href="#方案三：A100-80GB-×-8卡（高性能）" class="headerlink" title="方案三：A100-80GB × 8卡（高性能）"></a>方案三：A100-80GB × 8卡（高性能）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">总显存: 640GB</span><br><span class="line">可用显存: 576GB</span><br><span class="line">每卡分配: ~27GB</span><br><span class="line">数据精度: BF16</span><br><span class="line">张量并行度: 8</span><br><span class="line">最大序列长度: 32768</span><br><span class="line">预估并发: 256+ requests</span><br><span class="line">适用场景: 超大规模服务、超长上下文</span><br><span class="line"></span><br><span class="line">说明: 每卡显存占用极低</span><br><span class="line">      适合处理256K上下文和海量并发</span><br><span class="line">      成本较高，适合高价值场景</span><br></pre></td></tr></table></figure>

<h4 id="方案四：A100-80GB-×-2卡-FP8量化（性价比）⭐"><a href="#方案四：A100-80GB-×-2卡-FP8量化（性价比）⭐" class="headerlink" title="方案四：A100-80GB × 2卡 + FP8量化（性价比）⭐"></a>方案四：A100-80GB × 2卡 + FP8量化（性价比）⭐</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">总显存: 160GB</span><br><span class="line">可用显存: 144GB</span><br><span class="line">每卡分配: ~57GB</span><br><span class="line">数据精度: FP8量化</span><br><span class="line">张量并行度: 2</span><br><span class="line">最大序列长度: 8192</span><br><span class="line">预估并发: 32-64 requests</span><br><span class="line">适用场景: 成本敏感、中等性能需求</span><br><span class="line"></span><br><span class="line">说明: FP8量化后，2卡即可运行</span><br><span class="line">      相比BF16节省约50%成本</span><br><span class="line">      性能略有下降（~5%）</span><br></pre></td></tr></table></figure>

<h4 id="方案五：A100-40GB-×-4卡-FP8量化（经济型）"><a href="#方案五：A100-40GB-×-4卡-FP8量化（经济型）" class="headerlink" title="方案五：A100-40GB × 4卡 + FP8量化（经济型）"></a>方案五：A100-40GB × 4卡 + FP8量化（经济型）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">总显存: 160GB</span><br><span class="line">可用显存: 144GB</span><br><span class="line">每卡分配: ~28GB</span><br><span class="line">数据精度: FP8量化</span><br><span class="line">张量并行度: 4</span><br><span class="line">最大序列长度: 8192</span><br><span class="line">预估并发: 32-48 requests</span><br><span class="line">适用场景: 预算受限、开发测试</span><br><span class="line"></span><br><span class="line">说明: 使用40GB显卡降低硬件成本</span><br><span class="line">      需要FP8量化才能运行</span><br><span class="line">      硬件成本节省约50%</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>配置建议总结</strong>：</p>
<ul>
<li><strong>BF16精度</strong>：最少需要3卡A100-80GB，推荐4卡</li>
<li><strong>FP8量化</strong>：最少需要2卡A100-80GB或4卡A100-40GB</li>
<li><strong>单卡部署</strong>：不推荐，显存不足（BF16需215GB）</li>
</ul>
</blockquote>
<h3 id="3-4-显存使用估算工具"><a href="#3-4-显存使用估算工具" class="headerlink" title="3.4 显存使用估算工具"></a>3.4 显存使用估算工具</h3><h4 id="方法1：试运行监控法（推荐）"><a href="#方法1：试运行监控法（推荐）" class="headerlink" title="方法1：试运行监控法（推荐）"></a>方法1：试运行监控法（推荐）</h4><p>最简单的方法是直接启动服务，配合 <code>nvidia-smi</code> 观察显存占用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 在一个终端启动监控</span></span><br><span class="line">watch -n 1 nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 在另一个终端启动vLLM（使用最小并发配置）</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --dtype bfloat16 \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --max-num-seqs 1 \</span><br><span class="line">    --gpu-memory-utilization 0.9</span><br></pre></td></tr></table></figure>

<p><strong>如何解读：</strong></p>
<ol>
<li>vLLM启动时会预先申请 <code>gpu-memory-utilization</code> (默认90%) 的显存。</li>
<li>观察日志中的 <code># GPU blocks</code> 数量，这代表了可用于 KV Cache 的空间。</li>
<li>如果启动成功且 <code>nvidia-smi</code> 显示显存占用稳定，说明配置可行。</li>
<li>如果发生 OOM (Out Of Memory)，则需要降低 <code>max-model-len</code> 或增加 GPU 数量。</li>
</ol>
<h4 id="方法2：通过Python脚本测量（修正版）"><a href="#方法2：通过Python脚本测量（修正版）" class="headerlink" title="方法2：通过Python脚本测量（修正版）"></a>方法2：通过Python脚本测量（修正版）</h4><p>由于vLLM使用多进程架构（Multiprocessing），模型权重加载在子进程中，因此<strong>不能</strong>直接使用 <code>torch.cuda.memory_allocated()</code> 在主进程查看显存。</p>
<p>建议使用以下脚本，通过调用 <code>nvidia-smi</code> 获取真实的系统级显存占用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_gpu_memory</span>(<span class="params">label=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过nvidia-smi获取实际显存占用&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n=== <span class="subst">&#123;label&#125;</span>显存状态 ===&quot;</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 查询显存使用情况</span></span><br><span class="line">        result = subprocess.check_output(</span><br><span class="line">            [<span class="string">&#x27;nvidia-smi&#x27;</span>, <span class="string">&#x27;--query-gpu=index,memory.used,memory.total&#x27;</span>, <span class="string">&#x27;--format=csv,noheader,nounits&#x27;</span>],</span><br><span class="line">            encoding=<span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">        )</span><br><span class="line">        lines = result.strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        total_used = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line.strip(): <span class="keyword">continue</span></span><br><span class="line">            idx, used, total = line.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            used_mb = <span class="built_in">float</span>(used.strip())</span><br><span class="line">            total_used += used_mb</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;GPU <span class="subst">&#123;idx&#125;</span>: <span class="subst">&#123;used_mb:<span class="number">.0</span>f&#125;</span> MiB / <span class="subst">&#123;total.strip()&#125;</span> MiB&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;总计已用: <span class="subst">&#123;total_used/<span class="number">1024</span>:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;平均每卡: <span class="subst">&#123;total_used/<span class="built_in">len</span>(lines)/<span class="number">1024</span>:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> total_used</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;获取显存信息失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 打印初始状态</span></span><br><span class="line">print_gpu_memory(<span class="string">&quot;加载前&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&gt;&gt;&gt; 开始加载模型...&quot;</span>)</span><br><span class="line"><span class="comment"># 2. 加载模型</span></span><br><span class="line"><span class="comment"># 注意：vLLM会预占 gpu_memory_utilization 比例的显存</span></span><br><span class="line">llm = LLM(</span><br><span class="line">    model=<span class="string">&quot;/data/models/Qwen3-Next-80B-A3B-Instruct&quot;</span>,</span><br><span class="line">    tensor_parallel_size=<span class="number">4</span>,</span><br><span class="line">    dtype=<span class="string">&quot;bfloat16&quot;</span>,</span><br><span class="line">    max_model_len=<span class="number">8192</span>,</span><br><span class="line">    gpu_memory_utilization=<span class="number">0.9</span>,</span><br><span class="line">    max_num_seqs=<span class="number">1</span>,</span><br><span class="line">    trust_remote_code=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 打印加载后状态</span></span><br><span class="line">print_gpu_memory(<span class="string">&quot;加载后&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>注意</strong>：脚本运行结束后可能会报 <code>EngineCore died unexpectedly</code>，这是因为脚本退出导致子进程被强制关闭，属于正常现象，只要能看到”加载后显存状态”的输出即可。</p>
</blockquote>
<h4 id="方法3：手动计算公式（精确版）"><a href="#方法3：手动计算公式（精确版）" class="headerlink" title="方法3：手动计算公式（精确版）"></a>方法3：手动计算公式（精确版）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">estimate_moe_memory</span>(<span class="params"></span></span><br><span class="line"><span class="params">    total_params_b=<span class="number">80</span>,           <span class="comment"># 总参数量（B）</span></span></span><br><span class="line"><span class="params">    active_params_b=<span class="number">3</span>,           <span class="comment"># 激活参数量（B）</span></span></span><br><span class="line"><span class="params">    dtype_bytes=<span class="number">2</span>,               <span class="comment"># 数据类型字节数（FP16/BF16=2, FP8=1）</span></span></span><br><span class="line"><span class="params">    max_seq_len=<span class="number">8192</span>,            <span class="comment"># 最大序列长度</span></span></span><br><span class="line"><span class="params">    max_batch_size=<span class="number">64</span>,           <span class="comment"># 最大批次大小</span></span></span><br><span class="line"><span class="params">    num_layers=<span class="number">80</span>,               <span class="comment"># 层数（估算）</span></span></span><br><span class="line"><span class="params">    hidden_size=<span class="number">8192</span>,            <span class="comment"># 隐藏层大小（估算）</span></span></span><br><span class="line"><span class="params">    num_attention_heads=<span class="number">64</span>,      <span class="comment"># 注意力头数（估算）</span></span></span><br><span class="line"><span class="params">    tensor_parallel=<span class="number">4</span>            <span class="comment"># 张量并行度</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    MoE模型显存精确估算</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;=== MoE模型显存估算 ===&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;配置: <span class="subst">&#123;total_params_b&#125;</span>B参数, <span class="subst">&#123;active_params_b&#125;</span>B激活&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;推理配置: seq_len=<span class="subst">&#123;max_seq_len&#125;</span>, batch_size=<span class="subst">&#123;max_batch_size&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;并行度: <span class="subst">&#123;tensor_parallel&#125;</span>卡张量并行\n&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. 模型权重（全量加载，所有专家都在显存）</span></span><br><span class="line">    model_weights_gb = (total_params_b * dtype_bytes * <span class="number">1e9</span>) / (<span class="number">1024</span>**<span class="number">3</span>)</span><br><span class="line">    per_gpu_weights_gb = model_weights_gb / tensor_parallel</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;1. 模型权重:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;   总大小: <span class="subst">&#123;model_weights_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;   每GPU: <span class="subst">&#123;per_gpu_weights_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. KV缓存（与激活参数相关性较弱，主要看序列长度）</span></span><br><span class="line">    <span class="comment"># 公式: batch_size * seq_len * num_layers * hidden_size * 2 (K和V) * dtype_bytes</span></span><br><span class="line">    kv_cache_gb = (</span><br><span class="line">        max_batch_size * max_seq_len * num_layers * </span><br><span class="line">        hidden_size * <span class="number">2</span> * dtype_bytes</span><br><span class="line">    ) / (<span class="number">1024</span>**<span class="number">3</span>) / tensor_parallel</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n2. KV缓存:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;   每GPU: <span class="subst">&#123;kv_cache_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 激活值（只需激活参数的计算）</span></span><br><span class="line">    <span class="comment"># MoE的优势：激活值显著减少</span></span><br><span class="line">    <span class="comment"># Dense模型激活值 vs MoE激活值比例约为 total_params / active_params</span></span><br><span class="line">    dense_activation_ratio = total_params_b / active_params_b</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Dense模型的激活值估算</span></span><br><span class="line">    dense_activation_gb = (</span><br><span class="line">        max_batch_size * max_seq_len * hidden_size * </span><br><span class="line">        num_layers * dtype_bytes * <span class="number">8</span>  <span class="comment"># 8个中间tensor（FFN, attention等）</span></span><br><span class="line">    ) / (<span class="number">1024</span>**<span class="number">3</span>) / tensor_parallel</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># MoE只需激活部分</span></span><br><span class="line">    moe_activation_gb = dense_activation_gb / dense_activation_ratio * <span class="number">1.2</span>  <span class="comment"># 1.2是路由开销</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n3. 激活值和中间结果:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;   Dense模型需要: <span class="subst">&#123;dense_activation_gb:<span class="number">.2</span>f&#125;</span> GB/GPU&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;   MoE实际需要: <span class="subst">&#123;moe_activation_gb:<span class="number">.2</span>f&#125;</span> GB/GPU&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;   节省比例: <span class="subst">&#123;(<span class="number">1</span> - moe_activation_gb/dense_activation_gb)*<span class="number">100</span>:<span class="number">.1</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 系统开销（CUDA context, 碎片等）</span></span><br><span class="line">    overhead_gb = <span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n4. 系统开销:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;   每GPU: <span class="subst">&#123;overhead_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. 总计</span></span><br><span class="line">    total_per_gpu = per_gpu_weights_gb + kv_cache_gb + moe_activation_gb + overhead_gb</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n<span class="subst">&#123;<span class="string">&#x27;=&#x27;</span>*<span class="number">50</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;每GPU总计: <span class="subst">&#123;total_per_gpu:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;建议显存: <span class="subst">&#123;total_per_gpu * <span class="number">1.1</span>:<span class="number">.2</span>f&#125;</span> GB (含10%安全余量)&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;总显存需求: <span class="subst">&#123;total_per_gpu * tensor_parallel:<span class="number">.2</span>f&#125;</span> GB (<span class="subst">&#123;tensor_parallel&#125;</span>卡)&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;=&#x27;</span>*<span class="number">50</span>&#125;</span>\n&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 6. 配置建议</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;GPU配置建议:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> total_per_gpu * <span class="number">1.1</span> &lt;= <span class="number">40</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;✅ 可用A100-40GB (每卡<span class="subst">&#123;total_per_gpu*<span class="number">1.1</span>:<span class="number">.1</span>f&#125;</span>GB &lt; 40GB)&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> total_per_gpu * <span class="number">1.1</span> &lt;= <span class="number">80</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;✅ 可用A100-80GB (每卡<span class="subst">&#123;total_per_gpu*<span class="number">1.1</span>:<span class="number">.1</span>f&#125;</span>GB &lt; 80GB)&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;❌ 需要更大显存或更多卡 (每卡需<span class="subst">&#123;total_per_gpu*<span class="number">1.1</span>:<span class="number">.1</span>f&#125;</span>GB)&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;per_gpu_total&#x27;</span>: total_per_gpu,</span><br><span class="line">        <span class="string">&#x27;per_gpu_weights&#x27;</span>: per_gpu_weights_gb,</span><br><span class="line">        <span class="string">&#x27;per_gpu_kv_cache&#x27;</span>: kv_cache_gb,</span><br><span class="line">        <span class="string">&#x27;per_gpu_activation&#x27;</span>: moe_activation_gb,</span><br><span class="line">        <span class="string">&#x27;per_gpu_overhead&#x27;</span>: overhead_gb</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例1：4卡A100-80GB，BF16精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【示例1：生产环境配置】&quot;</span>)</span><br><span class="line">result = estimate_moe_memory(</span><br><span class="line">    total_params_b=<span class="number">80</span>,</span><br><span class="line">    active_params_b=<span class="number">3</span>,</span><br><span class="line">    dtype_bytes=<span class="number">2</span>,          <span class="comment"># BF16</span></span><br><span class="line">    max_seq_len=<span class="number">8192</span>,</span><br><span class="line">    max_batch_size=<span class="number">64</span>,</span><br><span class="line">    tensor_parallel=<span class="number">4</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例2：2卡A100-80GB，FP8量化</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n【示例2：FP8量化配置】&quot;</span>)</span><br><span class="line">result = estimate_moe_memory(</span><br><span class="line">    total_params_b=<span class="number">80</span>,</span><br><span class="line">    active_params_b=<span class="number">3</span>,</span><br><span class="line">    dtype_bytes=<span class="number">1</span>,          <span class="comment"># FP8</span></span><br><span class="line">    max_seq_len=<span class="number">8192</span>,</span><br><span class="line">    max_batch_size=<span class="number">32</span>,</span><br><span class="line">    tensor_parallel=<span class="number">2</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><strong>输出示例</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">=== MoE模型显存估算 ===</span><br><span class="line">配置: 80B参数, 3B激活</span><br><span class="line">推理配置: seq_len=8192, batch_size=64</span><br><span class="line">并行度: 4卡张量并行</span><br><span class="line"></span><br><span class="line">1. 模型权重:</span><br><span class="line">   总大小: 160.00 GB</span><br><span class="line">   每GPU: 40.00 GB</span><br><span class="line"></span><br><span class="line">2. KV缓存:</span><br><span class="line">   每GPU: 8.19 GB</span><br><span class="line"></span><br><span class="line">3. 激活值和中间结果:</span><br><span class="line">   Dense模型需要: 26.21 GB/GPU</span><br><span class="line">   MoE实际需要: 1.18 GB/GPU</span><br><span class="line">   节省比例: 95.5%</span><br><span class="line"></span><br><span class="line">4. 系统开销:</span><br><span class="line">   每GPU: 3.00 GB</span><br><span class="line"></span><br><span class="line">==================================================</span><br><span class="line">每GPU总计: 52.37 GB</span><br><span class="line">建议显存: 57.61 GB (含10%安全余量)</span><br><span class="line">总显存需求: 209.48 GB (4卡)</span><br><span class="line">==================================================</span><br><span class="line"></span><br><span class="line">GPU配置建议:</span><br><span class="line">✅ 可用A100-80GB (每卡57.6GB &lt; 80GB)</span><br></pre></td></tr></table></figure>

<h3 id="3-5-MoE模型性能特点"><a href="#3-5-MoE模型性能特点" class="headerlink" title="3.5 MoE模型性能特点"></a>3.5 MoE模型性能特点</h3><h4 id="显存优势"><a href="#显存优势" class="headerlink" title="显存优势"></a>显存优势</h4><table>
<thead>
<tr>
<th>模型类型</th>
<th>总参数</th>
<th>激活参数</th>
<th>BF16显存需求</th>
<th>优势</th>
</tr>
</thead>
<tbody><tr>
<td>Dense 80B</td>
<td>80B</td>
<td>80B</td>
<td>~190GB</td>
<td>无</td>
</tr>
<tr>
<td>MoE 80B-A3B</td>
<td>80B</td>
<td>3B</td>
<td>~190GB</td>
<td>计算量减少96%</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>关键</strong>：虽然MoE模型的<strong>权重显存</strong>与Dense模型相同（都需要存储所有参数），但<strong>计算显存</strong>（激活值）大幅减少，使得：</p>
<ul>
<li>推理速度更快（只计算3B参数）</li>
<li>功耗更低</li>
<li>相同显存下可支持更大batch size</li>
</ul>
</blockquote>
<h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><p>✅ <strong>MoE模型特别适合：</strong></p>
<ul>
<li>高吞吐量推理服务</li>
<li>长上下文处理（显存省在激活值上）</li>
<li>多任务处理（不同专家处理不同任务）</li>
<li>成本敏感场景（更少GPU即可达到80B性能）</li>
</ul>
<h3 id="3-6-在线显存计算器"><a href="#3-6-在线显存计算器" class="headerlink" title="3.6 在线显存计算器"></a>3.6 在线显存计算器</h3><p>使用HuggingFace官方计算器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 访问在线计算器</span></span><br><span class="line">https://huggingface.co/spaces/hf-accelerate/model-memory-usage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入参数（注意：标准计算器不完全支持MoE）</span></span><br><span class="line">Model: Qwen/Qwen3-Next-80B-A3B-Instruct</span><br><span class="line">Precision: bfloat16</span><br><span class="line">Number of GPUs: 4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：结果需要手动调整，因为标准计算器按Dense模型计算</span></span><br><span class="line"><span class="comment"># 实际MoE模型的KV缓存和激活值会更小</span></span><br></pre></td></tr></table></figure>

<h2 id="四、模型文件下载"><a href="#四、模型文件下载" class="headerlink" title="四、模型文件下载"></a>四、模型文件下载</h2><h3 id="4-1-环境准备"><a href="#4-1-环境准备" class="headerlink" title="4.1 环境准备"></a>4.1 环境准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装依赖工具</span></span><br><span class="line">pip install huggingface_hub</span><br><span class="line">pip install modelscope</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置代理（如需要）</span></span><br><span class="line"><span class="built_in">export</span> HTTP_PROXY=http://your-proxy:port</span><br><span class="line"><span class="built_in">export</span> HTTPS_PROXY=http://your-proxy:port</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置HuggingFace token（私有模型需要）</span></span><br><span class="line">huggingface-cli login</span><br></pre></td></tr></table></figure>

<h3 id="4-2-使用HuggingFace下载（国外服务器）"><a href="#4-2-使用HuggingFace下载（国外服务器）" class="headerlink" title="4.2 使用HuggingFace下载（国外服务器）"></a>4.2 使用HuggingFace下载（国外服务器）</h3><h4 id="方法1：使用huggingface-cli"><a href="#方法1：使用huggingface-cli" class="headerlink" title="方法1：使用huggingface-cli"></a>方法1：使用huggingface-cli</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载完整模型</span></span><br><span class="line">huggingface-cli download Qwen/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --local-dir /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --local-dir-use-symlinks False</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载特定文件</span></span><br><span class="line">huggingface-cli download Qwen/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --include <span class="string">&quot;*.json&quot;</span> <span class="string">&quot;*.safetensors&quot;</span> \</span><br><span class="line">    --local-dir /data/models/Qwen3-Next-80B-A3B-Instruct</span><br><span class="line"></span><br><span class="line"><span class="comment"># 断点续传（自动支持）</span></span><br><span class="line"><span class="comment"># 如果下载中断，重新执行相同命令即可继续</span></span><br></pre></td></tr></table></figure>

<h4 id="方法2：使用Python脚本"><a href="#方法2：使用Python脚本" class="headerlink" title="方法2：使用Python脚本"></a>方法2：使用Python脚本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> snapshot_download</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载完整模型</span></span><br><span class="line">model_path = snapshot_download(</span><br><span class="line">    repo_id=<span class="string">&quot;Qwen/Qwen3-Next-80B-A3B-Instruct&quot;</span>,</span><br><span class="line">    cache_dir=<span class="string">&quot;/data/models&quot;</span>,</span><br><span class="line">    local_dir=<span class="string">&quot;/data/models/Qwen3-Next-80B-A3B-Instruct&quot;</span>,</span><br><span class="line">    local_dir_use_symlinks=<span class="literal">False</span>,</span><br><span class="line">    resume_download=<span class="literal">True</span>,  <span class="comment"># 支持断点续传</span></span><br><span class="line">    max_workers=<span class="number">8</span>  <span class="comment"># 并发下载线程数</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型已下载到: <span class="subst">&#123;model_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-使用ModelScope下载（国内服务器推荐）"><a href="#4-3-使用ModelScope下载（国内服务器推荐）" class="headerlink" title="4.3 使用ModelScope下载（国内服务器推荐）"></a>4.3 使用ModelScope下载（国内服务器推荐）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装modelscope</span></span><br><span class="line">pip install modelscope</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置ModelScope为首选源</span></span><br><span class="line"><span class="built_in">export</span> VLLM_USE_MODELSCOPE=True</span><br><span class="line"><span class="built_in">export</span> MODELSCOPE_CACHE=/data/models/modelscope_cache</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载模型</span></span><br><span class="line">modelscope download \</span><br><span class="line">    --model qwen/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --local_dir /data/models/Qwen3-Next-80B-A3B-Instruct</span><br></pre></td></tr></table></figure>

<p>使用Python下载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> snapshot_download</span><br><span class="line"></span><br><span class="line">model_dir = snapshot_download(</span><br><span class="line">    <span class="string">&#x27;qwen/Qwen3-Next-80B-A3B-Instruct&#x27;</span>,</span><br><span class="line">    cache_dir=<span class="string">&#x27;/data/models&#x27;</span>,</span><br><span class="line">    local_dir=<span class="string">&#x27;/data/models/Qwen3-Next-80B-A3B-Instruct&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型下载完成: <span class="subst">&#123;model_dir&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-4-验证下载完整性"><a href="#4-4-验证下载完整性" class="headerlink" title="4.4 验证下载完整性"></a>4.4 验证下载完整性</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查文件完整性</span></span><br><span class="line"><span class="built_in">cd</span> /data/models/Qwen3-Next-80B-A3B-Instruct</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件列表和大小</span></span><br><span class="line"><span class="built_in">ls</span> -lh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证配置文件</span></span><br><span class="line"><span class="built_in">cat</span> config.json | jq .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查模型分片是否完整</span></span><br><span class="line"><span class="built_in">ls</span> -1 pytorch_model-*.bin | <span class="built_in">wc</span> -l</span><br><span class="line"><span class="comment"># 应该与 pytorch_model.bin.index.json 中的分片数一致</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算磁盘占用</span></span><br><span class="line"><span class="built_in">du</span> -sh .</span><br></pre></td></tr></table></figure>

<h3 id="4-5-下载加速技巧"><a href="#4-5-下载加速技巧" class="headerlink" title="4.5 下载加速技巧"></a>4.5 下载加速技巧</h3><h4 id="使用镜像站（国内）"><a href="#使用镜像站（国内）" class="headerlink" title="使用镜像站（国内）"></a>使用镜像站（国内）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用HF-Mirror</span></span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line">huggingface-cli download Qwen/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --local-dir /data/models/Qwen3-Next-80B-A3B-Instruct</span><br></pre></td></tr></table></figure>

<h2 id="五、模型部署"><a href="#五、模型部署" class="headerlink" title="五、模型部署"></a>五、模型部署</h2><h3 id="5-1-单卡部署（测试环境）"><a href="#5-1-单卡部署（测试环境）" class="headerlink" title="5.1 单卡部署（测试环境）"></a>5.1 单卡部署（测试环境）</h3><p>适用于小规模测试或显存充足的单卡场景：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基础启动（需要足够显存）</span></span><br><span class="line">vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --host 0.0.0.0 \</span><br><span class="line">    --port 8000 \</span><br><span class="line">    --dtype bfloat16 \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --gpu-memory-utilization 0.9</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用本地模型路径</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --host 0.0.0.0 \</span><br><span class="line">    --port 8000</span><br></pre></td></tr></table></figure>

<h3 id="5-2-多卡张量并行部署（生产环境推荐）"><a href="#5-2-多卡张量并行部署（生产环境推荐）" class="headerlink" title="5.2 多卡张量并行部署（生产环境推荐）"></a>5.2 多卡张量并行部署（生产环境推荐）</h3><h4 id="4卡A100配置"><a href="#4卡A100配置" class="headerlink" title="4卡A100配置"></a>4卡A100配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建启动脚本 start_vllm.sh</span></span><br><span class="line"><span class="built_in">cat</span> &gt; start_vllm.sh &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置环境变量</span></span><br><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=0,1,2,3</span><br><span class="line"><span class="built_in">export</span> VLLM_WORKER_MULTIPROC_METHOD=spawn</span><br><span class="line"><span class="built_in">export</span> PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动vLLM服务</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --host 0.0.0.0 \</span><br><span class="line">    --port 8000 \</span><br><span class="line">    --api-key <span class="string">&quot;your-secure-api-key-here&quot;</span> \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --dtype bfloat16 \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --max-num-seqs 64 \</span><br><span class="line">    --gpu-memory-utilization 0.85 \</span><br><span class="line">    --enable-chunked-prefill \</span><br><span class="line">    --enable-prefix-caching \</span><br><span class="line">    --disable-log-requests \</span><br><span class="line">    --served-model-name qwen3-80b</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="built_in">chmod</span> +x start_vllm.sh</span><br><span class="line">./start_vllm.sh</span><br></pre></td></tr></table></figure>

<h4 id="8卡A100高性能配置"><a href="#8卡A100高性能配置" class="headerlink" title="8卡A100高性能配置"></a>8卡A100高性能配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; start_vllm_8gpu.sh &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7</span><br><span class="line"><span class="built_in">export</span> VLLM_WORKER_MULTIPROC_METHOD=spawn</span><br><span class="line"><span class="built_in">export</span> PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True</span><br><span class="line"></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --host 0.0.0.0 \</span><br><span class="line">    --port 8000 \</span><br><span class="line">    --api-key <span class="string">&quot;your-secure-api-key-here&quot;</span> \</span><br><span class="line">    --tensor-parallel-size 8 \</span><br><span class="line">    --dtype bfloat16 \</span><br><span class="line">    --max-model-len 16384 \</span><br><span class="line">    --max-num-seqs 128 \</span><br><span class="line">    --max-num-batched-tokens 16384 \</span><br><span class="line">    --gpu-memory-utilization 0.9 \</span><br><span class="line">    --enable-chunked-prefill \</span><br><span class="line">    --enable-prefix-caching \</span><br><span class="line">    --trust-remote-code \</span><br><span class="line">    --served-model-name qwen3-80b</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="built_in">chmod</span> +x start_vllm_8gpu.sh</span><br><span class="line">./start_vllm_8gpu.sh</span><br></pre></td></tr></table></figure>

<h3 id="5-3-使用量化加速部署"><a href="#5-3-使用量化加速部署" class="headerlink" title="5.3 使用量化加速部署"></a>5.3 使用量化加速部署</h3><h4 id="FP8量化部署"><a href="#FP8量化部署" class="headerlink" title="FP8量化部署"></a>FP8量化部署</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要先转换模型为FP8格式，或使用已量化模型</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --quantization fp8 \</span><br><span class="line">    --kv-cache-dtype fp8 \</span><br><span class="line">    --max-model-len 16384 \</span><br><span class="line">    --gpu-memory-utilization 0.9</span><br></pre></td></tr></table></figure>

<h3 id="5-4-MoE专家CPU-Offload配置（高级）"><a href="#5-4-MoE专家CPU-Offload配置（高级）" class="headerlink" title="5.4 MoE专家CPU Offload配置（高级）"></a>5.4 MoE专家CPU Offload配置（高级）</h3><blockquote>
<p><strong>警告</strong>：截至vLLM 0.6.x版本，MoE模型的CPU offload功能还在实验阶段，可能存在性能和稳定性问题。</p>
</blockquote>
<h4 id="方法1：使用cpu-offload-gb参数"><a href="#方法1：使用cpu-offload-gb参数" class="headerlink" title="方法1：使用cpu-offload-gb参数"></a>方法1：使用cpu-offload-gb参数</h4><p>vLLM提供了<code>--cpu-offload-gb</code>参数，可以将部分模型参数offload到CPU内存：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基础CPU offload配置</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 2 \</span><br><span class="line">    --dtype bfloat16 \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --gpu-memory-utilization 0.85 \</span><br><span class="line">    --cpu-offload-gb 120 \</span><br><span class="line">    --enforce-eager</span><br></pre></td></tr></table></figure>

<p><strong>参数说明</strong>：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
<th>推荐值</th>
</tr>
</thead>
<tbody><tr>
<td><code>--cpu-offload-gb</code></td>
<td>offload到CPU的显存量(GB)</td>
<td>60-120</td>
</tr>
<tr>
<td><code>--enforce-eager</code></td>
<td>禁用CUDA graph（必需）</td>
<td>必须设置</td>
</tr>
<tr>
<td><code>--swap-space</code></td>
<td>CPU交换空间大小(GB)</td>
<td>4-16</td>
</tr>
</tbody></table>
<h4 id="方法2：环境变量控制"><a href="#方法2：环境变量控制" class="headerlink" title="方法2：环境变量控制"></a>方法2：环境变量控制</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置环境变量</span></span><br><span class="line"><span class="built_in">export</span> VLLM_CPU_OFFLOAD_GB=100</span><br><span class="line"><span class="built_in">export</span> VLLM_ENABLE_CPU_OFFLOAD=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 2 \</span><br><span class="line">    --dtype bfloat16 \</span><br><span class="line">    --enforce-eager</span><br></pre></td></tr></table></figure>

<h4 id="实际效果测试"><a href="#实际效果测试" class="headerlink" title="实际效果测试"></a>实际效果测试</h4><p><strong>测试配置</strong>：2卡A100-80GB + 256GB CPU内存</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动命令</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 2 \</span><br><span class="line">    --dtype bfloat16 \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --max-num-seqs 32 \</span><br><span class="line">    --gpu-memory-utilization 0.80 \</span><br><span class="line">    --cpu-offload-gb 100 \</span><br><span class="line">    --enforce-eager \</span><br><span class="line">    --disable-log-requests</span><br></pre></td></tr></table></figure>

<p><strong>性能对比</strong>：</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>GPU显存&#x2F;卡</th>
<th>CPU内存</th>
<th>TTFT</th>
<th>TPOT</th>
<th>吞吐量</th>
</tr>
</thead>
<tbody><tr>
<td>无offload (4卡)</td>
<td>54GB</td>
<td>-</td>
<td>500ms</td>
<td>40ms</td>
<td>2500 tok&#x2F;s</td>
</tr>
<tr>
<td>CPU offload (2卡)</td>
<td>60GB</td>
<td>100GB</td>
<td>800ms</td>
<td>80ms</td>
<td>1250 tok&#x2F;s</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>性能影响</strong>：启用CPU offload后，推理速度约降低40-50%，但可节省50%的GPU成本。</p>
</blockquote>
<h4 id="注意事项和限制"><a href="#注意事项和限制" class="headerlink" title="注意事项和限制"></a>注意事项和限制</h4><p><strong>❌ 当前限制</strong>：</p>
<ol>
<li><strong>vLLM原生支持有限</strong>：vLLM对MoE专家的智能offload支持不完善</li>
<li><strong>性能损失</strong>：频繁的GPU-CPU数据传输导致延迟增加</li>
<li><strong>PCIe带宽瓶颈</strong>：需要PCIe 4.0 x16以上带宽</li>
<li><strong>CUDA Graph不兼容</strong>：必须使用eager模式，进一步降低性能</li>
</ol>
<p><strong>✅ 适用场景</strong>：</p>
<ul>
<li>预算极其受限，无法配置足够GPU</li>
<li>开发测试环境，对性能要求不高</li>
<li>低并发场景（&lt;10 QPS）</li>
<li>批量离线推理</li>
</ul>
<p><strong>🔧 系统要求</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CPU内存需求</span></span><br><span class="line">至少 200GB 可用内存（推荐256GB+）</span><br><span class="line"></span><br><span class="line"><span class="comment"># PCIe带宽检查</span></span><br><span class="line">lspci -vv | grep -i <span class="string">&quot;lnkcap\|lnksta&quot;</span></span><br><span class="line"><span class="comment"># 确保有PCIe 4.0 x16或更高</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 内存带宽检查</span></span><br><span class="line">sudo apt install mbw</span><br><span class="line">mbw 128  <span class="comment"># 应该有40GB/s以上</span></span><br></pre></td></tr></table></figure>

<h4 id="方法3：使用DeepSpeed-MII（推荐替代方案）"><a href="#方法3：使用DeepSpeed-MII（推荐替代方案）" class="headerlink" title="方法3：使用DeepSpeed-MII（推荐替代方案）"></a>方法3：使用DeepSpeed-MII（推荐替代方案）</h4><p>如果需要更成熟的CPU offload支持，建议使用DeepSpeed-MII：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装DeepSpeed-MII</span></span><br><span class="line">pip install deepspeed-mii</span><br><span class="line"></span><br><span class="line"><span class="comment"># Python部署代码</span></span><br><span class="line">import mii</span><br><span class="line"></span><br><span class="line">mii_config = &#123;</span><br><span class="line">    <span class="string">&quot;tensor_parallel&quot;</span>: 2,</span><br><span class="line">    <span class="string">&quot;dtype&quot;</span>: <span class="string">&quot;bf16&quot;</span>,</span><br><span class="line">    <span class="string">&quot;max_length&quot;</span>: 8192,</span><br><span class="line">    <span class="string">&quot;offload_config&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;device&quot;</span>: <span class="string">&quot;cpu&quot;</span>,</span><br><span class="line">        <span class="string">&quot;nvme_path&quot;</span>: <span class="string">&quot;/nvme/offload&quot;</span>  <span class="comment"># 可选：使用NVMe加速</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">client = mii.serve(</span><br><span class="line">    model_name_or_path=<span class="string">&quot;/data/models/Qwen3-Next-80B-A3B-Instruct&quot;</span>,</span><br><span class="line">    deployment_name=<span class="string">&quot;qwen3-80b&quot;</span>,</span><br><span class="line">    **mii_config</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>DeepSpeed优势</strong>：更成熟的offload实现，智能预取，性能损失更小（约20-30%）</p>
</blockquote>
<h4 id="监控offload性能"><a href="#监控offload性能" class="headerlink" title="监控offload性能"></a>监控offload性能</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 监控脚本</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">monitor_offload</span>():</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># GPU显存使用</span></span><br><span class="line">        gpu_mem = subprocess.check_output([</span><br><span class="line">            <span class="string">&#x27;nvidia-smi&#x27;</span>, <span class="string">&#x27;--query-gpu=memory.used&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;--format=csv,noheader,nounits&#x27;</span></span><br><span class="line">        ]).decode()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># CPU内存使用</span></span><br><span class="line">        cpu_mem = subprocess.check_output([</span><br><span class="line">            <span class="string">&#x27;free&#x27;</span>, <span class="string">&#x27;-g&#x27;</span></span><br><span class="line">        ]).decode()</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;GPU Memory: <span class="subst">&#123;gpu_mem.strip()&#125;</span> MB&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;CPU Memory:\n<span class="subst">&#123;cpu_mem&#125;</span>&quot;</span>)</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">monitor_offload()</span><br></pre></td></tr></table></figure>

<h3 id="5-5-使用Systemd管理服务"><a href="#5-5-使用Systemd管理服务" class="headerlink" title="5.5 使用Systemd管理服务"></a>5.5 使用Systemd管理服务</h3><p>创建系统服务配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建服务文件</span></span><br><span class="line">sudo <span class="built_in">tee</span> /etc/systemd/system/vllm-qwen.service &gt; /dev/null &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=vLLM Qwen3-Next-80B Service</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">User=ubuntu</span><br><span class="line">WorkingDirectory=/home/ubuntu</span><br><span class="line">Environment=<span class="string">&quot;CUDA_VISIBLE_DEVICES=0,1,2,3&quot;</span></span><br><span class="line">Environment=<span class="string">&quot;VLLM_WORKER_MULTIPROC_METHOD=spawn&quot;</span></span><br><span class="line">Environment=<span class="string">&quot;PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True&quot;</span></span><br><span class="line">ExecStart=/home/ubuntu/vllm-env/bin/vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --host 0.0.0.0 \</span><br><span class="line">    --port 8000 \</span><br><span class="line">    --api-key your-api-key \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --dtype bfloat16 \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --gpu-memory-utilization 0.85 \</span><br><span class="line">    --enable-chunked-prefill \</span><br><span class="line">    --enable-prefix-caching</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line">StandardOutput=append:/var/log/vllm-qwen.log</span><br><span class="line">StandardError=append:/var/log/vllm-qwen-error.log</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重载systemd</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">sudo systemctl start vllm-qwen</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置开机自启</span></span><br><span class="line">sudo systemctl <span class="built_in">enable</span> vllm-qwen</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看服务状态</span></span><br><span class="line">sudo systemctl status vllm-qwen</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看日志</span></span><br><span class="line">sudo journalctl -u vllm-qwen -f</span><br></pre></td></tr></table></figure>

<h3 id="5-6-Docker部署"><a href="#5-6-Docker部署" class="headerlink" title="5.6 Docker部署"></a>5.6 Docker部署</h3><h4 id="方法1：使用官方镜像（推荐）⭐"><a href="#方法1：使用官方镜像（推荐）⭐" class="headerlink" title="方法1：使用官方镜像（推荐）⭐"></a>方法1：使用官方镜像（推荐）⭐</h4><p>vLLM提供了官方Docker镜像，开箱即用，无需构建：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拉取最新的vLLM官方镜像</span></span><br><span class="line">docker pull vllm/vllm-openai:latest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或使用nightly版本（包含最新特性）</span></span><br><span class="line">docker pull vllm/vllm-openai:nightly</span><br></pre></td></tr></table></figure>

<p><strong>基础运行示例</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用官方镜像启动vLLM服务</span></span><br><span class="line">docker run -d \</span><br><span class="line">    --name vllm-qwen3 \</span><br><span class="line">    --gpus all \</span><br><span class="line">    -p 8000:8000 \</span><br><span class="line">    -v /data/models:/models \</span><br><span class="line">    --ipc=host \</span><br><span class="line">    vllm/vllm-openai:latest \</span><br><span class="line">    --model /models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --host 0.0.0.0 \</span><br><span class="line">    --port 8000 \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --dtype auto \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --gpu-memory-utilization 0.85</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看日志</span></span><br><span class="line">docker logs -f vllm-qwen3</span><br></pre></td></tr></table></figure>

<p><strong>完整配置示例（生产环境）</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">    --name vllm-qwen3-prod \</span><br><span class="line">    --gpus <span class="string">&#x27;&quot;device=0,1,2,3&quot;&#x27;</span> \</span><br><span class="line">    -p 8000:8000 \</span><br><span class="line">    -v /data/models:/models \</span><br><span class="line">    -v /data/cache:/root/.cache \</span><br><span class="line">    --ipc=host \</span><br><span class="line">    --<span class="built_in">ulimit</span> memlock=-1 \</span><br><span class="line">    --<span class="built_in">ulimit</span> stack=67108864 \</span><br><span class="line">    --restart unless-stopped \</span><br><span class="line">    -e CUDA_VISIBLE_DEVICES=0,1,2,3 \</span><br><span class="line">    -e VLLM_WORKER_MULTIPROC_METHOD=spawn \</span><br><span class="line">    -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \</span><br><span class="line">    -e NCCL_P2P_DISABLE=0 \</span><br><span class="line">    -e NCCL_IB_DISABLE=0\</span><br><span class="line">    -e NCCL_ASYNC_ERROR_HANDLING=1 \</span><br><span class="line">    vllm/vllm-openai:latest \</span><br><span class="line">    --model /models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --served-model-name qwen3-80b \</span><br><span class="line">    --host 0.0.0.0 \</span><br><span class="line">    --port 8000 \</span><br><span class="line">    --api-key your-secure-api-key \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --dtype auto \</span><br><span class="line">    --max-model-len 8192 \</span><br><span class="line">    --max-num-seqs 64 \</span><br><span class="line">    --gpu-memory-utilization 0.85 \</span><br><span class="line">    --enable-chunked-prefill \</span><br><span class="line">    --enable-prefix-caching \</span><br><span class="line">    --trust-remote-code \</span><br><span class="line">    --disable-log-requests \</span><br><span class="line">    --disable-log-stats</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器状态</span></span><br><span class="line">docker ps -a | grep vllm-qwen3-prod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看实时日志</span></span><br><span class="line">docker logs -f vllm-qwen3-prod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看资源使用</span></span><br><span class="line">docker stats vllm-qwen3-prod</span><br></pre></td></tr></table></figure>

<p><strong>Docker Compose配置</strong>：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker-compose.yml</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3.8&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">vllm-qwen3:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">vllm/vllm-openai:latest</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">vllm-qwen3</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">unless-stopped</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">reservations:</span></span><br><span class="line">          <span class="attr">devices:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">driver:</span> <span class="string">nvidia</span></span><br><span class="line">              <span class="attr">device_ids:</span> [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>]</span><br><span class="line">              <span class="attr">capabilities:</span> [<span class="string">gpu</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8000:8000&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/data/models:/models</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/data/cache:/root/.cache</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">CUDA_VISIBLE_DEVICES=0,1,2,3</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">VLLM_WORKER_MULTIPROC_METHOD=spawn</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">NCCL_DEBUG=WARN</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">NCCL_ASYNC_ERROR_HANDLING=1</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">NCCL_IB_DISABLE=0</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">NCCL_P2P_DISABLE=0</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">ipc:</span> <span class="string">host</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">ulimits:</span></span><br><span class="line">      <span class="attr">memlock:</span></span><br><span class="line">        <span class="attr">soft:</span> <span class="number">-1</span></span><br><span class="line">        <span class="attr">hard:</span> <span class="number">-1</span></span><br><span class="line">      <span class="attr">stack:</span></span><br><span class="line">        <span class="attr">soft:</span> <span class="number">67108864</span></span><br><span class="line">        <span class="attr">hard:</span> <span class="number">67108864</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--model</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/models/Qwen3-Next-80B-A3B-Instruct</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--served-model-name</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">qwen3-80b</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--host</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--port</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8000&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--api-key</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">$&#123;VLLM_API_KEY:-your-api-key&#125;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--tensor-parallel-size</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;4&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--dtype</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">auto</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--max-model-len</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8192&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--gpu-memory-utilization</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;0.85&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--enable-chunked-prefill</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--enable-prefix-caching</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--disable-log-requests</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">--disable-log-stats</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用方式</span></span><br><span class="line"><span class="comment"># docker-compose up -d</span></span><br><span class="line"><span class="comment"># docker-compose logs -f</span></span><br><span class="line"><span class="comment"># docker-compose down</span></span><br></pre></td></tr></table></figure>

<h4 id="Docker部署最佳实践"><a href="#Docker部署最佳实践" class="headerlink" title="Docker部署最佳实践"></a>Docker部署最佳实践</h4><p><strong>1. 网络配置</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建专用网络</span></span><br><span class="line">docker network create vllm-network</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用专用网络运行</span></span><br><span class="line">docker run -d \</span><br><span class="line">    --name vllm-qwen3 \</span><br><span class="line">    --network vllm-network \</span><br><span class="line">    --gpus all \</span><br><span class="line">    -p 8000:8000 \</span><br><span class="line">    vllm/vllm-openai:latest \</span><br><span class="line">    --model /models/Qwen3-Next-80B-A3B-Instruct</span><br></pre></td></tr></table></figure>

<p><strong>2. 健康检查</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在docker-compose.yml中添加健康检查</span></span><br><span class="line"><span class="attr">healthcheck:</span></span><br><span class="line">  <span class="attr">test:</span> [<span class="string">&quot;CMD&quot;</span>, <span class="string">&quot;curl&quot;</span>, <span class="string">&quot;-f&quot;</span>, <span class="string">&quot;http://localhost:8000/health&quot;</span>]</span><br><span class="line">  <span class="attr">interval:</span> <span class="string">30s</span></span><br><span class="line">  <span class="attr">timeout:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">start_period:</span> <span class="string">60s</span></span><br></pre></td></tr></table></figure>

<p><strong>3. 日志管理</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 限制日志大小</span></span><br><span class="line">docker run -d \</span><br><span class="line">    --name vllm-qwen3 \</span><br><span class="line">    --log-driver json-file \</span><br><span class="line">    --log-opt max-size=100m \</span><br><span class="line">    --log-opt max-file=5 \</span><br><span class="line">    vllm/vllm-openai:latest \</span><br><span class="line">    --model /models/Qwen3-Next-80B-A3B-Instruct</span><br></pre></td></tr></table></figure>

<p><strong>4. 资源限制</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置CPU和内存限制</span></span><br><span class="line">docker run -d \</span><br><span class="line">    --name vllm-qwen3 \</span><br><span class="line">    --gpus all \</span><br><span class="line">    --cpus=<span class="string">&quot;32&quot;</span> \</span><br><span class="line">    --memory=<span class="string">&quot;200g&quot;</span> \</span><br><span class="line">    --memory-swap=<span class="string">&quot;250g&quot;</span> \</span><br><span class="line">    vllm/vllm-openai:latest \</span><br><span class="line">    --model /models/Qwen3-Next-80B-A3B-Instruct</span><br></pre></td></tr></table></figure>

<h3 id="5-7-验证部署"><a href="#5-7-验证部署" class="headerlink" title="5.7 验证部署"></a>5.7 验证部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查服务是否启动</span></span><br><span class="line">curl http://localhost:8000/health</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型列表</span></span><br><span class="line">curl http://localhost:8000/v1/models</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单推理测试</span></span><br><span class="line">curl http://localhost:8000/v1/chat/completions \</span><br><span class="line">    -H <span class="string">&quot;Content-Type: application/json&quot;</span> \</span><br><span class="line">    -H <span class="string">&quot;Authorization: Bearer your-api-key&quot;</span> \</span><br><span class="line">    -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">        &quot;model&quot;: &quot;qwen3-80b&quot;,</span></span><br><span class="line"><span class="string">        &quot;messages&quot;: [</span></span><br><span class="line"><span class="string">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好，请介绍一下你自己&quot;&#125;</span></span><br><span class="line"><span class="string">        ],</span></span><br><span class="line"><span class="string">        &quot;max_tokens&quot;: 100</span></span><br><span class="line"><span class="string">    &#125;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="六、模型监控"><a href="#六、模型监控" class="headerlink" title="六、模型监控"></a>六、模型监控</h2><h3 id="6-1-vLLM内置监控指标"><a href="#6-1-vLLM内置监控指标" class="headerlink" title="6.1 vLLM内置监控指标"></a>6.1 vLLM内置监控指标</h3><p>vLLM提供Prometheus格式的监控指标：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 访问metrics端点</span></span><br><span class="line">curl http://localhost:8000/metrics</span><br></pre></td></tr></table></figure>

<h4 id="关键指标说明"><a href="#关键指标说明" class="headerlink" title="关键指标说明"></a>关键指标说明</h4><table>
<thead>
<tr>
<th>指标名称</th>
<th>说明</th>
<th>类型</th>
</tr>
</thead>
<tbody><tr>
<td><code>vllm:num_requests_running</code></td>
<td>正在运行的请求数</td>
<td>Gauge</td>
</tr>
<tr>
<td><code>vllm:num_requests_waiting</code></td>
<td>等待中的请求数</td>
<td>Gauge</td>
</tr>
<tr>
<td><code>vllm:gpu_cache_usage_perc</code></td>
<td>GPU缓存使用率</td>
<td>Gauge</td>
</tr>
<tr>
<td><code>vllm:num_preemptions_total</code></td>
<td>总抢占次数</td>
<td>Counter</td>
</tr>
<tr>
<td><code>vllm:time_to_first_token_seconds</code></td>
<td>首token时间</td>
<td>Histogram</td>
</tr>
<tr>
<td><code>vllm:time_per_output_token_seconds</code></td>
<td>每token生成时间</td>
<td>Histogram</td>
</tr>
<tr>
<td><code>vllm:e2e_request_latency_seconds</code></td>
<td>端到端延迟</td>
<td>Histogram</td>
</tr>
</tbody></table>
<h3 id="6-2-配置Prometheus监控"><a href="#6-2-配置Prometheus监控" class="headerlink" title="6.2 配置Prometheus监控"></a>6.2 配置Prometheus监控</h3><h4 id="安装Prometheus"><a href="#安装Prometheus" class="headerlink" title="安装Prometheus"></a>安装Prometheus</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载Prometheus</span></span><br><span class="line">wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz</span><br><span class="line">tar xvfz prometheus-*.tar.gz</span><br><span class="line"><span class="built_in">cd</span> prometheus-*</span><br></pre></td></tr></table></figure>

<h4 id="配置Prometheus"><a href="#配置Prometheus" class="headerlink" title="配置Prometheus"></a>配置Prometheus</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prometheus.yml</span></span><br><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span> <span class="string">15s</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span></span><br><span class="line"></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;vllm-qwen3&#x27;</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:8000&#x27;</span>]</span><br><span class="line">    <span class="attr">metrics_path:</span> <span class="string">&#x27;/metrics&#x27;</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">10s</span></span><br></pre></td></tr></table></figure>

<h4 id="启动Prometheus"><a href="#启动Prometheus" class="headerlink" title="启动Prometheus"></a>启动Prometheus</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./prometheus --config.file=prometheus.yml \</span><br><span class="line">    --storage.tsdb.path=./data \</span><br><span class="line">    --web.listen-address=:9090</span><br></pre></td></tr></table></figure>

<h3 id="6-3-配置Grafana可视化"><a href="#6-3-配置Grafana可视化" class="headerlink" title="6.3 配置Grafana可视化"></a>6.3 配置Grafana可视化</h3><h4 id="安装Grafana"><a href="#安装Grafana" class="headerlink" title="安装Grafana"></a>安装Grafana</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ubuntu/Debian</span></span><br><span class="line">sudo apt-get install -y grafana</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Grafana</span></span><br><span class="line">sudo systemctl start grafana-server</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> grafana-server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问 http://localhost:3000</span></span><br><span class="line"><span class="comment"># 默认账号密码: admin/admin</span></span><br></pre></td></tr></table></figure>

<h4 id="配置数据源"><a href="#配置数据源" class="headerlink" title="配置数据源"></a>配置数据源</h4><ol>
<li>登录Grafana</li>
<li>添加Prometheus数据源</li>
<li>URL: <code>http://localhost:9090</code></li>
<li>点击”Save &amp; Test”</li>
</ol>
<h4 id="导入vLLM监控面板"><a href="#导入vLLM监控面板" class="headerlink" title="导入vLLM监控面板"></a>导入vLLM监控面板</h4><p>创建Dashboard配置文件 <code>vllm-dashboard.json</code>：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;dashboard&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;vLLM Qwen3-80B Monitoring&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;panels&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Running Requests&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;targets&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;expr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;vllm:num_requests_running&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;graph&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;GPU Cache Usage&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;targets&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;expr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;vllm:gpu_cache_usage_perc&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gauge&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Time to First Token (P95)&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;targets&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;expr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;histogram_quantile(0.95, rate(vllm:time_to_first_token_seconds_bucket[5m]))&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;graph&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Request Latency (P99)&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;targets&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;expr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;histogram_quantile(0.99, rate(vllm:e2e_request_latency_seconds_bucket[5m]))&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;graph&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="6-4-GPU监控"><a href="#6-4-GPU监控" class="headerlink" title="6.4 GPU监控"></a>6.4 GPU监控</h3><h4 id="使用nvidia-smi监控"><a href="#使用nvidia-smi监控" class="headerlink" title="使用nvidia-smi监控"></a>使用nvidia-smi监控</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实时监控</span></span><br><span class="line">watch -n 1 nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录GPU使用情况到文件</span></span><br><span class="line">nvidia-smi --query-gpu=timestamp,index,utilization.gpu,utilization.memory,memory.used,memory.total,temperature.gpu \</span><br><span class="line">    --format=csv -l 5 &gt; gpu_metrics.csv</span><br></pre></td></tr></table></figure>

<h4 id="使用DCGM监控（推荐）"><a href="#使用DCGM监控（推荐）" class="headerlink" title="使用DCGM监控（推荐）"></a>使用DCGM监控（推荐）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装DCGM</span></span><br><span class="line">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb</span><br><span class="line">sudo dpkg -i cuda-keyring_1.0-1_all.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y datacenter-gpu-manager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动DCGM</span></span><br><span class="line">sudo systemctl start nvidia-dcgm</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> nvidia-dcgm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装dcgm-exporter for Prometheus</span></span><br><span class="line">docker run -d \</span><br><span class="line">    --gpus all \</span><br><span class="line">    --name dcgm-exporter \</span><br><span class="line">    -p 9400:9400 \</span><br><span class="line">    nvcr.io/nvidia/k8s/dcgm-exporter:3.1.8-3.2.0-ubuntu22.04</span><br></pre></td></tr></table></figure>

<h4 id="配置Prometheus抓取GPU指标"><a href="#配置Prometheus抓取GPU指标" class="headerlink" title="配置Prometheus抓取GPU指标"></a>配置Prometheus抓取GPU指标</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加到prometheus.yml</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;dcgm&#x27;</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9400&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h3 id="6-5-日志监控"><a href="#6-5-日志监控" class="headerlink" title="6.5 日志监控"></a>6.5 日志监控</h3><h4 id="vLLM日志配置"><a href="#vLLM日志配置" class="headerlink" title="vLLM日志配置"></a>vLLM日志配置</h4><p>vLLM通过环境变量和启动参数控制日志：</p>
<p><strong>方法1：环境变量控制日志级别</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置日志级别</span></span><br><span class="line"><span class="built_in">export</span> VLLM_LOGGING_LEVEL=INFO  <span class="comment"># 可选: DEBUG, INFO, WARNING, ERROR</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用详细日志</span></span><br><span class="line"><span class="built_in">export</span> VLLM_TRACE_FUNCTION=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用请求日志（生产环境建议禁用）</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    <span class="comment"># 不加 --disable-log-requests 参数</span></span><br></pre></td></tr></table></figure>

<p><strong>方法2：启动参数控制</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生产环境：禁用请求日志</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --disable-log-requests \</span><br><span class="line">    --disable-log-stats</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开发环境：启用详细日志</span></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --log-requests</span><br></pre></td></tr></table></figure>

<p><strong>日志输出示例</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vLLM默认日志格式</span></span><br><span class="line">INFO 11-24 10:30:15 llm_engine.py:123] Initializing engine...</span><br><span class="line">INFO 11-24 10:30:20 llm_engine.py:456] <span class="comment"># GPU blocks: 2048</span></span><br><span class="line">INFO 11-24 10:30:20 model_runner.py:123] Loading model...</span><br><span class="line">INFO 11-24 10:30:45 api_server.py:234] vLLM API server started at http://0.0.0.0:8000</span><br></pre></td></tr></table></figure>

<h4 id="日志重定向和持久化"><a href="#日志重定向和持久化" class="headerlink" title="日志重定向和持久化"></a>日志重定向和持久化</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：重定向到文件</span></span><br><span class="line">vllm serve model-path --tensor-parallel-size 4 \</span><br><span class="line">    &gt; /var/log/vllm-qwen3.<span class="built_in">log</span> 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：使用tee同时输出到终端和文件</span></span><br><span class="line">vllm serve model-path --tensor-parallel-size 4 \</span><br><span class="line">    2&gt;&amp;1 | <span class="built_in">tee</span> -a /var/log/vllm-qwen3.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法3：Systemd管理（自动记录日志）</span></span><br><span class="line"><span class="comment"># 日志通过journalctl查看</span></span><br><span class="line">sudo journalctl -u vllm-qwen -f</span><br></pre></td></tr></table></figure>

<h4 id="日志轮转配置"><a href="#日志轮转配置" class="headerlink" title="日志轮转配置"></a>日志轮转配置</h4><p>创建logrotate配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 /etc/logrotate.d/vllm-qwen</span></span><br><span class="line">sudo <span class="built_in">tee</span> /etc/logrotate.d/vllm-qwen &gt; /dev/null &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">/var/log/vllm-qwen*.<span class="built_in">log</span> &#123;</span><br><span class="line">    daily</span><br><span class="line">    rotate 7</span><br><span class="line">    compress</span><br><span class="line">    delaycompress</span><br><span class="line">    missingok</span><br><span class="line">    notifempty</span><br><span class="line">    create 0644 ubuntu ubuntu</span><br><span class="line">    sharedscripts</span><br><span class="line">    postrotate</span><br><span class="line">        systemctl reload vllm-qwen &gt; /dev/null 2&gt;&amp;1 || <span class="literal">true</span></span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试配置</span></span><br><span class="line">sudo logrotate -d /etc/logrotate.d/vllm-qwen</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动执行轮转</span></span><br><span class="line">sudo logrotate -f /etc/logrotate.d/vllm-qwen</span><br></pre></td></tr></table></figure>

<h4 id="使用Loki收集日志"><a href="#使用Loki收集日志" class="headerlink" title="使用Loki收集日志"></a>使用Loki收集日志</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装Promtail</span></span><br><span class="line">wget https://github.com/grafana/loki/releases/download/v2.8.0/promtail-linux-amd64.zip</span><br><span class="line">unzip promtail-linux-amd64.zip</span><br><span class="line"><span class="built_in">chmod</span> +x promtail-linux-amd64</span><br></pre></td></tr></table></figure>

<p>配置Promtail (<code>promtail-config.yml</code>)：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">http_listen_port:</span> <span class="number">9080</span></span><br><span class="line">  <span class="attr">grpc_listen_port:</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="attr">positions:</span></span><br><span class="line">  <span class="attr">filename:</span> <span class="string">/tmp/positions.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">clients:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">url:</span> <span class="string">http://localhost:3100/loki/api/v1/push</span></span><br><span class="line"></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">vllm-logs</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">localhost</span></span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">job:</span> <span class="string">vllm-qwen3</span></span><br><span class="line">          <span class="attr">__path__:</span> <span class="string">/var/log/vllm-qwen*.log</span></span><br></pre></td></tr></table></figure>

<h3 id="6-6-告警配置"><a href="#6-6-告警配置" class="headerlink" title="6.6 告警配置"></a>6.6 告警配置</h3><h4 id="Prometheus告警规则"><a href="#Prometheus告警规则" class="headerlink" title="Prometheus告警规则"></a>Prometheus告警规则</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># alerts.yml</span></span><br><span class="line"><span class="attr">groups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">vllm_alerts</span></span><br><span class="line">    <span class="attr">interval:</span> <span class="string">30s</span></span><br><span class="line">    <span class="attr">rules:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">alert:</span> <span class="string">HighRequestWaitTime</span></span><br><span class="line">        <span class="attr">expr:</span> <span class="string">vllm:num_requests_waiting</span> <span class="string">&gt;</span> <span class="number">50</span></span><br><span class="line">        <span class="attr">for:</span> <span class="string">2m</span></span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">severity:</span> <span class="string">warning</span></span><br><span class="line">        <span class="attr">annotations:</span></span><br><span class="line">          <span class="attr">summary:</span> <span class="string">&quot;Too many requests waiting&quot;</span></span><br><span class="line">          <span class="attr">description:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; $value &#125;&#125;</span> requests waiting for processing&quot;</span></span><br><span class="line">      </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">alert:</span> <span class="string">HighGPUMemoryUsage</span></span><br><span class="line">        <span class="attr">expr:</span> <span class="string">vllm:gpu_cache_usage_perc</span> <span class="string">&gt;</span> <span class="number">95</span></span><br><span class="line">        <span class="attr">for:</span> <span class="string">5m</span></span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">severity:</span> <span class="string">critical</span></span><br><span class="line">        <span class="attr">annotations:</span></span><br><span class="line">          <span class="attr">summary:</span> <span class="string">&quot;GPU memory usage is very high&quot;</span></span><br><span class="line">          <span class="attr">description:</span> <span class="string">&quot;GPU cache usage at <span class="template-variable">&#123;&#123; $value &#125;&#125;</span>%&quot;</span></span><br><span class="line">      </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">alert:</span> <span class="string">SlowTimeToFirstToken</span></span><br><span class="line">        <span class="attr">expr:</span> <span class="string">histogram_quantile(0.95,</span> <span class="string">rate(vllm:time_to_first_token_seconds_bucket[5m]))</span> <span class="string">&gt;</span> <span class="number">2</span></span><br><span class="line">        <span class="attr">for:</span> <span class="string">5m</span></span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">severity:</span> <span class="string">warning</span></span><br><span class="line">        <span class="attr">annotations:</span></span><br><span class="line">          <span class="attr">summary:</span> <span class="string">&quot;Slow time to first token&quot;</span></span><br><span class="line">          <span class="attr">description:</span> <span class="string">&quot;P95 TTFT is <span class="template-variable">&#123;&#123; $value &#125;&#125;</span>s&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="七、模型压测"><a href="#七、模型压测" class="headerlink" title="七、模型压测"></a>七、模型压测</h2><h3 id="7-1-使用vLLM官方压测工具"><a href="#7-1-使用vLLM官方压测工具" class="headerlink" title="7.1 使用vLLM官方压测工具"></a>7.1 使用vLLM官方压测工具</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装压测工具</span></span><br><span class="line">pip install aiohttp tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载压测脚本</span></span><br><span class="line">wget https://raw.githubusercontent.com/vllm-project/vllm/main/benchmarks/benchmark_serving.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行压测</span></span><br><span class="line">python benchmark_serving.py \</span><br><span class="line">    --host localhost \</span><br><span class="line">    --port 8000 \</span><br><span class="line">    --endpoint /v1/completions \</span><br><span class="line">    --model qwen3-80b \</span><br><span class="line">    --dataset-name random \</span><br><span class="line">    --request-rate 10 \</span><br><span class="line">    --num-prompts 500 \</span><br><span class="line">    --output-len 128 \</span><br><span class="line">    --seed 2024</span><br></pre></td></tr></table></figure>

<h4 id="压测参数说明"><a href="#压测参数说明" class="headerlink" title="压测参数说明"></a>压测参数说明</h4><table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
<th>推荐值</th>
</tr>
</thead>
<tbody><tr>
<td><code>--request-rate</code></td>
<td>每秒请求数(QPS)</td>
<td>1-50</td>
</tr>
<tr>
<td><code>--num-prompts</code></td>
<td>总请求数</td>
<td>100-1000</td>
</tr>
<tr>
<td><code>--input-len</code></td>
<td>输入长度</td>
<td>512-2048</td>
</tr>
<tr>
<td><code>--output-len</code></td>
<td>输出长度</td>
<td>128-512</td>
</tr>
<tr>
<td><code>--dataset-name</code></td>
<td>数据集</td>
<td>random&#x2F;sharegpt</td>
</tr>
</tbody></table>
<h3 id="7-2-使用Apache-Bench进行简单压测"><a href="#7-2-使用Apache-Bench进行简单压测" class="headerlink" title="7.2 使用Apache Bench进行简单压测"></a>7.2 使用Apache Bench进行简单压测</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装ab</span></span><br><span class="line">sudo apt-get install apache2-utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建测试请求文件</span></span><br><span class="line"><span class="built_in">cat</span> &gt; test_request.json &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;qwen3-80b&quot;</span>,</span><br><span class="line">  <span class="string">&quot;messages&quot;</span>: [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;请介绍一下人工智能的发展历史&quot;</span>&#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;max_tokens&quot;</span>: 200,</span><br><span class="line">  <span class="string">&quot;temperature&quot;</span>: 0.7</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行压测</span></span><br><span class="line">ab -n 100 -c 10 \</span><br><span class="line">   -p test_request.json \</span><br><span class="line">   -T application/json \</span><br><span class="line">   -H <span class="string">&quot;Authorization: Bearer your-api-key&quot;</span> \</span><br><span class="line">   http://localhost:8000/v1/chat/completions</span><br></pre></td></tr></table></figure>

<h3 id="7-3-使用Locust进行复杂压测"><a href="#7-3-使用Locust进行复杂压测" class="headerlink" title="7.3 使用Locust进行复杂压测"></a>7.3 使用Locust进行复杂压测</h3><h4 id="安装Locust"><a href="#安装Locust" class="headerlink" title="安装Locust"></a>安装Locust</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install locust</span><br></pre></td></tr></table></figure>

<h4 id="创建压测脚本"><a href="#创建压测脚本" class="headerlink" title="创建压测脚本"></a>创建压测脚本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># locustfile.py</span></span><br><span class="line"><span class="keyword">from</span> locust <span class="keyword">import</span> HttpUser, task, between</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VLLMUser</span>(<span class="title class_ inherited__">HttpUser</span>):</span><br><span class="line">    wait_time = between(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_start</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;初始化时执行&quot;&quot;&quot;</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Authorization&quot;</span>: <span class="string">&quot;Bearer your-api-key&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 准备测试prompt</span></span><br><span class="line">        self.prompts = [</span><br><span class="line">            <span class="string">&quot;请解释一下什么是深度学习&quot;</span>,</span><br><span class="line">            <span class="string">&quot;介绍一下自然语言处理的应用&quot;</span>,</span><br><span class="line">            <span class="string">&quot;什么是Transformer架构&quot;</span>,</span><br><span class="line">            <span class="string">&quot;如何优化大模型推理性能&quot;</span>,</span><br><span class="line">            <span class="string">&quot;请写一个Python快速排序算法&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @task(<span class="params"><span class="number">3</span></span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chat_completion</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;聊天补全压测&quot;&quot;&quot;</span></span><br><span class="line">        payload = &#123;</span><br><span class="line">            <span class="string">&quot;model&quot;</span>: <span class="string">&quot;qwen3-80b&quot;</span>,</span><br><span class="line">            <span class="string">&quot;messages&quot;</span>: [</span><br><span class="line">                &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: random.choice(self.prompts)&#125;</span><br><span class="line">            ],</span><br><span class="line">            <span class="string">&quot;max_tokens&quot;</span>: <span class="number">200</span>,</span><br><span class="line">            <span class="string">&quot;temperature&quot;</span>: <span class="number">0.7</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> self.client.post(</span><br><span class="line">            <span class="string">&quot;/v1/chat/completions&quot;</span>,</span><br><span class="line">            json=payload,</span><br><span class="line">            headers=self.headers,</span><br><span class="line">            catch_response=<span class="literal">True</span></span><br><span class="line">        ) <span class="keyword">as</span> response:</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                data = response.json()</span><br><span class="line">                <span class="comment"># 记录首token时间等指标</span></span><br><span class="line">                response.success()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                response.failure(<span class="string">f&quot;Failed: <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @task(<span class="params"><span class="number">1</span></span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">streaming_completion</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;流式补全压测&quot;&quot;&quot;</span></span><br><span class="line">        payload = &#123;</span><br><span class="line">            <span class="string">&quot;model&quot;</span>: <span class="string">&quot;qwen3-80b&quot;</span>,</span><br><span class="line">            <span class="string">&quot;messages&quot;</span>: [</span><br><span class="line">                &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;请详细介绍机器学习的基本概念&quot;</span>&#125;</span><br><span class="line">            ],</span><br><span class="line">            <span class="string">&quot;max_tokens&quot;</span>: <span class="number">500</span>,</span><br><span class="line">            <span class="string">&quot;stream&quot;</span>: <span class="literal">True</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> self.client.post(</span><br><span class="line">            <span class="string">&quot;/v1/chat/completions&quot;</span>,</span><br><span class="line">            json=payload,</span><br><span class="line">            headers=self.headers,</span><br><span class="line">            stream=<span class="literal">True</span>,</span><br><span class="line">            catch_response=<span class="literal">True</span></span><br><span class="line">        ) <span class="keyword">as</span> response:</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="comment"># 读取流式响应</span></span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> response.iter_lines():</span><br><span class="line">                    <span class="keyword">if</span> line:</span><br><span class="line">                        <span class="keyword">pass</span></span><br><span class="line">                response.success()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                response.failure(<span class="string">f&quot;Failed: <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="运行Locust压测"><a href="#运行Locust压测" class="headerlink" title="运行Locust压测"></a>运行Locust压测</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Locust</span></span><br><span class="line">locust -f locustfile.py --host=http://localhost:8000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或使用命令行模式</span></span><br><span class="line">locust -f locustfile.py \</span><br><span class="line">    --host=http://localhost:8000 \</span><br><span class="line">    --<span class="built_in">users</span> 50 \</span><br><span class="line">    --spawn-rate 5 \</span><br><span class="line">    --run-time 10m \</span><br><span class="line">    --headless \</span><br><span class="line">    --html=locust_report.html</span><br></pre></td></tr></table></figure>

<h3 id="7-4-压测结果分析"><a href="#7-4-压测结果分析" class="headerlink" title="7.4 压测结果分析"></a>7.4 压测结果分析</h3><h4 id="关键性能指标"><a href="#关键性能指标" class="headerlink" title="关键性能指标"></a>关键性能指标</h4><table>
<thead>
<tr>
<th>指标</th>
<th>说明</th>
<th>优秀标准</th>
<th>可接受标准</th>
</tr>
</thead>
<tbody><tr>
<td><strong>TTFT</strong> (Time To First Token)</td>
<td>首token延迟</td>
<td>&lt; 500ms</td>
<td>&lt; 1s</td>
</tr>
<tr>
<td><strong>TPOT</strong> (Time Per Output Token)</td>
<td>每token生成时间</td>
<td>&lt; 50ms</td>
<td>&lt; 100ms</td>
</tr>
<tr>
<td><strong>Throughput</strong></td>
<td>吞吐量(tokens&#x2F;s)</td>
<td>&gt; 2000</td>
<td>&gt; 1000</td>
</tr>
<tr>
<td><strong>QPS</strong></td>
<td>每秒查询数</td>
<td>&gt; 20</td>
<td>&gt; 10</td>
</tr>
<tr>
<td><strong>P99 Latency</strong></td>
<td>99分位延迟</td>
<td>&lt; 3s</td>
<td>&lt; 5s</td>
</tr>
<tr>
<td><strong>Error Rate</strong></td>
<td>错误率</td>
<td>&lt; 0.1%</td>
<td>&lt; 1%</td>
</tr>
</tbody></table>
<h4 id="生成压测报告"><a href="#生成压测报告" class="headerlink" title="生成压测报告"></a>生成压测报告</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># analyze_benchmark.py</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">analyze_results</span>(<span class="params">results_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;分析压测结果&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(results_file, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        results = json.load(f)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 提取关键指标</span></span><br><span class="line">    ttft = results[<span class="string">&#x27;time_to_first_token&#x27;</span>]</span><br><span class="line">    tpot = results[<span class="string">&#x27;time_per_output_token&#x27;</span>]</span><br><span class="line">    throughput = results[<span class="string">&#x27;throughput&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 统计分析</span></span><br><span class="line">    stats = &#123;</span><br><span class="line">        <span class="string">&#x27;TTFT_mean&#x27;</span>: <span class="built_in">sum</span>(ttft) / <span class="built_in">len</span>(ttft),</span><br><span class="line">        <span class="string">&#x27;TTFT_p95&#x27;</span>: <span class="built_in">sorted</span>(ttft)[<span class="built_in">int</span>(<span class="built_in">len</span>(ttft) * <span class="number">0.95</span>)],</span><br><span class="line">        <span class="string">&#x27;TTFT_p99&#x27;</span>: <span class="built_in">sorted</span>(ttft)[<span class="built_in">int</span>(<span class="built_in">len</span>(ttft) * <span class="number">0.99</span>)],</span><br><span class="line">        <span class="string">&#x27;TPOT_mean&#x27;</span>: <span class="built_in">sum</span>(tpot) / <span class="built_in">len</span>(tpot),</span><br><span class="line">        <span class="string">&#x27;Throughput&#x27;</span>: throughput</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=== 压测结果分析 ===&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;平均首token延迟: <span class="subst">&#123;stats[<span class="string">&#x27;TTFT_mean&#x27;</span>]:<span class="number">.2</span>f&#125;</span>ms&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;P95首token延迟: <span class="subst">&#123;stats[<span class="string">&#x27;TTFT_p95&#x27;</span>]:<span class="number">.2</span>f&#125;</span>ms&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;P99首token延迟: <span class="subst">&#123;stats[<span class="string">&#x27;TTFT_p99&#x27;</span>]:<span class="number">.2</span>f&#125;</span>ms&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;平均每token时间: <span class="subst">&#123;stats[<span class="string">&#x27;TPOT_mean&#x27;</span>]:<span class="number">.2</span>f&#125;</span>ms&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;吞吐量: <span class="subst">&#123;stats[<span class="string">&#x27;Throughput&#x27;</span>]:<span class="number">.2</span>f&#125;</span> tokens/s&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘制图表</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line">    axes[<span class="number">0</span>, <span class="number">0</span>].hist(ttft, bins=<span class="number">50</span>)</span><br><span class="line">    axes[<span class="number">0</span>, <span class="number">0</span>].set_title(<span class="string">&#x27;Time to First Token Distribution&#x27;</span>)</span><br><span class="line">    axes[<span class="number">0</span>, <span class="number">0</span>].set_xlabel(<span class="string">&#x27;TTFT (ms)&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    axes[<span class="number">0</span>, <span class="number">1</span>].hist(tpot, bins=<span class="number">50</span>)</span><br><span class="line">    axes[<span class="number">0</span>, <span class="number">1</span>].set_title(<span class="string">&#x27;Time per Output Token Distribution&#x27;</span>)</span><br><span class="line">    axes[<span class="number">0</span>, <span class="number">1</span>].set_xlabel(<span class="string">&#x27;TPOT (ms)&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">0</span>].plot(ttft)</span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">0</span>].set_title(<span class="string">&#x27;TTFT Over Time&#x27;</span>)</span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">0</span>].set_xlabel(<span class="string">&#x27;Request Number&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">1</span>].plot(tpot)</span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">1</span>].set_title(<span class="string">&#x27;TPOT Over Time&#x27;</span>)</span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">1</span>].set_xlabel(<span class="string">&#x27;Token Number&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(<span class="string">&#x27;benchmark_analysis.png&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;图表已保存至 benchmark_analysis.png&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    analyze_results(<span class="string">&#x27;benchmark_results.json&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="八、模型推理Trace追踪"><a href="#八、模型推理Trace追踪" class="headerlink" title="八、模型推理Trace追踪"></a>八、模型推理Trace追踪</h2><h3 id="8-1-启用OpenTelemetry追踪"><a href="#8-1-启用OpenTelemetry追踪" class="headerlink" title="8.1 启用OpenTelemetry追踪"></a>8.1 启用OpenTelemetry追踪</h3><h4 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install opentelemetry-api opentelemetry-sdk</span><br><span class="line">pip install opentelemetry-instrumentation-requests</span><br><span class="line">pip install opentelemetry-exporter-jaeger</span><br></pre></td></tr></table></figure>

<h4 id="配置OpenTelemetry"><a href="#配置OpenTelemetry" class="headerlink" title="配置OpenTelemetry"></a>配置OpenTelemetry</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tracing_config.py</span></span><br><span class="line"><span class="keyword">from</span> opentelemetry <span class="keyword">import</span> trace</span><br><span class="line"><span class="keyword">from</span> opentelemetry.sdk.trace <span class="keyword">import</span> TracerProvider</span><br><span class="line"><span class="keyword">from</span> opentelemetry.sdk.trace.export <span class="keyword">import</span> BatchSpanProcessor</span><br><span class="line"><span class="keyword">from</span> opentelemetry.exporter.jaeger.thrift <span class="keyword">import</span> JaegerExporter</span><br><span class="line"><span class="keyword">from</span> opentelemetry.sdk.resources <span class="keyword">import</span> Resource</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_tracing</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;配置分布式追踪&quot;&quot;&quot;</span></span><br><span class="line">    resource = Resource.create(&#123;</span><br><span class="line">        <span class="string">&quot;service.name&quot;</span>: <span class="string">&quot;vllm-qwen3-80b&quot;</span>,</span><br><span class="line">        <span class="string">&quot;service.version&quot;</span>: <span class="string">&quot;1.0.0&quot;</span></span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">    tracer_provider = TracerProvider(resource=resource)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 配置Jaeger导出器</span></span><br><span class="line">    jaeger_exporter = JaegerExporter(</span><br><span class="line">        agent_host_name=<span class="string">&quot;localhost&quot;</span>,</span><br><span class="line">        agent_port=<span class="number">6831</span>,</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    span_processor = BatchSpanProcessor(jaeger_exporter)</span><br><span class="line">    tracer_provider.add_span_processor(span_processor)</span><br><span class="line">    </span><br><span class="line">    trace.set_tracer_provider(tracer_provider)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> trace.get_tracer(__name__)</span><br><span class="line"></span><br><span class="line">tracer = setup_tracing()</span><br></pre></td></tr></table></figure>

<h3 id="8-2-部署Jaeger追踪系统"><a href="#8-2-部署Jaeger追踪系统" class="headerlink" title="8.2 部署Jaeger追踪系统"></a>8.2 部署Jaeger追踪系统</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Docker快速部署Jaeger</span></span><br><span class="line">docker run -d \</span><br><span class="line">    --name jaeger \</span><br><span class="line">    -p 5775:5775/udp \</span><br><span class="line">    -p 6831:6831/udp \</span><br><span class="line">    -p 6832:6832/udp \</span><br><span class="line">    -p 5778:5778 \</span><br><span class="line">    -p 16686:16686 \</span><br><span class="line">    -p 14268:14268 \</span><br><span class="line">    -p 14250:14250 \</span><br><span class="line">    -p 9411:9411 \</span><br><span class="line">    jaegertracing/all-in-one:latest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问Jaeger UI: http://localhost:16686</span></span><br></pre></td></tr></table></figure>

<h3 id="8-3-客户端追踪示例"><a href="#8-3-客户端追踪示例" class="headerlink" title="8.3 客户端追踪示例"></a>8.3 客户端追踪示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># client_with_tracing.py</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> opentelemetry <span class="keyword">import</span> trace</span><br><span class="line"><span class="keyword">from</span> opentelemetry.trace <span class="keyword">import</span> SpanKind</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取tracer</span></span><br><span class="line">tracer = trace.get_tracer(__name__)</span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    api_key=<span class="string">&quot;your-api-key&quot;</span>,</span><br><span class="line">    base_url=<span class="string">&quot;http://localhost:8000/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">traced_chat_completion</span>(<span class="params">messages</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带追踪的聊天补全&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> tracer.start_as_current_span(</span><br><span class="line">        <span class="string">&quot;chat_completion&quot;</span>,</span><br><span class="line">        kind=SpanKind.CLIENT</span><br><span class="line">    ) <span class="keyword">as</span> span:</span><br><span class="line">        <span class="comment"># 记录输入</span></span><br><span class="line">        span.set_attribute(<span class="string">&quot;model&quot;</span>, <span class="string">&quot;qwen3-80b&quot;</span>)</span><br><span class="line">        span.set_attribute(<span class="string">&quot;num_messages&quot;</span>, <span class="built_in">len</span>(messages))</span><br><span class="line">        span.set_attribute(<span class="string">&quot;input_text&quot;</span>, messages[-<span class="number">1</span>][<span class="string">&quot;content&quot;</span>])</span><br><span class="line">        </span><br><span class="line">        start_time = time.time()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response = client.chat.completions.create(</span><br><span class="line">                model=<span class="string">&quot;qwen3-80b&quot;</span>,</span><br><span class="line">                messages=messages,</span><br><span class="line">                max_tokens=<span class="number">200</span>,</span><br><span class="line">                temperature=<span class="number">0.7</span></span><br><span class="line">            )</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 记录输出</span></span><br><span class="line">            span.set_attribute(<span class="string">&quot;completion_tokens&quot;</span>, response.usage.completion_tokens)</span><br><span class="line">            span.set_attribute(<span class="string">&quot;prompt_tokens&quot;</span>, response.usage.prompt_tokens)</span><br><span class="line">            span.set_attribute(<span class="string">&quot;total_tokens&quot;</span>, response.usage.total_tokens)</span><br><span class="line">            </span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            span.set_attribute(<span class="string">&quot;latency_seconds&quot;</span>, elapsed)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> response</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            span.record_exception(e)</span><br><span class="line">            span.set_status(trace.Status(trace.StatusCode.ERROR, <span class="built_in">str</span>(e)))</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个有用的AI助手&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;请解释什么是深度学习&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = traced_chat_completion(messages)</span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure>

<h3 id="8-4-vLLM内部追踪"><a href="#8-4-vLLM内部追踪" class="headerlink" title="8.4 vLLM内部追踪"></a>8.4 vLLM内部追踪</h3><p>启用vLLM详细日志：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动时设置环境变量</span></span><br><span class="line"><span class="built_in">export</span> VLLM_LOGGING_LEVEL=DEBUG</span><br><span class="line"><span class="built_in">export</span> VLLM_TRACE_FUNCTION=1</span><br><span class="line"></span><br><span class="line">vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --disable-log-requests <span class="literal">false</span></span><br></pre></td></tr></table></figure>


<h2 id="九、生产环境最佳实践"><a href="#九、生产环境最佳实践" class="headerlink" title="九、生产环境最佳实践"></a>九、生产环境最佳实践</h2><h3 id="9-1-安全加固"><a href="#9-1-安全加固" class="headerlink" title="9.1 安全加固"></a>9.1 安全加固</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 使用强API密钥</span></span><br><span class="line">openssl rand -hex 32 &gt; /etc/vllm/api_key.txt</span><br><span class="line"><span class="built_in">chmod</span> 600 /etc/vllm/api_key.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 配置防火墙</span></span><br><span class="line">sudo ufw allow from 10.0.0.0/8 to any port 8000</span><br><span class="line">sudo ufw deny 8000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 启用HTTPS</span></span><br><span class="line"><span class="comment"># 使用Nginx反向代理配置SSL</span></span><br></pre></td></tr></table></figure>

<h3 id="9-2-高可用部署"><a href="#9-2-高可用部署" class="headerlink" title="9.2 高可用部署"></a>9.2 高可用部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用HAProxy进行负载均衡</span></span><br><span class="line"><span class="built_in">cat</span> &gt; /etc/haproxy/haproxy.cfg &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">global</span><br><span class="line">    maxconn 4096</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode http</span><br><span class="line">    <span class="built_in">timeout</span> connect 10s</span><br><span class="line">    <span class="built_in">timeout</span> client 300s</span><br><span class="line">    <span class="built_in">timeout</span> server 300s</span><br><span class="line"></span><br><span class="line">frontend vllm_frontend</span><br><span class="line">    <span class="built_in">bind</span> *:80</span><br><span class="line">    default_backend vllm_backend</span><br><span class="line"></span><br><span class="line">backend vllm_backend</span><br><span class="line">    balance roundrobin</span><br><span class="line">    option httpchk GET /health</span><br><span class="line">    server vllm1 10.0.0.1:8000 check</span><br><span class="line">    server vllm2 10.0.0.2:8000 check</span><br><span class="line">    server vllm3 10.0.0.3:8000 check</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo systemctl restart haproxy</span><br></pre></td></tr></table></figure>
<p>也可使用<a href="/ai/litellm-proxy-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%EF%BC%9Adocker%E9%83%A8%E7%BD%B2%E3%80%81vllm%E4%BB%A3%E7%90%86/">litellm-proxy</a>来实现负载均衡</p>
<h3 id="9-3-故障排查"><a href="#9-3-故障排查" class="headerlink" title="9.3 故障排查"></a>9.3 故障排查</h3><h4 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h4><table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody><tr>
<td>CUDA OOM</td>
<td>显存不足</td>
<td>降低<code>--gpu-memory-utilization</code>或增加GPU数量</td>
</tr>
<tr>
<td>请求超时</td>
<td>并发过高</td>
<td>调整<code>--max-num-seqs</code>和<code>--max-num-batched-tokens</code></td>
</tr>
<tr>
<td>启动失败</td>
<td>模型文件损坏</td>
<td>重新下载模型文件</td>
</tr>
<tr>
<td>推理慢</td>
<td>配置不当</td>
<td>启用<code>--enable-prefix-caching</code></td>
</tr>
</tbody></table>
<h4 id="诊断脚本"><a href="#诊断脚本" class="headerlink" title="诊断脚本"></a>诊断脚本</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># diagnose.sh</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;=== vLLM诊断脚本 ===&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查GPU</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;1. 检查GPU状态&quot;</span></span><br><span class="line">nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查服务状态</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;2. 检查vLLM服务&quot;</span></span><br><span class="line">curl -s http://localhost:8000/health || <span class="built_in">echo</span> <span class="string">&quot;服务未响应&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查模型文件</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;3. 检查模型文件&quot;</span></span><br><span class="line"><span class="built_in">ls</span> -lh /data/models/Qwen3-Next-80B-A3B-Instruct/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查日志</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;4. 最近的错误日志&quot;</span></span><br><span class="line"><span class="built_in">tail</span> -100 /var/log/vllm-qwen.log | grep -i error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查端口占用</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;5. 检查端口&quot;</span></span><br><span class="line">netstat -tlnp | grep 8000</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;=== 诊断完成 ===&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="十、总结"><a href="#十、总结" class="headerlink" title="十、总结"></a>十、总结</h2><p>通过本文，我们完整介绍了使用vLLM部署Qwen3-Next-80B-A3B-Instruct（MoE架构）大模型的全流程，包括：</p>
<h3 id="核心要点"><a href="#核心要点" class="headerlink" title="核心要点"></a>核心要点</h3><ol>
<li><p><strong>模型理解</strong></p>
<ul>
<li>掌握MoE架构特点：80B总参数，3B激活参数</li>
<li>理解MoE模型的显存和计算优势</li>
<li>认识MoE模型适用场景和限制</li>
</ul>
</li>
<li><p><strong>模型准备</strong></p>
<ul>
<li>从HuggingFace或ModelScope查找和下载模型</li>
<li>验证模型文件完整性</li>
<li>合理规划存储空间（MoE模型约160GB）</li>
</ul>
</li>
<li><p><strong>资源规划</strong></p>
<ul>
<li>理解MoE模型的特殊显存需求（权重显存高，计算显存低）</li>
<li>选择合适的GPU配置（2卡A100-80GB即可运行）</li>
<li>使用显存预估工具辅助决策</li>
</ul>
</li>
<li><p><strong>部署配置</strong></p>
<ul>
<li>多卡张量并行部署提升性能</li>
<li>合理设置关键参数优化资源利用</li>
<li>使用Systemd或Docker管理服务</li>
</ul>
</li>
<li><p><strong>监控运维</strong></p>
<ul>
<li>配置Prometheus+Grafana监控系统</li>
<li>实时追踪GPU和模型性能指标</li>
<li>设置合理的告警规则</li>
</ul>
</li>
<li><p><strong>性能优化</strong></p>
<ul>
<li>通过压测找到最优配置</li>
<li>启用前缀缓存和分块预填充</li>
<li>分析trace数据优化推理链路</li>
</ul>
</li>
<li><p><strong>生产实践</strong></p>
<ul>
<li>实施安全加固措施</li>
<li>部署高可用架构</li>
<li>建立完善的故障排查流程</li>
</ul>
</li>
</ol>
<h3 id="MoE模型特殊优势"><a href="#MoE模型特殊优势" class="headerlink" title="MoE模型特殊优势"></a>MoE模型特殊优势</h3><p>Qwen3-Next-80B-A3B-Instruct作为MoE模型，相比传统Dense模型具有显著优势：</p>
<p>✅ <strong>成本优势</strong></p>
<ul>
<li>仅需2卡A100-80GB即可运行（Dense 80B通常需要4-8卡）</li>
<li>推理速度快20-25倍，相同时间内可处理更多请求</li>
<li>功耗更低，运营成本降低60%以上</li>
</ul>
<p>✅ <strong>性能优势</strong></p>
<ul>
<li>相同显存下可支持更大batch size（提升吞吐量）</li>
<li>长上下文处理更高效（KV缓存占用更少）</li>
<li>冷启动快，首token延迟低</li>
</ul>
<p>✅ <strong>灵活性优势</strong></p>
<ul>
<li>不同专家可处理不同类型任务</li>
<li>便于针对特定领域fine-tune单个专家</li>
<li>支持专家级别的A&#x2F;B测试</li>
</ul>
<h3 id="后续优化方向"><a href="#后续优化方向" class="headerlink" title="后续优化方向"></a>后续优化方向</h3><ul>
<li><strong>模型量化</strong>：使用FP8量化进一步降低显存（可在单卡A100-80GB运行）</li>
<li><strong>专家调优</strong>：针对特定任务fine-tune相关专家网络</li>
<li><strong>专家路由优化</strong>：研究专家选择策略，提升特定任务性能</li>
<li><strong>混合部署</strong>：结合专家卸载技术，在更低成本硬件上运行</li>
<li><strong>推理优化</strong>：探索Flash Attention、分块预填充等加速技术</li>
<li><strong>成本优化</strong>：使用Spot实例或混合云降低成本</li>
<li><strong>功能扩展</strong>：集成RAG、Function Calling等高级功能</li>
</ul>
<p>本指南为您提供了完整的MoE大模型部署和运维知识，通过实践这些步骤，您将能够快速搭建一个高性能、低成本的生产级别大语言模型推理服务。MoE架构的优势使得80B级模型的部署和运营变得更加可行和经济。</p>
<h2 id="相关资源"><a href="#相关资源" class="headerlink" title="相关资源"></a>相关资源</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.vllm.ai/">vLLM官方文档</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/Qwen">Qwen模型主页</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm">vLLM GitHub</a></li>
<li><a target="_blank" rel="noopener" href="https://prometheus.io/docs/">Prometheus文档</a></li>
<li><a target="_blank" rel="noopener" href="https://grafana.com/docs/">Grafana文档</a></li>
<li><a target="_blank" rel="noopener" href="https://opentelemetry.io/docs/">OpenTelemetry文档</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/dcgm">NVIDIA DCGM</a></li>
<li><a target="_blank" rel="noopener" href="https://locust.io/">Locust压测工具</a></li>
</ul>
<blockquote>
<p>本文由 AI 辅助生成，如有错误或建议,欢迎指出。</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://xhua.eu.org">Michael Pan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://xhua.eu.org/posts/2c6cec23da85.html">https://xhua.eu.org/posts/2c6cec23da85.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://xhua.eu.org" target="_blank">Michael Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/vllm/">vLLM</a><a class="post-meta__tags" href="/tags/qwen/">Qwen</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/">大模型部署</a><a class="post-meta__tags" href="/tags/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/">模型推理</a><a class="post-meta__tags" href="/tags/gpu/">GPU</a><a class="post-meta__tags" href="/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/">性能优化</a><a class="post-meta__tags" href="/tags/%E6%A8%A1%E5%9E%8B%E7%9B%91%E6%8E%A7/">模型监控</a></div><div class="post-share"><div class="social-share" data-image="https://img.xhua.eu.org/daaca0b411fbe28fb31f728aff34d87083919fff6300004fea34ff5fce7ad91b.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="/pluginsSrc/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="/pluginsSrc/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/b165687e5798.html" title="大语言模型各类版本详解：Base、Instruct、MoE、量化、Thinking 等到底是什么意思？"><img class="cover" src="https://img.xhua.eu.org/176e234d6c59bc9e31fdec2fc747174523a9b90c4328abc4f9a4279a1e540a7f.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">大语言模型各类版本详解：Base、Instruct、MoE、量化、Thinking 等到底是什么意思？</div></div><div class="info-2"><div class="info-item-1">一、为什么要搞懂大模型的各种「版本」？近年来，各种大模型名字后面越来越“花”：  Base &#x2F; Instruct &#x2F; Chat MoE（Mixture of Experts） AWQ &#x2F; GPTQ &#x2F; INT4 &#x2F; FP8 量化 Thinking &#x2F; DeepThink &#x2F; Step &#x2F; Reasoning  如果不了解这些后缀的含义，我们就很难：  正确选择模型：是用 Base 还是 Instruct？是要 MoE 还是稠密模型？ 合理评估效果：为什么同一家模型，Instruct 版本比 Base 用起来舒服很多？ 看懂论文与技术文档：里面充满了 dense、MoE、SFT、RLHF、quantization 等术语。  这篇文章的目标是：  用通俗语言 + 对比表格，解释常见大模型版本名背后的含义、原理与适用场景 帮助你在选型、部署与使用大模型时，做到：心中有数，不再迷茫   二、从「Base 模型」到「Instruct 模型」2.1 Base 模型：会“说话”，但不一定听得懂你**Base 模型...</div></div></div></a><a class="pagination-related" href="/posts/b0a1603977e7.html" title="生产级大语言模型平台系统设计：多期落地方案与实践"><img class="cover" src="https://img.xhua.eu.org/0c48a51774caee38ab8195ab16d9895325b3056f41cb0b06ee3bff5c009bc2d4.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">生产级大语言模型平台系统设计：多期落地方案与实践</div></div><div class="info-2"><div class="info-item-1">背景与目标随着大语言模型在企业内的应用场景不断扩展，单一模型服务或简单的 API + 网关 架构已经难以满足生产环境下的多租户管理、资源隔离、安全合规、可观测性以及快速迭代等要求。企业需要一套生产级别的大语言模型平台系统，以平台化的方式统一承载模型推理、Agent 编排、MCP 工具生态及 RAG 检索能力。 本文面向有一定 DevOps&#x2F;平台工程基础的读者，设计一套可生产落地的大语言模型平台，从整体架构到关键模块拆解，涵盖：  模型部署与运行时管理 多集群 &#x2F; 多云资源管理与调度 监控、日志、链路追踪与容量管理 安全与访问控制 RAG 平台 Agent 平台 MCP（Model Context Protocol）生态集成 平台运维与发布管理  并按照优先级划分为多期落地路线，便于企业按阶段实施。  本文更偏向平台架构设计与关键实现要点，不绑定某个具体云厂商，可结合 Kubernetes、Service Mesh、向量数据库等基础设施实施。  多期落地规划概览为了降低一次性建设的复杂度，建议将大模型平台拆分为多期，逐步演进：  一期（核心推理与基础运维能力，必...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/48e8b33bb2e1.html" title="LiteLLM Proxy 使用指南：Docker 部署、vLLM 代理"><img class="cover" src="https://img.xhua.eu.org/48dd231882a51333c98c4f74930be9150976bfd432f67e16827749bb8e563df2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-30</div><div class="info-item-2">LiteLLM Proxy 使用指南：Docker 部署、vLLM 代理</div></div><div class="info-2"><div class="info-item-1">背景与目标LiteLLM Proxy 是一个 OpenAI API 兼容的模型网关，支持将来自 OpenAI、Azure OpenAI、Bedrock、Vertex AI 以及本地&#x2F;自建的 OpenAI 兼容推理服务（如 vLLM）统一到一套接口之下，并提供虚拟 API Key、用量与预算、速率限制、缓存、日志&#x2F;指标、路由、负载均衡与回退等能力。本文将演示：  如何用 Docker 快速部署 LiteLLM Proxy（含最小可用与带数据库的完整模式） 如何把 vLLM 暴露的 OpenAI 兼容接口接入到 LiteLLM Proxy 进行统一代理 如何生成虚拟 Key、设置每分钟请求数（RPM）限速 如何查询模型列表等常用“免费”功能  参考与更多细节请见官方文档：  LiteLLM Proxy Docker 快速上手 vLLM Provider 文档  你将学到什么 用 Docker 启动 LiteLLM Proxy，并验证 /chat/completions 将本地 vLLM（OpenAI 兼容接口）纳入代理，统一用 OpenAI 协议调用 配置同名模型...</div></div></div></a><a class="pagination-related" href="/posts/c076d9f3f05d.html" title="vLLM高性能大模型推理引擎使用指南"><img class="cover" src="https://img.xhua.eu.org/daaca0b411fbe28fb31f728aff34d87083919fff6300004fea34ff5fce7ad91b.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-05</div><div class="info-item-2">vLLM高性能大模型推理引擎使用指南</div></div><div class="info-2"><div class="info-item-1">在当今AI快速发展的时代，大模型推理性能直接影响着应用的用户体验和成本效益。vLLM作为一个高性能的大模型推理引擎，为开发者提供了快速、高效的模型服务解决方案。本文将详细介绍如何使用vLLM进行离线推理和在线服务部署，特别是如何利用uv工具进行快速环境管理，以及如何部署兼容OpenAI API的模型服务。 什么是vLLMvLLM（Very Large Language Model）是由UC Berkeley开发的高性能大语言模型推理和服务引擎。它具有以下特点：  高吞吐量：通过PagedAttention等技术优化，显著提升推理速度 内存效率：动态内存管理，减少显存占用 易于使用：提供简洁的Python API和OpenAI兼容接口 灵活部署：支持批量推理和在线服务两种模式  环境准备与安装系统要求 操作系统：Linux Python版本：3.9 - 3.12 硬件：NVIDIA GPU（推荐）  使用uv工具快速安装uv是一个超快的Python环境管理器，可以显著加速环境创建和包安装过程。 1. 安装uv工具12345# 在Linux/macOS上安装uvcurl -LsSf ...</div></div></div></a><a class="pagination-related" href="/posts/7dcd88b03636.html" title="Mac Apple Silicon LLM 微调实战指南：从原理到多场景应用"><img class="cover" src="https://img.xhua.eu.org/68658d9962fcadac50f2aeada47d66c20bebfa94d25f209bcb103bdba65cb749.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-09</div><div class="info-item-2">Mac Apple Silicon LLM 微调实战指南：从原理到多场景应用</div></div><div class="info-2"><div class="info-item-1">Mac Apple Silicon LLM 微调实战指南：从原理到多场景应用随着 Apple Silicon (M1&#x2F;M2&#x2F;M3&#x2F;M4) 芯片的普及，Mac 已经成为一个强大的 AI 开发工作站。凭借其统一内存架构 (Unified Memory Architecture)，Mac 能够处理比同等配置显卡更大的模型。本文将介绍如何在 Mac 上使用 MLX 框架高效微调大语言模型（如 Qwen、Llama、Mistral 等），并探讨微调在不同业务场景中的应用。 一、 核心概念解析在开始动手之前，我们需要理解几个关键的技术术语。 1. 什么是微调 (Fine-tuning)？微调是在预训练模型（Base Model）的基础上，使用特定领域的数据进行进一步训练。就像是一个已经读完大学的“通才”，通过学习法律卷宗，变成了一位“律师”。 2. SFT (监督微调)SFT (Supervised Fine-Tuning) 是最常用的微调方式。它通过 (Input, Output) 对来教导模型如何响应指令。  编程场景示例:  输入: “帮我写一个 Pyth...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E8%AF%BB%E8%80%85"><span class="toc-text">目标读者</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%A8%A1%E5%9E%8B%E6%9F%A5%E6%89%BE%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="toc-text">一、模型查找与选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Qwen3-Next-80B-A3B-Instruct%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D"><span class="toc-text">1.1 Qwen3-Next-80B-A3B-Instruct模型介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%9C%A8HuggingFace%E6%9F%A5%E6%89%BE%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.2 在HuggingFace查找模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%9C%A8ModelScope%E6%9F%A5%E6%89%BE%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%9B%BD%E5%86%85%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="toc-text">1.3 在ModelScope查找模型（国内推荐）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%E8%AF%B4%E6%98%8E"><span class="toc-text">1.4 模型文件结构说明</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-text">二、关键参数说明</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E5%8F%82%E6%95%B0"><span class="toc-text">2.1 模型加载参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0"><span class="toc-text">基础配置参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%8F%82%E6%95%B0"><span class="toc-text">性能优化参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E5%8F%82%E6%95%B0"><span class="toc-text">量化参数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-API%E6%9C%8D%E5%8A%A1%E5%8F%82%E6%95%B0"><span class="toc-text">2.2 API服务参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%8E%A8%E7%90%86%E7%94%9F%E6%88%90%E5%8F%82%E6%95%B0"><span class="toc-text">2.3 推理生成参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%85%B3%E9%94%AE%E6%80%A7%E8%83%BD%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3-%E2%AD%90"><span class="toc-text">2.4 关键性能参数详解 ⭐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#max-model-len%EF%BC%9A%E6%9C%80%E5%A4%A7%E5%BA%8F%E5%88%97%E9%95%BF%E5%BA%A6"><span class="toc-text">--max-model-len：最大序列长度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#max-num-seqs%EF%BC%9A%E6%9C%80%E5%A4%A7%E5%B9%B6%E5%8F%91%E5%BA%8F%E5%88%97%E6%95%B0"><span class="toc-text">--max-num-seqs：最大并发序列数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#max-num-batched-tokens%EF%BC%9A%E6%89%B9%E6%AC%A1%E6%9C%80%E5%A4%A7token%E6%95%B0"><span class="toc-text">--max-num-batched-tokens：批次最大token数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E4%B8%AA%E5%8F%82%E6%95%B0%E7%9A%84%E5%8D%8F%E5%90%8C%E9%85%8D%E7%BD%AE"><span class="toc-text">三个参数的协同配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E5%B7%A5%E4%BD%9C%E6%B5%81"><span class="toc-text">参数调优工作流</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E9%80%9F%E6%9F%A5%E8%A1%A8"><span class="toc-text">显存占用速查表</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%98%BE%E5%AD%98%E9%A2%84%E4%BC%B0%E4%B8%8E%E8%B5%84%E6%BA%90%E8%A7%84%E5%88%92"><span class="toc-text">三、显存预估与资源规划</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-MoE%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E"><span class="toc-text">3.1 MoE模型架构说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AF%A6%E8%A7%A3"><span class="toc-text">3.2 显存占用详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E6%98%BE%E5%AD%98%E8%AE%A1%E7%AE%97"><span class="toc-text">3.3 模型权重显存计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#FP16-BF16-%E7%B2%BE%E5%BA%A6%EF%BC%88%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="toc-text">FP16&#x2F;BF16 精度（推荐）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FP8-INT8-%E9%87%8F%E5%8C%96"><span class="toc-text">FP8&#x2F;INT8 量化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-GPU%E9%85%8D%E7%BD%AE%E6%96%B9%E6%A1%88%EF%BC%88%E9%92%88%E5%AF%B9MoE%E6%A8%A1%E5%9E%8B%EF%BC%89"><span class="toc-text">3.4 GPU配置方案（针对MoE模型）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%A1%88%E4%B8%80%EF%BC%9AA100-80GB-%C3%97-3%E5%8D%A1%EF%BC%88%E6%9C%80%E5%B0%8F%E9%85%8D%E7%BD%AE%EF%BC%89"><span class="toc-text">方案一：A100-80GB × 3卡（最小配置）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%A1%88%E4%BA%8C%EF%BC%9AA100-80GB-%C3%97-4%E5%8D%A1%EF%BC%88%E7%94%9F%E4%BA%A7%E6%8E%A8%E8%8D%90%EF%BC%89%E2%AD%90"><span class="toc-text">方案二：A100-80GB × 4卡（生产推荐）⭐</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%A1%88%E4%B8%89%EF%BC%9AA100-80GB-%C3%97-8%E5%8D%A1%EF%BC%88%E9%AB%98%E6%80%A7%E8%83%BD%EF%BC%89"><span class="toc-text">方案三：A100-80GB × 8卡（高性能）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%A1%88%E5%9B%9B%EF%BC%9AA100-80GB-%C3%97-2%E5%8D%A1-FP8%E9%87%8F%E5%8C%96%EF%BC%88%E6%80%A7%E4%BB%B7%E6%AF%94%EF%BC%89%E2%AD%90"><span class="toc-text">方案四：A100-80GB × 2卡 + FP8量化（性价比）⭐</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%A1%88%E4%BA%94%EF%BC%9AA100-40GB-%C3%97-4%E5%8D%A1-FP8%E9%87%8F%E5%8C%96%EF%BC%88%E7%BB%8F%E6%B5%8E%E5%9E%8B%EF%BC%89"><span class="toc-text">方案五：A100-40GB × 4卡 + FP8量化（经济型）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%E4%BC%B0%E7%AE%97%E5%B7%A5%E5%85%B7"><span class="toc-text">3.4 显存使用估算工具</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%951%EF%BC%9A%E8%AF%95%E8%BF%90%E8%A1%8C%E7%9B%91%E6%8E%A7%E6%B3%95%EF%BC%88%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="toc-text">方法1：试运行监控法（推荐）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%952%EF%BC%9A%E9%80%9A%E8%BF%87Python%E8%84%9A%E6%9C%AC%E6%B5%8B%E9%87%8F%EF%BC%88%E4%BF%AE%E6%AD%A3%E7%89%88%EF%BC%89"><span class="toc-text">方法2：通过Python脚本测量（修正版）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%953%EF%BC%9A%E6%89%8B%E5%8A%A8%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F%EF%BC%88%E7%B2%BE%E7%A1%AE%E7%89%88%EF%BC%89"><span class="toc-text">方法3：手动计算公式（精确版）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-MoE%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E7%89%B9%E7%82%B9"><span class="toc-text">3.5 MoE模型性能特点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%BE%E5%AD%98%E4%BC%98%E5%8A%BF"><span class="toc-text">显存优势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">适用场景</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E5%9C%A8%E7%BA%BF%E6%98%BE%E5%AD%98%E8%AE%A1%E7%AE%97%E5%99%A8"><span class="toc-text">3.6 在线显存计算器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD"><span class="toc-text">四、模型文件下载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-text">4.1 环境准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%BD%BF%E7%94%A8HuggingFace%E4%B8%8B%E8%BD%BD%EF%BC%88%E5%9B%BD%E5%A4%96%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%89"><span class="toc-text">4.2 使用HuggingFace下载（国外服务器）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%951%EF%BC%9A%E4%BD%BF%E7%94%A8huggingface-cli"><span class="toc-text">方法1：使用huggingface-cli</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%952%EF%BC%9A%E4%BD%BF%E7%94%A8Python%E8%84%9A%E6%9C%AC"><span class="toc-text">方法2：使用Python脚本</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%BD%BF%E7%94%A8ModelScope%E4%B8%8B%E8%BD%BD%EF%BC%88%E5%9B%BD%E5%86%85%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="toc-text">4.3 使用ModelScope下载（国内服务器推荐）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E9%AA%8C%E8%AF%81%E4%B8%8B%E8%BD%BD%E5%AE%8C%E6%95%B4%E6%80%A7"><span class="toc-text">4.4 验证下载完整性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E4%B8%8B%E8%BD%BD%E5%8A%A0%E9%80%9F%E6%8A%80%E5%B7%A7"><span class="toc-text">4.5 下载加速技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%95%9C%E5%83%8F%E7%AB%99%EF%BC%88%E5%9B%BD%E5%86%85%EF%BC%89"><span class="toc-text">使用镜像站（国内）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><span class="toc-text">五、模型部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%8D%95%E5%8D%A1%E9%83%A8%E7%BD%B2%EF%BC%88%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%EF%BC%89"><span class="toc-text">5.1 单卡部署（测试环境）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%A4%9A%E5%8D%A1%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E9%83%A8%E7%BD%B2%EF%BC%88%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="toc-text">5.2 多卡张量并行部署（生产环境推荐）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E5%8D%A1A100%E9%85%8D%E7%BD%AE"><span class="toc-text">4卡A100配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8%E5%8D%A1A100%E9%AB%98%E6%80%A7%E8%83%BD%E9%85%8D%E7%BD%AE"><span class="toc-text">8卡A100高性能配置</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%BD%BF%E7%94%A8%E9%87%8F%E5%8C%96%E5%8A%A0%E9%80%9F%E9%83%A8%E7%BD%B2"><span class="toc-text">5.3 使用量化加速部署</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#FP8%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2"><span class="toc-text">FP8量化部署</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-MoE%E4%B8%93%E5%AE%B6CPU-Offload%E9%85%8D%E7%BD%AE%EF%BC%88%E9%AB%98%E7%BA%A7%EF%BC%89"><span class="toc-text">5.4 MoE专家CPU Offload配置（高级）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%951%EF%BC%9A%E4%BD%BF%E7%94%A8cpu-offload-gb%E5%8F%82%E6%95%B0"><span class="toc-text">方法1：使用cpu-offload-gb参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%952%EF%BC%9A%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E6%8E%A7%E5%88%B6"><span class="toc-text">方法2：环境变量控制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95"><span class="toc-text">实际效果测试</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%E5%92%8C%E9%99%90%E5%88%B6"><span class="toc-text">注意事项和限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%953%EF%BC%9A%E4%BD%BF%E7%94%A8DeepSpeed-MII%EF%BC%88%E6%8E%A8%E8%8D%90%E6%9B%BF%E4%BB%A3%E6%96%B9%E6%A1%88%EF%BC%89"><span class="toc-text">方法3：使用DeepSpeed-MII（推荐替代方案）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%91%E6%8E%A7offload%E6%80%A7%E8%83%BD"><span class="toc-text">监控offload性能</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-%E4%BD%BF%E7%94%A8Systemd%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1"><span class="toc-text">5.5 使用Systemd管理服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6-Docker%E9%83%A8%E7%BD%B2"><span class="toc-text">5.6 Docker部署</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%951%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%AE%98%E6%96%B9%E9%95%9C%E5%83%8F%EF%BC%88%E6%8E%A8%E8%8D%90%EF%BC%89%E2%AD%90"><span class="toc-text">方法1：使用官方镜像（推荐）⭐</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Docker%E9%83%A8%E7%BD%B2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="toc-text">Docker部署最佳实践</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-7-%E9%AA%8C%E8%AF%81%E9%83%A8%E7%BD%B2"><span class="toc-text">5.7 验证部署</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%A8%A1%E5%9E%8B%E7%9B%91%E6%8E%A7"><span class="toc-text">六、模型监控</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-vLLM%E5%86%85%E7%BD%AE%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87"><span class="toc-text">6.1 vLLM内置监控指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E6%8C%87%E6%A0%87%E8%AF%B4%E6%98%8E"><span class="toc-text">关键指标说明</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E9%85%8D%E7%BD%AEPrometheus%E7%9B%91%E6%8E%A7"><span class="toc-text">6.2 配置Prometheus监控</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Prometheus"><span class="toc-text">安装Prometheus</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEPrometheus"><span class="toc-text">配置Prometheus</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8Prometheus"><span class="toc-text">启动Prometheus</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E9%85%8D%E7%BD%AEGrafana%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">6.3 配置Grafana可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Grafana"><span class="toc-text">安装Grafana</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-text">配置数据源</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5vLLM%E7%9B%91%E6%8E%A7%E9%9D%A2%E6%9D%BF"><span class="toc-text">导入vLLM监控面板</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-GPU%E7%9B%91%E6%8E%A7"><span class="toc-text">6.4 GPU监控</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8nvidia-smi%E7%9B%91%E6%8E%A7"><span class="toc-text">使用nvidia-smi监控</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8DCGM%E7%9B%91%E6%8E%A7%EF%BC%88%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="toc-text">使用DCGM监控（推荐）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEPrometheus%E6%8A%93%E5%8F%96GPU%E6%8C%87%E6%A0%87"><span class="toc-text">配置Prometheus抓取GPU指标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7"><span class="toc-text">6.5 日志监控</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#vLLM%E6%97%A5%E5%BF%97%E9%85%8D%E7%BD%AE"><span class="toc-text">vLLM日志配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E9%87%8D%E5%AE%9A%E5%90%91%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-text">日志重定向和持久化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E8%BD%AE%E8%BD%AC%E9%85%8D%E7%BD%AE"><span class="toc-text">日志轮转配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Loki%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97"><span class="toc-text">使用Loki收集日志</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-%E5%91%8A%E8%AD%A6%E9%85%8D%E7%BD%AE"><span class="toc-text">6.6 告警配置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Prometheus%E5%91%8A%E8%AD%A6%E8%A7%84%E5%88%99"><span class="toc-text">Prometheus告警规则</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%A8%A1%E5%9E%8B%E5%8E%8B%E6%B5%8B"><span class="toc-text">七、模型压测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E4%BD%BF%E7%94%A8vLLM%E5%AE%98%E6%96%B9%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7"><span class="toc-text">7.1 使用vLLM官方压测工具</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%8B%E6%B5%8B%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-text">压测参数说明</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E4%BD%BF%E7%94%A8Apache-Bench%E8%BF%9B%E8%A1%8C%E7%AE%80%E5%8D%95%E5%8E%8B%E6%B5%8B"><span class="toc-text">7.2 使用Apache Bench进行简单压测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-%E4%BD%BF%E7%94%A8Locust%E8%BF%9B%E8%A1%8C%E5%A4%8D%E6%9D%82%E5%8E%8B%E6%B5%8B"><span class="toc-text">7.3 使用Locust进行复杂压测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Locust"><span class="toc-text">安装Locust</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%8E%8B%E6%B5%8B%E8%84%9A%E6%9C%AC"><span class="toc-text">创建压测脚本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E8%A1%8CLocust%E5%8E%8B%E6%B5%8B"><span class="toc-text">运行Locust压测</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-%E5%8E%8B%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-text">7.4 压测结果分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87"><span class="toc-text">关键性能指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%8E%8B%E6%B5%8B%E6%8A%A5%E5%91%8A"><span class="toc-text">生成压测报告</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86Trace%E8%BF%BD%E8%B8%AA"><span class="toc-text">八、模型推理Trace追踪</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E5%90%AF%E7%94%A8OpenTelemetry%E8%BF%BD%E8%B8%AA"><span class="toc-text">8.1 启用OpenTelemetry追踪</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"><span class="toc-text">安装依赖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEOpenTelemetry"><span class="toc-text">配置OpenTelemetry</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E9%83%A8%E7%BD%B2Jaeger%E8%BF%BD%E8%B8%AA%E7%B3%BB%E7%BB%9F"><span class="toc-text">8.2 部署Jaeger追踪系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%BD%E8%B8%AA%E7%A4%BA%E4%BE%8B"><span class="toc-text">8.3 客户端追踪示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-vLLM%E5%86%85%E9%83%A8%E8%BF%BD%E8%B8%AA"><span class="toc-text">8.4 vLLM内部追踪</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="toc-text">九、生产环境最佳实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-%E5%AE%89%E5%85%A8%E5%8A%A0%E5%9B%BA"><span class="toc-text">9.1 安全加固</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2"><span class="toc-text">9.2 高可用部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5"><span class="toc-text">9.3 故障排查</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="toc-text">常见问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%8A%E6%96%AD%E8%84%9A%E6%9C%AC"><span class="toc-text">诊断脚本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">十、总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E8%A6%81%E7%82%B9"><span class="toc-text">核心要点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MoE%E6%A8%A1%E5%9E%8B%E7%89%B9%E6%AE%8A%E4%BC%98%E5%8A%BF"><span class="toc-text">MoE模型特殊优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8E%E7%BB%AD%E4%BC%98%E5%8C%96%E6%96%B9%E5%90%91"><span class="toc-text">后续优化方向</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90"><span class="toc-text">相关资源</span></a></li></ol></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2026 By Michael Pan</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>