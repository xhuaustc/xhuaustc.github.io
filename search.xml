<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>OpenClaw：开源AI代理与技能生态系统详解</title>
    <url>/posts/7c7ed6b618d8.html</url>
    <content><![CDATA[引言在 2026 年初，一个名为 OpenClaw 的开源 AI 代理项目在全球范围内引发了广泛关注。这个项目经历了从 Clawdbot 到 Moltbot，再到 OpenClaw 的品牌演变，已经成为个人 AI 助手领域的重要参与者。
OpenClaw 的核心理念是通过 Gateway 网关架构，将各种聊天应用与 AI 智能体连接起来，让用户可以在任何平台上与 AI 助手交互。本文将深入介绍 OpenClaw 的核心功能、Gateway 架构，以及围绕它构建的庞大技能生态系统。
OpenClaw 是什么？OpenClaw 是一个适用于任何操作系统的 AI 智能体 Gateway（网关），它通过单个 Gateway 进程将聊天应用连接到 Pi 等编程智能体。OpenClaw 不仅仅是一个聊天机器人，而是一个真正能够自主执行任务的 AI 代理系统。
Gateway 架构OpenClaw 的核心是 Gateway 网关，它是会话、路由和渠道连接的唯一事实来源：
聊天应用 + 插件 → Gateway → Pi 智能体                        ↓                    CLI / Web 控制界面 / macOS 应用 / 移动节点

Gateway 网关负责：

管理多个聊天渠道的连接
路由消息到不同的智能体
维护会话状态
处理媒体文件
提供统一的管理界面

核心特性1. 本地化部署，保护隐私OpenClaw 运行在用户的本地计算机上（支持 Mac、Windows 和 Linux），这意味着：

所有对话历史、上下文和偏好设置都存储在本地
不依赖云端服务器，数据完全由用户控制
符合隐私保护的最高标准

2. 多渠道 Gateway 网关OpenClaw 通过单个 Gateway 进程支持多种消息平台：

WhatsApp：通过配对流程连接
Telegram：Bot API 集成
Discord：Bot 集成
iMessage：macOS 本地集成
插件渠道：通过扩展包添加 Mattermost 等更多渠道

这种统一的网关架构让用户可以在任何平台上与 AI 助手交互，所有消息都通过同一个 Gateway 处理。
3. 强大的系统访问能力OpenClaw 拥有广泛的系统权限：

文件系统访问：读写文件、管理目录
终端执行：运行命令、执行脚本
浏览器控制：自动化网页浏览、数据提取
代码执行：编写和运行程序

4. 持久化记忆系统OpenClaw 能够：

学习用户的偏好和习惯
记住历史对话上下文
根据过往经验优化响应
跨会话保持连续性

5. 24&#x2F;7 自主运行通过后台任务和 Webhook 机制，OpenClaw 可以：

定时执行任务
监控事件触发
自动处理日常事务
无需用户干预即可完成工作

6. 丰富的工具集成OpenClaw 可以与各种个人工具无缝集成：

日历管理（Google Calendar、Apple Calendar）
任务管理（Todoist、Things、Notion）
邮件处理（Gmail、Outlook）
财务软件
开发工具

OpenClaw 的实际应用场景日常任务自动化
管理邮件收件箱，自动分类和回复
安排日程，处理会议邀请
航班值机提醒
账单支付提醒

开发工作流
代码审查和重构建议
自动化测试执行
文档生成
Git 操作自动化

信息管理
网页内容提取和总结
RSS 订阅管理
知识库构建
笔记整理

智能家居控制
通过集成 Home Assistant 控制智能设备
自动化场景设置
设备状态监控

OpenClaw Skills：可扩展的能力系统OpenClaw 的真正强大之处在于其**技能（Skills）**系统。技能是可以扩展 OpenClaw 能力的插件，用户可以通过安装第三方技能来增强代理的功能。
什么是 Skills？Skills 遵循 Anthropic 开发的 Agent Skill 规范，这是一个开放标准，用于 AI 编码助手的能力扩展。每个 Skill 本质上是一个包含以下内容的包：

SKILL.md：技能的描述、使用说明和触发条件
可执行脚本或工具
配置文件
依赖声明

ClawHub：技能市场ClawHub 是 OpenClaw 的官方技能市场，截至 2026 年 2 月，已经托管了超过 3000+ 社区构建的技能。用户可以通过简单的命令安装技能：
npx clawhub@latest install &lt;skill-slug&gt;

Awesome OpenClaw Skills：精选技能集合awesome-openclaw-skills 是一个社区维护的精选技能列表，从 ClawHub 的 3000+ 技能中筛选出了1715+ 高质量技能，并按类别进行了组织。
筛选标准该列表排除了：

加密货币&#x2F;区块链&#x2F;DeFi 相关技能
垃圾或批量生成的内容
重复的技能
非英文描述的技能
潜在的风险或有害技能

主要技能分类1. Web &amp; 前端开发（46 个技能）
frontend-design：创建生产级前端界面
webapp-testing：使用 Playwright 进行 Web 应用测试
remotion-video-toolkit：使用 React 创建视频
vercel-deploy：部署应用到 Vercel

2. 编码代理 &amp; IDE（55 个技能）
claude-code-skill：MCP（模型上下文协议）集成
cursor-agent：Cursor CLI 代理
mcp-builder：创建 MCP 服务器
skill-creator：创建新技能的指南

3. Git &amp; GitHub（34 个技能）
github-pr：获取、预览、合并和测试 GitHub PR
git-sync：自动同步本地更改到远程仓库
conventional-commits：使用约定式提交规范

4. DevOps &amp; 云服务（144 个技能）
kubernetes：Kubernetes 和 OpenShift 集群管理
docker-diag：Docker 容器高级日志分析
aws-infra：基于聊天的 AWS 基础设施辅助
terraform：基础设施即代码

5. 浏览器 &amp; 自动化（69 个技能）
browser-use：基于云的浏览器自动化
playwright-cli：通过 Playwright CLI 进行浏览器自动化
firecrawl-skills：网页抓取和爬取

6. AI &amp; LLM（159 个技能）
gemini-deep-research：使用 Gemini 进行复杂研究
agent-memory：AI 代理的持久化内存系统
prompt-engineering-expert：提示工程专家
multi-viewpoint-debates：从多个角度辩论决策

7. 搜索 &amp; 研究（148 个技能）
perplexity：通过 Perplexity API 进行 AI 驱动的搜索
exa：通过 Exa AI API 进行神经网络搜索
arxiv-watcher：搜索和总结 ArXiv 论文
youtube-summarizer：获取 YouTube 转录并生成结构化摘要

8. 生产力 &amp; 任务管理（93 个技能）
todoist：管理 Todoist 任务和项目
notion：Notion API 集成
obsidian：Obsidian vault 管理
linear：Linear 问题跟踪

9. 通信（58 个技能）
telegram-bot：构建和管理 Telegram 机器人
discord：通过 Discord 工具控制 Discord
slack：Slack 集成
email-manager-lite：通过任何 IMAP&#x2F;SMTP 提供商发送和读取邮件

10. 健康 &amp; 健身（35 个技能）
garmin-health：Garmin 数据（睡眠、HRV、VO2 max）
oura-ring：Oura Ring 准备度和睡眠评分
strava：Strava 活动分析
workout-logger：记录锻炼、跟踪进度

技能安装示例# 安装前端设计技能npx clawhub@latest install frontend-design# 安装 GitHub PR 管理技能npx clawhub@latest install github-pr# 安装 Kubernetes 管理技能npx clawhub@latest install kubernetes

手动安装你也可以手动将技能文件夹复制到以下位置之一：

全局：~/.openclaw/skills/
工作区：&lt;project&gt;/skills/

优先级：工作区 &gt; 本地 &gt; 内置
安全考虑虽然 OpenClaw 和其技能生态系统非常强大，但也存在一些安全隐患需要注意：
1. 配置暴露风险网络安全研究人员发现，某些配置可能会在网络上暴露私人消息、凭证和 API 密钥。因此，在配置 OpenClaw 时需要特别注意：

不要将敏感配置文件提交到公共仓库
使用环境变量管理 API 密钥
定期审查访问权限

2. 恶意技能风险2026 年 2 月，安全研究人员在 ClawHub 上发现了341 个恶意技能，这些技能通过社会工程学攻击窃取用户数据。攻击方式包括：

伪装成合法技能（如 “solana-wallet-tracker”）
在文档中包含虚假的前置条件
诱导用户下载安装脚本
部署木马程序（如 Atomic Stealer）

安全最佳实践
审查技能代码：在安装前查看技能的源代码
使用可信来源：优先选择官方或社区验证的技能
最小权限原则：只授予必要的权限
定期更新：保持 OpenClaw 和技能的最新版本
监控活动：定期检查系统日志和网络活动
使用安全扫描工具：如 skillguard、skillvet 等技能可以扫描其他技能的安全性

OpenClaw 的技术架构核心组件架构图flowchart LR  A[&quot;聊天应用 + 插件&quot;] --&gt; B[&quot;Gateway 网关&quot;]  B --&gt; C[&quot;Pi 智能体&quot;]  B --&gt; D[&quot;CLI&quot;]  B --&gt; E[&quot;Web 控制界面&quot;]  B --&gt; F[&quot;macOS 应用&quot;]  B --&gt; G[&quot;iOS 和 Android 节点&quot;]

Gateway 网关核心功能Gateway 是 OpenClaw 的中枢，提供以下核心能力：
1. 多智能体路由
按智能体隔离会话
按工作区分离上下文
按发送者创建独立会话

2. 媒体支持
发送和接收图片
音频文件处理
文档传输

3. Web 控制界面
浏览器仪表板管理
聊天历史查看
配置管理
会话和节点管理

4. 移动节点
iOS 和 Android 节点配对
Canvas 功能支持
跨设备同步

配置系统OpenClaw 的配置文件位于 ~/.openclaw/openclaw.json，支持：

渠道白名单配置
群组提及规则
智能体路由规则
安全令牌管理

示例配置：
&#123;  &quot;channels&quot;: &#123;    &quot;whatsapp&quot;: &#123;      &quot;allowFrom&quot;: [&quot;+15555550123&quot;],      &quot;groups&quot;: &#123; &quot;*&quot;: &#123; &quot;requireMention&quot;: true &#125; &#125;    &#125;  &#125;,  &quot;messages&quot;: &#123;    &quot;groupChat&quot;: &#123;      &quot;mentionPatterns&quot;: [&quot;@openclaw&quot;]    &#125;  &#125;&#125;

与其他 AI 代理的比较


特性
OpenClaw
AutoGPT
LangChain Agents



开源
✅
✅
✅


本地运行
✅
✅
✅


技能市场
✅ (ClawHub)
❌
部分


多平台消息
✅
❌
❌


持久化记忆
✅
部分
部分


24&#x2F;7 运行
✅
部分
❌


浏览器控制
✅
部分
部分


社区与生态系统Moltbook：AI 代理的社交网络OpenClaw 生态系统还包括 Moltbook，一个专为 AI 代理设计的社交网络平台，代理可以在这里：

发布更新
与其他代理互动
分享经验和知识
参与社区讨论

开发者资源
官方文档（中文）：docs.openclaw.ai&#x2F;zh-CN
官方文档（英文）：docs.openclaw.ai
GitHub 仓库：github.com&#x2F;openclaw
Discord 社区：活跃的开发者和用户社区
技能开发指南：详细的技能创建教程

未来展望OpenClaw 项目正在快速发展，未来可能的方向包括：

更强的安全机制：改进技能审核流程，防止恶意代码
更好的多代理协作：允许多个 OpenClaw 实例协同工作
企业级功能：团队协作、权限管理、审计日志
更丰富的集成：支持更多第三方服务和平台
AI 模型优化：针对特定任务优化模型性能
跨平台移动端：iOS 和 Android 原生应用

如何开始使用 OpenClawOpenClaw 提供多种安装方式，适合不同的使用场景。
方式一：标准安装（推荐个人使用）第 1 步：安装 OpenClawnpm install -g openclaw@latest

第 2 步：新手引导并安装服务openclaw onboard --install-daemon

这个命令会启动交互式向导，引导你完成初始配置。
第 3 步：配对渠道并启动 Gateway# 配对 WhatsApp（或其他渠道）openclaw channels login# 启动 Gateway 网关openclaw gateway --port 18789

方式二：Docker 安装（推荐容器化部署）Docker 安装适合需要隔离环境或在 VPS 上运行的场景。
适用场景
✅ 需要隔离的、可丢弃的 Gateway 环境
✅ 在 VPS 或云服务器上运行
✅ 需要容器化部署
❌ 本地开发（标准安装更快）

快速开始（推荐）从 OpenClaw 仓库根目录运行：
./docker-setup.sh

这个脚本会自动完成：

构建 Gateway 镜像
运行新手引导向导
配置提供商设置
通过 Docker Compose 启动 Gateway
生成并保存 Gateway 令牌

手动安装流程# 1. 构建镜像docker build -t openclaw:local -f Dockerfile .# 2. 运行新手引导docker compose run --rm openclaw-cli onboard# 3. 启动 Gatewaydocker compose up -d openclaw-gateway

配置渠道# WhatsApp（QR 码）docker compose run --rm openclaw-cli channels login# Telegram（Bot Token）docker compose run --rm openclaw-cli channels add --channel telegram --token &quot;&lt;token&gt;&quot;# Discord（Bot Token）docker compose run --rm openclaw-cli channels add --channel discord --token &quot;&lt;token&gt;&quot;

访问控制界面# 获取带令牌的仪表板 URLdocker compose run --rm openclaw-cli dashboard --no-open

高级配置额外挂载目录：
export OPENCLAW_EXTRA_MOUNTS=&quot;$HOME/.codex:/home/node/.codex:ro,$HOME/github:/home/node/github:rw&quot;./docker-setup.sh

持久化容器 home 目录：
export OPENCLAW_HOME_VOLUME=&quot;openclaw_home&quot;./docker-setup.sh

安装系统包：
export OPENCLAW_DOCKER_APT_PACKAGES=&quot;ffmpeg build-essential&quot;./docker-setup.sh

智能体沙箱（Docker 隔离）OpenClaw 支持在 Docker 容器中运行智能体工具，实现安全隔离：
&#123;  &quot;agents&quot;: &#123;    &quot;defaults&quot;: &#123;      &quot;sandbox&quot;: &#123;        &quot;mode&quot;: &quot;non-main&quot;,        &quot;scope&quot;: &quot;agent&quot;,        &quot;workspaceAccess&quot;: &quot;none&quot;,        &quot;docker&quot;: &#123;          &quot;image&quot;: &quot;openclaw-sandbox:bookworm-slim&quot;,          &quot;network&quot;: &quot;none&quot;,          &quot;readOnlyRoot&quot;: true,          &quot;memory&quot;: &quot;1g&quot;,          &quot;cpus&quot;: 1        &#125;      &#125;    &#125;  &#125;&#125;

构建沙箱镜像：
scripts/sandbox-setup.sh

方式三：Ansible 自动化安装（推荐生产环境）Ansible 安装是生产服务器的推荐方式，提供安全优先的自动化部署。
适用场景
✅ 生产环境部署
✅ 需要安全加固
✅ VPS&#x2F;云服务器
✅ 团队协作环境

一键安装curl -fsSL https://raw.githubusercontent.com/openclaw/openclaw-ansible/main/install.sh | bash

安装内容Ansible playbook 会自动安装和配置：

Tailscale：Mesh VPN，安全远程访问
UFW 防火墙：仅允许 SSH + Tailscale 端口
Docker CE + Compose V2：用于智能体沙箱
Node.js 22.x + pnpm：运行时依赖
OpenClaw：基于主机运行（非容器化）
Systemd 服务：带安全加固的自动启动

4 层安全架构┌─────────────────────────────────────┐│ 1. 防火墙（UFW）                     ││    仅开放 SSH(22) + Tailscale       │├─────────────────────────────────────┤│ 2. VPN（Tailscale）                  ││    Gateway 仅通过 VPN 访问          │├─────────────────────────────────────┤│ 3. Docker 隔离                       ││    智能体工具在容器中执行            │├─────────────────────────────────────┤│ 4. Systemd 加固                      ││    NoNewPrivileges + 非特权用户     │└─────────────────────────────────────┘

安装后设置# 切换到 openclaw 用户sudo -i -u openclaw# 运行新手引导openclaw onboard# 配置渠道openclaw channels login# 检查服务状态sudo systemctl status openclaw# 查看日志sudo journalctl -u openclaw -f

系统要求
操作系统：Debian 11+ 或 Ubuntu 20.04+
权限：Root 或 sudo 权限
网络：互联网连接
Ansible：2.14+（自动安装）

验证安全性# 测试外部攻击面（应该只显示端口 22）nmap -p- YOUR_SERVER_IP

手动安装# 1. 安装先决条件sudo apt update &amp;&amp; sudo apt install -y ansible git# 2. 克隆仓库git clone https://github.com/openclaw/openclaw-ansible.gitcd openclaw-ansible# 3. 安装 Ansible collectionsansible-galaxy collection install -r requirements.yml# 4. 运行 playbook./run-playbook.sh

安装方式对比


特性
标准安装
Docker 安装
Ansible 安装



适用场景
个人开发
容器化部署
生产环境


安装难度
⭐
⭐⭐
⭐⭐⭐


安全性
中
高
最高


隔离性
无
容器隔离
多层隔离


自动更新
手动
手动
Systemd 管理


防火墙
需手动配置
需手动配置
自动配置


VPN 支持
需手动配置
需手动配置
自动集成 Tailscale


推荐用途
本地测试
VPS 部署
企业生产


访问控制界面Gateway 启动后，打开浏览器访问控制界面：

本地访问：http://127.0.0.1:18789/
远程访问：参考官方文档配置 SSH 或 Tailscale


配置访问控制（可选）编辑 ~/.openclaw/openclaw.json 配置文件：
&#123;  &quot;channels&quot;: &#123;    &quot;whatsapp&quot;: &#123;      &quot;allowFrom&quot;: [&quot;+15555550123&quot;],      &quot;groups&quot;: &#123; &quot;*&quot;: &#123; &quot;requireMention&quot;: true &#125; &#125;    &#125;  &#125;,  &quot;messages&quot;: &#123;    &quot;groupChat&quot;: &#123;      &quot;mentionPatterns&quot;: [&quot;@openclaw&quot;]    &#125;  &#125;&#125;

安装技能扩展# 安装技能管理工具npx clawhub@latest install skill-creator# 浏览可用技能npx clawhub@latest search# 安装常用技能npx clawhub@latest install github-prnpx clawhub@latest install todoistnpx clawhub@latest install email-manager-lite

开始使用在配置的消息平台上发送消息：
帮我检查今天的日程安排

总结我收件箱中未读的重要邮件

创建一个新的 GitHub PR 用于修复 bug #123

总结OpenClaw 代表了个人 AI 助手的新方向——一个真正能够自主执行任务、保护用户隐私、可无限扩展的开源解决方案。通过其丰富的技能生态系统（特别是 awesome-openclaw-skills 精选列表），用户可以轻松找到并安装满足特定需求的功能扩展。
然而，在享受 OpenClaw 带来的便利时，我们也必须保持警惕，注意安全风险，特别是在安装第三方技能时。遵循最佳实践，审查代码，使用可信来源，是确保安全使用 OpenClaw 的关键。
随着 AI 代理技术的不断发展，OpenClaw 及其社区正在塑造一个更加智能、自动化和个性化的数字未来。无论你是开发者、运维工程师，还是普通用户，OpenClaw 都值得你去探索和尝试。
参考资源
OpenClaw 官网：openclaw.ai
官方文档（中文）：docs.openclaw.ai&#x2F;zh-CN
ClawHub 技能市场：clawhub.com
Awesome OpenClaw Skills：github.com&#x2F;VoltAgent&#x2F;awesome-openclaw-skills
社区 Discord：加入官方 Discord 服务器获取支持
完整文档索引：docs.openclaw.ai&#x2F;llms.txt



本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>OpenClaw</tag>
        <tag>AI Agent</tag>
        <tag>自动化</tag>
        <tag>开源</tag>
      </tags>
  </entry>
  <entry>
    <title>Claude Code 使用最佳实践与技巧</title>
    <url>/posts/bc61d6a92d4e.html</url>
    <content><![CDATA[在 AI 辅助编程工具快速发展的今天，开发者面临一个共同的挑战：如何从”能用”到”用好”？Claude Code 作为 Anthropic 推出的终端优先的自主 AI 编程代理，凭借其强大的推理能力和 200k+ token 的上下文窗口，已经成为处理复杂代码任务的利器。但工具再强大，也需要正确的使用方式才能发挥最大价值。
本文将系统性地介绍 Claude Code 的最佳实践与高级技巧，帮助你从基础使用提升到专家级别，真正释放 AI 辅助编程的生产力。
Claude Code 核心理解什么是 Claude CodeClaude Code 是 Anthropic 开发的自主编程代理（Agentic Coding Tool），而非传统的代码补全工具。它的核心特点包括：

终端优先：通过 claude 命令在终端中交互，也支持 Web、桌面应用、IDE 插件等多种形式
自主执行：能够独立规划任务、执行多文件操作、运行命令、验证结果
大上下文：支持 200k+ token 的上下文窗口，适合处理大型代码库
多模态能力：不仅限于编程，还能处理文档编写、构建运行、文件搜索、主题研究等任何可从命令行执行的任务

Claude Code vs Cursor：选择指南理解两者的差异有助于选择合适的工具：



维度
Claude Code
Cursor



核心定位
终端自主代理
AI 增强 IDE


工作方式
探索式、并行执行
聚焦式、精准控制


最佳场景
复杂重构、架构决策、多文件任务
快速原型、MVP 开发、实时补全


上下文窗口
200k+ tokens
相对较小


学习曲线
需要理解 Agent 工作方式
熟悉 VS Code 即可上手


定价
Pro 订阅 $17-20&#x2F;月，无免费版
有免费 Hobby 计划


选择建议：

需要处理大型代码库的复杂重构 → Claude Code
快速迭代和原型开发 → Cursor
深度代码审查和理解 → Cursor
并行处理多个独立任务 → Claude Code

两者并非互斥，很多团队同时使用两者，根据任务特点灵活选择。
环境配置与初始化快速安装# macOS - Homebrewbrew install anthropics/claude/claude# macOS/Linux - 原生安装curl -fsSL https://code.claude.com/install.sh | sh# Windows - WinGetwinget install Anthropic.Claude

安装后运行 claude 命令即可开始使用，首次运行需要登录 Claude 账号（需要 Pro、Max、Teams 或 Enterprise 订阅）。
创建 CLAUDE.md：项目上下文的关键CLAUDE.md 是 Claude Code 最重要的配置文件，它为 Claude 提供持久化的项目上下文。这个文件应该放在项目根目录或父目录（适用于 monorepo）。
CLAUDE.md 应该包含什么# CLAUDE.mdThis file provides guidance to Claude Code when working with code in this repository.## Project Overview[项目简介、技术栈、架构概述]## Common Commands```bash# Developmentnpm run dev          # Start dev servernpm run test         # Run testsnpm run build        # Production build

Code Style Guidelines
Use TypeScript strict mode
Follow ESLint rules in .eslintrc.js
Prefer functional components with hooks
Write tests for all business logic

Testing Instructions
Unit tests: npm run test:unit
E2E tests: npm run test:e2e
Coverage threshold: 80%

Repository Etiquette
Create feature branches from main
Squash commits before merging
Update CHANGELOG.md for user-facing changes

Developer Environment Setup
Node.js 18+
PostgreSQL 14+
Redis for caching

#### CLAUDE.md 最佳实践1. **保持更新**：随着项目演进及时更新 CLAUDE.md2. **团队共识**：CLAUDE.md 应该反映团队的开发规范，纳入代码审查流程3. **分层配置**：Monorepo 可以在根目录放通用配置，子项目放特定配置4. **避免过度详细**：聚焦高频操作和容易出错的地方，不要写成完整文档### 环境优化配置#### 跳过权限提示（谨慎使用）```bash# 开发环境快速迭代时可以跳过权限确认claude --dangerously-skip-permissions# 或设置别名alias cc=&#x27;claude --dangerously-skip-permissions&#x27;

⚠️ 警告：此选项会让 Claude 直接执行文件修改、命令运行等操作，仅在信任的项目中使用。
企业代理配置# 设置 HTTP 代理export HTTP_PROXY=http://proxy.company.com:8080export HTTPS_PROXY=http://proxy.company.com:8080# 或在 Claude 配置中设置claude config set proxy http://proxy.company.com:8080

核心工作流程与最佳实践黄金法则：探索 → 规划 → 编码不要直接让 Claude 开始编码，这是最常见的误区。正确的流程是：
1. 探索阶段（Exploration）# 让 Claude 先理解代码库claude &quot;分析这个项目的认证流程实现&quot;# 探索特定功能claude &quot;找出所有使用 Redis 缓存的地方&quot;# 理解依赖关系claude &quot;这个 UserService 被哪些模块依赖？&quot;

2. 规划阶段（Planning）# 让 Claude 制定计划claude &quot;我需要添加用户登录限流功能，请先制定一个实现计划&quot;# 审查计划并调整# Claude 会列出步骤、涉及的文件、潜在风险# 此时你可以提供反馈：&quot;不要修改 auth.ts，在 middleware 层实现&quot;

为什么规划重要？

让你有机会在代码修改前调整方向
Claude 会考虑更全面的影响范围
避免返工和错误修改

3. 编码阶段（Coding）# 执行计划claude &quot;按照刚才的计划实现登录限流功能&quot;# 增量验证claude &quot;实现第一步后，写个测试验证是否工作&quot;

使用 Plan Mode 进行安全分析Plan Mode 是一个只读模式，适合在不修改代码的情况下进行深度分析：
# 进入 Plan Modeclaude plan &quot;分析这个重构方案的影响范围&quot;# Plan Mode 适用场景- 复杂重构前的影响分析- 代码审查和安全检查- 架构决策评估- 性能瓶颈诊断

Plan Mode 会给出详细的分析报告，但不会修改任何文件，让你可以安全地探索各种可能性。
频繁使用 /clear 保持会话清晰长对话会让 AI 代理变得不可预测，这是所有 LLM 的共性问题。最佳实践：
# 完成一个任务后立即清空/clear# 切换到新任务前清空/clear# 感觉 Claude 理解偏离时清空/clear

何时应该 /clear？

✅ 完成一个独立功能
✅ 开始新的不相关任务
✅ Claude 的回复开始偏离预期
✅ 上下文已经很长（&gt;10 轮对话）

何时不应该 /clear？

❌ 正在进行多步骤的复杂任务中途
❌ 需要 Claude 记住之前的决策和上下文
❌ 正在调试一个问题的过程中

提供丰富的上下文好的提示词 &#x3D; 清晰的意图 + 丰富的上下文
差的提示词示例# ❌ 太模糊claude &quot;优化这个函数&quot;# ❌ 缺少上下文claude &quot;修复这个 bug&quot;# ❌ 没有验证方式claude &quot;添加缓存功能&quot;

好的提示词示例# ✅ 清晰的意图 + 具体指标claude &quot;优化 src/utils/parser.ts 中的 parseUserData 函数，目标是将处理 10000 条记录的时间从 5 秒降低到 1 秒以内。当前瓶颈是 JSON.parse 调用过于频繁。&quot;# ✅ 包含错误信息和环境claude &quot;修复登录失败的问题。错误信息：Error: Invalid token signature  at jwt.verify (auth.ts:45)环境：Node.js 18, JWT 库版本 9.0.0复现步骤：使用过期 token 调用 /api/user 接口&quot;# ✅ 明确验证方式claude &quot;为 UserService 添加 Redis 缓存。要求：1. 缓存用户基本信息，TTL 5分钟2. 提供缓存失效机制3. 添加单元测试验证缓存命中率4. 实现后运行 npm test 验证所有测试通过&quot;

让 Claude 验证自己的工作不要假设 Claude 的输出是正确的，而是给它验证的方式：
# 要求运行测试claude &quot;实现用户注册功能，完成后运行 npm test 验证&quot;# 要求检查 linterclaude &quot;重构这个组件，完成后运行 eslint 检查是否有错误&quot;# 要求构建验证claude &quot;修改 Webpack 配置，完成后运行 npm run build 确保构建成功&quot;# 要求手动验证claude &quot;添加新的 API 端点，完成后用 curl 测试并展示响应&quot;

Claude 会自动执行这些验证命令，并根据结果调整代码，直到验证通过。
高级技巧与模式使用 Subagents 进行任务委派Subagent 是 Claude Code 的杀手级特性，允许你将复杂任务分解为并行的子任务：
# 主 Claude 会话claude &quot;我需要重构整个认证模块，包括：1. 将 JWT 验证逻辑从 controller 移到 middleware2. 添加 refresh token 机制3. 更新所有相关的单元测试4. 更新 API 文档请创建 subagents 并行处理这些任务&quot;

Claude 会自动：

创建多个 subagent
为每个 subagent 分配明确的任务
监控各个 subagent 的进度
整合所有结果

Subagent 适用场景：

✅ 多个独立模块的修改
✅ 大规模重构
✅ 并行的测试编写
✅ 多文件的格式化和清理

Subagent 不适用场景：

❌ 有强依赖关系的任务（必须按顺序执行）
❌ 需要频繁交互的任务
❌ 简单的单文件修改

使用 Git Worktrees 进行并行开发Git Worktrees 允许你在同一个仓库的不同分支上同时工作，结合 Claude Code 可以实现真正的并行开发：
# 创建 worktree 用于新特性git worktree add ../myproject-feature-auth feature/auth# 在新 worktree 中启动 Claudecd ../myproject-feature-authclaude &quot;实现 OAuth2 登录功能&quot;# 同时在主 worktree 中处理其他任务cd ../myprojectclaude &quot;修复用户头像上传的 bug&quot;

Worktrees 的优势：

无需频繁切换分支
可以同时运行多个 Claude 会话
避免未提交代码的冲突
方便对比不同实现方案

使用 Extended Thinking Mode对于特别复杂的问题，可以启用扩展思考模式，让 Claude 进行更深入的分析：
# 启用扩展思考claude --extended-thinking &quot;分析这个分布式事务的死锁问题，考虑所有可能的竞态条件和解决方案&quot;

Extended Thinking Mode 会：

花费更多时间分析问题
考虑更多边缘情况
提供更详细的推理过程
适合架构级别的复杂决策

何时使用：

复杂的并发问题
性能优化决策
架构重构方案
安全漏洞分析

Checkpointing：时光机功能Checkpointing 允许你保存会话状态并随时回退：
# 创建检查点/checkpoint save &quot;完成用户认证基础功能&quot;# 继续开发...claude &quot;添加多因素认证&quot;# 如果不满意，回退到检查点/checkpoint restore &quot;完成用户认证基础功能&quot;# 查看所有检查点/checkpoint list

Checkpointing 最佳实践：

在每个重要里程碑创建检查点
使用描述性的检查点名称
尝试激进方案前先创建检查点
定期清理不再需要的检查点

Model Context Protocol (MCP) 集成什么是 MCPMCP（Model Context Protocol）是 AI 应用的”USB-C 接口”，提供标准化的方式让 Claude Code 连接外部系统：

数据源：本地文件、数据库、API
工具：搜索引擎、计算器、代码分析工具
工作流：GitHub、Sentry、Jira、Slack

配置 MCP 服务器MCP 服务器通过 JSON 配置文件连接：
// ~/.config/claude/mcp_servers.json&#123;  &quot;github&quot;: &#123;    &quot;type&quot;: &quot;stdio&quot;,    &quot;command&quot;: &quot;npx&quot;,    &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-github&quot;],    &quot;env&quot;: &#123;      &quot;GITHUB_TOKEN&quot;: &quot;your_token_here&quot;    &#125;  &#125;,  &quot;postgres&quot;: &#123;    &quot;type&quot;: &quot;stdio&quot;,    &quot;command&quot;: &quot;npx&quot;,    &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-postgres&quot;],    &quot;env&quot;: &#123;      &quot;DATABASE_URL&quot;: &quot;postgresql://localhost/mydb&quot;    &#125;  &#125;,  &quot;figma&quot;: &#123;    &quot;type&quot;: &quot;http&quot;,    &quot;url&quot;: &quot;https://mcp.figma.com&quot;,    &quot;headers&quot;: &#123;      &quot;Authorization&quot;: &quot;Bearer your_figma_token&quot;    &#125;  &#125;&#125;

MCP 实际应用场景场景 1：从 GitHub Issue 生成代码claude &quot;读取 GitHub Issue #123，根据需求实现对应功能&quot;# Claude 会通过 MCP GitHub 服务器获取 Issue 内容，理解需求后实现

场景 2：数据库驱动开发claude &quot;连接到生产数据库（只读），分析 users 表的查询性能问题，找出慢查询并优化索引&quot;# Claude 通过 MCP Postgres 服务器执行 EXPLAIN ANALYZE

场景 3：从设计稿生成代码claude &quot;从 Figma 设计稿 &#x27;User Dashboard&#x27; 生成 React 组件&quot;# Claude 通过 MCP Figma 服务器获取设计规范，生成匹配的代码

常用 MCP 服务器


MCP 服务器
用途
安装



@modelcontextprotocol/server-github
GitHub 集成
npx -y @modelcontextprotocol/server-github


@modelcontextprotocol/server-postgres
PostgreSQL 数据库
npx -y @modelcontextprotocol/server-postgres


@modelcontextprotocol/server-sentry
错误追踪
npx -y @modelcontextprotocol/server-sentry


@modelcontextprotocol/server-filesystem
文件系统访问
npx -y @modelcontextprotocol/server-filesystem


完整的 MCP 服务器列表：https://github.com/modelcontextprotocol/servers
自定义与扩展Hooks：自动化工作流Hooks 是在 Claude Code 生命周期特定点执行的 Shell 命令，提供确定性的自动化控制：
创建 Hooks 配置// .claude/hooks.json&#123;  &quot;pre-edit&quot;: &#123;    &quot;command&quot;: &quot;npm run lint&quot;,    &quot;description&quot;: &quot;Run linter before editing&quot;  &#125;,  &quot;post-edit&quot;: &#123;    &quot;command&quot;: &quot;npm run format&quot;,    &quot;description&quot;: &quot;Auto-format after editing&quot;  &#125;,  &quot;pre-commit&quot;: &#123;    &quot;command&quot;: &quot;npm test&quot;,    &quot;description&quot;: &quot;Run tests before commit&quot;  &#125;,  &quot;on-error&quot;: &#123;    &quot;command&quot;: &quot;notify-send &#x27;Claude Code Error&#x27; &#x27;$ERROR_MESSAGE&#x27;&quot;,    &quot;description&quot;: &quot;Desktop notification on error&quot;  &#125;&#125;

Hooks 应用场景
自动格式化：每次编辑后运行 Prettier
自动测试：提交前运行测试套件
通知集成：任务完成后发送 Slack 通知
日志记录：记录所有 Claude 操作到审计日志
自定义权限：根据文件路径实现细粒度权限控制

Skills：扩展 Claude 的能力Skills 是 Claude Code 最强大的扩展机制之一，它允许你创建可重用的指令集，让 Claude 自动获得特定领域的知识和能力。
什么是 SkillsSkills 是遵循 Agent Skills 开放标准的扩展，通过创建 SKILL.md 文件来定义。Claude 会：

自动发现：在相关场景下自动加载 Skills
手动调用：通过 /skill-name 命令直接调用
跨工具兼容：Skills 标准可在多个 AI 工具中使用

创建第一个 Skill让我们创建一个代码解释 Skill，教会 Claude 用可视化图表和类比来解释代码：
# 1. 创建 Skill 目录mkdir -p ~/.claude/skills/explain-code# 2. 创建 SKILL.mdcat &gt; ~/.claude/skills/explain-code/SKILL.md &lt;&lt; &#x27;EOF&#x27;---name: explain-codedescription: 用可视化图表和类比解释代码。当用户询问&quot;这段代码如何工作&quot;或需要理解代码逻辑时使用。---# 代码解释 Skill当解释代码时，始终包含以下内容：## 1. 从类比开始将代码与日常生活中的事物进行比较，帮助理解抽象概念。## 2. 绘制图表使用 ASCII 艺术展示流程、结构或关系：
┌─────────┐     ┌─────────┐     ┌─────────┐│ 输入数据 │ ──&gt; │ 处理逻辑 │ ──&gt; │ 输出结果 │└─────────┘     └─────────┘     └─────────┘
## 3. 逐步讲解分步骤解释代码执行过程，标注关键决策点。## 4. 指出常见陷阱说明容易出错的地方或常见误解。保持解释对话化，对于复杂概念使用多个类比。EOF

Skill 存放位置Skills 可以放在不同位置，影响其作用范围：



位置
路径
适用范围



企业级
通过管理配置部署
组织内所有用户


个人级
~/.claude/skills/&lt;skill-name&gt;/SKILL.md
你的所有项目


项目级
.claude/skills/&lt;skill-name&gt;/SKILL.md
当前项目


插件级
&lt;plugin&gt;/skills/&lt;skill-name&gt;/SKILL.md
插件启用的地方


优先级：企业级 &gt; 个人级 &gt; 项目级。插件 Skills 使用 plugin-name:skill-name 命名空间，不会冲突。
Skill 结构最佳实践基础结构：
---name: skill-namedescription: 简洁描述 Skill 的功能和使用场景---# Skill 主体内容[具体的指令和指导]

高级结构（支持文件）：
my-skill/├── SKILL.md              # 主指令（必需）├── reference.md          # 详细参考文档（按需加载）├── examples.md           # 使用示例（按需加载）└── scripts/    ├── validate.sh       # 验证脚本（可执行）    └── helper.py         # 辅助脚本

渐进式披露模式：
SKILL.md 作为概览，指向详细资料：
# PDF 处理 Skill## 快速开始使用 pdfplumber 提取文本：```pythonimport pdfplumberwith pdfplumber.open(&quot;file.pdf&quot;) as pdf:    text = pdf.pages[0].extract_text()

高级功能
表单填充：详见 FORMS.md
API 参考：详见 REFERENCE.md
示例代码：详见 EXAMPLES.md

Claude 只在需要时才加载这些额外文件，节省 token。#### Skill 配置选项通过 frontmatter 控制 Skill 行为：```yaml---name: deploydescription: 部署应用到生产环境disable-model-invocation: true  # 仅允许手动调用allowed-tools: Bash(*)          # 限制可用工具context: fork                   # 在独立上下文中运行agent: Explore                  # 指定使用的 agent 类型---

关键配置项：



配置项
说明
使用场景



disable-model-invocation: true
禁止 Claude 自动调用
有副作用的操作（部署、提交）


user-invocable: false
从菜单隐藏，仅 Claude 可用
背景知识类 Skills


allowed-tools
限制可用工具
只读模式、安全限制


context: fork
在子 agent 中运行
隔离执行、并行任务


实用 Skill 示例示例 1：Git Commit 消息生成器
---name: commit-messagedescription: 分析 git diff 生成规范的 commit 消息。当用户需要提交代码或询问如何写 commit 消息时使用。---# Commit 消息生成器## 格式规范遵循 Conventional Commits 标准：
(): 



## Type 类型- `feat`: 新功能- `fix`: Bug 修复- `docs`: 文档更新- `style`: 代码格式（不影响功能）- `refactor`: 重构- `test`: 测试相关- `chore`: 构建/工具配置## 示例**输入**：添加用户认证功能**输出**：
feat(auth): implement JWT-based authentication


Add login endpoint with email&#x2F;password validation
Implement token generation and verification middleware
Add refresh token mechanism for session management

Closes #123
## 生成步骤1. 运行 `git diff --staged` 查看变更2. 分析变更类型和影响范围3. 生成简洁但完整的消息4. 包含相关 issue 编号（如有）

示例 2：代码审查 Skill
---name: code-reviewdescription: 系统化审查代码变更，检查质量、安全和最佳实践。用于 PR 审查或代码质量检查。disable-model-invocation: true---# 代码审查 Skill## 审查清单### 1. 功能正确性- [ ] 代码实现符合需求- [ ] 边缘情况处理完善- [ ] 错误处理适当### 2. 代码质量- [ ] 命名清晰有意义- [ ] 函数职责单一- [ ] 避免重复代码- [ ] 注释必要且准确### 3. 安全性- [ ] 输入验证充分- [ ] 无 SQL 注入风险- [ ] 敏感数据加密- [ ] 权限检查正确### 4. 性能- [ ] 无明显性能瓶颈- [ ] 数据库查询优化- [ ] 避免 N+1 查询### 5. 测试- [ ] 有对应单元测试- [ ] 测试覆盖关键路径- [ ] 测试用例有意义## 审查流程1. 阅读变更概述和相关 issue2. 逐文件审查代码3. 运行测试套件4. 检查是否符合项目规范5. 提供具体、可操作的反馈

示例 3：数据库查询优化 Skill
---name: optimize-queriesdescription: 分析和优化数据库查询性能。当遇到慢查询或需要性能优化时使用。allowed-tools: Bash(psql *), Read, Grep---# 数据库查询优化 Skill## 诊断流程### 1. 识别慢查询```bash# PostgreSQL 慢查询日志grep &quot;duration:&quot; /var/log/postgresql/postgresql.log | sort -k3 -n | tail -20

2. 分析查询计划EXPLAIN ANALYZESELECT * FROM users WHERE email = &#x27;user@example.com&#x27;;

3. 检查关键指标
Seq Scan：全表扫描，通常需要添加索引
Nested Loop：嵌套循环，数据量大时性能差
Cost：查询成本，越低越好

常见优化策略添加索引-- 单列索引CREATE INDEX idx_users_email ON users(email);-- 复合索引（注意顺序）CREATE INDEX idx_orders_user_date ON orders(user_id, created_at);-- 部分索引（减少索引大小）CREATE INDEX idx_active_users ON users(email) WHERE active = true;

查询重写-- ❌ 避免：SELECT *SELECT * FROM users;-- ✅ 推荐：只选择需要的列SELECT id, email, name FROM users;-- ❌ 避免：子查询SELECT * FROM orders WHERE user_id IN (SELECT id FROM users WHERE active = true);-- ✅ 推荐：JOINSELECT o.* FROM orders oINNER JOIN users u ON o.user_id = u.idWHERE u.active = true;

使用缓存# Redis 缓存热点数据import redisr = redis.Redis()def get_user(user_id):    # 先查缓存    cached = r.get(f&quot;user:&#123;user_id&#125;&quot;)    if cached:        return json.loads(cached)        # 缓存未命中，查数据库    user = db.query(&quot;SELECT * FROM users WHERE id = %s&quot;, user_id)    r.setex(f&quot;user:&#123;user_id&#125;&quot;, 300, json.dumps(user))  # 5分钟 TTL    return user

验证优化效果
记录优化前的查询时间
应用优化方案
运行 EXPLAIN ANALYZE 对比
在生产环境监控性能指标

#### Skills 编写最佳实践**1. 简洁至上**Claude 已经很聪明，只添加它不知道的信息：```markdown# ❌ 过于啰嗦（150 tokens）## 提取 PDF 文本PDF（Portable Document Format）是一种常见的文件格式，包含文本、图像等内容。要提取 PDF 文本，需要使用库。有很多库可用，但我们推荐 pdfplumber...# ✅ 简洁明了（50 tokens）## 提取 PDF 文本使用 pdfplumber：```pythonimport pdfplumberwith pdfplumber.open(&quot;file.pdf&quot;) as pdf:    text = pdf.pages[0].extract_text()
**2. 设置适当的自由度**- **高自由度**（文本指令）：多种方法都有效时- **中等自由度**（伪代码/带参数脚本）：有推荐模式但允许变化- **低自由度**（具体脚本）：操作易错或需要严格一致性**3. 使用工作流模式**对于复杂任务，提供清晰的步骤和检查清单：```markdown## 部署工作流复制此清单并跟踪进度：
部署进度：

 步骤 1：运行测试套件
 步骤 2：构建生产版本
 步骤 3：备份当前版本
 步骤 4：部署到生产环境
 步骤 5：验证部署成功

**步骤 1：运行测试套件**```bashnpm run test
所有测试必须通过才能继续。
步骤 2：构建生产版本…
**4. 实现反馈循环**关键操作要有验证步骤：```markdown## 文档编辑流程1. 修改 `word/document.xml`2. **立即验证**：`python scripts/validate.py`3. 如果验证失败：   - 仔细查看错误信息   - 修复 XML 中的问题   - 再次运行验证4. **只有验证通过才能继续**5. 重新打包：`python scripts/pack.py`

5. 提供实用脚本
预制脚本比让 Claude 生成更可靠：
## 实用脚本**analyze_form.py**：从 PDF 提取表单字段```bashpython scripts/analyze_form.py input.pdf &gt; fields.json

validate.py：验证字段映射
python scripts/validate.py fields.json# 返回：&quot;OK&quot; 或列出冲突
#### Skills 与其他功能的配合**Skills + Subagents**：```markdown---name: deep-researchdescription: 深度研究某个主题context: forkagent: Explore---研究 $ARGUMENTS：1. 使用 Glob 和 Grep 查找相关文件2. 阅读并分析代码3. 总结发现，包含具体文件引用

Skills + MCP：
---name: github-issue-workflowdescription: 从 GitHub Issue 创建实现---## 工作流1. 使用 `GitHub:get_issue` 工具获取 Issue 详情2. 分析需求并制定计划3. 实现功能4. 使用 `GitHub:create_pr` 创建 Pull Request

Skills + Hooks：
&#123;  &quot;hooks&quot;: &#123;    &quot;post-edit&quot;: &#123;      &quot;command&quot;: &quot;python scripts/format.py&quot;,      &quot;description&quot;: &quot;Auto-format after editing&quot;    &#125;  &#125;&#125;

管理和分享 Skills查看可用 Skills：
# 列出所有 Skillsclaude &quot;有哪些可用的 Skills？&quot;# 查看 Skill 详情/help explain-code

分享 Skills：

项目级：将 .claude/skills/ 提交到版本控制
插件：在插件中创建 skills/ 目录
企业级：通过管理配置部署

Skills 故障排查：



问题
可能原因
解决方案



Skill 未触发
description 不够具体
添加关键词和使用场景


触发过于频繁
description 太宽泛
更具体的描述或添加 disable-model-invocation: true


Claude 看不到 Skill
超出字符预算
简化 description 或增加 SLASH_COMMAND_TOOL_CHAR_BUDGET


创建自定义 Subagent对于重复的专业任务，可以创建自定义 subagent：
# .claude/subagents/test-writer.py&quot;&quot;&quot;Custom subagent for writing comprehensive tests&quot;&quot;&quot;def generate_tests(file_path, coverage_target=80):    &quot;&quot;&quot;    Generate unit tests for given file    Args:        file_path: Path to source file        coverage_target: Target coverage percentage    &quot;&quot;&quot;    # Subagent logic here    pass

# 使用自定义 subagentclaude &quot;使用 test-writer subagent 为 src/services/ 下所有文件生成测试，目标覆盖率 90%&quot;

插件系统Claude Code 支持插件扩展功能：
# 安装插件claude plugin install code-review-assistant# 启用插件claude plugin enable code-review-assistant# 使用插件功能claude &quot;使用 code-review-assistant 审查最近的 commit&quot;

常用插件类型：

代码质量检查
安全漏洞扫描
性能分析
文档生成
依赖管理

团队协作与规模化编写团队级 CLAUDE.md团队使用 Claude Code 时，CLAUDE.md 应该成为团队知识库的一部分：
# CLAUDE.md - Team Edition## Project Overview[项目背景、业务领域、技术栈]## Team Conventions- **Branch naming**: feature/*, bugfix/*, hotfix/*- **Commit messages**: Follow Conventional Commits- **PR process**: Require 2 approvals, all checks must pass- **Code ownership**: See CODEOWNERS file## Common Workflows### Adding a New API Endpoint1. Define schema in `src/schemas/`2. Implement handler in `src/handlers/`3. Add route in `src/routes/`4. Write integration test in `tests/integration/`5. Update OpenAPI spec in `docs/api.yaml`### Database Migrations```bashnpm run migration:create -- --name add_user_roles# Edit migration filenpm run migration:runnpm run migration:test

Troubleshooting
Build fails with “Module not found”: Run npm ci to clean install
Tests timeout: Increase timeout in jest.config.js
Database connection fails: Check VPN connection

Contact
Architecture questions: @architecture-team
DevOps issues: @devops-team
Security concerns: @security-team

### 多会话并行工作大型项目可以启动多个 Claude 会话并行工作：```bash# Terminal 1: 前端开发cd frontendclaude &quot;实现用户仪表板组件&quot;# Terminal 2: 后端 APIcd backendclaude &quot;实现用户统计 API&quot;# Terminal 3: 数据库迁移cd databaseclaude &quot;创建用户活动日志表的迁移脚本&quot;# Terminal 4: 文档更新cd docsclaude &quot;根据新 API 更新文档&quot;

Headless Mode：自动化与 CI&#x2F;CDClaude Code 支持 headless 模式，适合集成到 CI&#x2F;CD 流程：
# 在 CI 中运行 Claude Codeclaude --headless --task &quot;运行所有测试，如果失败则分析原因并尝试修复&quot;# GitHub Actions 集成# .github/workflows/claude-review.ymlname: Claude Code Reviewon: [pull_request]jobs:  review:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v3      - name: Run Claude Code Review        run: |          claude --headless --task &quot;审查这个 PR 的代码变更，          检查是否符合团队规范，是否有潜在 bug&quot;        env:          CLAUDE_API_KEY: $&#123;&#123; secrets.CLAUDE_API_KEY &#125;&#125;

企业级配置管理企业环境可以使用集中式配置管理：
// /etc/claude/enterprise-config.json&#123;  &quot;allowed_operations&quot;: [&quot;read&quot;, &quot;write&quot;, &quot;execute&quot;],  &quot;forbidden_paths&quot;: [&quot;/etc&quot;, &quot;/var&quot;, &quot;~/.ssh&quot;],  &quot;mcp_servers&quot;: &#123;    &quot;github&quot;: &#123; &quot;org&quot;: &quot;your-company&quot; &#125;,    &quot;jira&quot;: &#123; &quot;project&quot;: &quot;PROJ&quot; &#125;  &#125;,  &quot;audit_log&quot;: &#123;    &quot;enabled&quot;: true,    &quot;destination&quot;: &quot;syslog&quot;  &#125;,  &quot;rate_limits&quot;: &#123;    &quot;requests_per_hour&quot;: 1000  &#125;&#125;

常见陷阱与解决方案陷阱 1：过度依赖长对话问题：在一个会话中处理太多不相关的任务，导致 Claude 混淆上下文。
解决方案：

每完成一个独立任务就 /clear
使用 subagents 分离不同任务
为不同类型的工作启动独立会话

陷阱 2：没有验证就信任输出问题：假设 Claude 生成的代码是正确的，直接提交。
解决方案：

始终要求 Claude 运行测试验证
使用 hooks 自动运行 linter 和测试
Code review 流程不能省略
关键代码手动审查

陷阱 3：CLAUDE.md 过时或缺失问题：项目规范变化了，但 CLAUDE.md 没有更新，导致 Claude 使用错误的模式。
解决方案：

将 CLAUDE.md 纳入代码审查流程
重大架构变更时同步更新 CLAUDE.md
定期审查 CLAUDE.md 的准确性
在 PR 模板中提醒更新 CLAUDE.md

陷阱 4：忽略权限和安全问题：使用 --dangerously-skip-permissions 在不信任的代码库中，导致安全风险。
解决方案：

仅在信任的项目中使用 skip permissions
使用 hooks 实现细粒度权限控制
审计 Claude 的所有文件操作
敏感项目使用沙箱环境

陷阱 5：不合理的任务分解问题：给 Claude 一个过于庞大的任务，如”重写整个应用”。
解决方案：

将大任务分解为可管理的子任务
使用 Plan Mode 先制定计划
逐步验证每个子任务的结果
使用 checkpoints 保存进度

性能优化建议减少 Token 消耗# ❌ 低效：让 Claude 读取整个大文件claude &quot;优化 src/utils/large-file.ts&quot;# ✅ 高效：指定具体函数claude &quot;优化 src/utils/large-file.ts 中的 processData 函数&quot;# ✅ 使用 .claudeignore 排除无关文件# .claudeignorenode_modules/dist/*.logcoverage/.git/

利用缓存# Claude Code 会自动缓存：- 文件内容- 命令输出- MCP 响应# 利用缓存的最佳实践：- 避免频繁修改 CLAUDE.md（会使缓存失效）- 重复的命令会直接使用缓存结果- 使用 /clear 会清空缓存，谨慎使用

并行化操作# ❌ 串行处理claude &quot;为 A.ts 添加测试&quot;# 等待完成...claude &quot;为 B.ts 添加测试&quot;# 等待完成...# ✅ 并行处理claude &quot;为 A.ts 和 B.ts 添加测试，使用 subagents 并行处理&quot;

实战案例案例 1：大规模重构场景：将 REST API 迁移到 GraphQL
# 第 1 步：探索和规划claude plan &quot;分析当前 REST API 结构，制定迁移到 GraphQL 的计划&quot;# 第 2 步：审查计划并调整# Claude 会列出：# - 需要创建的 GraphQL schema# - 需要修改的 resolver# - 需要更新的客户端代码# - 测试策略# 第 3 步：创建检查点/checkpoint save &quot;开始 GraphQL 迁移&quot;# 第 4 步：使用 subagents 并行执行claude &quot;按照计划执行 GraphQL 迁移，创建 subagents 分别处理：1. 定义 GraphQL schema2. 实现 resolvers3. 更新客户端查询4. 迁移测试用例&quot;# 第 5 步：验证claude &quot;运行完整测试套件，确保所有功能正常&quot;# 第 6 步：文档更新claude &quot;更新 API 文档，添加 GraphQL 使用示例&quot;

案例 2：性能优化场景：优化慢查询
# 第 1 步：诊断问题claude &quot;连接到数据库（通过 MCP），找出最慢的 10 个查询&quot;# 第 2 步：分析根因claude --extended-thinking &quot;分析这些慢查询的根本原因，考虑索引、查询计划、数据量等因素&quot;# 第 3 步：制定优化方案claude &quot;为每个慢查询提供优化方案，包括：- 索引建议- 查询重写- 缓存策略评估每个方案的收益和风险&quot;# 第 4 步：实施优化claude &quot;实施优化方案，先在开发环境验证效果&quot;# 第 5 步：性能测试claude &quot;运行性能测试，对比优化前后的查询时间&quot;

案例 3：Bug 修复场景：生产环境出现间歇性错误
# 第 1 步：收集信息claude &quot;从 Sentry（通过 MCP）获取最近 24 小时的错误报告，聚焦 &#x27;UserService.updateProfile&#x27; 相关的错误&quot;# 第 2 步：复现问题claude &quot;根据错误堆栈和用户报告，编写能够复现问题的测试用例&quot;# 第 3 步：调试claude &quot;分析 UserService.updateProfile 的代码，找出可能导致间歇性错误的竞态条件或边缘情况&quot;# 第 4 步：修复验证claude &quot;修复问题后，运行测试用例 100 次，确保问题不再出现&quot;# 第 5 步：回归测试claude &quot;运行完整测试套件，确保修复没有引入新问题&quot;

学习资源与社区官方资源
官方文档：https://code.claude.com/docs
最佳实践指南：https://www.anthropic.com/engineering/claude-code-best-practices
MCP 协议规范：https://modelcontextprotocol.io/
GitHub Changelog：https://github.com/anthropics/claude-code/releases

社区资源
Claude Code Discord：官方 Discord 服务器，活跃的社区支持
Reddit r&#x2F;ClaudeAI：用户分享使用技巧和案例
YouTube 教程：搜索 “Claude Code tutorial” 有大量视频教程
GitHub Awesome List：https://github.com/topics/claude-code

进阶学习路径
基础阶段（1-2 周）

熟悉基本命令和工作流
编写第一个 CLAUDE.md
完成简单的代码生成任务


进阶阶段（1 个月）

掌握 Plan Mode 和 Subagents
配置 MCP 服务器
使用 Hooks 自动化工作流


专家阶段（持续学习）

创建自定义 Subagents
开发 MCP 服务器
优化团队协作流程
贡献社区插件和最佳实践



总结Claude Code 不仅仅是一个代码生成工具，而是一个自主编程伙伴。要充分发挥其价值，关键在于：
核心要点
建立正确的心智模型：理解 Claude Code 是自主代理，不是简单的补全工具
投资于配置：编写高质量的 CLAUDE.md，配置 MCP 和 Hooks
遵循最佳流程：探索 → 规划 → 编码，而非直接编码
保持会话清晰：频繁使用 /clear，避免长对话混乱
验证验证验证：永远不要盲目信任 AI 输出，要求自动化验证
善用高级特性：Subagents、Plan Mode、Checkpoints 是效率倍增器
团队协作：将 CLAUDE.md 纳入团队规范，统一工作方式

下一步行动
 为你的项目创建 CLAUDE.md
 尝试使用 Plan Mode 分析一个复杂任务
 配置至少一个 MCP 服务器（建议从 GitHub 开始）
 使用 Subagents 处理一个多文件任务
 设置 Hooks 自动化你的常用工作流
 在团队中分享 Claude Code 最佳实践

参考资源
Claude Code 官方文档
Anthropic 工程博客：Claude Code 最佳实践
Model Context Protocol 规范
Claude Code vs Cursor 深度对比
100 Essential Claude Code Tips


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>MCP</tag>
        <tag>Claude</tag>
        <tag>AI编程助手</tag>
        <tag>开发效率</tag>
        <tag>最佳实践</tag>
      </tags>
  </entry>
  <entry>
    <title>Cursor IDE 使用最佳实践与技巧：从入门到精通</title>
    <url>/posts/11eb1b4f9e5f.html</url>
    <content><![CDATA[在 AI 辅助编程时代，选择正确的工具和掌握最佳实践能够显著提升开发效率。Cursor IDE 作为一款原生集成 AI 能力的代码编辑器，正在改变开发者的工作方式。据用户反馈，合理使用 Cursor 的 Agent 模式配合上下文管理，可以将功能开发时间缩短 60%。本文将系统性地介绍 Cursor 的核心功能、最佳实践和常见陷阱，帮助你充分发挥这款工具的潜力。
为什么选择 CursorCursor vs GitHub Copilot在选择 AI 编程助手时，了解不同工具的优势至关重要：
Cursor 的核心优势：

全项目代码库理解：能够理解整个代码库的上下文，而不仅仅是当前文件
多文件编辑能力：通过 Composer 功能同时编辑多个文件
深度 AI 集成：AI 能力原生内置于 IDE，而非仅作为扩展
复杂项目优势：在处理大型、复杂代码库时表现出色，具备完整的架构感知能力
模型灵活性：支持切换不同的 AI 模型（Claude 4.5 Opus、Sonnet 等）

GitHub Copilot 的优势：

广泛的 IDE 兼容性：作为插件可在 VS Code、JetBrains、Vim 等多个编辑器中使用
快速自动补全：逐行建议速度快，特别适合 TypeScript 和 React 组件
更低的成本门槛：提供免费层级（2000 次补全 + 50 次聊天&#x2F;月），Pro 版本 $10&#x2F;月
成熟的生态系统：已建立的工具，与现有开发工作流集成良好

选择建议：

如果你希望保持当前编辑器设置且偏好低成本，选择 GitHub Copilot
如果你愿意切换编辑器并需要处理复杂代码库的强大上下文能力，选择 Cursor

Cursor 核心功能详解1. Composer：AI 编码助手Composer 是 Cursor 的核心 AI 助手，通过 ⌘I（Mac）或 Ctrl+I（Windows&#x2F;Linux）调用。
两种布局模式：

Pane 模式：侧边栏聊天 + 代码编辑器并排显示
Editor 模式：单窗口视图，最大化代码编辑空间

Normal Mode（普通模式）：

代码库和文档搜索
Web 搜索能力
文件创建和写入
扩展的 @ 符号命令

Agent Mode（智能代理模式）：
通过 ⌘.（Mac）或 Ctrl+.（Windows&#x2F;Linux）启用，获得主动式编码伙伴：

自动拉取相关上下文（使用 @Recommended）
运行终端命令
创建和修改文件
语义化代码搜索
执行文件操作


重要提示：Agent 模式支持最多 25 次工具调用后停止，目前仅支持 Claude 模型。

2. Tab 自动补全Cursor Tab 是一个定制的自动补全模型，能够预测你的下一步操作：

利用 AI 预测实现闪电般的编码速度
完成函数、生成样板代码、创建测试用例
AI 从打开的文件中理解上下文，提供更好的结果

启用方式：在设置中启用 Tab 功能及其高级特性。
3. 上下文管理使用 @ 符号访问上下文：

@ + 文件名：引用特定文件
@Recommended：自动推荐相关上下文
聊天顶部的上下文标签显示活动上下文，可以添加或删除

使用 # 符号：

# + 文件名：聚焦于特定文件

4. 其他关键功能Checkpoints（检查点）：每次代码生成都会创建检查点，用于版本控制和回滚更改。
History（历史记录）：通过历史面板访问之前的 Composer 会话。
Lint Iteration（Beta）：Composer 会自动尝试修复生成代码中的 linting 问题。
最佳实践：提升生产力的关键结构化规划与文档1. 维护项目里程碑文档
在大型项目中，创建 Project_milestones.md 文件并在 .cursorrules 中引用：
# Project Milestones## Phase 1: Authentication System- [x] User registration- [x] Login/logout- [ ] Password reset- [ ] OAuth integration## Phase 2: Core Features- [ ] Dashboard implementation- [ ] Data visualization- [ ] Export functionality

在 .cursorrules 文件中引用：
@Project_milestones.mdFollow the implementation plan defined in the milestones document.

2. 请求实现概览
在让 AI 生成代码之前，先请求实现概览：

防止浪费时间
及早发现 AI 幻觉
确保方案符合预期

示例提示词：
Before implementing the user authentication system, please provide an overview of your planned approach, including architecture decisions, libraries to use, and potential security considerations.

增量开发策略小步快跑，而非大功能投放：
❌ 错误做法：
Implement the entire e-commerce checkout flow including cart, payment, shipping, and order confirmation.

✅ 正确做法：
Step 1: Implement shopping cart add/remove functionalityStep 2: Create cart summary viewStep 3: Add shipping address formStep 4: Integrate payment gatewayStep 5: Implement order confirmation

测试驱动开发（TDD）：
先实现测试，再编写代码：
# Step 1: Write test firstdef test_user_registration():    user = register_user(&quot;test@example.com&quot;, &quot;password123&quot;)    assert user.email == &quot;test@example.com&quot;    assert user.is_active == True# Step 2: Let AI implement the function to pass the test

上下文管理策略1. 创建参考文档
为一致的上下文创建参考文档：
project/├── docs/│   ├── prd.md          # Product Requirements Document│   ├── specs.md        # Technical Specifications│   ├── architecture.md # System Architecture│   └── todo.md         # Current Tasks

2. 分层上下文方法
针对大型代码库（50 万行以上），采用分层上下文策略：

即时上下文：当前文件 + 直接依赖
模块上下文：相关文件
子系统上下文：更广泛的架构视图

3. 标记所有必要文件
在提供上下文时，使用 @ 标记所有相关文件：
@src/auth/login.ts @src/auth/types.ts @src/utils/validation.tsRefactor the login function to use the new validation utilities.

工作流优化1. 任务分离原则
为每个任务启动新的聊天，避免上下文膨胀：
Chat 1: Implement user authentication ✓Chat 2: Add email verification ✓  Chat 3: Create password reset flow ← New chat

2. 模式切换策略

Ask 模式：用于规划和探索
Agent 模式：用于实现和执行

3. 模型选择策略

推理模型（如 Claude 3.7 Max）：用于规划、架构设计
常规模型（如 Claude 4.5 Sonnet）：用于代码实现

性能优化：大型代码库策略1. 智能索引配置
创建 .cursorignore 文件排除不必要的文件：
# Build artifactsdist/build/*.min.js*.map# Dependenciesnode_modules/vendor/.venv/# Generated files*.generated.*__generated__/# Test snapshots**/__snapshots__/# Logs*.loglogs/

配置增量更新：
在设置中配置 5 分钟的增量更新间隔。
2. 多根工作区策略
对于 monorepo，按优先级组织工作区文件夹：
workspace/├── packages/│   ├── core/          # High priority - keep open│   ├── ui/            # High priority - keep open│   ├── utils/         # Medium priority - close when not needed│   └── legacy/        # Low priority - keep closed


提示：Cursor 仅索引打开的工作区，关闭不活跃的文件夹可减少内存使用。

3. PRD → Plan → Todo 方法论
采用文档驱动的开发方法：
1. PRD (Product Requirements Document)   ↓2. Implementation Plan   ↓3. Todo List with Checkpoints   ↓4. Incremental Implementation

高级技巧1. Cursor Rules 配置在 .cursor/rules 目录创建领域特定知识：
.cursor/└── rules/    ├── coding-standards.md    ├── api-conventions.md    └── security-guidelines.md

示例 coding-standards.md：
# Coding Standards## TypeScript- Use strict mode- Prefer interfaces over types for object shapes- Always define return types for functions- Use const assertions for literal types## Error Handling- Always use try-catch for async operations- Log errors with context information- Return meaningful error messages to users## Testing- Minimum 80% code coverage- Write unit tests for all business logic- Use integration tests for API endpoints

2. 提示词精确化❌ 模糊提示：
Make the app faster

✅ 精确提示：
Optimize the user list component by:1. Implementing virtual scrolling for lists &gt; 100 items2. Memoizing expensive calculations with useMemo3. Adding React.memo to prevent unnecessary re-renders4. Lazy loading user avatars

3. MCP（Model Context Protocol）集成设置 MCP 服务器扩展 Cursor 能力：
GitHub MCP 示例：

仓库管理
代码审查
问题跟踪

配置方式：在 Cursor 设置中配置 MCP 服务器：
// ~/.cursor/mcp.json&#123;  &quot;mcpServers&quot;: &#123;    &quot;github&quot;: &#123;      &quot;command&quot;: &quot;npx&quot;,      &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-github&quot;],      &quot;env&quot;: &#123;        &quot;GITHUB_TOKEN&quot;: &quot;your_token_here&quot;      &#125;    &#125;,    &quot;filesystem&quot;: &#123;      &quot;command&quot;: &quot;npx&quot;,      &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/path/to/allowed/dir&quot;]    &#125;  &#125;&#125;

4. Skills：扩展 Cursor 的能力Skills 是 Cursor 强大的扩展机制，允许你创建可重用的指令集，让 AI 自动获得特定领域的知识和能力。
什么是 SkillsSkills 遵循 Agent Skills 开放标准，通过创建 SKILL.md 文件来定义。Cursor 会：

自动发现：在相关场景下自动加载 Skills
手动调用：通过 /skill-name 命令直接调用
跨工具兼容：Skills 标准可在 Cursor、Claude Code 等多个 AI 工具中使用

创建 Skill在 .cursor/skills/ 目录下创建 Skill：
# 创建 Skill 目录mkdir -p .cursor/skills/code-review# 创建 SKILL.md

示例：代码审查 Skill
---name: code-reviewdescription: 系统化审查代码变更，检查质量、安全和最佳实践。用于 PR 审查或代码质量检查。---# 代码审查 Skill## 审查清单### 1. 功能正确性- [ ] 代码实现符合需求- [ ] 边缘情况处理完善- [ ] 错误处理适当### 2. 代码质量- [ ] 命名清晰有意义- [ ] 函数职责单一- [ ] 避免重复代码### 3. 安全性- [ ] 输入验证充分- [ ] 无 SQL 注入风险- [ ] 敏感数据加密### 4. 性能- [ ] 无明显性能瓶颈- [ ] 数据库查询优化## 审查流程1. 阅读变更概述和相关 issue2. 逐文件审查代码3. 运行测试套件4. 提供具体、可操作的反馈

Skill 存放位置与优先级


位置
路径
适用范围



个人级
~/.cursor/skills/&lt;skill-name&gt;/SKILL.md
你的所有项目


项目级
.cursor/skills/&lt;skill-name&gt;/SKILL.md
当前项目


实用 Skill 示例Commit 消息生成器：
---name: commit-messagedescription: 分析 git diff 生成规范的 commit 消息---# Commit 消息生成器## 格式规范遵循 Conventional Commits 标准：
(): 

## Type 类型- `feat`: 新功能- `fix`: Bug 修复- `docs`: 文档更新- `refactor`: 重构- `test`: 测试相关## 生成步骤1. 运行 `git diff --staged` 查看变更2. 分析变更类型和影响范围3. 生成简洁但完整的消息

代码解释 Skill：
---name: explain-codedescription: 用可视化图表和类比解释代码逻辑---# 代码解释 Skill当解释代码时：## 1. 从类比开始将代码与日常生活中的事物进行比较。## 2. 绘制图表使用 ASCII 艺术展示流程：
┌─────────┐     ┌─────────┐     ┌─────────┐│ 输入数据 │ ──&gt; │ 处理逻辑 │ ──&gt; │ 输出结果 │└─────────┘     └─────────┘     └─────────┘
## 3. 逐步讲解分步骤解释代码执行过程。

Skills 最佳实践
简洁至上：只添加 AI 不知道的信息，避免冗余
渐进式披露：SKILL.md 作为概览，详细内容放在单独文件中按需加载
提供验证步骤：关键操作要有验证方式
团队共享：将 .cursor/skills/ 提交到版本控制

5. 团队协作共享配置：
project/├── .cursorrules          # Shared AI behavior rules├── .cursorignore        # Shared ignore patterns└── .cursor/    └── rules/           # Team coding standards

将这些文件提交到版本控制，确保团队成员获得一致的 AI 行为。
Cursor 与 Claude Code 集成为什么同时使用两个工具Cursor 和 Claude Code 是互补而非竞争的关系。两者各有所长，结合使用可以覆盖更广泛的开发场景：



场景
推荐工具
原因



快速原型开发
Cursor
IDE 集成，实时补全，所见即所得


复杂重构
Claude Code
200k+ token 上下文，自主执行多文件操作


代码审查
Cursor
内联 diff，可视化对比


架构级决策
Claude Code
Extended Thinking，深度分析能力


调试和修复
Cursor
即时反馈，快速迭代


批量代码生成
Claude Code
Subagents 并行处理


文档编写
Cursor
Markdown 预览，实时编辑


CI&#x2F;CD 集成
Claude Code
Headless 模式，脚本化执行


配置共享共享 MCP 服务器Cursor 和 Claude Code 都支持 MCP，可以共享相同的 MCP 服务器配置：
// 共享的 MCP 配置逻辑// Cursor: ~/.cursor/mcp.json// Claude Code: ~/.config/claude/mcp_servers.json&#123;  &quot;github&quot;: &#123;    &quot;command&quot;: &quot;npx&quot;,    &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-github&quot;],    &quot;env&quot;: &#123;      &quot;GITHUB_TOKEN&quot;: &quot;$&#123;GITHUB_TOKEN&#125;&quot;    &#125;  &#125;,  &quot;postgres&quot;: &#123;    &quot;command&quot;: &quot;npx&quot;,    &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-postgres&quot;],    &quot;env&quot;: &#123;      &quot;DATABASE_URL&quot;: &quot;$&#123;DATABASE_URL&#125;&quot;    &#125;  &#125;&#125;

共享 Skills由于 Skills 遵循开放标准，可以在项目中创建统一的 Skills 目录：
project/├── .cursor/│   └── rules/              # Cursor 专用规则├── .claude/│   └── hooks.json          # Claude Code 专用 hooks├── skills/                 # 共享 Skills（两个工具都可读取）│   ├── code-review/│   │   └── SKILL.md│   └── commit-message/│       └── SKILL.md└── CLAUDE.md              # 项目上下文（两个工具都识别）

协同工作流工作流 1：规划 + 实现# Step 1: 使用 Claude Code 进行深度规划claude plan &quot;设计用户认证系统，考虑安全性、可扩展性和性能&quot;# Step 2: 审查 Claude Code 的规划输出# Step 3: 在 Cursor 中实现# 打开 Cursor，使用 Agent 模式按计划逐步实现# Cursor 提供更好的代码可视化和即时反馈

工作流 2：并行开发# Terminal 1: Claude Code 处理后端重构claude &quot;重构 UserService，将认证逻辑迁移到独立模块&quot;# 同时在 Cursor 中处理前端# 使用 Cursor 的实时补全和预览功能开发 UI 组件

工作流 3：代码审查 + 修复# Step 1: 使用 Claude Code 进行全面代码审查claude &quot;审查最近的 PR 变更，检查安全漏洞和性能问题&quot;# Step 2: 在 Cursor 中查看和修复问题# 利用 Cursor 的 diff 视图和内联编辑能力快速修复

在 Cursor 中使用 Claude Code 扩展Cursor 支持安装 Claude Code VSCode 扩展，实现深度集成：
安装方式：

打开 Cursor 的扩展面板
搜索 “Claude Code”
安装官方扩展

集成功能：

在 Cursor 内直接调用 Claude Code 终端
共享编辑器上下文
统一的快捷键绑定

// settings.json - 配置 Claude Code 扩展&#123;  &quot;claudeCode.autoActivate&quot;: true,  &quot;claudeCode.showInlineHints&quot;: true,  &quot;claudeCode.terminalIntegration&quot;: true&#125;

最佳实践：工具选择决策树需要完成什么任务？│├── 简单代码修改 / 快速补全│   └── ✅ 使用 Cursor│├── 单文件重构 / UI 开发│   └── ✅ 使用 Cursor│├── 多文件重构 / 架构变更│   └── ✅ 使用 Claude Code│├── 需要深度分析 / 复杂决策│   └── ✅ 使用 Claude Code (Extended Thinking)│├── 需要并行处理多个任务│   └── ✅ 使用 Claude Code (Subagents)│├── CI/CD 集成 / 自动化脚本│   └── ✅ 使用 Claude Code (Headless)│└── 实时协作 / 代码审查    └── ✅ 使用 Cursor

配置同步脚本创建一个脚本来同步两个工具的配置：
#!/bin/bash# sync-ai-tools.sh# 同步 MCP 配置cp ~/.cursor/mcp.json ~/.config/claude/mcp_servers.json 2&gt;/dev/null || true# 同步 Skills 到两个位置if [ -d &quot;skills&quot; ]; then  mkdir -p .cursor/skills .claude/skills  cp -r skills/* .cursor/skills/  cp -r skills/* .claude/skills/fiecho &quot;AI 工具配置同步完成&quot;

将此脚本添加到项目的 package.json 或 Makefile 中，确保配置保持同步。
常见错误与陷阱错误 1：忽略控制台错误问题：遇到问题时直接重装 Cursor，而不先检查开发者工具。
解决方案：

打开开发者工具：Help &gt; Toggle Developer Tools
过滤控制台查看：
“WARN” 消息（搜索提供程序问题）
“ERROR” 日志（原生模块失败）
扩展激活失败



错误 2：误诊扩展冲突问题：认为所有问题都是扩展冲突导致。
解决方案：

先在原版 VS Code 中测试，确认问题不是系统级的
在开发者工具中检查内置扩展状态
逐个禁用扩展定位问题

错误 3：忽略架构不匹配问题：系统架构不匹配（特别是 ARM64 vs x64）导致功能失效。
解决方案：

检查错误消息中的架构线索（如 “win32-arm64”）
验证 Cursor 版本与操作系统架构匹配
必要时重新下载正确架构的版本

错误 4：浪费时间在表面修复上问题：删除配置文件夹或切换实验性设置，但问题依旧。
解决方案：系统性排查：

验证 Cursor 版本
检查架构兼容性
在终端测试 git grep
创建测试项目确认问题

错误 5：错过更新问题：自动更新可能静默失败，ARM64 构建有时滞后。
解决方案：

定期从官网手动重新下载 Cursor
设置和扩展会自动迁移

故障排除速查GitHub 登录问题Command Palette &gt; Sign Out of GitHub然后重新登录

企业代理阻止 Cmd K&#x2F;Tab在设置中添加：
&#123;  &quot;cursor.general.disableHttp2&quot;: true&#125;


原因：HTTP&#x2F;2 可能被 Zscaler 等代理阻止。

订阅未更新从设置中退出并重新登录。
Remote SSH 问题
注意：Mac 和 Windows 的 SSH 连接目前不受支持。

收集故障排除信息报告问题时，收集以下信息：

截图
复现步骤
系统信息（Help &gt; About）
VPN&#x2F;Zscaler 状态
开发者工具控制台错误
日志文件：
Windows: C:\Users\&lt;用户名&gt;\AppData\Roaming\Cursor\logs
Mac: ~/Library/Application Support/Cursor/logs
Linux: ~/.config/Cursor/logs



实战案例：从零到生产场景：构建用户认证系统Step 1: 规划阶段（Ask 模式）
@architecture.md @security-guidelines.mdI need to implement a user authentication system. Please provide an implementation plan considering:- Session vs JWT approach- Password hashing strategy- Rate limiting for login attempts- OAuth integration requirements

Step 2: 创建 Todo
基于 AI 的规划，创建 todo.md：
# User Authentication Implementation## Phase 1: Core Auth- [ ] Set up database schema for users- [ ] Implement password hashing with bcrypt- [ ] Create registration endpoint- [ ] Create login endpoint- [ ] Implement JWT token generation## Phase 2: Security- [ ] Add rate limiting middleware- [ ] Implement refresh token mechanism- [ ] Add email verification- [ ] Create password reset flow## Phase 3: OAuth- [ ] Google OAuth integration- [ ] GitHub OAuth integration

Step 3: 增量实现（Agent 模式）
@todo.md @src/auth/Implement Phase 1, Task 1: Set up database schema for users.Use Prisma ORM and include fields for email, password hash, created_at, updated_at, and is_verified.

Step 4: 测试驱动
@src/auth/register.tsWrite unit tests for the registration function covering:- Successful registration- Duplicate email handling- Password validation- Email format validation

Step 5: 代码审查
使用 Cursor 的检查点功能回顾更改，确保质量。
总结Cursor IDE 通过深度 AI 集成和强大的上下文理解能力，正在重新定义开发者的工作方式。掌握以下关键实践，可以最大化你的生产力：

结构化规划：使用 PRD、规范文档和 todo 列表保持组织性
增量开发：小步快跑，采用测试驱动开发
上下文管理：创建参考文档，使用分层上下文方法
工作流优化：任务分离，合理选择模式和模型
性能优化：配置 .cursorignore，使用多根工作区
Skills 扩展：创建可重用的 Skills，标准化 AI 行为
工具协同：结合 Claude Code 使用，覆盖更广泛的开发场景
团队协作：共享 Cursor 规则、Skills 和配置

记住，AI 是你的编码伙伴，而非替代品。通过精确的提示词、良好的上下文管理、Skills 扩展和与 Claude Code 的协同使用，你可以将 AI 辅助编程的潜力发挥到极致。
参考资源Cursor 相关
Cursor 官方文档
Cursor Best Practices
Cursor IDE Guide
Developer Toolkit - Cursor Tips
Cursor Community Forum

Claude Code 与集成
Claude Code 官方文档
Agent Skills 开放标准
Model Context Protocol
Claude Code vs Cursor 深度对比


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>Claude Code</tag>
        <tag>Cursor</tag>
        <tag>AI编程</tag>
        <tag>开发工具</tag>
        <tag>生产力</tag>
        <tag>VSCode</tag>
        <tag>Skills</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent Skill：从使用到原理，一次讲清</title>
    <url>/posts/76da86423267.html</url>
    <content><![CDATA[引言你可能遇到过这种情况：同一个任务（会议纪要、代码评审、发布流程、合规检查）每次都要把“长长的规则”再贴一遍；而当这些规则越来越复杂，又会把上下文挤爆、让模型抓不住重点。
这期视频把 Agent Skill 讲得很透：它不是“更强的提示词”，而是一种可复用的能力封装方式，并且通过 渐进式披露（progressive disclosure） 做到“只在需要时加载”。视频还强调了一个关键对比：Skill 和 MCP 能力有交集，但定位不同，很多场景应该组合使用。Agent Skill 从使用到原理，一次讲清
本文会基于视频主线，结合 Agent Skills 规范与主流工具文档，把“从使用到原理”讲清楚，并给你一套可直接复用的写 Skill 方法论。
Agent Skill 是什么用一句话概括：Agent Skill 是一个目录（folder），里面包含 SKILL.md 指令文件，以及可选的脚本、参考资料、资源文件；AI 工具会在合适的时机发现并加载它们。Agent Skills 概览 Skills explained
从规范角度看，一个最小 Skill 的目录结构如下：Agent Skills Specification
skill-name/  SKILL.md

而更实用的结构通常会把“长知识&#x2F;长规则”放到 references/，把“可执行自动化”放到 scripts/：
meeting-minutes/  SKILL.md  references/    FINANCE.md  scripts/    upload.py

核心机制：渐进式披露（Progressive Disclosure）Agent Skills 的关键不是“把提示词收纳起来”，而是 加载策略。规范把它分成三层：Agent Skills Specification

Metadata（约 ~100 tokens）：启动时只加载 name、description，让 agent 知道“有哪些技能”和“什么时候用”。
Instructions（建议 &lt; 5000 tokens）：当 agent 认为某个 Skill 相关时，才加载 SKILL.md 正文指令。
Resources（按需）：只有当指令引用到 references/、scripts/、assets/ 等文件时，才会进一步读取&#x2F;执行。

VS Code 文档也用类似的“三层加载”描述了这一点：Use Agent Skills in VS Code

注意：这套设计直接解决了“可复用规则很长”与“上下文很贵”之间的矛盾：你可以装很多 Skill，但每次只为当前任务付出必要的上下文成本。

基础用法：写一个会议纪要 Skill（可复用、可迁移）1) 先写对目录名和 FrontmatterAgent Skills 规范对 name 有硬性约束：小写字母&#x2F;数字&#x2F;连字符（hyphen），最长 64，且必须与父目录名一致。Agent Skills Specification
例如目录叫 meeting-minutes/，则 SKILL.md 的 frontmatter 应该是：
---name: meeting-minutesdescription: Summarize meeting transcripts into attendees, agenda items, and decisions. Use when the user asks for meeting minutes, meeting summary, or action items.compatibility: Works with Agent Skills compatible tools. Requires ability to read local files.---

2) 再写正文指令（Instruction）建议把正文写成“可执行的 SOP”，包括输出格式、步骤、边界条件、示例输入输出：
---name: meeting-minutesdescription: Summarize meeting transcripts into attendees, agenda items, and decisions. Use when the user asks for meeting minutes, meeting summary, or action items.---## GoalProduce clear meeting minutes from a raw transcript.## Output format (required)### Attendees- &lt;name&gt;### Agenda1. &lt;topic&gt;### Decisions- &lt;decision&gt;### Action Items- &lt;owner&gt;: &lt;task&gt; (due: &lt;date or TBD&gt;)## Procedure1. Read the transcript carefully and infer attendee names if stated.2. Group discussion into agenda topics.3. Extract explicit decisions; do not invent decisions.4. If budgets, procurement, expenses, or payments are mentioned, follow the Finance Check section and read references/FINANCE.md.5. If the user asks to upload/sync/send to a server, run scripts/upload.py with the minutes content.


注意：正文越长，触发 Skill 后的上下文成本越高。规范建议把 SKILL.md 控制在 500 行以内，把长材料下沉到 references/。Agent Skills Specification

高级用法一：Reference（按需读取长资料）视频里用“集团财务手册”演示了 Reference 的价值：只有当会议内容提到费用&#x2F;预算等关键词时，才加载财务规则；否则不占用上下文 token。Agent Skill 从使用到原理，一次讲清
你可以在 references/FINANCE.md 放合规规则（越聚焦越好）：
# Finance policy (excerpt)## Hotel- Max hotel reimbursement: 500 CNY per night## Meals- Max meal reimbursement: 300 CNY per person per day

然后在 SKILL.md 里写清楚“何时读取”与“读完要做什么”，并用相对路径引用：Agent Skills Specification
## Finance CheckOnly when budget, procurement, expenses, reimbursement, or payments are mentioned:- Read [references/FINANCE.md](references/FINANCE.md)- Flag any amount that exceeds the policy and specify the required approver

高级用法二：Script（执行自动化，而不是堆更多上下文）视频强调了 Script 的一个关键点：脚本是被执行的，通常不会被当作长文本读进上下文；因此你可以把复杂逻辑放到脚本里，避免把上下文“塞爆”。Agent Skill 从使用到原理，一次讲清
例如 scripts/upload.py（示例为安全起见使用环境变量，不要硬编码密钥）：
import osimport sysimport jsonimport urllib.requestdef main() -&gt; int:    upload_url = os.environ.get(&quot;UPLOAD_URL&quot;, &quot;https://example.com/upload&quot;)    token = os.environ.get(&quot;UPLOAD_TOKEN&quot;, &quot;&quot;)    content = sys.stdin.read()    if not content.strip():        print(&quot;No content to upload.&quot;, file=sys.stderr)        return 2    payload = json.dumps(&#123;&quot;content&quot;: content&#125;).encode(&quot;utf-8&quot;)    req = urllib.request.Request(upload_url, data=payload, method=&quot;POST&quot;)    req.add_header(&quot;Content-Type&quot;, &quot;application/json&quot;)    if token:        req.add_header(&quot;Authorization&quot;, f&quot;Bearer &#123;token&#125;&quot;)    with urllib.request.urlopen(req, timeout=30) as resp:        body = resp.read().decode(&quot;utf-8&quot;, errors=&quot;replace&quot;)        print(body)    return 0if __name__ == &quot;__main__&quot;:    raise SystemExit(main())


警告：任何“会产生副作用”的脚本（发请求、改线上、发消息）都要做权限与确认机制。不同平台对脚本执行、自动批准（auto-approve）有不同的安全控制，务必先读文档、再在受控环境中试运行。Use Agent Skills in VS Code

Skills 的制作（含最佳实践）目前 Skills 的制作方式大致可以分成两类：

本地制作（以 Claude Code 为代表）：借助 skill-creator 等技能，在本地文件系统里生成&#x2F;迭代 Skill（更可控，但对工具与环境有一定门槛）。
云端制作（以扣子为代表）：全程自然语言对话，在云端沙盒中生成&#x2F;调试&#x2F;部署 Skill（上手更轻量，适合大多数“先做出来再优化”的场景）。

在 Claude Code 中制作 Skills（以 skill-creator 为核心）这里默认你已经安装并能正常使用 Claude Code。若是初次接触，可以先参考官方文档的安装与能力说明，再回到本文继续（重点在“如何把流程固化成 Skill”）。
案例一：做一个「英文博客翻译」Skill（把固定流程固化为可复用 SOP）这是一个典型的“高频处理 + 可复用流程”任务：你经常读英文博客，遇到好文章想保存为中英双语两份文档，步骤通常比较固定：

获取博客内容并保存为英文版 Markdown
基于英文 Markdown 翻译为中文
审阅译文并润色，输出中文最终版

Step 1：安装 skill-creator

仓库：https://github.com/anthropics/skills/tree/main/skills/skill-creator

Step 2：梳理需求并写成“可执行提示词”
为了让模型更稳定地产出你想要的 Skill，建议把提示词拆成 4 类信息（越明确越不跑偏）：

用途：这个 Skill 用来解决什么问题（做什么）。
触发方式：什么场景&#x2F;用户表达会触发（什么时候用）。
输出要求：输出格式、命名规则、保存位置、边界条件（怎么输出）。
所需资源（可选）：脚本、术语表、润色规则、参考资料（用什么支撑它做对）。

下面是一份“英文博客翻译”Skill 的真实提示词示例（可直接复制给 skill-creator 使用，后续再逐步补齐脚本&#x2F;术语表&#x2F;润色规则）：
我要创建一个「英文博客翻译」Skill，这个Skill可以获取任意英文博客URL并将它转换为Markdown文件保存在本地，同时支持将英文Markdown文件翻译为中文保存在同一目录下。1、核心流程：步骤一：获取英文博客内容并保存为英文版Markdown（用python脚本固定）步骤二：根据英文版Markdown翻译为中文（将翻译prompt固定）步骤三：审阅译文并润色输出为中文最终版（将润色prompt和术语表固定，稍后提供）2、输出文件：一份英文版Markdown，一份中文版Markdown3、输出文件要求：保存的Markdown文件按域名组织，例如：&lt;项目根目录&gt;/└── &lt;domain&gt;/    └── &lt;blog&gt;.md    └── &lt;中译-blog&gt;.md- &lt;domain&gt;：即站点名，如github.com、mp.weixin.qq.com- 英文版Markdown文件命名：&lt;blog&gt;，从页面标题（首选）或 URL 路径中提取信息，转换为 kebab-case 格式，2-6 个单词- 中文版Markdown命名：&lt;中译-blog&gt;，即在英文版Markdown文件命名前加上  中译- 进行连接4、触发时机：当用户想要保存并翻译博客文章时使用。


小建议：提示词里“输出文件要求”一定要写得非常具体（保存位置、命名规则、目录结构），否则模型很容易在这一步自由发挥，导致后续脚本&#x2F;测试都不好收敛。

Step 3：用 skill-creator 生成 Skill
把 Step 2 的提示词交给 Claude Code。若你发现它触发了“别的 Skill”（比如触发到 superpowers），通常意味着：

你的 description&#x2F;触发条件不够精准，关键词与意图描述太泛；
或你同时安装了多个相近技能，导致匹配冲突。

解决思路很简单：收敛 description、加上更具体的触发语句与关键词，必要时把某些技能设为 disable-model-invocation: true 只允许手动调用。
Step 4：测试并迭代
建议重点验证两个稳定性：

稳定触发：在预期场景下是否能触发（多换几种用户表达方式）。
稳定执行：流程是否会跳步&#x2F;漏步、输出是否符合命名与目录要求（多换几个不同站点&#x2F;不同结构的博客 URL）。

Step 5：分享（可选）：制作 Plugin&#x2F;Marketplace
如果你希望把自定义 Skill 分享给更多人，除了直接分享本地文件，也可以按官方插件文档把它打包成可安装&#x2F;可更新的形式：

文档：https://code.claude.com/docs/en/plugins

案例二：做一个「Chrome 插件图标」Skill（先验证脚本，再打包进 Skill）这个案例的目标很工程化：解决“AI 写 Chrome 插件但总忘了配置图标&#x2F;图标格式不对”的问题，并支持两种输入路径：

有现成 SVG：直接把 SVG 转成 Chrome 插件所需的 4 种 PNG 尺寸（16×16、32×32、48×48、128×128）。
没有现成 SVG：先在图标库搜索并下载 SVG，再做格式转换。

经验教训是：这类技能更适合先把脚本跑通，再让 skill-creator 把脚本与说明一起“封装进 Skill”。你可以把流程拆成更可复用的“原子能力”：

搜索&#x2F;下载图标：一个脚本负责（输入关键词，输出 SVG）
格式转换：另一个脚本负责（输入 SVG，输出多尺寸 PNG）

这样就能同时兼容“只想转格式”和“需要从零找图标”两种场景，而不是被一个大而全脚本绑死。
参考（文字版）：https://mp.weixin.qq.com/s/7VUIzD0tnkImN8atADEGRg
在扣子中制作 Skills（自然语言对话 + 云端沙盒）扣子这类平台的优势是门槛更低：你可以先用自然语言把“需求 → 技能 → 调试 → 部署”跑通，再逐步把规则写得更严谨、更可复用。
案例一：一次性提示词生成「中英文混合排版优化」Skill典型痛点：中文与【英文&#x2F;数字】之间需要保留 1 个半角空格，手动改很重复。它特别适合做成 Skill，因为规则明确、可批量复用。
制作要点同样是 4 件事：用途、触发方式、输出要求、所需资源。然后在右侧调试区用多份真实文本测试：

稳定触发：在用户说“帮我优化中英文内容排版”等表达时是否能加载技能
稳定执行：是否能一致地补空格、且不引入额外格式破坏

完成后部署上线，即可在对话中直接调用。
案例二：多轮对话得到满意产物，再“固化为 Skill”当你一开始说不清楚规范时，可以先用多轮对话把产物打磨到满意（例如：初翻 → 译后审校 → 最终润色），然后再让平台把“对话流程”总结并打包成 Skill。
需要注意的是：对话固化时可能会丢失细节（平台会做摘要&#x2F;归纳），因此最终一定要用新样例做回归测试，必要时把关键规则（术语表、语体要求、输出结构）显式写进 Skill 里。
一个比较稳的做法是把“审校维度”和“术语表”明确下来，让 Skill 有可复用的约束条件。例如审校维度可以固定为：准确性、流畅性、风格适配、术语统一；术语表可以作为可选输入参数传入（或固化在 references/ 中）。示例术语映射（节选）：

AGI → 通用人工智能
LLM&#x2F;Large Language Model → 大语言模型
Generative AI → 生成式 AI
AI Agent → AI 智能体
few-shot → 少样本学习
fine-tuning → 微调
Agent Skills → Agent Skills
Skills → Skills

同时把输出结构写死也很重要，例如固定输出为：原始译文 + 审校报告 + 润色后的最终译文。这样测试时更容易判断“有没有漏步骤&#x2F;跑偏”。
另外，云端平台通常会提供版本记录与回滚能力：当你某次迭代不满意时，可以直接回到上一版本，减少试错成本。
Skills 制作最佳实践（整理自官方建议）这里的要点主要来自官方的“Skill authoring best practices”与渐进式披露思想，适合当作“写 Skill 的默认规范”。Skill authoring best practices
核心原则
简洁是关键：只添加 AI 原本不知道、但对任务质量至关重要的上下文。
设定适当的自由度：任务越脆弱&#x2F;越容易出错，越应该减少自由度（更多脚本&#x2F;更明确格式）；任务越开放，越可以保留弹性（更多原则&#x2F;示例&#x2F;边界条件）。

结构建议（SKILL.md 与资源层）
name 命名清晰：尽量用动名词&#x2F;可执行动作表达（如 processing-pdfs、writing-documentation），避免 helper、documents 这类含糊名称。
description 写清“做什么 + 什么时候用”：让 agent 能稳定匹配触发条件。
渐进式披露：SKILL.md 正文建议控制在 500 行以内；接近上限时，把长内容拆到 references/。
资源层一层深度：资源文件建议放在与 SKILL.md 同级的一级目录（如 references/、scripts/），并且在 SKILL.md 中显式链接&#x2F;引用，确保 agent “需要时能读到”。
资源文件超过 100 行加目录：在文件顶部加 TOC，方便预览截断时仍能定位到信息。

工作流与反馈回路
把复杂任务拆成清晰步骤：必要时提供可复制的检查清单，让 agent 执行时逐项勾选。
引入反馈回路：用“运行验证器 → 修复错误 → 重复”的循环，显著提升稳定性与一致性。
双 Agent 开发：用 Agent A 负责创建&#x2F;修改 Skill，用 Agent B 负责黑盒测试与挑错，把问题再反馈给 Agent A 迭代。

发布前自检清单（可直接复用）核心质量

 description 具体，包含关键术语
 description 同时包含“功能”与“使用时机”
 SKILL.md 正文在 500 行以内
 长细节拆分到独立文件（如有需要）
 全 Skill 术语一致
 示例具体，不抽象
 文件引用保持一层深度
 渐进式披露使用得当
 工作流步骤清晰

代码与脚本

 脚本是为了解决问题，而不是把问题“推回给 Claude”
 错误处理清晰、可定位、提示有帮助
 没有“神秘常量”（所有关键值都有合理解释&#x2F;来源）
 列出并验证所需依赖&#x2F;软件包
 脚本文档清晰（如何运行、输入输出、失败模式）
 避免 Windows 风格路径（统一使用正斜杠 /，提升跨平台复用性）
 关键操作有验证步骤（例如生成文件后检查是否存在&#x2F;尺寸是否正确）

测试

 至少用 3 个真实样例做评估
 在不同模型&#x2F;配置上做过测试（如适用）
 覆盖真实使用场景与边界情况
 团队&#x2F;同伴反馈已纳入（如适用）

什么任务值得做成 Skills？一个更“工程化”的判断标准是：当你需要 Claude 持续、高效、稳定地执行某个特定任务时，就值得用 Skill 来固化流程。Skills explained
典型适用方向包括：

组织工作流程：品牌指南、合规流程、文档模板
专业领域：Excel 公式、PDF 处理、数据分析
个人偏好：笔记系统、编码模式、研究方法

结合社区安装热度（如 skills.sh 的榜单）常见任务类型大致呈现如下分布（示例统计）：



类型
数量



增长&#x2F;营销（SEO&#x2F;CRO&#x2F;投放&#x2F;定价&#x2F;增长策略）
23


软件工程：前端&#x2F;全栈 UI 与最佳实践
19


内容生产（图文&#x2F;幻灯片&#x2F;图片&#x2F;媒体处理&#x2F;社媒分发）
17


工程方法论（计划&#x2F;执行&#x2F;协作&#x2F;评审&#x2F;调试等流程）
13


文档与办公（PDF&#x2F;DOCX&#x2F;PPTX&#x2F;XLSX&#x2F;文档协作）
8


Agent 工程（MCP&#x2F;工具链&#x2F;浏览器自动化等元技能）
5


软件工程：移动开发（Expo&#x2F;RN&#x2F;iOS&#x2F;Android）
4


软件工程：DevOps&#x2F;发布&#x2F;CI&#x2F;CD&#x2F;依赖维护
4


软件工程：后端&#x2F;API&#x2F;数据与数据库
4


沟通与管理（会议&#x2F;汇报&#x2F;内部沟通）
2


软件工程：测试&#x2F;质量保障&#x2F;验证（偏测试方法）
1


从任务特征上看，值得做成 Skill 的任务通常落在三类（可重叠）：

高频处理 + 可复用流程：步骤固定、目标明确（如 code review、CI&#x2F;CD、SEO 审计）。
强模板化产出：产物格式&#x2F;风格强约束（如报告、周报、PPT、品牌设计）。
多模块&#x2F;多流程组合：需要串并联多个子流程（如社媒发布：选题 → 写作 → 配图 → 分发）。

最简单的二分法也很好用：一次性任务不值得做 Skill；需要反复做的任务，优先固化成 Skill。实践中建议遵循这个顺序：

手动跑通，沉淀标准：先手动执行几次，厘清固定步骤与高质量标准。
示范教学，封装 Skill：在支持 Skills 的工具里完整演示一遍，让 agent 把流程封装为 Skill。
循环使用，持续迭代：后续直接调用 Skill；每次输出后做校验，按反馈不断修正。

Skill 放哪儿？不同工具的常见落地点同一个 Skill 的目录结构与 SKILL.md 格式是通用的（Agent Skills 标准），但各工具的“扫描路径”和“优先级”不同。
Claude CodeClaude Code 文档给出了清晰的层级与路径：个人级 ~/.claude/skills/&lt;skill-name&gt;/SKILL.md，项目级 .claude/skills/&lt;skill-name&gt;/SKILL.md，并支持用 /skill-name 显式调用。Extend Claude with skills
OpenAI CodexCodex 支持从 .codex/skills（以及用户&#x2F;管理员等多个层级）加载 skills，并支持显式&#x2F;隐式调用两种模式。Agent Skills (Codex)
VS Code GitHub CopilotVS Code 推荐把项目 skills 放在 .github/skills/（同时兼容 .claude/skills/ 作为 legacy），并且目前处于预览阶段，需要开启 chat.useAgentSkills。Use Agent Skills in VS Code
CursorCursor 也提供了 Agent Skills 的落地文档，并且把“目录结构、触发方式、如何查看&#x2F;安装&#x2F;迁移”写得很具体。Agent Skills | Cursor Docs
1) 技能目录（项目级 vs 全局）根据 Cursor 文档，Cursor 会自动从以下位置发现并加载技能（优先用 .cursor/skills/ 做项目级管理，兼容 .claude/skills/ 和 .codex/skills/ 便于跨工具复用）：Agent Skills | Cursor Docs

项目级：.cursor/skills/
项目级（兼容 Claude）：.claude/skills/
项目级（兼容 Codex）：.codex/skills/
用户级（全局）：~/.cursor/skills/
用户级（全局，兼容 Claude）：~/.claude/skills/
用户级（全局，兼容 Codex）：~/.codex/skills/

2) 创建一个项目级 Skill（推荐）Cursor 推荐的最小目录结构如下（每个 Skill 是一个文件夹，包含 SKILL.md）：Agent Skills | Cursor Docs
.cursor/  skills/    meeting-minutes/      SKILL.md

SKILL.md 的写法与 Agent Skills 标准一致（YAML frontmatter + Markdown 正文）。示例（内容建议用英文写清楚关键词，提升匹配命中率）：
---name: meeting-minutesdescription: Summarize meeting transcripts into attendees, agenda items, decisions, and action items. Use when the user asks for meeting minutes or a meeting summary.---## Output format### Attendees- &lt;name&gt;### Agenda1. &lt;topic&gt;### Decisions- &lt;decision&gt;### Action Items- &lt;owner&gt;: &lt;task&gt; (due: &lt;date or TBD&gt;)

3) 在 Cursor 里如何触发 SkillsCursor 文档明确了两种方式：Agent Skills | Cursor Docs

自动触发：Cursor 启动时会发现并加载技能，Agent 会“看到”所有技能，并根据当前上下文决定何时调用。
手动触发：在 Agent 对话中输入 /，搜索技能名称并显式调用。

如果你希望把某个 Skill 变成“只允许手动调用”的斜杠命令效果，可以在 frontmatter 里加 disable-model-invocation: true，这样它只有在 /skill-name 显式调用时才会被包含进上下文。Agent Skills | Cursor Docs
4) 如何在 Cursor UI 里查看已发现的 SkillsCursor 提供了一个查看入口，便于排查“为什么我写了 Skill 但 Agent 没用”：Agent Skills | Cursor Docs

打开 Cursor Settings（Windows&#x2F;Linux：Ctrl+Shift+J）
进入 Rules
技能会显示在 Agent Decides 部分

5) 从 GitHub 安装&#x2F;导入 Skills（团队共享）如果你想把 skills 放在一个 GitHub 仓库里共享给团队，Cursor 文档给出的操作路径是：Agent Skills | Cursor Docs

Cursor Settings → Rules
在 Project Rules 部分点击 Add Rule
选择 Remote Rule (Github)
输入 GitHub 仓库 URL

6) 从 Rules&#x2F;Slash Commands 迁移到 Skills（Cursor 2.4+）如果你之前已经积累了不少“动态规则（Apply Intelligently）”或斜杠命令，Cursor 2.4 内置了 /migrate-to-skills，可以辅助迁移到 .cursor/skills/。Agent Skills | Cursor Docs
文档也给了迁移的边界条件（建议迁移前先读一遍，避免误解）：例如 alwaysApply: true 或带特定 globs 的规则不会被迁移；用户规则也不会被迁移（因为不存储在文件系统中）。Agent Skills | Cursor Docs
共享&#x2F;可复用的 Agent Skills 项目（GitHub）如果你不想从零写 Skill，最实用的方式就是“先复用、再改成自己的版本”。这里把你列出的 GitHub 项目按定位做了分组，并给出我建议的阅读顺序（先官方&#x2F;准官方 → 再精选合集 → 最后按领域挑专用 skills）。

注意：无论从哪里拉取 skills，都建议先人工 Review 一遍 SKILL.md 与 scripts/，特别是会产生副作用的脚本（发请求、改配置、部署、删数据等）。必要时把这类技能设为 disable-model-invocation: true，只允许手动触发。

0) Skills 目录站（可检索&#x2F;发现）
Skillsmp：Skills 目录&#x2F;发现站（可检索&#x2F;浏览）  
网站：https://skillsmp.com/



1) 官方&#x2F;准官方 Skills Catalog（优先看）
Anthropic Agent Skills：anthropics/skills（公共 Agent Skills 仓库&#x2F;示例&#x2F;规范相关目录）  
GitHub：https://github.com/anthropics/skills


OpenAI Codex Skills Catalog：openai/skills（Codex Skills Catalog）  
GitHub：https://github.com/openai/skills


Hugging Face Skills：huggingface/skills  
GitHub：https://github.com/huggingface/skills



2) 社区“技能包&#x2F;技能合集”（适合快速找灵感&#x2F;模板）
Awesome Claude Skills（聚合列表）：ComposioHQ/awesome-claude-skills  
GitHub：https://github.com/ComposioHQ/awesome-claude-skills


200+ Skills 合集：sickn33/antigravity-awesome-skills（覆盖 Claude Code&#x2F;Antigravity&#x2F;Cursor 等生态）  
GitHub：https://github.com/sickn33/antigravity-awesome-skills


安全审计&#x2F;漏洞方向 Skills：trailofbits/skills（安全研究&#x2F;漏洞检测&#x2F;审计工作流）  
GitHub：https://github.com/trailofbits/skills


Claude Code 配置+skills&#x2F;hooks&#x2F;MCP 汇总：affaan-m/everything-claude-code  
GitHub：https://github.com/affaan-m/everything-claude-code



3) 面向特定产品&#x2F;工程场景的 Skills（更“即插即用”）
Obsidian Skills：kepano/obsidian-skills  
GitHub：https://github.com/kepano/obsidian-skills


Expo 项目技能：expo/skills（Expo 项目与 EAS 相关）  
GitHub：https://github.com/expo/skills


Remotion Agent Skills：remotion-dev/skills  
GitHub：https://github.com/remotion-dev/skills


n8n 工作流技能：czlonkowski/n8n-skills  
GitHub：https://github.com/czlonkowski/n8n-skills


UI&#x2F;UX Pro Max Skill：nextlevelbuilder/ui-ux-pro-max-skill（面向 UI&#x2F;UX 设计的风格、配色、字体与组件建议能力集）  
GitHub：https://github.com/nextlevelbuilder/ui-ux-pro-max-skill



4) AutoGen Studio（可直接复制粘贴的 skills）
madtank/autogenstudio-skills  
GitHub：https://github.com/madtank/autogenstudio-skills


aj47/autogen-studio-skills  
GitHub：https://github.com/aj47/autogen-studio-skills



5) 在 Cursor 里复用这些仓库的推荐姿势结合 Cursor 的 skills 目录约定（项目级 .cursor/skills/ + 全局 ~/.cursor/skills/）以及其 GitHub 导入入口，你通常有两种落地方式：Agent Skills | Cursor Docs

直接复制某个 skill 文件夹到你的项目里（最稳、最可控）：

# Copy a skill folder into your project-level skills directorymkdir -p .cursor/skillscp -R /path/to/repo/skills/&lt;skill-name&gt; .cursor/skills/


用 Cursor 的 GitHub 导入入口做团队共享：Cursor Settings → Rules → Project Rules → Add Rule → Remote Rule (Github)，把 skills 仓库当作“可版本控制的共享来源”。

6) 实用 Skills 推荐（案例向）下面这些是我认为“上手就能用、并且很适合拿来学习工作流拆解”的技能。它们覆盖了 UI 设计、头脑风暴&#x2F;方案设计、上下文工程、自动循环执行、个人知识库写作、笔记体系、React 工程规范、浏览器自动化，以及“用 Skill 来创建 Skill”等方向。

提醒：一些技能仓库会带 scripts/ 或插件机制（hooks&#x2F;stop hook）。首次使用前建议先扫一遍代码，明确它会不会产生副作用（发请求、写文件、改 git、执行命令等）。

1) frontend-design（UI 设计风格纠偏）适合用来“纠正 AI 总爱出紫色渐变 UI”的问题，尤其在你想要更现代、更克制、更像真实产品的前端设计时很有效。

Frontend-design skill：https://github.com/anthropics/claude-plugins-official/tree/main/plugins/frontend-design/skills/frontend-design
相似能力技能：ui-ux-pro-max-skill：https://github.com/nextlevelbuilder/ui-ux-pro-max-skill

2) superpowers（把头脑风暴与工程化流程做成“固定工作流”）它解决的不是“会不会写代码”，而是“方案设计阶段问得够不够细、够不够准”，并且把从设计到开发、测试、评审的流程做成了可复用的工作流模板，非常值得当作“团队标准流程”的参考。

Superpower skills：https://github.com/obra/superpowers
工作流亮点（值得抄作业）：
brainstorming：结构化对话把粗想法变成可落地设计，探索多方案、权衡、增量验证
using-git-worktrees：用 git worktrees 隔离不同功能分支，避免互相干扰
writing-plans：把工作拆到具体文件路径、完整代码与验证步骤
执行开发：可选子代理驱动开发（任务间审查）或批量执行计划（人工分批审批）
test-driven-development：强制 RED-GREEN-REFACTOR，防止过度设计并确保每步可验证
requesting-code-review：按严重等级输出审查问题（Critical&#x2F;Important&#x2F;Minor）
finishing-a-development-branch：收尾合并&#x2F;PR&#x2F;保留&#x2F;丢弃，并清理 worktrees



3) planning-with-files（用文件系统做“外部工作记忆”）这是典型的“上下文工程”思路：把阶段计划、研究记录、最终交付拆成独立文件，让 agent 在长任务里不容易漂移，也更容易断点续跑。

planning-with-files：https://github.com/OthmanAdi/planning-with-files

4) ralph-loop &#x2F; ralph-wiggum（循环执行：一次启动，自动迭代到目标）ralph-loop 是 Anthropic 官方插件仓库里的循环执行器，核心思路是用 Stop Hook 拦截退出，并把相同提示词再次喂回去，让 Claude Code 反复迭代（读代码、看 git、修测试）直到达成目标或达到预设次数。

ralph-loop：https://github.com/anthropics/claude-plugins-official/tree/main/plugins/ralph-loop
ralph-wiggum：https://github.com/anthropics/claude-code/tree/main/plugins/ralph-wiggum
推荐组合用法：先用 planning-with-files 产出稳定计划&#x2F;里程碑文件，再用 /ralph-loop 做自动迭代，能显著降低“跑偏”的概率。

5) notebooklm-skill（把 NotebookLM 的“知识库能力”带到支持 Skills 的工具）适合 AI 写作、以及任何需要高质量 RAG&#x2F;知识库的场景：你可以把资料组织在 NotebookLM&#x2F;类似体系里，再用 Skill 把能力带到 Claude Code 等支持 Skills 的工具中。

NotebookLM Skill：https://github.com/PleasePrompto/notebooklm-skill

6) obsidian-skills（把 Obsidian 工作流技能化）Obsidian 团队&#x2F;社区围绕“如何让 AI 更好地融入 Obsidian”做了很实用的技能集合，尤其适合做知识管理、知识可视化与结构化笔记。

Obsidian Skills：https://github.com/kepano/obsidian-skills
包含的代表性技能：
Obsidian Markdown Skill：输出 Obsidian 风格 Markdown（wikilinks、embeds、callouts、properties 等）
Obsidian Bases Skill：把笔记变成数据库（表格&#x2F;卡片&#x2F;列表&#x2F;地图等视图）
JSON Canvas Skill：用 Canvas 做思维导图&#x2F;流程图，更适合知识可视化



7) react-best-practices（Vercel 的 React 工程“经验浓缩包”）如果你在做 React 项目（尤其是 Next.js 生态），这类“最佳实践 Skill”比泛化建议更稳定，适合当作 code review 的“默认标准”。

介绍：https://vercel.com/blog/introducing-react-best-practices
React Best Practices skill：https://github.com/vercel-labs/agent-skills/tree/main/skills/react-best-practices

8) agent-browser（轻量交互式浏览器壳层）适合一次性任务或简单自动化：相对“重量级的浏览器自动化方案”，它更轻、交互成本更低，很多情况下也更省 token。

Agent Browser：https://github.com/vercel-labs/agent-browser

9) skill-creator（用 Skill 来创建 Skill）这是 Anthropic 官方提供的“创建 Skill 的 Skill”。你后面如果要批量沉淀团队工作流，强烈建议先用它做一次标准化的目录结构与写法练习，再逐步形成自己的模板。

skill-creator：https://github.com/anthropics/skills/tree/main/skills/skill-creator

7) 高质量 Skills 资源库（用来“找技能&#x2F;学写法&#x2F;抄工作流”）除了本文前面列出的官方&#x2F;社区仓库，这里补充一批我认为质量更稳定、覆盖更广、也更适合检索的资源库与目录站：
Claude &#x2F; Anthropic 官方与准官方
Anthropic Skills（早期官方仓库）：https://github.com/anthropics/skills
Anthropic 插件&#x2F;skills 集合（更新更活跃）：https://github.com/anthropics/claude-plugins-official

awesome-claude-skills（合集类索引）GitHub 的 awesome 系列通常是“找灵感&#x2F;找同类方案”的最快入口，这里是多个 stars 较高的合集：

https://github.com/ComposioHQ/awesome-claude-skills
https://github.com/travisvn/awesome-claude-skills
https://github.com/BehiSecc/awesome-claude-skills
https://github.com/VoltAgent/awesome-claude-skills

skillsmp（超大规模收录 + 语义搜索）目前收录量非常大，并支持语义搜索与筛选&#x2F;排序，适合“我知道我想要什么能力，但不知道技能叫啥”的场景。

skillsmp：https://skillsmp.com

skills.sh（安装排行榜 + 快速安装生态）把多个技能项目整合成网站，并提供安装排行榜（总榜&#x2F;24h 榜），适合快速捕捉近期热门技能。

skills.sh：https://skills.sh/

科研方向资源库（垂直领域）如果你是科研&#x2F;工程研究方向，这个集合覆盖了很多学科的技能模板与流程。

claude-scientific-skills：https://github.com/K-Dense-AI/claude-scientific-skills

个人高质量工作流仓库（适合学拆解）个人开发者的技能仓库往往更“工作流导向”，特别适合学习如何把真实生产流程拆成可复用的 Skill。

@宝玉 Skills：https://github.com/JimLiu/baoyu-skills/tree/main/skills

moltbot（clawbot）生态技能索引如果你在用 moltbot（原 clawbot）相关生态，这个索引比较集中：

GitHub：https://github.com/VoltAgent/awesome-moltbot-skills
在线版：https://clawdhub.com/skills

Skill vs MCP：到底该选哪个？视频引用了 Anthropic 的一句话，非常精准：Skills explained

MCP connects Claude to data; Skills teach Claude what to do with that data.

把它翻译成工程视角就是：

MCP：解决“连得上”——把 agent 接到数据源&#x2F;业务系统&#x2F;工具上，让它能读写、能查、能操作。
Skills：解决“会做事”——把流程、规范、模板、判断规则固化下来，让 agent 用一致的方法处理数据并产出结果。

选型建议（实践向）
如果你现在的问题是“模型总是忘了规则&#x2F;输出不稳定&#x2F;每次都要贴提示词”——优先写 Skill。
如果你现在的问题是“模型拿不到数据&#x2F;无法执行操作&#x2F;需要对接系统”——优先上 MCP。
如果你要做的是“既要拿数据，又要按公司流程处理并输出”——Skill + MCP 组合通常最强：MCP 负责连接与能力暴露，Skill 负责步骤、格式、合规、边界条件。

常见坑与最佳实践清单
name&#x2F;目录名不一致：规范要求 name 必须匹配父目录名，且必须小写+连字符。Agent Skills Specification
description 太泛：description 应该同时描述“做什么”和“什么时候用”，并包含关键词以便检索与匹配。Agent Skills Specification
多个 Skill 意图相近导致误触发：当你安装了多个“功能看起来差不多”的技能时，如果触发条件写得不够具体，就可能调用错 Skill。解决方式是收敛 description、补充更明确的触发语句与关键词，必要时对某些技能启用 disable-model-invocation: true 只允许手动调用。
SKILL.md 太长：把长材料拆到 references/，保持主文件精炼，避免触发后上下文暴涨。Agent Skills Specification
带副作用的 Skill 让模型自动触发：在 Claude Code 里可以用 frontmatter 控制自动触发（例如 disable-model-invocation: true），把“部署&#x2F;提交&#x2F;发布”等动作收回到人工显式调用。Extend Claude with skills
“对话固化为 Skill”信息丢失：云端平台把多轮对话总结为 Skill 时，可能会丢掉关键细节（术语表、输出结构、边界条件）。建议把关键规则写成显式约束（或下沉到 references/），并用新样例做回归测试。
脚本依赖不清：脚本必须写清运行方式、依赖与错误提示，否则 agent 可能会退回去“读代码找怎么跑”，反而浪费上下文并引入风险（视频也提到了这一点）。Agent Skill 从使用到原理，一次讲清

总结
Agent Skill &#x3D; 可移植的能力包：指令（SKILL.md）+ 参考资料（references&#x2F;）+ 自动化脚本（scripts&#x2F;）。
渐进式披露是精髓：先元数据、再指令、最后按需资源，既复用又省上下文。
Skill vs MCP 不纠结：MCP 负责连接数据与工具，Skill 负责流程与规则；多数工程场景是组合拳。

参考资料
视频：Agent Skill 从使用到原理，一次讲清
Agent Skills 概览
Agent Skills Specification
Anthropic：Skills explained（含 Skill vs MCP）
Claude Code：Extend Claude with skills
OpenAI Codex：Agent Skills
VS Code：Use Agent Skills in VS Code
Cursor Docs：Agent Skills
飞书文档：Agent Skill 参考文档
飞书文档：实用 Skills &amp; Skills 资源（持续更新）
GitHub：anthropics&#x2F;claude-plugins-official
GitHub：obra&#x2F;superpowers
GitHub：OthmanAdi&#x2F;planning-with-files
GitHub：PleasePrompto&#x2F;notebooklm-skill
Vercel：Introducing React Best Practices
GitHub：vercel-labs&#x2F;agent-skills
GitHub：vercel-labs&#x2F;agent-browser
网站：skills.sh
Claude Code：Plugins
Claude Docs：Skill authoring best practices
微信文章：Chrome 插件图标 Skill（文字版）
GitHub：travisvn&#x2F;awesome-claude-skills
GitHub：BehiSecc&#x2F;awesome-claude-skills
GitHub：VoltAgent&#x2F;awesome-claude-skills
GitHub：K-Dense-AI&#x2F;claude-scientific-skills
GitHub：JimLiu&#x2F;baoyu-skills
GitHub：VoltAgent&#x2F;awesome-moltbot-skills
网站：clawdhub.com&#x2F;skills
GitHub：anthropics&#x2F;skills
GitHub：openai&#x2F;skills
GitHub：huggingface&#x2F;skills
GitHub：ComposioHQ&#x2F;awesome-claude-skills
GitHub：sickn33&#x2F;antigravity-awesome-skills
GitHub：trailofbits&#x2F;skills
GitHub：affaan-m&#x2F;everything-claude-code
GitHub：kepano&#x2F;obsidian-skills
GitHub：expo&#x2F;skills
GitHub：remotion-dev&#x2F;skills
GitHub：czlonkowski&#x2F;n8n-skills
GitHub：nextlevelbuilder&#x2F;ui-ux-pro-max-skill
GitHub：madtank&#x2F;autogenstudio-skills
GitHub：aj47&#x2F;autogen-studio-skills


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>Agent Skills</tag>
        <tag>Claude Code</tag>
        <tag>GitHub Copilot</tag>
        <tag>Codex</tag>
        <tag>MCP</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker BuildKit 新特性与最佳实践：更快构建、更强缓存与供应链元数据</title>
    <url>/posts/92b9c5787fcf.html</url>
    <content><![CDATA[写在前面：BuildKit&#x2F;Buildx 到底是什么关系？很多人日常只用 docker build，但真正让构建“又快又稳又可审计”的关键在 BuildKit。

BuildKit：Docker 的下一代构建引擎，提供并行构建、增量缓存、RUN --mount、远程缓存导入导出、供应链元数据（SBOM&#x2F;Provenance）等能力。
Buildx：Docker 的构建前端（CLI 插件&#x2F;子命令），用来驱动 BuildKit，并提供多平台、远程 builder、Bake 编排、导出&#x2F;推送、attestation 等高级功能。

截至 2026-01（以官方发布为准），Buildx 最新稳定版本仍在 0.30.x（2025-11 发布 0.30.0/0.30.1），其中包含了对 imagetools 与 attestation 持久化等能力的增强，以及对部分旧命令的弃用提示。
适用读者与版本建议本文面向：

需要在本地&#x2F;CI 中加速镜像构建的人
需要多架构镜像（amd64/arm64）的人
需要把缓存、SBOM、Provenance 一并纳入发布流程的人

版本建议（越新越好）：

Docker Engine &#x2F; Docker Desktop：建议使用近两年的版本
docker buildx：建议 0.29+，最好接近 0.30.x
Dockerfile frontend：建议显式声明 #syntax=docker/dockerfile:1.8（或更高）

一张图理解 BuildKit 的“新能力地图”BuildKit 的“新特性”可以按场景分成四类：

性能：并行、细粒度缓存、cache mount、减少上下文传输
可复现：更严格的 Dockerfile 解析&#x2F;校验、平台一致性检查
多平台：一次构建产出 linux/amd64 + linux/arm64 并推送 manifest list
供应链：Provenance（SLSA 语义）与 SBOM attestation，提升可审计性

下面按“你能立刻用起来”的角度展开。
新特性 1：RUN --mount 让构建更快（缓存&#x2F;Secret&#x2F;SSH）1.1 缓存挂载：把“下载依赖”从每次构建中剥离出来以 Node.js 为例，传统 RUN npm ci 每次都会重新下载依赖（或者依赖层缓存命中不稳定）。BuildKit 的 cache mount 可以把下载缓存放到一个专用缓存里，让构建在本地和 CI 都更稳。
# syntax=docker/dockerfile:1.8FROM node:20-bookworm-slim AS buildWORKDIR /appCOPY package.json package-lock.json ./# Cache npm download directory to speed up builds.RUN --mount=type=cache,target=/root/.npm \    npm ciCOPY . .RUN npm run build

要点：

cache mount 不会进入最终镜像层，仅用于加速构建步骤
对 Go &#x2F; Maven &#x2F; pip 等生态同样有效（目标目录不同）

1.2 Secret 挂载：把 Token 从镜像层里彻底移除很多团队会在构建时拉私有依赖（NPM token、pip index token、GitHub token）。正确做法是用 --secret 在构建阶段临时注入，不落盘进镜像层。
# syntax=docker/dockerfile:1.8FROM alpine:3.20RUN --mount=type=secret,id=my_token,target=/run/secrets/my_token \    sh -c &#x27;test -s /run/secrets/my_token &amp;&amp; echo &quot;Secret is present&quot;&#x27;

构建时传入 secret：
# Pass secret from local file.docker buildx build \  --secret id=my_token,src=./my_token.txt \  -t demo:secret .

注意：

secret 只在该条 RUN 指令可见
不要用 ARG&#x2F;ENV 传敏感信息（容易进入层或被构建日志泄露）

1.3 SSH 挂载：安全拉取私有 Git 仓库当你需要 git clone 私有仓库时，可以用 SSH agent 转发，而不是把私钥复制进镜像。
# syntax=docker/dockerfile:1.8FROM alpine:3.20RUN apk add --no-cache git openssh-clientRUN --mount=type=ssh \    git clone git@github.com:YOUR_ORG/YOUR_PRIVATE_REPO.git /src

构建时启用：
# Forward default SSH agent socket.docker buildx build --ssh default -t demo:ssh .

多个 SSH Key 的常见做法做法 A：在同一个 ssh-agent 里加载多把 key（推荐）BuildKit 转发的是 SSH agent，因此多 key 场景最稳的方式是：在宿主机把多把 key 都 ssh-add 进同一个 agent，然后构建时仍然只需要 --ssh default。
# Start ssh-agent in current shell.eval &quot;$(ssh-agent -s)&quot;# Add multiple keys into the same agent.ssh-add ~/.ssh/id_ed25519_githubssh-add ~/.ssh/id_ed25519_gitlab# Build with forwarded agent.docker buildx build --ssh default -t demo:ssh .

说明：

agent 会根据目标主机与可用 key 自动尝试匹配；如果你有 GitHub&#x2F;GitLab&#x2F;自建 Git 多个来源，这是最省心的方式。
如果 key 带 passphrase，使用 agent 方式也更友好（避免在构建里交互输入）。

做法 B：用多个 --ssh id=... 分别转发不同 key（便于精确控制）如果你想在 Dockerfile 的不同步骤里明确使用不同的 SSH 凭据，可以为 --mount=type=ssh 指定 id，并在构建时传入多个 --ssh。
# syntax=docker/dockerfile:1.8FROM alpine:3.20RUN apk add --no-cache git openssh-clientRUN --mount=type=ssh,id=github \    git clone git@github.com:YOUR_ORG/YOUR_PRIVATE_REPO.git /src/githubRUN --mount=type=ssh,id=gitlab \    git clone git@gitlab.com:YOUR_ORG/YOUR_PRIVATE_REPO.git /src/gitlab

构建时分别指定不同 id 的来源：
# Forward different keys (or sockets) with different ids.docker buildx build \  --ssh github=$HOME/.ssh/id_ed25519_github \  --ssh gitlab=$HOME/.ssh/id_ed25519_gitlab \  -t demo:ssh-multi .

提示：

如果你使用的是 passphrase key，更推荐做法 A（agent），然后把 --ssh github=$SSH_AUTH_SOCK &#x2F; --ssh gitlab=$SSH_AUTH_SOCK 都指向同一个 agent socket（本质上仍由 agent 管理解密与选择）。

新特性 2：远程缓存（--cache-to/--cache-from）让 CI 构建“越跑越快”仅靠本地层缓存，CI 往往命中率很差。BuildKit 支持把缓存导出到 registry（或其他后端），再在后续构建导入。
下面是一个常见的 registry 缓存模式（适合 GitHub Actions&#x2F;Jenkins&#x2F;GitLab CI 等）：
# Export cache to registry and reuse in next builds.docker buildx build \  --platform linux/amd64,linux/arm64 \  --cache-from type=registry,ref=registry.example.com/myapp:buildcache \  --cache-to type=registry,ref=registry.example.com/myapp:buildcache,mode=max \  -t registry.example.com/myapp:$(git rev-parse --short HEAD) \  --push .

建议：

只在推送镜像时写入远程缓存，否则会产生无效写入
mode=max 缓存更完整但占用更大；可按项目权衡

新特性 3：多平台镜像从“麻烦事”变成“默认能力”Buildx 的典型价值之一就是多平台构建。你可以用一条命令构建并推送多架构镜像：
docker buildx build \  --platform linux/amd64,linux/arm64 \  -t registry.example.com/myapp:latest \  --push .

常见坑：

--load 只能加载单平台镜像到本地 Docker 镜像仓库（并且可能丢失部分元数据），多平台通常要用 --push。
某些依赖（如下载的二进制）需要按 TARGETARCH 分支处理，否则会构建出“能推送但不能运行”的镜像。

新特性 4：Dockerfile frontend（#syntax）持续进化：更强表达力 + 更严格校验Dockerfile 顶部的 #syntax=... 让你选择 Dockerfile frontend 版本，从而获得新语义和新能力。
截至近年的更新中，值得关注的方向包括：

变量展开能力增强：更接近 shell parameter expansion 的用法，写复杂镜像逻辑更可控（frontend 1.7+）。
更严格的校验与检查：新增&#x2F;增强检查规则，帮助在构建前发现平台不一致、ARG 扩展错误等问题（frontend 1.8+）。

实践建议：

业务镜像建议显式写 # syntax=docker/dockerfile:1.8（或你团队验证过的更高版本）
如果团队想把 Dockerfile 质量门禁做起来，可以研究 #check &#x2F; BUILDKIT_DOCKERFILE_CHECK 相关能力，在 CI 中先做校验再做构建

新特性 5：供应链元数据（SBOM &#x2F; Provenance）让镜像“可审计”5.1 为什么需要 SBOM 与 Provenance？当你需要回答这些问题时，SBOM&#x2F;Provenance 就很关键：

这个镜像里到底包含哪些依赖与组件？（SBOM）
这个镜像是用什么源码、什么构建参数、什么环境构建出来的？（Provenance）

BuildKit 已经支持为镜像生成并附加这些 attestation（in-toto 语义），并且与 Docker 的构建体系集成。
5.2 开启方式（示例）docker buildx build \  --platform linux/amd64,linux/arm64 \  --provenance=true \  --sbom=true \  -t registry.example.com/myapp:latest \  --push .

更精细的控制（示例，强调可读性）：
# Generate more detailed provenance (may expose more build details).docker buildx build \  --attest type=provenance,mode=max \  --attest type=sbom \  -t registry.example.com/myapp:latest \  --push .

重要提示：

注意：在某些导出方式下（例如 --load 或使用默认本地镜像存储），attestation 可能无法完整保留。生产发布链路建议以 --push 为主，并在你的镜像仓库侧验证元数据是否可见。

新特性 6：Buildx 的一些近期变化（截至 2025-11）从近一年的发布说明看，Buildx 的演进重点之一是“把构建产物的元数据保留下来，并让多工具链更好协作”。
你可能会关心的点：

imagetools create 对 attestation manifest 与 cosign 相关签名的保留增强（适合做镜像重组&#x2F;迁移时不丢元数据）。
一些旧命令（如 docker buildx install/uninstall）出现弃用提示，更倾向于直接使用 docker buildx 的默认行为（以官方 release note 为准）。

一个可直接复用的 CI 模板（多平台 + 远程缓存 + SBOM&#x2F;Provenance）把下面这段“按需替换 registry&#x2F;name&#x2F;tag”就能跑：
REGISTRY=registry.example.comIMAGE=$REGISTRY/myteam/myappTAG=$(git rev-parse --short HEAD)CACHE_REF=$IMAGE:buildcachedocker buildx build \  --platform linux/amd64,linux/arm64 \  --cache-from type=registry,ref=$CACHE_REF \  --cache-to type=registry,ref=$CACHE_REF,mode=max \  --provenance=true \  --sbom=true \  -t $IMAGE:$TAG \  -t $IMAGE:latest \  --push .

常见问题与排查思路1）为什么我开了 --sbom/--provenance，但仓库里看不到？优先排查：

你是否用了 --push（建议）而不是 --load
你的镜像仓库&#x2F;代理是否会剥离 OCI artifact&#x2F;attestation（某些老旧组件会）
是否用 docker buildx imagetools inspect 检查远端镜像清单与元数据

2）为什么多平台镜像构建很慢？通常原因是：

缓存没有跨 CI 复用（建议启用 registry cache）
Dockerfile 没有拆分依赖层，导致无效重建
某些步骤无法并行（例如单个 RUN 做了过多工作）

3）cache mount 会不会把敏感文件缓存出去？cache mount 的内容确实会进入缓存存储（本地或远程缓存后端），因此：

不要把 secret 写进 cache 目录
secret 用 --secret，并确保只在单条 RUN 内使用

总结BuildKit&#x2F;Buildx 在近几年的核心价值可以概括为三句话：

更快：RUN --mount=type=cache + 远程缓存让构建从“分钟级”变成“秒级可预期”
更强：多平台构建与更严格的 Dockerfile 语义&#x2F;校验，让交付更稳定
更可信：SBOM&#x2F;Provenance 让镜像可审计，能对接更完整的软件供应链治理

参考资料
Docker Build - Release notes
Dockerfile frontend (BuildKit) release notes
Build attestations: overview
Generate SBOM attestations
SLSA provenance attestations
Buildx releases (GitHub)


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>container</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>buildkit</tag>
        <tag>buildx</tag>
        <tag>container</tag>
        <tag>cicd</tag>
      </tags>
  </entry>
  <entry>
    <title>用 vCluster 在 Kubernetes 集群中构建虚拟集群</title>
    <url>/posts/0ba149dc4d1d.html</url>
    <content><![CDATA[引言在企业集群规模增长后，平台团队常常需要同时满足多团队隔离、快速环境交付和成本控制。vCluster 提供了一种折中方案：在同一个宿主 Kubernetes 集群中创建多个虚拟集群，每个虚拟集群有独立的 API Server 和控制平面，但共享底层节点资源。这样比“每个团队一个完整集群”更省成本，也比“只用 Namespace”更强隔离。GitHub 仓库 与 官网 给出了完整定位和文档入口。
本文以通用 Kubernetes 集群为背景，介绍 vCluster 的核心概念、部署要点与实际使用场景。
vCluster 是什么vCluster 是运行在宿主集群命名空间内的虚拟 Kubernetes 集群。核心特征如下：

控制平面独立：每个 vCluster 有自己的 API Server 与控制组件，更接近“真实集群”的使用体验。GitHub 仓库
共享底层资源：节点与资源来自宿主集群，成本更低，创建更快。GitHub 仓库
适合多租户：相比单纯 Namespace，隔离性更好，适合平台工程和团队自治场景。官网

Kubernetes 环境的部署要点在通用 Kubernetes 环境中部署 vCluster，通常关注以下几点：

访问与权限：确保在目标命名空间内有创建控制平面组件的权限。
存储后端：根据可靠性与运维要求选择嵌入式存储或外部数据库。
网络与同步：根据场景选择需要同步的资源类型，避免和宿主集群冲突。

参考文档：vCluster 文档入口
安装 vCluster安装流程通常包含“准备条件”和“安装 CLI”两步：
1. 准备条件
Kubernetes 版本：宿主集群建议满足官方版本要求（示例为 v1.28+）。官方文档
kubectl：已安装并配置好上下文，且具备目标命名空间的操作权限。官方文档
Helm（可选）：若使用 Helm 管理 vCluster，需要 Helm v3.10.0+。官方文档

2. 安装 vCluster CLI# Install vCluster CLI via Homebrewbrew install loft-sh/tap/vcluster# Verify installationvcluster --version

如果需要其他平台的安装方式，可参考官方安装页：vCluster 安装
3. Helm 方式（可选）# Install or upgrade with Helmhelm upgrade --install my-vcluster vcluster \  --repo https://charts.loft.sh \  --namespace team-x \  --create-namespace \  --version &lt;version&gt;

参考文档：官方安装说明
快速部署步骤（示例）
注意：以下步骤为通用流程示例，具体参数和版本请以官方文档为准。

1. 准备命名空间# Create a namespace for the virtual clusterkubectl create namespace team-x

2. 准备 vCluster 配置（示例）# vcluster.yamlcontrolPlane:  backingStore:    database:      embedded:        enabled: truesync:  toHost:    ingresses:      enabled: false

3. 创建 vCluster# Create a virtual cluster with a values filevcluster create my-vcluster --namespace team-x --values vcluster.yaml

4. 连接并验证# Connect to the virtual clustervcluster connect my-vcluster --namespace team-x# Verify accesskubectl get nodes

更多参数与部署方式见官方文档：vCluster 文档入口
vCluster 与宿主集群的 Service 互访vCluster 内部 Pod 运行在宿主集群网络中，Service 对象可由 vCluster 的同步机制映射到宿主集群，从而实现双向访问。官方提供了 networking.replicateServices 配置用于控制同步方向与目标。文档
1. 从宿主集群访问 vCluster 内服务通过 toHost 把虚拟集群里的 Service 映射到宿主集群：
# vcluster.yamlnetworking:  replicateServices:    toHost:      - from: app-ns/api        to: vcluster-api

上面示例会将 vCluster 内 app-ns/api 同步成宿主集群中的 vcluster-api Service，宿主集群工作负载可以直接访问。
2. 从 vCluster 访问宿主集群服务通过 fromHost 把宿主集群的 Service 映射到 vCluster：
# vcluster.yamlnetworking:  replicateServices:    fromHost:      - from: platform-ns/obs-gateway        to: platform/obs-gateway

这样 vCluster 内的应用可以使用 platform/obs-gateway 访问宿主集群的共享服务。文档
StorageClass、PVC 与宿主集群的关系vCluster 支持把宿主集群的 StorageClass 同步到虚拟集群，并将虚拟集群中的 PVC&#x2F;PV 反向同步到宿主集群，从而复用宿主存储系统。核心关系如下：文档

StorageClass 同步方向：宿主集群 → vCluster  
PVC&#x2F;PV 同步方向：vCluster → 宿主集群  
可控范围：可通过 selector 限制哪些 StorageClass 对虚拟集群可见

StorageClass 同步示例# vcluster.yamlsync:  fromHost:    storageClasses:      enabled: true      selector:        matchLabels:          vcluster: &quot;true&quot;


注意：如果不配置 selector，默认会同步宿主集群中的所有 StorageClass。

vCluster 内使用 PVC 示例# pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: app-data  namespace: app-nsspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 20Gi  storageClassName: fast-ssd

当 fast-ssd 来自宿主集群同步时，PVC 会在宿主侧实际创建并绑定对应 PV。
将 vCluster 服务暴露给外部访问对外暴露可分为两类：控制平面 API 与 应用服务。官方给出多种访问方式与配置示例。文档
1. 暴露 vCluster 控制平面 API常见方式是通过 Ingress 或 LoadBalancer 让 API Server 可外部访问：
# vcluster.yamlcontrolPlane:  ingress:    enabled: true    host: vcluster.example.com  proxy:    extraSANs:      - vcluster.example.comexportKubeConfig:  server: https://vcluster.example.com:443

如果使用 Ingress，需要确保宿主集群 Ingress Controller 支持 SSL Passthrough。文档
2. 暴露 vCluster 内应用服务方式 A：在 vCluster 内创建 Ingress，并将其同步到宿主集群，由宿主集群统一对外提供入口。
# vcluster.yamlsync:  toHost:    ingresses:      enabled: true

方式 B：在 vCluster 内创建 LoadBalancer 或 NodePort 服务，并通过宿主集群网络对外访问。适合云平台已有负载均衡器的场景。文档
实际使用场景
平台团队多租户为每个团队或业务线创建专属虚拟集群，团队拥有接近独立集群的控制体验，同时共享底层资源，降低成本。GitHub 仓库

CI &#x2F; 临时环境在同一宿主集群中快速创建&#x2F;销毁测试环境，适合流水线集成测试和短期验证场景。官网

Kubernetes 版本与兼容性验证使用虚拟集群做版本适配或 Operator 兼容性验证，避免影响宿主生产环境。vCluster 文档

统一平台服务复用监控、日志、Ingress 等基础组件由宿主集群统一提供，虚拟集群专注应用层，减少重复运维成本。官网


注意事项
安全与权限：确保在目标命名空间内有创建控制平面组件的权限，避免 RBAC 导致的安装失败。
资源隔离边界：vCluster 只虚拟控制平面，底层 CPU&#x2F;内存&#x2F;存储仍共享，需要配合配额与调度策略。
版本与兼容性：虚拟集群版本、控制平面分布和后端存储类型可能影响功能表现，建议锁定版本并在测试环境验证。vCluster 文档

总结vCluster 让 Kubernetes 集群中的多租户与环境交付更轻量：既能提供接近独立集群的体验，又保留共享底层资源的成本优势。对于平台团队而言，它是在隔离性、交付效率与成本之间的一种高性价比选择。
参考资料
vCluster GitHub 仓库
vCluster 官网
vCluster 文档：什么是虚拟集群


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>openshift</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>DevOps</tag>
        <tag>vCluster</tag>
        <tag>多租户</tag>
        <tag>平台工程</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac Apple Silicon LLM 微调实战指南：从原理到多场景应用</title>
    <url>/posts/7dcd88b03636.html</url>
    <content><![CDATA[Mac Apple Silicon LLM 微调实战指南：从原理到多场景应用随着 Apple Silicon (M1&#x2F;M2&#x2F;M3&#x2F;M4) 芯片的普及，Mac 已经成为一个强大的 AI 开发工作站。凭借其统一内存架构 (Unified Memory Architecture)，Mac 能够处理比同等配置显卡更大的模型。本文将介绍如何在 Mac 上使用 MLX 框架高效微调大语言模型（如 Qwen、Llama、Mistral 等），并探讨微调在不同业务场景中的应用。
一、 核心概念解析在开始动手之前，我们需要理解几个关键的技术术语。
1. 什么是微调 (Fine-tuning)？微调是在预训练模型（Base Model）的基础上，使用特定领域的数据进行进一步训练。就像是一个已经读完大学的“通才”，通过学习法律卷宗，变成了一位“律师”。
2. SFT (监督微调)SFT (Supervised Fine-Tuning) 是最常用的微调方式。它通过 (Input, Output) 对来教导模型如何响应指令。

编程场景示例: 
输入: “帮我写一个 Python 快速排序函数”
输出: “[正确的 Python 代码]”


医疗问答示例:
输入: “患者头晕且伴随耳鸣，可能的原因是什么？”
输出: “基于您的症状，可能的原因包括梅尼埃病、前庭神经炎等，建议及时就医进行专业检查。”


角色扮演示例:
输入: “你现在是苏格拉底，请跟我探讨什么是正义。”
输出: “我的朋友，让我们先从定义什么是‘正义’开始，你认为它是强者的利益，还是每个人应得的报酬？”



3. LoRA 与 QLoRA
LoRA (Low-Rank Adaptation): 这种技术不需要更新模型的所有参数（几十亿个），而是在原有的模型层旁插入一些极小的“旁路矩阵”（Low-Rank Matrices）。训练时只更新这些小矩阵，这极大减少了显存消耗。
QLoRA: 在 LoRA 的基础上，将原始模型量化到 4-bit。这让 16GB 甚至更小内存的 Mac 也能微调 7B 规模的模型。

二、 为什么要微调？（应用场景）并不是所有问题都需要微调，但在以下场景中，微调是不可替代的：

特定领域知识 (Domain Specific): 当通用模型无法满足极其专业的垂直领域（如医疗诊断、法律合同分析、特定企业内部工作流）时，微调能显著提升准确性。
特定的输出格式: 强制模型严格输出特定的 JSON 结构，或遵循复杂的业务逻辑格式。
语气与角色对齐: 让 AI 助手拥有特定的品牌性格、客服语调，或是模拟特定历史人物的谈吐。
长文本与长指令遵循: 在处理超长上下文或需要严格遵守多步指令的任务中，微调可以提升模型的稳定性。

三、 工欲善其事：基础模型选择的技巧选择一个合适的基础模型（Base Model）是微调成功的基石。并不是越大的模型就越好，关键在于“适配”。
1. 模型尺寸与显存的博弈在 Mac 上，显存（统一内存）是核心限制因素：

1.5B - 3B 模型: 极其轻量，适合移动端或极其简单的分类&#x2F;指令遵循任务。16GB 内存的 Mac 可以轻松运行。
7B - 9B 模型: 黄金选择。如 Qwen2.5-7B 或 Llama-3.1-8B。它们在逻辑能力和显存占用之间达到了完美的平衡，适合大多数垂直领域任务。

💡 为什么目前更推荐 Qwen2.5 而非最新的 Qwen3？虽然 Qwen3 带来了更强的推理能力（如 Thinking Mode）和 MoE 架构，但在微调实践中，Qwen2.5 仍有其独特优势：

生态适配极佳：MLX 框架对 Qwen2.5 的算子优化已极其成熟，微调过程非常稳定。
专项能力突出：尤其是 Qwen2.5-Coder 系列，在代码编写和逻辑推理上的表现依然是开源界的标杆。
资源消耗可控：Qwen2.5 采用稠密架构（Dense），在 Mac 的统一内存上表现非常可预测，相比 MoE 架构（Qwen3 部分版本）在微调时更容易控制显存峰值。
成熟的微调脚本：社区积累了大量针对 Qwen2.5 的 LoRA 实践经验，避坑更容易。


14B - 32B 模型: 适合复杂的逻辑推理。32GB 以上内存的 Mac 建议尝试。

2. 预训练背景的考量
通用场景: Llama-3.1 或 Qwen2.5 是目前最稳妥的选择，它们在海量多语言数据上训练过，泛化能力强。
中文场景: Qwen (通义千问) 系列或 DeepSeek 系列对中文语境的理解更地道，避坑首选。
编程场景: 如果你的目标是微调一个代码助手，直接选用 Qwen2.5-Coder 或 CodeLlama 作为基础模型，会比从通用模型开始微调省力得多。

3. Base vs Instruct 版
Base 版: 适合续写，没有对话能力。如果你想让模型学习某种特殊的写作风格或大量的纯知识，选 Base 版。
Instruct&#x2F;Chat 版: 已经过初步指令对齐，具备对话能力。大多数 SFT 任务建议从 Instruct 版开始，模型更容易“听懂”指令。

4. 协议与商用务必关注模型的开源协议（如 Apache 2.0, Llama 3 License 等）。如果用于企业内部或产品发布，确保基础模型的协议允许商用。
5. 跨平台视角：Linux + NVIDIA GPU 选型建议如果你的微调环境是带有 NVIDIA GPU 的 Linux 服务器（如配备 A100, H100 或 RTX 3090&#x2F;4090），选型逻辑会略有不同：

Qwen3-30B-A3B &#x2F; Qwen3-72B: 强烈推荐。Qwen3 在 Linux + CUDA 环境下能完美释放其 MoE（混合专家）架构的潜力。其独特的“思维模式（Thinking Mode）”在处理复杂推理、多语言（支持 119 种）任务时表现惊人。Linux 下的 vLLM 和 LLaMA-Factory 对其算子优化非常超前。
DeepSeek-V3&#x2F;R1: 如果你的显存足够（或使用多卡并行），DeepSeek 系列在 Linux&#x2F;CUDA 生态下有极佳的算子优化，尤其适合需要极高逻辑能力的场景。
Llama-3.1-70B&#x2F;405B: 在 Linux 环境下，通过 unsloth 或 vLLM 框架，可以比 Mac 更高效地运行和微调超大规模模型。
Mistral&#x2F;Mixtral: 欧洲的“明星”模型，在 Linux 生态下支持非常成熟，适合对隐私和多语言有特殊要求的欧洲或国际业务。
优势工具链: 在 Linux 下，你可以使用 Unsloth（速度提升 2x，内存减少 70%）或 LLaMA-Factory。这些工具对 NVIDIA GPU 有原生优化，微调效率远超 Mac。


四、 数据是灵魂：训练数据的获取与管理在大模型领域有一句话：“Garbage in, garbage out”（输入的是垃圾，输出的也是垃圾）。微调的效果 80% 取决于数据的质量。
1. 数据的组成结构SFT 数据通常由三个部分组成：

System: 设置模型的人设（如：“你是一个资深律师”）。
User: 用户的提问或指令。
Assistant: 标准的、高质量的回答。

2. 数据获取渠道
开源数据集: Hugging Face 和 ModelScope (魔搭社区) 是获取通用数据集的首选。
业务日志挖掘: 从真实的业务对话日志中提取 (Input, Output) 对，这是最贴合实际业务的数据来源。
合成数据 (Synthetic Data): 使用更强大的模型（如 GPT-4o 或 Claude 3.5）来生成训练数据。

3. 如何自建高质量数据？如果你没有现成的数据，可以尝试以下方法：

Self-Instruct: 给模型几个示例，让它仿照示例生成更多的指令和回答。
文档转化: 将公司内部的 PDF、Markdown 文档，通过脚本转化为问答对。
人工标注: 对于精度要求极高的场景，人工对模型生成的回答进行纠错和润色，形成“黄金数据集”。


五、 实战：环境搭建与微调步骤本章节将结合环境配置与实际操作，带你完成一次完整的微调流程。
1. 环境准备 (使用 MLX 框架)本方案推荐使用 MLX，这是苹果专门为 Apple Silicon 优化的深度学习框架。
硬件要求
芯片: Apple M1 或更高版本。
内存: 建议 32GB 及以上（16GB 内存可微调 7B 量化版）。

数据准备missvector&#x2F;linux-commands是一个非常适合初学者练习 Linux 命令微调的高质量数据集。
软件安装 (使用 uv 管理)# 使用 uv 快速创建环境uv venv --python 3.12 --seedsource .venv/bin/activate# 安装 MLX 核心库uv pip install mlx-lm datasets pandas

2. 数据预处理MLX 推荐使用 .jsonl 格式。数据格式如下：
&#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个 Linux 专家&quot;&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;如何查看进程？&quot;&#125;, &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;使用 ps -ef 或 top 命令。&quot;&#125;]&#125;

3. 执行微调创建一个 config.yaml 配置文件。这里以 Qwen2.5-7B 为例，你也可以替换为 Llama-3 或 Mistral 等模型：
model: &quot;mlx-community/Qwen2.5-7B-Instruct-4bit&quot; train: truedata: &quot;./data&quot; iters: 1000    batch_size: 4learning_rate: 2e-5lora_parameters:  rank: 32       scale: 2.0     dropout: 0.05

参数详解与调优实践：
iters (迭代次数): 训练步数。数据量小时（如 &lt; 100 条），建议 100-500 次；数据量大时可增加。注意：次数过多会导致模型“过拟合”，只会背书不会思考。
learning_rate (学习率): 决定了模型学习的“步子”有多大。通常在 1e-5 到 5e-5 之间。学习率太大模型会学“崩”，太小则学得太慢。
rank (LoRA 秩): 核心参数。rank: 8 适合简单任务，rank: 32 或 64 适合逻辑复杂或需要学习大量新知识的任务。
scale (缩放因子): 决定了微调权重对原始模型的影响程度。通常设为 2.0。
batch_size: 每次训练喂给模型的数据条数。Mac 内存有限，建议设为 1 或 4。

调参技巧：
如果模型回答开始变得重复或胡言乱语，通常是过拟合了。此时应尝试：1. 减少 iters；2. 调小 learning_rate；3. 增加训练数据的多样性。

运行微调命令：
python -m mlx_lm lora --config config.yaml

4. 部署微调后的模型对于量化模型，我们采用“基础模型 + 适配器（Adapters）”的方式：
python -m mlx_lm server \    --model mlx-community/Qwen2.5-7B-Instruct-4bit \    --adapter-path ./adapters \    --port 8080


六、 效果对比：Cherry Studio 展示微调效果如何，通过对比一目了然。我们可以使用 Cherry Studio 的“对比模式”进行验证。

配置本地端点: 在 Cherry Studio 中添加两个模型：一个指向原始的基础模型，另一个指向加载了 Adapter 的本地服务（端口 8080）。
开启对比聊天: 发送一个针对微调场景的特定任务（例如：医疗诊断建议或特定风格的文案）。

对比结果示例（以客服场景为例）：

微调前: 模型回答：“对不起，我不清楚您指的‘计划 A’是什么。”（通用知识，不了解内部业务）。
微调后: 模型回答：“我们的‘计划 A’包含年度基础维护和每季度的安全检查，具体您可以查看内部手册第 5 页。”（精准匹配业务数据）。

  
左侧是基础模型，右侧是微调后的模型七、 性能小贴士
统一内存: 在 Mac 上，你可以调整 max_seq_length。如果遇到内存溢出（OOM），适当调小 batch_size。
监控: 训练时打开“活动监视器”，观察 GPU 的负载。你会发现 M 芯片的 GPU 在矩阵运算上非常高效。

总结微调不再是昂贵服务器的专利。借助 MLX 框架和 LoRA 技术，每一位 Mac 用户都可以在本地训练出属于自己的“专家模型”。这不仅保护了数据隐私，更为个性化 AI 应用打开了大门。

本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>Fine-tuning</tag>
        <tag>Mac</tag>
        <tag>MLX</tag>
        <tag>Llama</tag>
        <tag>Qwen</tag>
      </tags>
  </entry>
  <entry>
    <title>RAGFlow 使用指南：从深度解析到生产化部署运维全攻略</title>
    <url>/posts/f3be144d72f7.html</url>
    <content><![CDATA[RAGFlow 使用指南：从深度解析到生产化部署运维全攻略1. 引言：为什么选择 RAGFlow？在 RAG（检索增强生成）领域，业界公认的挑战在于：“Garbage in, garbage out”。如果输入的上下文质量低下、版式混乱，LLM 再强也无法给出准确答案。
RAGFlow 的核心优势在于它对高质量数据接入的执着。它不只是简单的“向量化工具”，而是强调两点：

细粒度文档解析（DeepDoc）：针对图片、表格等复杂版式，通过 OCR 和版面分析，确保文档被“吃透”。
可追溯引用：每一个答案都能精准追溯到原始文档片段，有效降低大模型幻觉。

如果你需要处理大量复杂的 PDF、扫描件、金融财报或技术手册，RAGFlow 提供的“数据质量优先”路径将是你的不二之选。

2. 核心功能深度解析2.1 知识库（Datasets）与 DeepDoc 解析知识库是 RAGFlow 的底座。它将非结构化文件转化为可检索的证据库。

深度解析（DeepDoc）：这是 RAGFlow 的杀手锏。它在解析阶段执行 OCR、表格结构识别等重度预处理。
切分策略（Chunking）：
通用文档：建议使用中等 chunk + 适度重叠，兼顾语义完整性。
手册&#x2F;规范：推荐按标题层级切分，减少跨章节误导。


最佳实践：
先小样验证：先导入 5-10 份典型文档调试效果，再大规模导入。
元数据过滤：利用分类、版本等标签，在检索时减少噪声。



2.2 搜索与对话：AI Search vs AI ChatRAGFlow 提供了两种不同的检索交互方式：



特性
AI Search (单轮搜索)
AI Chat (多轮对话)



定位
智能百科，追求快、稳、证据直观
业务助理，追求深度理解、多轮推进


交互
单轮（不维护上下文）
多轮（保留历史对话）


策略
预设的混合检索（Hybrid Search）
可自定义 RAG 策略与 Agent 流程


展示
直接在答案下方列出所有 chunks
通过引用标签展示来源，更像成品体验


建议场景
故障排查、条款快速定位
方案咨询、复杂文档对比


2.3 Chat Assistant：面向业务的入口Assistant 是 RAGFlow 的封装层，将模型、知识库、Prompt 整合为可运营的服务。

System Prompt：定义角色人设和规则（如“无依据则拒答”）。
参数精修：针对不同助理配置不同的温度、Top-K 和 Rerank 模型。
工具调用：支持挂载外部工具（搜索、计算等），赋予助理行动力。

2.4 Agent 与 Workflow：从问答升级到自动化Workflow 将 RAG 从简单的“一问一答”升级为“多步任务”。

核心组件：
基础节点：Begin, Answer, Message。
检索节点：Retrieval, Rerank, Rewrite（重写查询词）。
逻辑节点：Generate, Categorize, Switch, Python Executor。


注意：若要在 Workflow 中运行 Python Executor 节点进行自定义逻辑计算，必须部署 Sandbox Executor Manager 服务，否则代码将无法执行。
设计原则：Workflow 优先，Agent 其次。能用固定流程解决的，就不要让模型自由发挥，以确保结果的一致性。

2.5 Memory：实现长期协作记忆Memory 将对话转为可检索的结构化记忆，存入 DOC_ENGINE（ES 或 Infinity）。

记忆类型：Raw（原始）、Semantic（语义）、Episodic（情景）、Procedural（流程）。
读写逻辑：通过 LLM 自动抽取关键信息，并基于向量相似度召回。
调参旋钮：keywords_similarity_weight（文本 vs 向量权重）和 similarity_threshold（阈值控制召回灵活性）。


3. 团队协作与组织管理RAGFlow 支持多人协作，提供了细粒度的资源管理能力：

资源共享：支持团队成员间共享 Dataset 和 Agent。
Admin 工具链：
Admin UI：可视化管理用户、修改密码、查看服务状态。
Admin CLI：应急时通过命令行管理资源、恢复服务。


权限边界：企业版支持更高级的模型和助理共享功能。


4. 架构设计与生产部署4.1 核心组件概览基于 docker-compose 的标准部署包含：

存储层：MySQL（元数据）、Elasticsearch&#x2F;Infinity（文档引擎）、MinIO（对象存储）。
中间件：Redis&#x2F;Valkey（任务队列）。
计算层：RAGFlow API、Admin Server、Worker（处理解析任务）、Sandbox Executor Manager（可选，代码执行沙箱）。

4.2 部署实战
快速启动：

# 进入 docker 目录并启动 (默认 CPU 模式)cd ragflow/dockerdocker compose up -d

生产化思路：Web 与 Worker 分离解析任务（Worker）极为消耗资源，建议将其部署在带有 GPU 的节点上，而 Web&#x2F;API 则部署在常规节点，实现负载隔离。

GPU 加速解析：如果需要 GPU 加速，在 .env 中设置 DEVICE=gpu。对于自定义环境，建议使用 uv 管理依赖，并安装匹配 CUDA 版本的 torch。


4.3 部署 Sandbox Executor (实现 Workflow 代码运行)如果你需要使用 Workflow 中的 Python Executor 运行自定义代码，必须额外部署沙箱环境：当前ragflow 沙箱依赖 gVisor 提供内核级隔离，请确保宿主机已安装。如果使用pod sidecar的方式部署的话，需要构建支持gVisor的Dind容器。
FROM docker:27-dindRUN apk add --no-cache curl bashRUN curl -Lo /usr/local/bin/runsc https://storage.googleapis.com/gvisor/releases/release/latest/x86_64/runsc &amp;&amp; \    chmod +x /usr/local/bin/runscRUN mkdir -p /etc/docker &amp;&amp; \    echo &#x27;&#123; \        &quot;runtimes&quot;: &#123; \            &quot;runsc&quot;: &#123; \                &quot;path&quot;: &quot;/usr/local/bin/runsc&quot; \            &#125; \        &#125; \    &#125;&#x27; &gt; /etc/docker/daemon.jsonEXPOSE 2375 2376ENTRYPOINT [&quot;dockerd-entrypoint.sh&quot;]

4.4 可观测性与运维
健康检查：访问 /v1/system/healthz 接口进行监控接入。
全链路追踪（Tracing）：集成 Langfuse。无需修改代码，只需在后台配置 Key，即可实时监控检索、重排、生成的完整链路。
安全性：严禁使用默认密码。务必修改 .env 中的 MYSQL_PASSWORD、MINIO_PASSWORD 等敏感配置。


5. 选型建议与对比


维度
RAGFlow
Dify
LangChain &#x2F; LlamaIndex



核心优势
复杂文档解析、可追溯性
应用搭建、工作流编排
研发灵活性、高度可组合


适用团队
追求检索效果、有运维能力
追求快速上线、业务优先
深度研发、自研平台


主要挑战
部署较重、解析耗时较长
RAG 深度调节受限
维护成本高、需补齐运维面


推荐路径：

如果文档包含大量表格、公式或扫描件：必选 RAGFlow。
如果需要一个全能的 AI 应用平台：优先 Dify。
如果需要集成到现有成熟系统作为 RAG 引擎：RAGFlow &#x2F; 自研。


6. 总结RAGFlow 是一个“硬核”的 RAG 引擎。它不追求花哨的 UI，而是通过 DeepDoc 把数据底座打扎实，通过可观测性把 RAG 链路变得透明。在生产化过程中，建议将解析任务视作独立的计算任务进行治理，并充分利用其可追溯能力来构建业务闭环。

相关资源
RAGFlow GitHub Repository
官方 0.22.1 文档中心
Langfuse 追踪配置指南


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>Agent</tag>
        <tag>RAGFlow</tag>
        <tag>知识库</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>排障记录：删除Namespace olm一直Terminating的处理过程</title>
    <url>/posts/577b82f4a03d.html</url>
    <content><![CDATA[引言在集群日常维护中，删除一个 Namespace 看起来是“删掉一堆资源”的简单动作，但如果控制面无法完成资源发现（discovery），或某些资源的 finalizers 无法被对应控制器清理，Namespace 就会长期卡在 Terminating。
本文记录一次删除 olm Namespace 失败的排障过程：先是 NamespaceDeletionDiscoveryFailure，解决后又卡在 ClusterServiceVersion（CSV）清理阶段，最终通过移除 CSV 的 finalizer 完成删除。文末附一套可复用的排查清单与风险提示。

警告：olm 相关资源通常属于 Operator Lifecycle Manager（OLM）核心组件。生产集群中不建议随意删除；本文的“强制移除 finalizer &#x2F; 删除 APIService”属于应急手段，务必理解影响范围后再执行。

现象：Namespace olm 一直 Terminating删除 olm 后一直停留在 Terminating，查看 Namespace 状态发现：
Discovery failed for some groups, 1 failing: unable to retrieve thecomplete list of server APIs: packages.operators.coreos.com/v1: stale GroupVersiondiscovery: packages.operators.coreos.com/v1&#x27;reason: DiscoveryFailedstatus: &quot;True&quot;type: NamespaceDeletionDiscoveryFailure

这条信息的关键点是：控制面在做“列出该 Namespace 下所有资源类型并逐一清理”之前，需要先完成 API discovery；而 discovery 过程中，packages.operators.coreos.com/v1 这个聚合 API（aggregated API）无法正常返回，导致删除流程被阻塞。
第一步：从 Namespace Conditions 定位阻塞点建议第一时间看 Namespace 的 conditions（比“盲删 finalizers”更快定位根因）：
kubectl describe namespace olm

其中 NamespaceDeletionDiscoveryFailure=True 通常意味着：有 API group&#x2F;version 在 discovery 阶段不可用，Namespace lifecycle controller 无法确认“有哪些资源类型需要被列出并清理”。
根因一：v1.packages.operators.coreos.com APIService 不可用（ServiceNotFound）根据报错指向的 group&#x2F;version，去查对应的 APIService：
kubectl get apiservice v1.packages.operators.coreos.com

输出类似：
NAME                                SERVICE                     AVAILABLE   AGEv1.packages.operators.coreos.com     olm/packageserver-service   False       163d

这里的信号非常明确：该聚合 API 通过 APIService 挂到 kube-apiserver 的 aggregation layer 上，后端需要一个真实存在且健康的 Service（这里是 olm/packageserver-service）。当它变成 False (ServiceNotFound) 时，kube-apiserver 仍会把这个 API 组暴露在 discovery 路径里，但向后端代理请求会失败，从而触发 Namespace 删除流程的 discovery failure。
你可以进一步核实 Service 是否存在：
kubectl -n olm get svc packageserver-service

处理一：优先恢复 packageserver（推荐），否则删除 APIService（应急）方案A：恢复后端服务（推荐）如果这是生产集群或你仍需要 OLM 能力，优先做“把 packageserver-service 对应的 Deployment&#x2F;Pod 修复回来”，让 APIService 变为可用（AVAILABLE=True）。这样 deletion discovery 能自然恢复，后续清理也更可控。
方案B：删除不可用的 APIService（应急）如果你明确不再需要该聚合 API（例如正在拆除 OLM &#x2F; 清理残留），可以删除这个 APIService，让 kube-apiserver 的 discovery 不再包含该 group&#x2F;version，从而解除 Namespace 删除的 discovery 阻塞：
kubectl delete apiservice v1.packages.operators.coreos.com


注意：删除后会影响依赖该 API 的能力（例如 OLM 的 packagemanifest 查询等），请在维护窗口评估影响后再做。

执行后你会观察到：很多此前卡在 Terminating 的 Namespace 会被快速清理。但这次事故里，olm 仍然没有被删掉，说明还有第二个阻塞点。
第二步：Discovery 恢复后，继续看“剩余资源 + 剩余 finalizers”此时再次 describe Namespace，会看到 discovery 相关条件已恢复，但出现另外两条：
NamespaceContentRemaining True  SomeResourcesRemain  ... clusterserviceversions.operators.coreos.com has 1 resource instancesNamespaceFinalizersRemaining True SomeFinalizersRemain ... operators.coreos.com/csv-cleanup in 1 resource instances

这两条意味着：

Namespace 里仍有资源实例没删干净（这里是 ClusterServiceVersion）
且该资源实例上仍挂着 finalizer（这里是 operators.coreos.com/csv-cleanup），控制器没有完成清理并移除 finalizer

根因二：CSV packageserver 残留，csv-cleanup finalizer 无法被清理列出 olm 下的 CSV：
kubectl get csv -n olm

输出示例：
NAMESPACE   NAME          DISPLAY          VERSION   REPLACES   PHASEolm         packageserver Package Server   0.32.0              Pending

此时典型情况是：OLM 相关控制器（负责执行 operators.coreos.com/csv-cleanup）已经不可用或不再工作，导致 finalizer 永远不会被自动移除，进而阻塞 CSV 删除，也阻塞 Namespace 最终删除。
处理二：手动清理 CSV（最后手段：移除 finalizers）优先尝试正常删除如果控制器仍然健康，直接删除 CSV 即可：
kubectl delete csv packageserver -n olm

控制器失效时：移除 CSV finalizers当 CSV 一直删不掉（或你已确认需要强制清理），可通过 patch 移除 finalizers：
kubectl patch csv packageserver -n olm \  -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null&#125;&#125;&#x27; --type=merge

执行后，CSV 会被立即释放，随后 olm Namespace 也会自动完成终止并消失。
为什么“删 namespace 的 spec.finalizers”不一定有用？很多人第一反应是把 Namespace 的 spec.finalizers 清空，但这在本案里无效，原因有两类：

Discovery 阶段就失败：控制器连“这个 Namespace 里有哪些资源类型要清理”都无法完整发现，删除流程会直接卡住。
资源级 finalizer 才是硬阻塞：即使 Namespace 自身 finalizer 被清空，只要 Namespace 内仍有带 finalizer 的资源（例如 CSV），最终删除仍会被阻塞。

换句话说：Namespace 是“结果”，资源与控制器的健康才是“原因”。
一套可复用的排查清单（推荐按顺序执行）1. 看清楚卡在哪一类 conditionkubectl describe namespace &lt;ns&gt;


NamespaceDeletionDiscoveryFailure=True：优先查 APIService&#x2F;聚合 API&#x2F;网络&#x2F;后端服务
NamespaceContentRemaining=True：说明仍有资源残留
NamespaceFinalizersRemaining=True：说明仍有资源 finalizer 没被清理

2. 找出所有不可用的 APIServicekubectl get apiservice | grep Falsekubectl describe apiservice &lt;apiservice-name&gt;

3. 枚举 Namespace 内仍然存在的资源（通用方法）kubectl api-resources --verbs=list --namespaced -o name | \  xargs -n 1 kubectl get -n &lt;ns&gt;

4. 针对“有 finalizer 的残留资源”做精准清理以本案为例：
kubectl get csv -n &lt;ns&gt;kubectl get csv &lt;name&gt; -n &lt;ns&gt; -o yaml

确认 finalizers 后，再决定是否移除（强制手段）。
5. 最后手段：强制完成 Namespace finalize（谨慎）如果你已经：

修复&#x2F;移除了不可用的 APIService（不再触发 discovery failure）
手工清理了 Namespace 内的残留资源（或你已接受残留风险）

仍然无法删除 Namespace，可以考虑通过 finalize 端点强制移除 spec.finalizers（参考 GKE 文档的流程）。这是高风险操作，可能导致集群中遗留“看不见但还存在”的资源。
kubectl get ns &lt;ns&gt; -o yaml &gt; ns-terminating.ymlkubectl proxycurl -H &quot;Content-Type: application/yaml&quot; -X PUT \  --data-binary @ns-terminating.yml \  http://127.0.0.1:8001/api/v1/namespaces/&lt;ns&gt;/finalize

风险提示与建议
能修复控制器就别强拆 finalizer：finalizer 本意是保证清理动作被执行；直接移除意味着你承诺“后果自负”，后续可能留下孤儿资源或脏数据。
删除 APIService 是在“切断聚合 API 的入口”：它能让 discovery 变干净，但也会让对应 API 彻底不可用。更推荐先恢复后端服务。
系统组件 Namespace 要谨慎：如果是 OpenShift 集群内置的 OLM（例如 openshift-operator-lifecycle-manager 等），不要照搬本文对系统命名空间的删除操作。

参考链接
Kubernetes Finalizers：https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/
Kubernetes API Aggregation Layer：https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/
Configure the Aggregation Layer：https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/
GKE：Troubleshoot namespace stuck in the Terminating state：https://cloud.google.com/kubernetes-engine/docs/troubleshooting/terminating-namespaces
OLM：ClusterServiceVersion（概念）https://olm.operatorframework.io/docs/concepts/crds/clusterserviceversion/
OLM：PackageManifest API（列出可安装 Operator）https://olm.operatorframework.io/docs/tasks/list-operators-available-to-install/


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>openshift</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>OLM</tag>
        <tag>Namespace</tag>
        <tag>APIService</tag>
        <tag>Finalizer</tag>
      </tags>
  </entry>
  <entry>
    <title>Fabric：开源AI工作流与Prompt辅助框架详解</title>
    <url>/posts/0ca62b9873a9.html</url>
    <content><![CDATA[在 AI 技术爆发的今天，我们拥有了无数强大的大模型和工具，但如何高效地将这些能力集成到日常工作流中，仍然是一个巨大的挑战。通常我们面临的问题不是”AI 能做什么”，而是”如何让 AI 帮我做这件事”。
Fabric 正是为了解决这个问题而诞生的。它是一个旨在通过 AI 增强人类能力的开源框架，核心理念是将 AI 的原子能力封装成标准化的”模式”（Patterns），让我们能够像使用命令行工具一样方便地调用 AI 能力。
什么是 Fabric？Fabric 由安全专家 Daniel Miessler 创建，它不仅仅是一个工具，更是一种使用 AI 的方法论。
核心痛点
Prompt 管理混乱：每个人都在写 Prompt，但很难复用、版本控制和分享。
集成困难：在这个应用里用 ChatGPT，在那个应用里用 Claude，缺乏统一的入口。
上下文切换：为了使用 AI，需要在不同窗口间频繁切换，打断心流。

核心特性
Patterns（模式）：Fabric 将高质量的 Prompt 封装为 Pattern，每个 Pattern 解决一个具体问题（如”提取视频摘要”、”分析代码安全”、”撰写技术文章”）。
命令行优先：通过 CLI 接口，Fabric 可以轻松接入 Shell 管道（Pipeline），与其他 Linux 工具（如 grep, pbpaste, cat）组合使用。
多模型支持：支持 OpenAI, Anthropic, Google, Ollama 等多种后端模型。
YouTube 集成：内置强大的 YouTube 视频内容提取功能。

安装与配置Fabric 使用 Go 语言重构后，安装变得非常简单。
1. 安装 FabricLinux &#x2F; macOS (推荐一键安装):
curl -fsSL https://raw.githubusercontent.com/danielmiessler/fabric/main/scripts/installer/install.sh | bash

Windows (PowerShell):
iwr -useb https://raw.githubusercontent.com/danielmiessler/fabric/main/scripts/installer/install.ps1 | iex

使用 Go 安装 (适用于开发者):
go install github.com/danielmiessler/fabric/cmd/fabric@latest

2. 配置安装完成后，运行 setup 命令进行初始化，配置 API Key 和相关设置：
fabric --setup

在此过程中，你需要提供各大模型提供商的 API Key（如 OpenAI API Key）。配置信息会保存在 ~/.config/fabric/ 目录下。
常用功能与使用示例Fabric 的强大之处在于它可以融入你的命令行工作流。
基础用法基本语法结构：
fabric --pattern [pattern_name]# 或者简写fabric -p [pattern_name]

场景一：总结文章内容你可以将剪贴板的内容直接投喂给 Fabric：
# 读取剪贴板内容并总结pbpaste | fabric --pattern summarize# 将文件内容传递给 Fabric 进行分析cat config.yaml | fabric --pattern analyze_claims

场景二：YouTube 视频智慧提取这是 Fabric 最受欢迎的功能之一。它可以直接抓取 YouTube 视频字幕，并提取其中的核心观点和智慧，而不仅仅是简单的摘要。依赖 YouTube APIKEY
# 提取视频中的核心智慧（Wisdom）fabric -y &quot;https://www.youtube.com/watch?v=VIDEO_ID&quot; --pattern extract_wisdom

场景三：代码分析与解释# 解释一段代码cat complex_function.py | fabric --pattern explain_code# 为代码生成文档cat main.go | fabric --pattern create_design_document

场景四：即时流式输出使用 --stream 或 -s 参数可以获得类似 ChatGPT 的打字机效果：
echo &quot;解释量子纠缠&quot; | fabric --pattern explain_terms --stream

常用核心命令详解为了让大家更直观地理解 Fabric 的能力，这里精选了几个最具代表性的 Pattern 进行详细说明：



Pattern 名称
功能描述
典型应用场景



summarize
内容摘要专家。将输入内容压缩为一句话总结、10个主要观点和5个核心收获（Takeaways）。
快速阅读长文章、会议纪要或复杂的文档。


extract_wisdom
智慧提取器（Fabric 核心）。从内容中提取核心观点、洞见、名言、习惯和事实，不仅仅是摘要，而是深度提炼。
深度阅读文章、视频或书籍，提取最有价值的信息。


explain_code
代码解释器。解释代码的功能、逻辑，如果是安全工具输出则解释其含义，如果是配置文件则解释设置项。
接手新项目时快速理解代码，或学习新的编程语言&#x2F;框架。


create_command
根据工具的帮助文档（Help）生成正确的 CLI 命令。它能通过阅读文档，生成精准的 Bash 命令。
忘记某个复杂 Linux 命令的参数时，直接把 help 信息投喂给它生成命令。


create_formal_email
商务邮件写作助手。根据上下文和目的，撰写语气得体、专业规范的正式邮件。
回复客户咨询、发送正式通知或进行商务沟通。


humanize
人性化重写工具。将 AI 生成的生硬文本重写为自然、流畅、具有”人味”的内容。它会优化句子节奏、去除机器味，使其更像真人的表达。
让 ChatGPT 生成的邮件、文案或对话看起来更自然、更有亲和力。


fix_typos
文本校对专家。专注于修复拼写错误、语法问题和标点符号错误，不改变原意，只做清洗和专业化处理。
发送重要文档、邮件或发布文章前的最后一道检查。


clean_text
文本清洗工具。修复断行、格式混乱的文本，添加正确的段落和标点，不做内容修改。
处理 PDF 复制出来的乱码文本，或 OCR 识别后的混乱文本。


write_essay
写作专家。根据输入的主题，模仿特定作者（默认或指定）的风格撰写清晰、有深度的文章。
撰写博客、专栏文章或深度评论。


summarize_meeting
会议纪要生成器。提取会议概览、关键点、任务分配、决策、挑战、时间线和后续步骤。
整理 Zoom&#x2F;Teams 会议转录文本，快速生成可分发的会议纪要。


create_prd
产品需求文档（PRD）生成器。将产品创意或描述转化为包含目标、功能、用户故事、技术要求等章节的完整 PRD。
产品经理快速草拟需求文档，或将头脑风暴的结果结构化。


create_design_document
软件架构设计专家。基于输入的需求描述，使用 C4 模型（Context, Container, Deployment）生成详细的设计文档，包含 Mermaid 图表代码。
架构师或技术负责人进行系统设计和文档编写。


create_git_diff_commit
Git 提交信息生成器。分析代码变更（git diff），生成符合 Conventional Commits 规范的提交命令和 commit message。
开发者提交代码时，自动生成规范、清晰的 commit log。


analyze_terraform_plan
专家级 Terraform 计划分析器。它能分析 Terraform plan 的输出，评估基础设施变更、安全风险、成本影响和合规性考量。
DevOps 工程师在执行 terraform apply 前进行自动化代码审查。


analyze_claims
真实性分析专家。分析内容中的核心主张，提供支持证据、反驳证据，并识别逻辑谬误，进行真实性评级（A-F）。
阅读具有争议性的文章、新闻或辩论，进行批判性思考和事实核查。


analyze_candidates
候选人&#x2F;竞选者对比分析。分析并对比两名候选人在关键议题上的立场、政策优劣势及背景信息。
选举期间对比候选人政纲，或用于对比两个待选方案&#x2F;人物。


suggest_pattern
Fabric 导航员。根据你的需求描述，智能推荐最适合的 Fabric Pattern，并解释原因和用法。
刚接触 Fabric，不知道该用哪个 Pattern 解决手头问题时。


improve_prompt
Prompt 优化专家。基于 Prompt Engineering 最佳实践（如 CoT、Few-Shot），优化输入的 Prompt，使其更清晰、高效。
当你的 Prompt 效果不佳时，用它来”修一修”。


create_pattern
Pattern 制造机。将一段普通的 Prompt 或指令，转化为符合 Fabric 标准结构的 Pattern（包含 Identity, Steps, Output Instructions 等）。
想要将自己常用的 Prompt 封装成 Fabric Pattern 分享或复用时。


Fabric Patterns 大全Pattern 是 Fabric 的灵魂。以下是 Fabric 目前支持的所有 Pattern 分类列表（按功能前缀分类）：
Analyze（分析类）这类 Pattern 用于对输入内容进行深度分析，挖掘潜在信息、逻辑漏洞或安全风险。

analyze_answers: 分析回答的质量和相关性。
analyze_bill: 分析账单或发票。
analyze_bill_short: 简短版账单分析。
analyze_candidates: 分析求职者简历。
analyze_cfp_submission: 分析会议征稿提交内容。
analyze_claims: 核心功能，分析文本中提出的所有主张（Claims）及其真实性。
analyze_comments: 分析评论区的情感和观点。
analyze_debate: 分析辩论双方的论点和逻辑。
analyze_email_headers: 分析邮件头，用于安全取证。
analyze_incident: 分析安全事件或事故报告。
analyze_interviewer_techniques: 分析面试官的提问技巧。
analyze_logs: 运维必备，智能分析系统日志。
analyze_malware: 分析恶意软件特征（需谨慎使用）。
analyze_military_strategy: 分析军事策略。
analyze_mistakes: 分析失败原因和错误。
analyze_paper: 深度分析学术论文。
analyze_paper_simple: 简单版论文分析。
analyze_patent: 分析专利文档。
analyze_personality: 基于文本分析人物性格。
analyze_presentation: 分析演示文稿结构和内容。
analyze_product_feedback: 分析产品用户反馈。
analyze_proposition: 分析提案或建议。
analyze_prose: 分析散文的写作风格。
analyze_prose_json: 以 JSON 格式输出散文分析结果。
analyze_prose_pinker: 基于 Pinker 风格的散文分析。
analyze_risk: 风险评估分析。
analyze_sales_call: 分析销售通话记录。
analyze_spiritual_text: 分析宗教或精神类文本。
analyze_tech_impact: 分析技术对社会的影响。
analyze_terraform_plan: DevOps必备，分析 Terraform 执行计划。
analyze_threat_report: 分析安全威胁报告。
analyze_threat_report_cmds: 提取威胁报告中的命令。
analyze_threat_report_trends: 分析威胁趋势。

Create（创作与生成类）用于生成各种格式的文档、代码、可视化图表或创意内容。

create_5_sentence_summary: 生成五句式摘要。
create_academic_paper: 生成学术论文草稿。
create_ai_jobs_analysis: 生成 AI 就业分析报告。
create_aphorisms: 生成格言或警句。
create_art_prompt: 生成 AI 绘画提示词。
create_better_frame: 重构问题框架。
create_coding_feature: 生成代码功能实现思路。
create_coding_project: 生成代码项目结构。
create_command: 生成命令行指令。
create_conceptmap: 生成概念图。
create_cyber_summary: 生成网络安全摘要。
create_design_document: 开发必备，生成软件设计文档。
create_diy: 生成 DIY 指南。
create_excalidraw_visualization: 生成 Excalidraw 可视化数据。
create_flash_cards: 生成学习闪卡。
create_formal_email: 生成正式邮件。
create_git_diff_commit: 基于 git diff 生成 Commit 信息。
create_graph_from_input: 从输入生成图表数据。
create_hormozi_offer: 生成类似 Alex Hormozi 风格的商业报价。
create_idea_compass: 生成创意指南针。
create_investigation_visualization: 生成调查可视化。
create_keynote: 生成 Keynote 演讲大纲。
create_loe_document: 生成工作量估算（LOE）文档。
create_logo: 生成 Logo 设计灵感。
create_markmap_visualization: 生成思维导图（Markmap）。
create_mermaid_visualization: 生成 Mermaid 图表代码。
create_mermaid_visualization_for_github: 专为 GitHub 优化的 Mermaid 图表。
create_micro_summary: 生成微型摘要。
create_mnemonic_phrases: 生成记忆口诀。
create_network_threat_landscape: 生成网络威胁态势图。
create_newsletter_entry: 生成通讯录条目。
create_npc: 生成游戏 NPC 设定。
create_pattern: 元功能，用于创建新的 Fabric Pattern。
create_prd: 产品经理必备，生成产品需求文档（PRD）。
create_prediction_block: 生成预测模块。
create_quiz: 生成测试题。
create_reading_plan: 生成阅读计划。
create_recursive_outline: 生成递归大纲。
create_report_finding: 生成报告发现。
create_rpg_summary: 生成 RPG 游戏摘要。
create_security_update: 生成安全更新通告。
create_show_intro: 生成节目介绍。
create_sigma_rules: 生成 Sigma 安全规则。
create_story_about_people_interaction: 生成人物互动故事。
create_story_about_person: 生成人物故事。
create_story_explanation: 生成故事解释。
create_stride_threat_model: 生成 STRIDE 威胁模型。
create_summary: 生成通用摘要。
create_tags: 自动生成标签。
create_threat_scenarios: 生成威胁场景。
create_ttrc_graph: 生成 TTRC 图表。
create_ttrc_narrative: 生成 TTRC 叙述。
create_upgrade_pack: 生成升级包说明。
create_user_story: 生成用户故事。
create_video_chapters: 生成视频章节划分。
create_visualization: 生成通用可视化描述。

Extract（提取类）用于从长文本或复杂内容中提炼关键信息。

extract_algorithm_update_recommendations: 提取算法更新建议。
extract_alpha: 提取高价值信息（Alpha）。
extract_article_wisdom: 提取文章智慧。
extract_book_ideas: 提取书籍核心创意。
extract_book_recommendations: 提取书籍推荐。
extract_business_ideas: 提取商业创意。
extract_characters: 提取人物角色。
extract_controversial_ideas: 提取有争议的观点。
extract_core_message: 提取核心信息。
extract_ctf_writeup: 提取 CTF 解题思路。
extract_domains: 提取域名信息。
extract_extraordinary_claims: 提取非凡的主张。
extract_ideas: 提取创意。
extract_insights: 提取洞察。
extract_insights_dm: Daniel Miessler 风格的洞察提取。
extract_instructions: 提取操作指令。
extract_jokes: 提取笑话。
extract_latest_video: 提取最新视频信息。
extract_main_activities: 提取主要活动。
extract_main_idea: 提取中心思想。
extract_mcp_servers: 提取 MCP 服务器配置。
extract_most_redeeming_thing: 提取最有价值的一点。
extract_patterns: 提取模式。
extract_poc: 提取概念验证（PoC）。
extract_predictions: 提取预测信息。
extract_primary_problem: 提取主要问题。
extract_primary_solution: 提取主要解决方案。
extract_product_features: 提取产品特性。
extract_questions: 提取问题。
extract_recipe: 提取食谱。
extract_recommendations: 提取推荐建议。
extract_references: 提取参考文献。
extract_skills: 提取技能点。
extract_song_meaning: 提取歌曲含义。
extract_sponsors: 提取赞助商信息。
extract_videoid: 提取视频 ID。
extract_wisdom: 明星功能，提取核心智慧和洞见。
extract_wisdom_agents: 提取 Agent 相关的智慧。
extract_wisdom_dm: DM 风格的智慧提取。
extract_wisdom_nometa: 不包含元数据的智慧提取。

Summarize（摘要类）专注于不同场景下的文本摘要。

summarize: 通用摘要。
summarize_board_meeting: 董事会会议摘要。
summarize_debate: 辩论摘要。
summarize_git_changes: Git 变更摘要。
summarize_git_diff: Git Diff 摘要。
summarize_lecture: 讲座&#x2F;课程摘要。
summarize_legislation: 法律法规摘要。
summarize_meeting: 会议纪要摘要。
summarize_micro: 微型摘要。
summarize_newsletter: 通讯摘要。
summarize_paper: 论文摘要。
summarize_prompt: Prompt 摘要。
summarize_pull-requests: PR 摘要。
summarize_rpg_session: RPG 游戏进程摘要。

Coding &amp; Technical（编程与技术类）
clean_text: 清理文本格式。
coding_master: 编程专家模式。
explain_code: 解释代码逻辑。
explain_docs: 解释文档。
explain_project: 解释项目结构。
export_data_as_csv: 将数据导出为 CSV。
fix_typos: 修正拼写错误。
generate_code_rules: 生成代码规范。
improve_prompt: 优化 Prompt。
review_code: 代码审查。
review_design: 设计审查。
sanitize_broken_html_to_markdown: 修复并转换 HTML 为 Markdown。
write_latex: 编写 LaTeX。
write_nuclei_template_rule: 编写 Nuclei 扫描规则。
write_pull-request: 编写 PR 描述。
write_semgrep_rule: 编写 Semgrep 规则。

Others（其他实用工具）
agility_story: 敏捷故事编写。
ai: 通用 AI 对话。
answer_interview_question: 回答面试题。
apply_ul_tags: 添加无序列表标签。
ask_secure_by_design_questions: 询问”安全设计”相关问题。
ask_uncle_duke: 咨询 Uncle Duke（虚拟角色）。
capture_thinkers_work: 捕捉思想家的工作。
check_agreement: 检查协议。
compare_and_contrast: 比较与对比。
concall_summary: 电话会议摘要。
convert_to_markdown: 转换为 Markdown。
dialog_with_socrates: 与苏格拉底对话（苏格拉底式提问）。
enrich_blog_post: 丰富博客文章内容。
explain_math: 解释数学概念。
explain_terms: 解释术语。
find_female_life_partner: （趣味）寻找女性伴侣建议。
find_hidden_message: 寻找隐藏信息。
find_logical_fallacies: 寻找逻辑谬误。
get_wow_per_minute: 计算”惊叹时刻”频率。
heal_person: 疗愈对话。
humanize: 使文本更人性化。
identify_dsrp_...: DSRP 系统思维相关分析。
identify_job_stories: 识别 Job Stories。
improve_academic_writing: 优化学术写作。
improve_report_finding: 优化报告发现。
improve_writing: 优化通用写作。
judge_output: 评判输出质量。
label_and_rate: 标记并评分。
md_callout: Markdown 标注。
model_as_sherlock_freud: 扮演福尔摩斯或弗洛伊德。
official_pattern_template: 官方 Pattern 模板。
predict_person_actions: 预测人物行为。
prepare_7s_strategy: 准备 7S 战略分析。
provide_guidance: 提供指导。
rate_ai_response: 评分 AI 回答。
rate_ai_result: 评分 AI 结果。
rate_content: 内容评分。
rate_value: 价值评分。
raw_query: 原始查询。
recommend_artists: 推荐艺术家。
recommend_pipeline_upgrades: 推荐流水线升级。
recommend_talkpanel_topics: 推荐座谈会话题。
recommend_yoga_practice: 推荐瑜伽练习。
refine_design_document: 优化设计文档。
suggest_pattern: 建议 Pattern。
t_...: 一系列以 t_ 开头的思维工具（Thinkers）。
to_flashcards: 转换为闪卡。
transcribe_minutes: 记录会议纪要。
translate: 翻译。
tweet: 生成推文。
write_essay: 撰写文章。
write_essay_pg: Paul Graham 风格文章。
write_hackerone_report: 撰写 HackerOne 漏洞报告。
write_micro_essay: 撰写微型文章。
youtube_summary: YouTube 视频摘要。

总结Fabric 不仅仅是一个工具集合，它代表了一种将 AI 能力模块化、流程化的先进理念。通过使用 Fabric，我们可以：

复用优秀经验：直接使用专家打磨好的 Prompt (Pattern)。
提高效率：命令行接口让批量处理和自动化成为可能。
专注核心价值：从重复的 Prompt 编写中解放出来，专注于内容本身。

如果你是开发者、安全研究员、内容创作者，或者任何希望通过 AI 提升效率的人，Fabric 都值得你立即尝试。

本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>Fabric</tag>
        <tag>AI</tag>
        <tag>Prompt Engineering</tag>
        <tag>LLM</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>15个实用开源AI项目汇总：从PPT生成到语音克隆</title>
    <url>/posts/b7a7608ae340.html</url>
    <content><![CDATA[随着大语言模型（LLM）的爆发，GitHub 上涌现了大量优秀的开源 AI 项目。这些项目不仅降低了 AI 技术的使用门槛，还切实解决了许多工作和生活中的痛点。
本文精选了 15 个 偏向实用的开源 AI 项目，涵盖 PPT 自动生成、本地 LLM 交互、应用开发、前端生成、AI 搜索、私有云相册、工作流增强、语音转文字、图像生成、知识库、声音克隆 以及 数据库管理 等领域。无论你是开发者、产品经理还是普通用户，都能从中找到提升效率的利器。
1. Presenton：AI 自动生成 PPTPresenton 是一个开源的 AI 演示文稿生成器，可以看作是 Gamma、Beautiful.ai 的开源替代品。它完全在本地运行，支持使用 OpenAI、Gemini 或本地 Ollama 模型来生成内容。

GitHub: https://github.com/presenton/presenton
主要功能:
多模型支持: 支持 OpenAI, Gemini, Ollama 等多种 LLM 后端。
隐私安全: 数据掌握在自己手中，支持本地运行。
所见即所得: 生成大纲后可进行编辑，再生成最终幻灯片。
导出格式: 支持导出为 PPTX 和 PDF 格式。



快速上手 (Docker)最简单的部署方式是使用 Docker：
# 使用 OpenAI 作为后端docker run -it --name presenton \  -p 5000:80 \  -e LLM=&quot;openai&quot; \  -e OPENAI_API_KEY=&quot;sk-xxxx&quot; \  -e IMAGE_PROVIDER=&quot;dall-e-3&quot; \  -v &quot;./app_data:/app_data&quot; \  ghcr.io/presenton/presenton:latest

启动后，访问 http://localhost:5000 即可开始使用。

注意: 如果使用 Ollama 作为后端，建议配备 GPU 以获得更好的生成速度。

2. Open WebUI：功能强大的本地 LLM 界面Open WebUI（原 Ollama WebUI）是目前最流行的开源 LLM 交互界面之一。它不仅界面美观（类似 ChatGPT），而且功能极其丰富，完美适配 Ollama、OpenAI API 等多种推理后端。

GitHub: https://github.com/open-webui/open-webui
主要功能:
全能聊天: 支持多模态（图片上传）、代码高亮、Markdown 渲染。
RAG 支持: 支持上传文档（PDF, TXT, MD 等），实现基于知识库的问答。
Web 搜索: 集成搜索引擎，让模型可以联网获取最新信息。
多用户管理: 支持多用户注册、角色管理，适合团队内部部署。
模型竞技场: 可以同时对比两个模型的输出结果。



快速部署# 如果你的电脑上有 Nvidia GPUdocker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main# 如果没有 GPU (仅运行 WebUI)docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main

部署完成后，浏览器访问 http://localhost:3000。
3. Dify：一站式 AI 应用开发平台如果你想开发自己的 AI 应用（如客服机器人、文章生成器），Dify 是目前最好的开源选择之一。它是一个开源的 LLM 应用开发平台，提供“后端即服务”（Backend-as-a-Service）和可视化的 Prompt 编排。

GitHub: https://github.com/langgenius/dify
主要功能:
可视化编排: 通过拖拽节点设计 AI 工作流（Workflow）。
RAG 引擎: 内置高性能 RAG 管道，支持分段、清洗、索引等全流程。
Agent 能力: 支持 Function Calling，可以让 AI 调用外部工具。
API 发布: 一键将设计好的应用发布为 API，方便集成到现有系统。



部署与使用Dify 提供了 docker-compose 部署方案：
git clone https://github.com/langgenius/dify.gitcd dify/dockercp .env.example .envdocker compose up -d

启动后访问 http://localhost 即可进入控制台创建应用。
4. Screenshot-to-Code：截图即代码对于前端开发者来说，Screenshot-to-Code 简直是神一般的存在。你只需要把网页截图（或设计稿）拖进去，它就能利用 GPT-4 Vision 或 Claude 3 Sonnet 瞬间生成对应的 HTML&#x2F;Tailwind&#x2F;React 代码。

GitHub: https://github.com/abi/screenshot-to-code
主要功能:
多栈支持: 支持 HTML + Tailwind, React, Vue, Bootstrap 等。
AI 视觉识别: 利用大模型的视觉能力精准还原布局和组件。
实时预览: 生成代码的同时实时渲染网页效果。
再次编辑: 可以通过对话让 AI 微调生成的代码。



使用方法你需要拥有 OpenAI API Key (GPT-4 Vision) 或 Anthropic API Key。
git clone https://github.com/abi/screenshot-to-codecd screenshot-to-code/backend# 配置 API KEYecho &quot;OPENAI_API_KEY=sk-...&quot; &gt; .envpoetry installpoetry shellpoetry run uvicorn main:app --reload --port 7001

（同时也需要启动前端服务，详见官方文档）
5. Perplexica：开源 AI 搜索引擎Perplexica 是 Perplexity AI 的开源替代品。它是一个 AI 驱动的搜索引擎，能够深入互联网搜索信息，并以引经据典的方式回答你的问题。

GitHub: https://github.com/ItzCrazyKns/Perplexica
主要功能:
多模式搜索: 支持“学术搜索”、“YouTube 搜索”、“写作模式”等。
引用来源: 答案中会明确标注信息来源，减少幻觉。
本地模型: 支持使用 Ollama 等本地模型进行推理，保护隐私。
SearxNG 集成: 使用开源的元搜索引擎 SearxNG 获取搜索结果。



部署Perplexica 依赖 SearxNG 和 后端 LLM（如 Ollama 或 OpenAI API）。
git clone https://github.com/ItzCrazyKns/Perplexica.gitcd Perplexica# 配置 config.tomldocker compose up -d

6. Immich：AI 加持的私有云相册Immich 是一个高性能的、自托管的 Google Photos 替代方案。它不仅支持海量照片备份，更重要的是内置了强大的 AI 功能，支持人脸识别和语义搜索（例如搜索“海边的狗”）。

GitHub: https://github.com/immich-app/immich
主要功能:
智能搜索: 利用 CLIP 模型，支持自然语言搜索照片内容。
人脸聚类: 自动识别并分类照片中的人物。
多端同步: 拥有完善的 iOS 和 Android 客户端，支持后台自动备份。
高性能: 针对私有部署优化，响应速度极快。



部署Immich 推荐使用 Docker Compose 部署，虽然组件较多，但官方提供了一键启动配置。
wget -O docker-compose.yml https://github.com/immich-app/immich/releases/latest/download/docker-compose.ymlwget -O .env https://github.com/immich-app/immich/releases/latest/download/example.env# 修改 .env 中的密码等配置docker compose up -d

7. Fabric：人类 AI 增强框架Fabric 不是一个简单的聊天机器人，而是一个旨在“增强人类能力”的开源框架。它由知名安全专家 Daniel Miessler 创建，核心在于收集和优化了大量高质量的 Prompt（称为 Patterns），并通过 CLI 工具让你能随时随地调用 AI 解决具体问题（如提取摘要、分析安全报告、写代码注释）。

GitHub: https://github.com/danielmiessler/fabric
主要功能:
Patterns 库: 内置数百个经过打磨的 Prompt，涵盖写作、编程、安全分析等。
CLI 集成: 支持管道操作，例如 cat file.txt | fabric --pattern summarize。
多模型支持: 可以连接 OpenAI, Claude, 或本地 Ollama。



安装与使用# 需要 Go 环境go install github.com/danielmiessler/fabric@latestfabric --setup # 配置 API Key# 使用示例：总结 YouTube 视频字幕yt --transcript https://youtube.com/watch?v=xxx | fabric --pattern summarize

8. Buzz：本地实时语音转文字Buzz 是一个基于 OpenAI Whisper 模型的开源桌面软件（支持 Mac&#x2F;Windows&#x2F;Linux）。它能利用你电脑的 GPU 进行离线、高精度的语音转文字，甚至支持实时麦克风听写和翻译。

GitHub: https://github.com/chidiwilliams/buzz
主要功能:
离线转换: 基于 Whisper，无需联网，保护隐私。
实时听写: 支持实时语音转文字，适合会议记录。
多语言翻译: 支持将语音直接翻译成英文文本。
易用性: 图形化界面，拖拽音频文件即可开始转换。



9. ComfyUI：模块化 AI 绘画工作流ComfyUI 是目前最强大的 Stable Diffusion 节点式 GUI。它允许你通过拖拽节点的方式构建复杂的图像生成工作流，非常适合进阶用户和开发者探索 AI 绘画的无限可能。

GitHub: https://github.com/comfyanonymous/ComfyUI
主要功能:
节点式编辑: 像连线一样设计工作流，逻辑清晰，复用性强。
极致优化: 启动速度快，显存占用低，支持 SDXL, SD1.5, SD2.0 等多种模型。
社区生态: 拥有海量的自定义节点（Custom Nodes），可以实现 ControlNet, AnimateDiff 等高级功能。
工作流分享: 生成的图片自带工作流元数据，拖入界面即可复现。



快速启动git clone https://github.com/comfyanonymous/ComfyUIcd ComfyUI# 安装依赖 (推荐使用虚拟环境)pip install -r requirements.txt# 启动python main.py

10. RAGFlow：深度文档理解 RAG 引擎RAGFlow 是一个基于深度文档理解（DeepDoc）的开源 RAG 引擎。它不仅能检索文本，还能精准识别和解析 PDF 中的表格、图表和复杂布局，大大提升了知识库问答的准确性。

GitHub: https://github.com/infiniflow/ragflow
主要功能:
深度文档解析: 针对 PDF、Excel、Word 等格式进行深度结构化解析（OCR + 布局分析）。
可视化编排: 支持对解析后的 chunk 进行可视化管理和检索测试。
多路召回: 融合关键词检索和向量检索，提高召回率。
企业级特性: 支持多租户、权限管理，适合企业内部知识库构建。



部署git clone https://github.com/infiniflow/ragflow.gitcd ragflow/dockerdocker compose -f docker-compose-CN.yml up -d

11. GPT-SoVITS：强大的少样本语音转换与 TTSGPT-SoVITS 是目前开源界效果最惊艳的语音转换（Voice Conversion）和文本转语音（TTS）项目之一。只需要 1 分钟的语音样本，就能训练出高相似度的声音模型，支持中英日韩多语种。

GitHub: https://github.com/RVC-Boss/GPT-SoVITS
主要功能:
零样本&#x2F;少样本推理: 仅需 5 秒样本即可进行零样本 TTS，1 分钟样本可微调提升相似度。
跨语言支持: 支持中文、英文、日文混合朗读。
WebUI 界面: 提供整合包和 Web 界面，操作简单，无需写代码。
高音质: 生成的语音自然流畅，情感丰富。



12. Jan：运行在本地的 ChatGPT 替代品Jan 是一个开源的桌面端 AI 助手（支持 Mac&#x2F;Windows&#x2F;Linux），它的目标是成为 ChatGPT 的开源替代品。它允许你在本地下载并运行各种开源模型（如 Llama 3, Mistral），也可以连接 OpenAI 等远程 API。

GitHub: https://github.com/janhq/jan
主要功能:
本地优先: 所有聊天记录和模型数据都存储在本地，保护隐私。
模型中心: 内置模型下载市场，一键下载运行 GGUF 格式模型。
兼容性: 提供兼容 OpenAI 格式的本地 API Server。
扩展性: 支持通过扩展（Extensions）增强功能。



13. Upscayl：AI 图片无损放大Upscayl 是一个免费开源的 AI 图片放大工具，基于 Real-ESRGAN 等模型。它能将模糊的低分辨率图片无损放大 4 倍甚至更多，非常适合修复老照片或提升素材清晰度。

GitHub: https://github.com/upscayl/upscayl
主要功能:
一键放大: 拖入图片，选择模型，点击放大，操作极简。
本地处理: 利用本地 GPU 进行计算，无需上传图片。
多模型选择: 内置多种模型，针对二次元、写实照片等不同场景优化。
批量处理: 支持批量导入图片进行放大。



14. Open Interpreter：会写代码的 AI 终端助手Open Interpreter 是一个让 LLM 在你本地电脑上运行代码的工具。它就像是 ChatGPT 的 Code Interpreter，但运行在你的终端里，可以访问本地文件、网络和系统设置，帮你自动完成各种复杂任务。

GitHub: https://github.com/OpenInterpreter/open-interpreter
主要功能:
自然语言控制: 对它说“帮我把桌面上的 PDF 都转成 Word”，它会自动写 Python 代码并执行。
全能执行: 支持 Python, JavaScript, Shell 等多种语言。
人机协作: 在执行敏感操作前会请求用户确认，安全可控。
本地模型: 支持使用 CodeLlama 等本地模型，无需 API Key。



安装pip install open-interpreterinterpreter

15. Chat2DB：智能数据库客户端Chat2DB 是一款集成了 AI 能力的数据库管理工具。它打破了传统 SQL 客户端的模式，支持通过自然语言生成 SQL、解释 SQL 以及优化 SQL，大大降低了数据库操作的门槛。

GitHub: https://github.com/chat2db/Chat2DB
主要功能:
Text2SQL: 用自然语言描述需求，AI 自动生成 SQL 语句。
SQL 解释: 选中复杂的 SQL，让 AI 帮你解释它的含义。
多数据库支持: 支持 MySQL, PostgreSQL, Oracle, SQL Server 等主流数据库。
报表生成: 支持将查询结果自动生成简单的图表。



总结本文为您汇总了 15 个实用的开源 AI 项目，覆盖了从生产力工具到基础设施的方方面面：

Presenton: 搞定 PPT。
Open WebUI: 搞定 LLM 聊天界面。
Dify: 搞定 AI 应用开发。
Screenshot-to-Code: 搞定前端切图。
Perplexica: 搞定智能搜索。
Immich: 搞定照片管理与 AI 搜索。
Fabric: 搞定日常任务的 AI 增强与 Prompt 管理。
Buzz: 搞定语音转文字与会议记录。
ComfyUI: 搞定专业 AI 绘画工作流。
RAGFlow: 搞定复杂文档的深度解析与问答。
GPT-SoVITS: 搞定声音克隆与配音。
Jan: 搞定小白友好的本地 AI 助手。
Upscayl: 搞定图片高清放大。
Open Interpreter: 搞定自动化脚本编写与执行。
Chat2DB: 搞定数据库 SQL 编写。

这些项目大多支持 Docker 快速部署，建议挑一个最感兴趣的现在就动手试试！

本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>Tools</tag>
        <tag>Open Source</tag>
        <tag>GitHub</tag>
        <tag>Productivity</tag>
      </tags>
  </entry>
  <entry>
    <title>大语言模型各类版本详解：Base、Instruct、MoE、量化、Thinking 等到底是什么意思？</title>
    <url>/posts/b165687e5798.html</url>
    <content><![CDATA[一、为什么要搞懂大模型的各种「版本」？近年来，各种大模型名字后面越来越“花”：

Base &#x2F; Instruct &#x2F; Chat
MoE（Mixture of Experts）
AWQ &#x2F; GPTQ &#x2F; INT4 &#x2F; FP8 量化
Thinking &#x2F; DeepThink &#x2F; Step &#x2F; Reasoning

如果不了解这些后缀的含义，我们就很难：

正确选择模型：是用 Base 还是 Instruct？是要 MoE 还是稠密模型？
合理评估效果：为什么同一家模型，Instruct 版本比 Base 用起来舒服很多？
看懂论文与技术文档：里面充满了 dense、MoE、SFT、RLHF、quantization 等术语。

这篇文章的目标是：

用通俗语言 + 对比表格，解释常见大模型版本名背后的含义、原理与适用场景
帮助你在选型、部署与使用大模型时，做到：心中有数，不再迷茫


二、从「Base 模型」到「Instruct 模型」2.1 Base 模型：会“说话”，但不一定听得懂你**Base 模型（基座模型）一般指只经过预训练（Pre-training）**的大模型，它的训练目标通常是：

给定一段文本前缀，预测下一个 token（Next Token Prediction）

特点：

语言能力强，知识丰富
但没有专门对话或指令对齐训练
经常出现：回答跑偏、瞎编、忽略指令等现象

可以把 Base 模型想象成：

读了很多书的“学霸”，知识很强，但没有专门训练“如何做客服 &#x2F; 助理 &#x2F; 写代码工具”等应用场景。

2.2 Instruct &#x2F; Chat 模型：会「听人话」的助手Instruct 模型在 Base 模型基础上，额外做了对齐（Alignment）与指令微调（Instruction Tuning），常见训练流程：

收集大量指令数据：人类写的「指令 + 理想回答」
监督微调（SFT）：让模型学习这些指令-回复模式
可能再加上 RLHF、RLAIF 等方法做偏好对齐

特点：

更会「理解你的需求」
更倾向于一步一步解释、给出结构化答案
更适合作为：对话助手、代码助手、写作助手

可以把 Instruct 模型看成：

在 Base 的基础上，上过“客服&#x2F;助理&#x2F;讲解”专项训练营的版本。

2.3 Base vs Instruct 对比表


特性
Base 模型
Instruct &#x2F; Chat 模型



训练目标
预测下一个 token（预训练）
预训练 + 指令微调（SFT + 对齐）


对指令理解
一般，容易误解或跑题
好，能围绕指令展开回答


输出风格
偏“原始语料风格”，有时不够礼貌或结构化
通常更礼貌、有结构、更像“产品级”回答


适合场景
下游再微调、研究、生成原始语料
聊天、问答、代码助手、写作助手


灵活性
更接近“原始模型”，可塑性更强
已做对齐，某些研究场景可能更偏“产品化”行为


选型建议：

做终端应用（Chatbot、助手类产品）：优先选 Instruct &#x2F; Chat 版本
做二次训练 &#x2F; 研究：可以考虑从 Base 版本出发，自己做对齐


三、MoE 模型：Mixture of Experts —— “专家混合”架构3.1 什么是 MoE？传统 Transformer 是稠密模型（Dense Model）：

每一层的参数都参与每一次推理

而 MoE（Mixture of Experts） 的核心思想是：

每一层并不是只有一个大 FFN，而是有很多个“专家（Expert）”
每次只激活其中的一小部分（比如 2&#x2F;8、4&#x2F;16）

可以用一个简单的“路由图”来理解：

输入 token 经过 Router（路由器）
Router 根据输入特征，选出 一小部分 Expert 参与计算
再把各 Expert 的输出加权合并

3.2 MoE 与稠密模型对比


对比维度
Dense 模型
MoE 模型（Mixture of Experts）



参数参与度
每次推理，所有参数都参与
每次推理，只激活部分专家


总参数量
较少
总参数量可以很大（数千亿甚至万亿）


有效计算量
等于总参数量
小于总参数量，只计算被激活的部分


优势
结构简单，实现成熟
参数量大但计算可控，可提升模型容量


挑战
扩容 &#x3D; 参数、计算量一起涨
Router 训练难、负载均衡难、部署复杂


直观理解：

Dense：每个问题都找同一位“通才”
MoE：根据问题，把活分给少数几个相关专家

3.3 MoE 的意义
在相同算力预算下，MoE 可以让模型拥有更大的参数容量
对多任务、多语言、多领域场景，会有更好的表达能力与可扩展性
越来越多企业级模型与开源模型采用 MoE 架构，以实现 “大而不贵” 的效果


四、AWQ 与量化：让大模型“瘦身上岗”4.1 为什么要量化（Quantization）？原始大模型通常使用 FP16 &#x2F; BF16 等较高精度浮点格式，在部分新硬件上也开始支持 FP8：

显存占用巨大
对部署硬件要求高

量化（Quantization） 的核心目标：

用更少的位宽（如 INT8 &#x2F; INT4）表示权重或激活
减少：
参数存储空间（显存 &#x2F; 内存）
计算量（乘加操作更便宜）



4.2 常见量化方式概览


名称
典型精度
主要对象
说明



FP8
8 bit
权重 &#x2F; 激活
低精度浮点格式，在部分 GPU 上用于高效训练推理


INT8
8 bit
权重 &#x2F; 激活
最常见的通用量化，效果与损失较平衡


INT4
4 bit
权重为主
压缩更凶猛，需精细设计以控制精度损失


GPTQ
INT3&#x2F;4
权重
一种离线权重量化算法，常用于开源模型


AWQ
4 bit
权重
感知权重重要性，尽量保护“重要权重”


KV Cache 量化
INT8&#x2F;4
KV Cache 缓存
降低长上下文推理时的显存压力


4.3 AWQ（Activation-aware Weight Quantization）是啥？AWQ 是一种比较流行的权重离线量化方法，主要思路：

在量化权重时，考虑激活（Activation）信息
通过小规模数据推理，估计哪些权重对模型输出更敏感
对敏感权重给予更好的量化策略，减少精度损失

可以简单理解为：

不是“平均压扁”所有权重，而是“有选择性地压”，保护关键部分。

4.4 量化的收益与代价


维度
好处
代价 &#x2F; 风险



显存占用
显著下降（可节省 2–4 倍甚至更多）
需要额外量化流程，某些结构难以量化


推理速度
在支持的硬件上，推理可明显提速
若算子不支持，反而可能变慢


精度
轻量量化（INT8）影响较小
激进量化（INT4）可能带来明显精度退化


可用性
更容易在单卡 &#x2F; 边缘设备部署
需要考虑算子支持、框架版本、工具链兼容性


选型建议：

如果你是本地 &#x2F; 个人部署，优先考虑：
4-bit &#x2F; 8-bit 量化模型
例如：q4_k_m、AWQ 4bit、GPTQ 4bit 等


对精度要求高的线上服务：
训练或主推理精度优先使用 FP16 &#x2F; BF16，在硬件支持的情况下可以考虑 FP8 提升吞吐
对延迟或成本敏感的子任务，再结合 INT8 &#x2F; INT4 量化或 KV Cache 量化，并在评测集上验证质量后上线




五、「Thinking &#x2F; Reasoning &#x2F; 深度思考」模型：鼓励模型“想清楚再回答”5.1 为什么会出现 Thinking &#x2F; Reasoning 模型？传统 Chat &#x2F; Instruct 模型的训练目标通常是：

直接给出一个“好看”的最终答案

但对于：

复杂推理（数学、逻辑题）
多步骤规划（写代码、设计方案）

如果模型不“显式思考”，容易：

一步到位给错答案
中间推理过程完全不可见，难以检查和纠错

5.2 Thinking 模型的核心思路所谓 Thinking &#x2F; Reasoning &#x2F; DeepThink &#x2F; Step 系列模型，核心做法可以概括为：

在训练和推理时，显式鼓励“逐步思考”，而不是立刻给结论
常见训练方式包括：
使用带有**中间推理过程（Chain-of-Thought，CoT）**的数据
让模型先生成“思考过程”，再给最终答案
部分商用模型还会把“思考过程”隐藏或截断，只展示最终结论



5.3 Thinking 模型与普通 Instruct 模型对比


维度
普通 Instruct &#x2F; Chat 模型
Thinking &#x2F; Reasoning 模型



输出风格
直接给答案，解释较少
倾向于先分析过程，再给出结论


复杂任务表现
容易“自信地给错答案”
在数学、逻辑、代码调试等任务上更稳健


推理时长
通常较短
因为生成中间推理文本，响应更长、更慢


成本
Token 消耗相对较少
Token 消耗更多，成本上升


直观理解：

普通模型：像是一个很快给你答案的同学，但不一定对
Thinking 模型：像是一个会在草稿纸上演算一遍再告诉你结果的同学


六、把这些概念放在一起看：一个统一的「模型版本地图」下面用一个表格把前面讲到的概念串起来，帮助你建立整体认知。



维度
常见选项 &#x2F; 术语
主要解决什么问题？



模型对齐程度
Base &#x2F; Instruct &#x2F; Chat
从“仅预训练”到“更听指令、贴近人类偏好”


架构类型
Dense &#x2F; MoE
在算力预算下，如何扩展模型容量


权重精度
FP16 &#x2F; BF16 &#x2F; FP8 &#x2F; INT8 &#x2F; INT4
如何在有限显存下部署更大的模型


量化算法
GPTQ &#x2F; AWQ &#x2F; KV 量化
在保证精度的前提下，进一步压缩模型


推理风格
普通 Chat &#x2F; Thinking &#x2F; Reasoning
是“直接给答案”还是“显式推理后给答案”


你可以把模型“版本”理解为在这些维度上的不同组合：

Qwen2.5-32B-Base：大容量 Dense + Base + FP16&#x2F;BF16
Qwen2.5-7B-Instruct-AWQ：相对小模型 + Instruct + 4bit AWQ 量化
DeepSeek-R1 &#x2F; o3-mini 等：对推理过程做特殊优化的 Thinking &#x2F; Reasoning 类型模型


七、从工程实践角度：如何根据场景选择模型版本？下面从几个典型场景给出实用选型建议，便于在实际项目中落地。
7.1 本地玩玩 &#x2F; 个人知识助手
优先选择：
参数量在 7B–14B 左右
有 Instruct &#x2F; Chat 版本
支持 4-bit &#x2F; 8-bit 量化（AWQ &#x2F; GPTQ 等）


这样可以：
在单张消费级显卡 &#x2F; Mac 上运行
有不错的聊天、写作、问答体验



7.2 企业内网部署 &#x2F; 私有化知识问答
建议组合：
架构：Dense 或成熟的 MoE（优先选择社区使用多、工具链完善的）
版本：Instruct &#x2F; Chat
精度：优先 FP16 &#x2F; BF16，在支持 FP8 的 GPU 上可以评估 FP8 推理，通过验证后再做 INT8 &#x2F; INT4 量化


重点关注：
对齐效果：是否容易瞎编、是否能遵守安全策略
推理成本：QPS、延迟、显存占用
运维复杂度：MoE 部署与监控链路更复杂



7.3 需要复杂推理的场景（代码分析、数学、规划）
可以优先考虑：
标记为 Thinking &#x2F; Reasoning &#x2F; DeepThink &#x2F; Step 等版本
或支持显式 CoT 提示、工具调用的模型


同时注意：
成本：这类模型每次对话往往会多出不少 token
时延：生成“思考过程”会略微增加响应时间
隐私：如果中间推理过程包含敏感信息，需注意日志与存储策略




八、总结：理解「版本」背后，是在理解模型能力的维度最后用一张小表，作为这篇文章的整体总结。



关键词
核心一句话理解
你可以用它来解决什么问题？



Base
只做过预训练的“原始大脑”
作为微调基座、研究原始能力


Instruct
经过指令微调、更懂人话的助手
用于 Chatbot、写作、代码助手等终端应用


MoE
只激活部分专家的“专家混合”结构
在算力有限的前提下，让模型“更大更强”


量化 &#x2F; AWQ
把权重“压缩打包”，减少显存和计算成本
在消费级或边缘设备上部署大模型


Thinking
显式“想一想再回答”的推理风格
提升复杂任务、数学、代码调试等场景的可靠性


理解这些版本背后的含义，本质上是在理解：

模型能力的来源（预训练 vs 指令对齐）
模型容量与算力的权衡方式（Dense vs MoE，FP16 vs 量化）
模型行为风格（直接回答 vs 显式推理）

当你再看到一个新模型时，可以试着问自己：

它是 Base 还是 Instruct？
是 Dense 还是 MoE？
是 全精度还是量化？
是普通 Chat，还是号称 Thinking &#x2F; Reasoning？

只要能在这几个维度上给出答案，你对这个模型的预期、选型与部署策略，就会清晰很多。

本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>大语言模型</tag>
        <tag>模型版本</tag>
        <tag>量化</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>使用vLLM部署Qwen3-Next-80B-A3B-Instruct大模型完整指南</title>
    <url>/posts/2c6cec23da85.html</url>
    <content><![CDATA[在大模型时代，如何高效部署和运维一个80B级别的大语言模型服务是许多AI工程师面临的挑战。本文将详细介绍使用vLLM部署Qwen3-Next-80B-A3B-Instruct模型的完整流程，包括模型查找、参数配置、显存估算、下载部署、监控管理、性能压测以及推理追踪等关键环节。通过本文，您将能够快速搭建一个生产级别的大模型推理服务。
目标读者本文适合以下读者：

AI&#x2F;ML工程师，需要部署大规模语言模型服务
DevOps工程师，负责管理和运维大模型推理平台
技术架构师，评估大模型部署方案
研究人员，需要高性能推理环境

一、模型查找与选择1.1 Qwen3-Next-80B-A3B-Instruct模型介绍Qwen3-Next-80B-A3B-Instruct是阿里云通义千问团队推出的最新一代大语言模型，采用先进的MoE（Mixture of Experts）架构，具有以下特点：

模型架构：MoE混合专家模型，总参数80B，激活参数仅3B
性能优势：以3B的计算成本获得接近80B Dense模型的性能
上下文长度：支持最长256K tokens的上下文（推理时建议8K-32K以平衡性能）
多语言能力：中英文双语能力突出
指令跟随：经过指令微调，适合对话和任务执行
推理效率：相比传统80B Dense模型，推理速度提升约20-25倍


MoE架构说明：模型包含多个专家网络，每次推理只激活部分专家（约3B参数），其余专家处于休眠状态。这种设计使得大规模模型可以在相对较小的计算资源上高效运行，同时保持接近全量模型的性能表现。

1.2 在HuggingFace查找模型访问HuggingFace模型库查找模型：
# 浏览器访问https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct# 或在HuggingFace网站搜索https://huggingface.co/models?search=qwen3-next-80b-a3b

1.3 在ModelScope查找模型（国内推荐）对于国内用户，建议使用ModelScope：
# 浏览器访问https://modelscope.cn/models/qwen/Qwen3-Next-80B-A3B-Instruct# 或使用modelscope命令行工具搜索modelscope download --help

1.4 模型文件结构说明一个标准的HuggingFace模型包含以下关键文件：
Qwen3-Next-80B-A3B-Instruct/├── config.json                    # 模型配置文件├── tokenizer_config.json          # 分词器配置├── tokenizer.json                 # 分词器文件├── special_tokens_map.json        # 特殊token映射├── generation_config.json         # 生成配置├── pytorch_model.bin.index.json   # 模型权重索引├── pytorch_model-00001-of-00017.bin  # 模型权重文件（分片）├── pytorch_model-00002-of-00017.bin├── ...└── README.md                      # 模型说明文档

二、关键参数说明2.1 模型加载参数vLLM提供了丰富的参数来控制模型加载和推理行为：
基础配置参数


参数名称
说明
默认值
推荐值（80B模型）



--model
模型名称或路径
必填
Qwen/Qwen3-Next-80B-A3B-Instruct


--tensor-parallel-size
张量并行度（GPU数量）
1
4-8


--dtype
数据类型
auto
bfloat16 or float16


--max-model-len
最大序列长度
模型默认
8192-32768


--gpu-memory-utilization
GPU显存利用率
0.9
0.85-0.9


性能优化参数


参数名称
说明
默认值
推荐值



--max-num-seqs
最大并行序列数
256
64-128


--max-num-batched-tokens
批次最大token数
根据显存自动
8192


--enable-chunked-prefill
分块预填充
False
True


--enable-prefix-caching
前缀缓存
False
True


量化参数


参数名称
说明
可选值



--quantization
量化方法
awq, gptq, fp8


--kv-cache-dtype
KV缓存数据类型
auto, fp8


2.2 API服务参数


参数名称
说明
默认值
推荐值



--host
服务监听地址
127.0.0.1
0.0.0.0


--port
服务端口
8000
8000


--api-key
API密钥
None
设置强密钥


--served-model-name
服务中的模型名称
与模型名相同
自定义


--max-log-len
日志最大长度
None
100


2.3 推理生成参数这些参数在调用API时传递：



参数名称
说明
默认值
范围



temperature
生成温度，控制随机性
1.0
0.0-2.0


top_p
核采样概率
1.0
0.0-1.0


top_k
Top-K采样
-1（禁用）
1-100


max_tokens
最大生成长度
16
1-32768


frequency_penalty
频率惩罚
0.0
-2.0-2.0


presence_penalty
存在惩罚
0.0
-2.0-2.0


repetition_penalty
重复惩罚
1.0
1.0-2.0


2.4 关键性能参数详解 ⭐这三个参数对显存占用和推理性能有重大影响，需要仔细配置：
--max-model-len：最大序列长度作用：控制单个请求的输入+输出的最大token数
显存影响：
# KV缓存显存计算公式KV_cache_memory = (    max_model_len *           # 序列长度    num_layers *              # 层数（Qwen3-80B约80层）    hidden_size *             # 隐藏层大小（约8192）    2 *                       # K和V两个cache    dtype_bytes *             # 数据类型（BF16=2字节）    max_num_seqs              # 并发序列数) / (1024**3) / tensor_parallel_size# 示例：计算不同max-model-len的显存占用# 假设：80层，hidden_size=8192，BF16，64并发，4卡并行max_model_len=4096  -&gt; KV缓存约 8GB/GPUmax_model_len=8192  -&gt; KV缓存约 16GB/GPU  max_model_len=16384 -&gt; KV缓存约 32GB/GPUmax_model_len=32768 -&gt; KV缓存约 64GB/GPU

性能影响：

✅ 越大：支持更长的上下文，但显存占用成线性增长
⚠️ 过大：显存不足会导致OOM或降低并发能力
💡 建议：根据实际业务需求设置，不要盲目追求最大值

配置建议：



GPU配置
推荐max-model-len
显存占用
适用场景



4×A100-80GB (BF16)
8192
~16GB KV
通用对话


4×A100-80GB (BF16)
16384
~32GB KV
长文档分析


8×A100-80GB (BF16)
32768
~32GB KV
RAG检索


4×A100-80GB (FP8)
16384
~16GB KV
性价比高


--max-num-seqs：最大并发序列数作用：同时处理的请求数量上限
显存影响：
# 显存需求与max-num-seqs成正比total_memory = (    model_weights +                    # 模型权重（固定）    KV_cache * max_num_seqs +          # KV缓存（线性增长）    activation * active_seqs           # 激活值（动态）)# 示例：4卡A100-80GB，max-model-len=8192，BF16max_num_seqs=32  -&gt; KV缓存约 8GB/GPU  (可用显存充足)max_num_seqs=64  -&gt; KV缓存约 16GB/GPU (推荐配置)max_num_seqs=128 -&gt; KV缓存约 32GB/GPU (接近上限)max_num_seqs=256 -&gt; KV缓存约 64GB/GPU (可能OOM)

性能影响：

✅ 越大：支持更高并发，吞吐量提升
⚠️ 过大：显存不足，或导致batch过大降低响应速度
⚠️ 过小：并发能力受限，GPU利用率低
💡 最优值：使GPU利用率达到80-90%

配置建议：
# 保守配置（稳定优先）--max-num-seqs 32 \--max-model-len 8192# 平衡配置（推荐）--max-num-seqs 64 \--max-model-len 8192# 激进配置（吞吐优先）--max-num-seqs 128 \--max-model-len 8192# 长上下文配置--max-num-seqs 32 \--max-model-len 32768

--max-num-batched-tokens：批次最大token数作用：单个批次中所有序列的token总数上限
计算关系：
# 约束条件max_num_batched_tokens &gt;= max_model_lenmax_num_batched_tokens &lt;= max_num_seqs * max_model_len# vLLM动态批处理逻辑actual_batch_tokens = sum([    len(seq.prompt) + seq.output_len     for seq in current_batch])# 如果 actual_batch_tokens &gt; max_num_batched_tokens:#     等待下一个batch

显存影响：
# 影响激活值显存（对MoE模型影响较小）activation_memory = (    max_num_batched_tokens *     hidden_size *     num_layers *     dtype_bytes *     activation_factor  # MoE约为0.05，Dense约为1.0) / (1024**3) / tensor_parallel_size# 示例：MoE模型，4卡，BF16max_num_batched_tokens=8192  -&gt; 激活值约 0.5GB/GPUmax_num_batched_tokens=16384 -&gt; 激活值约 1.0GB/GPUmax_num_batched_tokens=32768 -&gt; 激活值约 2.0GB/GPU

性能影响：

✅ 合理设置：优化批处理效率
⚠️ 过小：限制批次大小，降低GPU利用率
⚠️ 过大：可能导致单个batch过大，增加延迟
💡 自动模式：不设置则vLLM自动计算（推荐）

配置建议：
# 方式1：自动模式（推荐）vllm serve model-path \    --max-model-len 8192 \    --max-num-seqs 64    # 不设置max-num-batched-tokens，让vLLM自动计算# 方式2：手动设置vllm serve model-path \    --max-model-len 8192 \    --max-num-seqs 64 \    --max-num-batched-tokens 16384  # 通常设为 max_model_len * 2# 方式3：高并发短序列vllm serve model-path \    --max-model-len 2048 \    --max-num-seqs 128 \    --max-num-batched-tokens 8192

三个参数的协同配置场景1：通用对话服务（推荐）
vllm serve model-path \    --tensor-parallel-size 4 \    --max-model-len 8192 \          # 支持8K上下文    --max-num-seqs 64 \             # 64并发    --gpu-memory-utilization 0.85   # 自动计算batched-tokens预期性能：- QPS: 20-40- 平均延迟: 1-2秒- GPU显存: 每卡约55GB- 吞吐量: 1500-2000 tokens/s

场景2：长文档处理
vllm serve model-path \    --tensor-parallel-size 8 \    --max-model-len 32768 \         # 支持32K上下文    --max-num-seqs 32 \             # 降低并发以节省显存    --gpu-memory-utilization 0.90预期性能：- QPS: 5-10- 平均延迟: 3-5秒- GPU显存: 每卡约60GB- 适合: RAG、文档分析

场景3：高吞吐批处理
vllm serve model-path \    --tensor-parallel-size 4 \    --max-model-len 4096 \          # 较短上下文    --max-num-seqs 128 \            # 高并发    --max-num-batched-tokens 16384 \    --gpu-memory-utilization 0.85预期性能：- QPS: 50-80- 平均延迟: 0.5-1秒- GPU显存: 每卡约50GB- 适合: API服务、批量推理

场景4：低显存环境
vllm serve model-path \    --tensor-parallel-size 4 \    --max-model-len 4096 \          # 限制序列长度    --max-num-seqs 16 \             # 低并发    --gpu-memory-utilization 0.75   # 保守设置预期性能：- QPS: 8-15- 平均延迟: 1-2秒- GPU显存: 每卡约45GB- 适合: 显存受限环境

参数调优工作流# 步骤1：从保守配置开始vllm serve model-path \    --max-model-len 8192 \    --max-num-seqs 32 \    --gpu-memory-utilization 0.80# 步骤2：监控GPU显存使用nvidia-smi dmon -s u -d 5# 步骤3：如果显存充足（&lt;70%），逐步提升# 方式A：增加并发--max-num-seqs 64  # 提升2倍# 方式B：增加上下文--max-model-len 16384  # 提升2倍# 方式C：两者平衡--max-model-len 12288 \--max-num-seqs 48# 步骤4：压测验证# 使用压测工具测试实际QPS和延迟# 步骤5：根据业务需求微调# 如果追求低延迟：降低max-num-seqs# 如果追求高吞吐：提升max-num-seqs

显存占用速查表4卡A100-80GB，BF16，Qwen3-80B-A3B配置：



max-model-len
max-num-seqs
每卡显存
适用QPS
适用场景



4096
32
~48GB
15-25
低并发


8192
32
~50GB
12-20
平衡


8192
64
~55GB
20-40
推荐⭐


8192
128
~65GB
40-60
高并发


16384
32
~58GB
10-15
长文档


16384
64
~68GB
15-25
长文档+并发


32768
16
~60GB
5-10
超长文档



关键原则：

显存占用 ≈ 模型权重 + (max_model_len × max_num_seqs × 系数)
MoE模型的”系数”比Dense模型小很多
优先保证稳定性，再追求性能
定期监控和调整


三、显存预估与资源规划3.1 MoE模型架构说明重要：Qwen3-Next-80B-A3B-Instruct 是一个 MoE（Mixture of Experts）架构的模型，具有以下特点：

总参数量：80B（800亿参数）
激活参数量：3B（30亿参数）
显存占用：需要加载全部80B参数到显存（约160GB for BF16）
计算优势：推理时只计算3B参数，大幅降低计算量和功耗


MoE工作原理：模型包含多个”专家”网络，所有专家权重都加载在GPU显存中。推理时，路由网络决定激活哪些专家（约3B参数），其余专家权重保持”休眠”状态（在显存中但不参与计算）。这种设计的关键优势是：

权重显存：与Dense模型相同（需要完整的80B）
计算显存：远小于Dense模型（只需3B的激活值和中间结果）
推理速度：快20-25倍（计算量只有3.75%）


3.2 显存占用详解理解MoE模型的显存分配很重要：



显存类型
Dense 80B
MoE 80B-A3B
说明



模型权重
160GB
160GB
相同（都需加载全部参数）


KV缓存
30GB
30GB
相同（取决于序列长度）


激活值&#x2F;梯度
~30GB
~2GB
大幅减少（只计算3B）


总显存
~220GB
~192GB
节省约13%



关键理解：MoE模型的优势主要体现在计算效率而非权重显存。虽然显存节省有限，但推理速度提升巨大。

3.3 模型权重显存计算vLLM默认加载方式：全量加载所有专家到GPU显存
对于MoE模型，显存需求计算公式：
显存需求（GB）= 模型权重显存 + KV缓存显存 + 激活值显存

FP16&#x2F;BF16 精度（推荐）# 1. 模型权重显存（所有80B参数）80B × 2 bytes = 160GB# 2. KV缓存（取决于batch size和序列长度）# 假设最大序列8192，batch size 64KV缓存 ≈ 20-30GB# 3. 激活值和中间结果（只需3B计算）激活值 ≈ 2-5GB  # 比Dense模型的30GB小得多# 4. 总显存需求基础显存 = 160GB + 30GB + 5GB = 195GB实际使用（含10%系统开销）≈ 195GB × 1.1 = 215GB# 5. 多卡分配（张量并行）每卡显存（4卡）= 215GB ÷ 4 ≈ 54GB ✅ A100-80GB可用每卡显存（2卡）= 215GB ÷ 2 ≈ 108GB ❌ 超出A100-80GB

FP8&#x2F;INT8 量化# 模型权重显存（量化后）80B × 1 byte = 80GB# KV缓存和激活值KV缓存 ≈ 15-20GB激活值 ≈ 1-3GB# 总显存需求基础显存 = 80GB + 20GB + 3GB = 103GB实际使用 ≈ 103GB × 1.1 = 113GB# 多卡分配每卡显存（2卡）= 113GB ÷ 2 ≈ 57GB ✅ A100-80GB可用每卡显存（单卡）= 113GB ❌ 超出A100-80GB


重要提示：

MoE模型的显存需求主要在模型权重（160GB）
激活值显存很小（5GB vs Dense的30GB），这是速度快的原因
不要期望MoE能显著减少显存需求，它的优势在计算速度
vLLM会将所有专家参数分布到所有GPU上


3.4 GPU配置方案（针对MoE模型）
配置前提：基于vLLM全量加载所有专家参数的方式

方案一：A100-80GB × 3卡（最小配置）总显存: 240GB可用显存（90%利用率）: 216GB数据精度: BF16张量并行度: 3最大序列长度: 8192预估并发: 24-48 requests适用场景: 开发测试、预算受限说明: 最小配置，215GB刚好能放下      每卡约72GB，留有少量余量

方案二：A100-80GB × 4卡（生产推荐）⭐总显存: 320GB可用显存（90%利用率）: 288GB每卡分配: ~54GB数据精度: BF16张量并行度: 4最大序列长度: 16384预估并发: 64-128 requests适用场景: 生产环境、高并发服务说明: 最佳平衡方案      每卡仅用54GB，余量充足      可支持更长上下文和更大batch

方案三：A100-80GB × 8卡（高性能）总显存: 640GB可用显存: 576GB每卡分配: ~27GB数据精度: BF16张量并行度: 8最大序列长度: 32768预估并发: 256+ requests适用场景: 超大规模服务、超长上下文说明: 每卡显存占用极低      适合处理256K上下文和海量并发      成本较高，适合高价值场景

方案四：A100-80GB × 2卡 + FP8量化（性价比）⭐总显存: 160GB可用显存: 144GB每卡分配: ~57GB数据精度: FP8量化张量并行度: 2最大序列长度: 8192预估并发: 32-64 requests适用场景: 成本敏感、中等性能需求说明: FP8量化后，2卡即可运行      相比BF16节省约50%成本      性能略有下降（~5%）

方案五：A100-40GB × 4卡 + FP8量化（经济型）总显存: 160GB可用显存: 144GB每卡分配: ~28GB数据精度: FP8量化张量并行度: 4最大序列长度: 8192预估并发: 32-48 requests适用场景: 预算受限、开发测试说明: 使用40GB显卡降低硬件成本      需要FP8量化才能运行      硬件成本节省约50%


配置建议总结：

BF16精度：最少需要3卡A100-80GB，推荐4卡
FP8量化：最少需要2卡A100-80GB或4卡A100-40GB
单卡部署：不推荐，显存不足（BF16需215GB）


3.4 显存使用估算工具方法1：试运行监控法（推荐）最简单的方法是直接启动服务，配合 nvidia-smi 观察显存占用：
# 1. 在一个终端启动监控watch -n 1 nvidia-smi# 2. 在另一个终端启动vLLM（使用最小并发配置）vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 4 \    --dtype bfloat16 \    --max-model-len 8192 \    --max-num-seqs 1 \    --gpu-memory-utilization 0.9

如何解读：

vLLM启动时会预先申请 gpu-memory-utilization (默认90%) 的显存。
观察日志中的 # GPU blocks 数量，这代表了可用于 KV Cache 的空间。
如果启动成功且 nvidia-smi 显示显存占用稳定，说明配置可行。
如果发生 OOM (Out Of Memory)，则需要降低 max-model-len 或增加 GPU 数量。

方法2：通过Python脚本测量（修正版）由于vLLM使用多进程架构（Multiprocessing），模型权重加载在子进程中，因此不能直接使用 torch.cuda.memory_allocated() 在主进程查看显存。
建议使用以下脚本，通过调用 nvidia-smi 获取真实的系统级显存占用：
import subprocessfrom vllm import LLMdef print_gpu_memory(label=&quot;&quot;):    &quot;&quot;&quot;通过nvidia-smi获取实际显存占用&quot;&quot;&quot;    print(f&quot;\n=== &#123;label&#125;显存状态 ===&quot;)    try:        # 查询显存使用情况        result = subprocess.check_output(            [&#x27;nvidia-smi&#x27;, &#x27;--query-gpu=index,memory.used,memory.total&#x27;, &#x27;--format=csv,noheader,nounits&#x27;],            encoding=&#x27;utf-8&#x27;        )        lines = result.strip().split(&#x27;\n&#x27;)        total_used = 0        for line in lines:            if not line.strip(): continue            idx, used, total = line.split(&#x27;,&#x27;)            used_mb = float(used.strip())            total_used += used_mb            print(f&quot;GPU &#123;idx&#125;: &#123;used_mb:.0f&#125; MiB / &#123;total.strip()&#125; MiB&quot;)                print(f&quot;总计已用: &#123;total_used/1024:.2f&#125; GB&quot;)        print(f&quot;平均每卡: &#123;total_used/len(lines)/1024:.2f&#125; GB&quot;)        return total_used    except Exception as e:        print(f&quot;获取显存信息失败: &#123;e&#125;&quot;)        return 0# 1. 打印初始状态print_gpu_memory(&quot;加载前&quot;)print(&quot;\n&gt;&gt;&gt; 开始加载模型...&quot;)# 2. 加载模型# 注意：vLLM会预占 gpu_memory_utilization 比例的显存llm = LLM(    model=&quot;/data/models/Qwen3-Next-80B-A3B-Instruct&quot;,    tensor_parallel_size=4,    dtype=&quot;bfloat16&quot;,    max_model_len=8192,    gpu_memory_utilization=0.9,    max_num_seqs=1,    trust_remote_code=True)# 3. 打印加载后状态print_gpu_memory(&quot;加载后&quot;)


注意：脚本运行结束后可能会报 EngineCore died unexpectedly，这是因为脚本退出导致子进程被强制关闭，属于正常现象，只要能看到”加载后显存状态”的输出即可。

方法3：手动计算公式（精确版）def estimate_moe_memory(    total_params_b=80,           # 总参数量（B）    active_params_b=3,           # 激活参数量（B）    dtype_bytes=2,               # 数据类型字节数（FP16/BF16=2, FP8=1）    max_seq_len=8192,            # 最大序列长度    max_batch_size=64,           # 最大批次大小    num_layers=80,               # 层数（估算）    hidden_size=8192,            # 隐藏层大小（估算）    num_attention_heads=64,      # 注意力头数（估算）    tensor_parallel=4            # 张量并行度):    &quot;&quot;&quot;    MoE模型显存精确估算    &quot;&quot;&quot;    print(f&quot;=== MoE模型显存估算 ===&quot;)    print(f&quot;配置: &#123;total_params_b&#125;B参数, &#123;active_params_b&#125;B激活&quot;)    print(f&quot;推理配置: seq_len=&#123;max_seq_len&#125;, batch_size=&#123;max_batch_size&#125;&quot;)    print(f&quot;并行度: &#123;tensor_parallel&#125;卡张量并行\n&quot;)        # 1. 模型权重（全量加载，所有专家都在显存）    model_weights_gb = (total_params_b * dtype_bytes * 1e9) / (1024**3)    per_gpu_weights_gb = model_weights_gb / tensor_parallel        print(f&quot;1. 模型权重:&quot;)    print(f&quot;   总大小: &#123;model_weights_gb:.2f&#125; GB&quot;)    print(f&quot;   每GPU: &#123;per_gpu_weights_gb:.2f&#125; GB&quot;)        # 2. KV缓存（与激活参数相关性较弱，主要看序列长度）    # 公式: batch_size * seq_len * num_layers * hidden_size * 2 (K和V) * dtype_bytes    kv_cache_gb = (        max_batch_size * max_seq_len * num_layers *         hidden_size * 2 * dtype_bytes    ) / (1024**3) / tensor_parallel        print(f&quot;\n2. KV缓存:&quot;)    print(f&quot;   每GPU: &#123;kv_cache_gb:.2f&#125; GB&quot;)        # 3. 激活值（只需激活参数的计算）    # MoE的优势：激活值显著减少    # Dense模型激活值 vs MoE激活值比例约为 total_params / active_params    dense_activation_ratio = total_params_b / active_params_b        # Dense模型的激活值估算    dense_activation_gb = (        max_batch_size * max_seq_len * hidden_size *         num_layers * dtype_bytes * 8  # 8个中间tensor（FFN, attention等）    ) / (1024**3) / tensor_parallel        # MoE只需激活部分    moe_activation_gb = dense_activation_gb / dense_activation_ratio * 1.2  # 1.2是路由开销        print(f&quot;\n3. 激活值和中间结果:&quot;)    print(f&quot;   Dense模型需要: &#123;dense_activation_gb:.2f&#125; GB/GPU&quot;)    print(f&quot;   MoE实际需要: &#123;moe_activation_gb:.2f&#125; GB/GPU&quot;)    print(f&quot;   节省比例: &#123;(1 - moe_activation_gb/dense_activation_gb)*100:.1f&#125;%&quot;)        # 4. 系统开销（CUDA context, 碎片等）    overhead_gb = 3        print(f&quot;\n4. 系统开销:&quot;)    print(f&quot;   每GPU: &#123;overhead_gb:.2f&#125; GB&quot;)        # 5. 总计    total_per_gpu = per_gpu_weights_gb + kv_cache_gb + moe_activation_gb + overhead_gb        print(f&quot;\n&#123;&#x27;=&#x27;*50&#125;&quot;)    print(f&quot;每GPU总计: &#123;total_per_gpu:.2f&#125; GB&quot;)    print(f&quot;建议显存: &#123;total_per_gpu * 1.1:.2f&#125; GB (含10%安全余量)&quot;)    print(f&quot;总显存需求: &#123;total_per_gpu * tensor_parallel:.2f&#125; GB (&#123;tensor_parallel&#125;卡)&quot;)    print(f&quot;&#123;&#x27;=&#x27;*50&#125;\n&quot;)        # 6. 配置建议    print(&quot;GPU配置建议:&quot;)    if total_per_gpu * 1.1 &lt;= 40:        print(f&quot;✅ 可用A100-40GB (每卡&#123;total_per_gpu*1.1:.1f&#125;GB &lt; 40GB)&quot;)    elif total_per_gpu * 1.1 &lt;= 80:        print(f&quot;✅ 可用A100-80GB (每卡&#123;total_per_gpu*1.1:.1f&#125;GB &lt; 80GB)&quot;)    else:        print(f&quot;❌ 需要更大显存或更多卡 (每卡需&#123;total_per_gpu*1.1:.1f&#125;GB)&quot;)        return &#123;        &#x27;per_gpu_total&#x27;: total_per_gpu,        &#x27;per_gpu_weights&#x27;: per_gpu_weights_gb,        &#x27;per_gpu_kv_cache&#x27;: kv_cache_gb,        &#x27;per_gpu_activation&#x27;: moe_activation_gb,        &#x27;per_gpu_overhead&#x27;: overhead_gb    &#125;# 示例1：4卡A100-80GB，BF16精度print(&quot;\n【示例1：生产环境配置】&quot;)result = estimate_moe_memory(    total_params_b=80,    active_params_b=3,    dtype_bytes=2,          # BF16    max_seq_len=8192,    max_batch_size=64,    tensor_parallel=4)# 示例2：2卡A100-80GB，FP8量化print(&quot;\n【示例2：FP8量化配置】&quot;)result = estimate_moe_memory(    total_params_b=80,    active_params_b=3,    dtype_bytes=1,          # FP8    max_seq_len=8192,    max_batch_size=32,    tensor_parallel=2)

输出示例：
=== MoE模型显存估算 ===配置: 80B参数, 3B激活推理配置: seq_len=8192, batch_size=64并行度: 4卡张量并行1. 模型权重:   总大小: 160.00 GB   每GPU: 40.00 GB2. KV缓存:   每GPU: 8.19 GB3. 激活值和中间结果:   Dense模型需要: 26.21 GB/GPU   MoE实际需要: 1.18 GB/GPU   节省比例: 95.5%4. 系统开销:   每GPU: 3.00 GB==================================================每GPU总计: 52.37 GB建议显存: 57.61 GB (含10%安全余量)总显存需求: 209.48 GB (4卡)==================================================GPU配置建议:✅ 可用A100-80GB (每卡57.6GB &lt; 80GB)

3.5 MoE模型性能特点显存优势


模型类型
总参数
激活参数
BF16显存需求
优势



Dense 80B
80B
80B
~190GB
无


MoE 80B-A3B
80B
3B
~190GB
计算量减少96%



关键：虽然MoE模型的权重显存与Dense模型相同（都需要存储所有参数），但计算显存（激活值）大幅减少，使得：

推理速度更快（只计算3B参数）
功耗更低
相同显存下可支持更大batch size


适用场景✅ MoE模型特别适合：

高吞吐量推理服务
长上下文处理（显存省在激活值上）
多任务处理（不同专家处理不同任务）
成本敏感场景（更少GPU即可达到80B性能）

3.6 在线显存计算器使用HuggingFace官方计算器：
# 访问在线计算器https://huggingface.co/spaces/hf-accelerate/model-memory-usage# 输入参数（注意：标准计算器不完全支持MoE）Model: Qwen/Qwen3-Next-80B-A3B-InstructPrecision: bfloat16Number of GPUs: 4# 注意：结果需要手动调整，因为标准计算器按Dense模型计算# 实际MoE模型的KV缓存和激活值会更小

四、模型文件下载4.1 环境准备# 安装依赖工具pip install huggingface_hubpip install modelscope# 配置代理（如需要）export HTTP_PROXY=http://your-proxy:portexport HTTPS_PROXY=http://your-proxy:port# 配置HuggingFace token（私有模型需要）huggingface-cli login

4.2 使用HuggingFace下载（国外服务器）方法1：使用huggingface-cli# 下载完整模型huggingface-cli download Qwen/Qwen3-Next-80B-A3B-Instruct \    --local-dir /data/models/Qwen3-Next-80B-A3B-Instruct \    --local-dir-use-symlinks False# 下载特定文件huggingface-cli download Qwen/Qwen3-Next-80B-A3B-Instruct \    --include &quot;*.json&quot; &quot;*.safetensors&quot; \    --local-dir /data/models/Qwen3-Next-80B-A3B-Instruct# 断点续传（自动支持）# 如果下载中断，重新执行相同命令即可继续

方法2：使用Python脚本from huggingface_hub import snapshot_download# 下载完整模型model_path = snapshot_download(    repo_id=&quot;Qwen/Qwen3-Next-80B-A3B-Instruct&quot;,    cache_dir=&quot;/data/models&quot;,    local_dir=&quot;/data/models/Qwen3-Next-80B-A3B-Instruct&quot;,    local_dir_use_symlinks=False,    resume_download=True,  # 支持断点续传    max_workers=8  # 并发下载线程数)print(f&quot;模型已下载到: &#123;model_path&#125;&quot;)

4.3 使用ModelScope下载（国内服务器推荐）# 安装modelscopepip install modelscope# 设置ModelScope为首选源export VLLM_USE_MODELSCOPE=Trueexport MODELSCOPE_CACHE=/data/models/modelscope_cache# 下载模型modelscope download \    --model qwen/Qwen3-Next-80B-A3B-Instruct \    --local_dir /data/models/Qwen3-Next-80B-A3B-Instruct

使用Python下载：
from modelscope import snapshot_downloadmodel_dir = snapshot_download(    &#x27;qwen/Qwen3-Next-80B-A3B-Instruct&#x27;,    cache_dir=&#x27;/data/models&#x27;,    local_dir=&#x27;/data/models/Qwen3-Next-80B-A3B-Instruct&#x27;)print(f&quot;模型下载完成: &#123;model_dir&#125;&quot;)

4.4 验证下载完整性# 检查文件完整性cd /data/models/Qwen3-Next-80B-A3B-Instruct# 查看文件列表和大小ls -lh# 验证配置文件cat config.json | jq .# 检查模型分片是否完整ls -1 pytorch_model-*.bin | wc -l# 应该与 pytorch_model.bin.index.json 中的分片数一致# 计算磁盘占用du -sh .

4.5 下载加速技巧使用镜像站（国内）# 使用HF-Mirrorexport HF_ENDPOINT=https://hf-mirror.comhuggingface-cli download Qwen/Qwen3-Next-80B-A3B-Instruct \    --local-dir /data/models/Qwen3-Next-80B-A3B-Instruct

五、模型部署5.1 单卡部署（测试环境）适用于小规模测试或显存充足的单卡场景：
# 基础启动（需要足够显存）vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct \    --host 0.0.0.0 \    --port 8000 \    --dtype bfloat16 \    --max-model-len 8192 \    --gpu-memory-utilization 0.9# 使用本地模型路径vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --host 0.0.0.0 \    --port 8000

5.2 多卡张量并行部署（生产环境推荐）4卡A100配置# 创建启动脚本 start_vllm.shcat &gt; start_vllm.sh &lt;&lt; &#x27;EOF&#x27;#!/bin/bash# 设置环境变量export CUDA_VISIBLE_DEVICES=0,1,2,3export VLLM_WORKER_MULTIPROC_METHOD=spawnexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True# 启动vLLM服务vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --host 0.0.0.0 \    --port 8000 \    --api-key &quot;your-secure-api-key-here&quot; \    --tensor-parallel-size 4 \    --dtype bfloat16 \    --max-model-len 8192 \    --max-num-seqs 64 \    --gpu-memory-utilization 0.85 \    --enable-chunked-prefill \    --enable-prefix-caching \    --disable-log-requests \    --served-model-name qwen3-80bEOFchmod +x start_vllm.sh./start_vllm.sh

8卡A100高性能配置cat &gt; start_vllm_8gpu.sh &lt;&lt; &#x27;EOF&#x27;#!/bin/bashexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7export VLLM_WORKER_MULTIPROC_METHOD=spawnexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:Truevllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --host 0.0.0.0 \    --port 8000 \    --api-key &quot;your-secure-api-key-here&quot; \    --tensor-parallel-size 8 \    --dtype bfloat16 \    --max-model-len 16384 \    --max-num-seqs 128 \    --max-num-batched-tokens 16384 \    --gpu-memory-utilization 0.9 \    --enable-chunked-prefill \    --enable-prefix-caching \    --trust-remote-code \    --served-model-name qwen3-80bEOFchmod +x start_vllm_8gpu.sh./start_vllm_8gpu.sh

5.3 使用量化加速部署FP8量化部署# 需要先转换模型为FP8格式，或使用已量化模型vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 4 \    --quantization fp8 \    --kv-cache-dtype fp8 \    --max-model-len 16384 \    --gpu-memory-utilization 0.9

5.4 MoE专家CPU Offload配置（高级）
警告：截至vLLM 0.6.x版本，MoE模型的CPU offload功能还在实验阶段，可能存在性能和稳定性问题。

方法1：使用cpu-offload-gb参数vLLM提供了--cpu-offload-gb参数，可以将部分模型参数offload到CPU内存：
# 基础CPU offload配置vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 2 \    --dtype bfloat16 \    --max-model-len 8192 \    --gpu-memory-utilization 0.85 \    --cpu-offload-gb 120 \    --enforce-eager

参数说明：



参数
说明
推荐值



--cpu-offload-gb
offload到CPU的显存量(GB)
60-120


--enforce-eager
禁用CUDA graph（必需）
必须设置


--swap-space
CPU交换空间大小(GB)
4-16


方法2：环境变量控制# 设置环境变量export VLLM_CPU_OFFLOAD_GB=100export VLLM_ENABLE_CPU_OFFLOAD=1# 启动服务vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 2 \    --dtype bfloat16 \    --enforce-eager

实际效果测试测试配置：2卡A100-80GB + 256GB CPU内存
# 启动命令vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 2 \    --dtype bfloat16 \    --max-model-len 8192 \    --max-num-seqs 32 \    --gpu-memory-utilization 0.80 \    --cpu-offload-gb 100 \    --enforce-eager \    --disable-log-requests

性能对比：



配置
GPU显存&#x2F;卡
CPU内存
TTFT
TPOT
吞吐量



无offload (4卡)
54GB
-
500ms
40ms
2500 tok&#x2F;s


CPU offload (2卡)
60GB
100GB
800ms
80ms
1250 tok&#x2F;s



性能影响：启用CPU offload后，推理速度约降低40-50%，但可节省50%的GPU成本。

注意事项和限制❌ 当前限制：

vLLM原生支持有限：vLLM对MoE专家的智能offload支持不完善
性能损失：频繁的GPU-CPU数据传输导致延迟增加
PCIe带宽瓶颈：需要PCIe 4.0 x16以上带宽
CUDA Graph不兼容：必须使用eager模式，进一步降低性能

✅ 适用场景：

预算极其受限，无法配置足够GPU
开发测试环境，对性能要求不高
低并发场景（&lt;10 QPS）
批量离线推理

🔧 系统要求：
# CPU内存需求至少 200GB 可用内存（推荐256GB+）# PCIe带宽检查lspci -vv | grep -i &quot;lnkcap\|lnksta&quot;# 确保有PCIe 4.0 x16或更高# 内存带宽检查sudo apt install mbwmbw 128  # 应该有40GB/s以上

方法3：使用DeepSpeed-MII（推荐替代方案）如果需要更成熟的CPU offload支持，建议使用DeepSpeed-MII：
# 安装DeepSpeed-MIIpip install deepspeed-mii# Python部署代码import miimii_config = &#123;    &quot;tensor_parallel&quot;: 2,    &quot;dtype&quot;: &quot;bf16&quot;,    &quot;max_length&quot;: 8192,    &quot;offload_config&quot;: &#123;        &quot;device&quot;: &quot;cpu&quot;,        &quot;nvme_path&quot;: &quot;/nvme/offload&quot;  # 可选：使用NVMe加速    &#125;&#125;client = mii.serve(    model_name_or_path=&quot;/data/models/Qwen3-Next-80B-A3B-Instruct&quot;,    deployment_name=&quot;qwen3-80b&quot;,    **mii_config)


DeepSpeed优势：更成熟的offload实现，智能预取，性能损失更小（约20-30%）

监控offload性能# 监控脚本import timeimport subprocessdef monitor_offload():    while True:        # GPU显存使用        gpu_mem = subprocess.check_output([            &#x27;nvidia-smi&#x27;, &#x27;--query-gpu=memory.used&#x27;,            &#x27;--format=csv,noheader,nounits&#x27;        ]).decode()                # CPU内存使用        cpu_mem = subprocess.check_output([            &#x27;free&#x27;, &#x27;-g&#x27;        ]).decode()                print(f&quot;GPU Memory: &#123;gpu_mem.strip()&#125; MB&quot;)        print(f&quot;CPU Memory:\n&#123;cpu_mem&#125;&quot;)        time.sleep(5)monitor_offload()

5.5 使用Systemd管理服务创建系统服务配置：
# 创建服务文件sudo tee /etc/systemd/system/vllm-qwen.service &gt; /dev/null &lt;&lt; &#x27;EOF&#x27;[Unit]Description=vLLM Qwen3-Next-80B ServiceAfter=network.target[Service]Type=simpleUser=ubuntuWorkingDirectory=/home/ubuntuEnvironment=&quot;CUDA_VISIBLE_DEVICES=0,1,2,3&quot;Environment=&quot;VLLM_WORKER_MULTIPROC_METHOD=spawn&quot;Environment=&quot;PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True&quot;ExecStart=/home/ubuntu/vllm-env/bin/vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --host 0.0.0.0 \    --port 8000 \    --api-key your-api-key \    --tensor-parallel-size 4 \    --dtype bfloat16 \    --max-model-len 8192 \    --gpu-memory-utilization 0.85 \    --enable-chunked-prefill \    --enable-prefix-cachingRestart=alwaysRestartSec=10StandardOutput=append:/var/log/vllm-qwen.logStandardError=append:/var/log/vllm-qwen-error.log[Install]WantedBy=multi-user.targetEOF# 重载systemdsudo systemctl daemon-reload# 启动服务sudo systemctl start vllm-qwen# 设置开机自启sudo systemctl enable vllm-qwen# 查看服务状态sudo systemctl status vllm-qwen# 查看日志sudo journalctl -u vllm-qwen -f

5.6 Docker部署方法1：使用官方镜像（推荐）⭐vLLM提供了官方Docker镜像，开箱即用，无需构建：
# 拉取最新的vLLM官方镜像docker pull vllm/vllm-openai:latest# 或使用nightly版本（包含最新特性）docker pull vllm/vllm-openai:nightly

基础运行示例：
# 使用官方镜像启动vLLM服务docker run -d \    --name vllm-qwen3 \    --gpus all \    -p 8000:8000 \    -v /data/models:/models \    --ipc=host \    vllm/vllm-openai:latest \    --model /models/Qwen3-Next-80B-A3B-Instruct \    --host 0.0.0.0 \    --port 8000 \    --tensor-parallel-size 4 \    --dtype auto \    --max-model-len 8192 \    --gpu-memory-utilization 0.85# 查看日志docker logs -f vllm-qwen3

完整配置示例（生产环境）：
docker run -d \    --name vllm-qwen3-prod \    --gpus &#x27;&quot;device=0,1,2,3&quot;&#x27; \    -p 8000:8000 \    -v /data/models:/models \    -v /data/cache:/root/.cache \    --ipc=host \    --ulimit memlock=-1 \    --ulimit stack=67108864 \    --restart unless-stopped \    -e CUDA_VISIBLE_DEVICES=0,1,2,3 \    -e VLLM_WORKER_MULTIPROC_METHOD=spawn \    -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \    -e NCCL_P2P_DISABLE=0 \    -e NCCL_IB_DISABLE=0\    -e NCCL_ASYNC_ERROR_HANDLING=1 \    vllm/vllm-openai:latest \    --model /models/Qwen3-Next-80B-A3B-Instruct \    --served-model-name qwen3-80b \    --host 0.0.0.0 \    --port 8000 \    --api-key your-secure-api-key \    --tensor-parallel-size 4 \    --dtype auto \    --max-model-len 8192 \    --max-num-seqs 64 \    --gpu-memory-utilization 0.85 \    --enable-chunked-prefill \    --enable-prefix-caching \    --trust-remote-code \    --disable-log-requests \    --disable-log-stats# 查看容器状态docker ps -a | grep vllm-qwen3-prod# 查看实时日志docker logs -f vllm-qwen3-prod# 查看资源使用docker stats vllm-qwen3-prod

Docker Compose配置：
# docker-compose.ymlversion: &#x27;3.8&#x27;services:  vllm-qwen3:    image: vllm/vllm-openai:latest    container_name: vllm-qwen3    restart: unless-stopped        deploy:      resources:        reservations:          devices:            - driver: nvidia              device_ids: [&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;]              capabilities: [gpu]        ports:      - &quot;8000:8000&quot;        volumes:      - /data/models:/models      - /data/cache:/root/.cache        environment:      - CUDA_VISIBLE_DEVICES=0,1,2,3      - VLLM_WORKER_MULTIPROC_METHOD=spawn      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True      - NCCL_DEBUG=WARN      - NCCL_ASYNC_ERROR_HANDLING=1      - NCCL_IB_DISABLE=0      - NCCL_P2P_DISABLE=0        ipc: host        ulimits:      memlock:        soft: -1        hard: -1      stack:        soft: 67108864        hard: 67108864        command:      - --model      - /models/Qwen3-Next-80B-A3B-Instruct      - --served-model-name      - qwen3-80b      - --host      - 0.0.0.0      - --port      - &quot;8000&quot;      - --api-key      - $&#123;VLLM_API_KEY:-your-api-key&#125;      - --tensor-parallel-size      - &quot;4&quot;      - --dtype      - auto      - --max-model-len      - &quot;8192&quot;      - --gpu-memory-utilization      - &quot;0.85&quot;      - --enable-chunked-prefill      - --enable-prefix-caching      - --disable-log-requests       - --disable-log-stats# 使用方式# docker-compose up -d# docker-compose logs -f# docker-compose down

Docker部署最佳实践1. 网络配置
# 创建专用网络docker network create vllm-network# 使用专用网络运行docker run -d \    --name vllm-qwen3 \    --network vllm-network \    --gpus all \    -p 8000:8000 \    vllm/vllm-openai:latest \    --model /models/Qwen3-Next-80B-A3B-Instruct

2. 健康检查
# 在docker-compose.yml中添加健康检查healthcheck:  test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:8000/health&quot;]  interval: 30s  timeout: 10s  retries: 3  start_period: 60s

3. 日志管理
# 限制日志大小docker run -d \    --name vllm-qwen3 \    --log-driver json-file \    --log-opt max-size=100m \    --log-opt max-file=5 \    vllm/vllm-openai:latest \    --model /models/Qwen3-Next-80B-A3B-Instruct

4. 资源限制
# 设置CPU和内存限制docker run -d \    --name vllm-qwen3 \    --gpus all \    --cpus=&quot;32&quot; \    --memory=&quot;200g&quot; \    --memory-swap=&quot;250g&quot; \    vllm/vllm-openai:latest \    --model /models/Qwen3-Next-80B-A3B-Instruct

5.7 验证部署# 检查服务是否启动curl http://localhost:8000/health# 查看模型列表curl http://localhost:8000/v1/models# 简单推理测试curl http://localhost:8000/v1/chat/completions \    -H &quot;Content-Type: application/json&quot; \    -H &quot;Authorization: Bearer your-api-key&quot; \    -d &#x27;&#123;        &quot;model&quot;: &quot;qwen3-80b&quot;,        &quot;messages&quot;: [            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好，请介绍一下你自己&quot;&#125;        ],        &quot;max_tokens&quot;: 100    &#125;&#x27;

六、模型监控6.1 vLLM内置监控指标vLLM提供Prometheus格式的监控指标：
# 访问metrics端点curl http://localhost:8000/metrics

关键指标说明


指标名称
说明
类型



vllm:num_requests_running
正在运行的请求数
Gauge


vllm:num_requests_waiting
等待中的请求数
Gauge


vllm:gpu_cache_usage_perc
GPU缓存使用率
Gauge


vllm:num_preemptions_total
总抢占次数
Counter


vllm:time_to_first_token_seconds
首token时间
Histogram


vllm:time_per_output_token_seconds
每token生成时间
Histogram


vllm:e2e_request_latency_seconds
端到端延迟
Histogram


6.2 配置Prometheus监控安装Prometheus# 下载Prometheuswget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gztar xvfz prometheus-*.tar.gzcd prometheus-*

配置Prometheus# prometheus.ymlglobal:  scrape_interval: 15s  evaluation_interval: 15sscrape_configs:  - job_name: &#x27;vllm-qwen3&#x27;    static_configs:      - targets: [&#x27;localhost:8000&#x27;]    metrics_path: &#x27;/metrics&#x27;    scrape_interval: 10s

启动Prometheus./prometheus --config.file=prometheus.yml \    --storage.tsdb.path=./data \    --web.listen-address=:9090

6.3 配置Grafana可视化安装Grafana# Ubuntu/Debiansudo apt-get install -y grafana# 启动Grafanasudo systemctl start grafana-serversudo systemctl enable grafana-server# 访问 http://localhost:3000# 默认账号密码: admin/admin

配置数据源
登录Grafana
添加Prometheus数据源
URL: http://localhost:9090
点击”Save &amp; Test”

导入vLLM监控面板创建Dashboard配置文件 vllm-dashboard.json：
&#123;  &quot;dashboard&quot;: &#123;    &quot;title&quot;: &quot;vLLM Qwen3-80B Monitoring&quot;,    &quot;panels&quot;: [      &#123;        &quot;title&quot;: &quot;Running Requests&quot;,        &quot;targets&quot;: [          &#123;            &quot;expr&quot;: &quot;vllm:num_requests_running&quot;          &#125;        ],        &quot;type&quot;: &quot;graph&quot;      &#125;,      &#123;        &quot;title&quot;: &quot;GPU Cache Usage&quot;,        &quot;targets&quot;: [          &#123;            &quot;expr&quot;: &quot;vllm:gpu_cache_usage_perc&quot;          &#125;        ],        &quot;type&quot;: &quot;gauge&quot;      &#125;,      &#123;        &quot;title&quot;: &quot;Time to First Token (P95)&quot;,        &quot;targets&quot;: [          &#123;            &quot;expr&quot;: &quot;histogram_quantile(0.95, rate(vllm:time_to_first_token_seconds_bucket[5m]))&quot;          &#125;        ],        &quot;type&quot;: &quot;graph&quot;      &#125;,      &#123;        &quot;title&quot;: &quot;Request Latency (P99)&quot;,        &quot;targets&quot;: [          &#123;            &quot;expr&quot;: &quot;histogram_quantile(0.99, rate(vllm:e2e_request_latency_seconds_bucket[5m]))&quot;          &#125;        ],        &quot;type&quot;: &quot;graph&quot;      &#125;    ]  &#125;&#125;

6.4 GPU监控使用nvidia-smi监控# 实时监控watch -n 1 nvidia-smi# 记录GPU使用情况到文件nvidia-smi --query-gpu=timestamp,index,utilization.gpu,utilization.memory,memory.used,memory.total,temperature.gpu \    --format=csv -l 5 &gt; gpu_metrics.csv

使用DCGM监控（推荐）# 安装DCGMwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.debsudo dpkg -i cuda-keyring_1.0-1_all.debsudo apt-get updatesudo apt-get install -y datacenter-gpu-manager# 启动DCGMsudo systemctl start nvidia-dcgmsudo systemctl enable nvidia-dcgm# 安装dcgm-exporter for Prometheusdocker run -d \    --gpus all \    --name dcgm-exporter \    -p 9400:9400 \    nvcr.io/nvidia/k8s/dcgm-exporter:3.1.8-3.2.0-ubuntu22.04

配置Prometheus抓取GPU指标# 添加到prometheus.ymlscrape_configs:  - job_name: &#x27;dcgm&#x27;    static_configs:      - targets: [&#x27;localhost:9400&#x27;]

6.5 日志监控vLLM日志配置vLLM通过环境变量和启动参数控制日志：
方法1：环境变量控制日志级别
# 设置日志级别export VLLM_LOGGING_LEVEL=INFO  # 可选: DEBUG, INFO, WARNING, ERROR# 启用详细日志export VLLM_TRACE_FUNCTION=1# 启用请求日志（生产环境建议禁用）vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 4 \    # 不加 --disable-log-requests 参数

方法2：启动参数控制
# 生产环境：禁用请求日志vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 4 \    --disable-log-requests \    --disable-log-stats# 开发环境：启用详细日志vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 4 \    --log-requests

日志输出示例：
# vLLM默认日志格式INFO 11-24 10:30:15 llm_engine.py:123] Initializing engine...INFO 11-24 10:30:20 llm_engine.py:456] # GPU blocks: 2048INFO 11-24 10:30:20 model_runner.py:123] Loading model...INFO 11-24 10:30:45 api_server.py:234] vLLM API server started at http://0.0.0.0:8000

日志重定向和持久化# 方法1：重定向到文件vllm serve model-path --tensor-parallel-size 4 \    &gt; /var/log/vllm-qwen3.log 2&gt;&amp;1# 方法2：使用tee同时输出到终端和文件vllm serve model-path --tensor-parallel-size 4 \    2&gt;&amp;1 | tee -a /var/log/vllm-qwen3.log# 方法3：Systemd管理（自动记录日志）# 日志通过journalctl查看sudo journalctl -u vllm-qwen -f

日志轮转配置创建logrotate配置：
# 创建 /etc/logrotate.d/vllm-qwensudo tee /etc/logrotate.d/vllm-qwen &gt; /dev/null &lt;&lt; &#x27;EOF&#x27;/var/log/vllm-qwen*.log &#123;    daily    rotate 7    compress    delaycompress    missingok    notifempty    create 0644 ubuntu ubuntu    sharedscripts    postrotate        systemctl reload vllm-qwen &gt; /dev/null 2&gt;&amp;1 || true    endscript&#125;EOF# 测试配置sudo logrotate -d /etc/logrotate.d/vllm-qwen# 手动执行轮转sudo logrotate -f /etc/logrotate.d/vllm-qwen

使用Loki收集日志# 安装Promtailwget https://github.com/grafana/loki/releases/download/v2.8.0/promtail-linux-amd64.zipunzip promtail-linux-amd64.zipchmod +x promtail-linux-amd64

配置Promtail (promtail-config.yml)：
server:  http_listen_port: 9080  grpc_listen_port: 0positions:  filename: /tmp/positions.yamlclients:  - url: http://localhost:3100/loki/api/v1/pushscrape_configs:  - job_name: vllm-logs    static_configs:      - targets:          - localhost        labels:          job: vllm-qwen3          __path__: /var/log/vllm-qwen*.log

6.6 告警配置Prometheus告警规则# alerts.ymlgroups:  - name: vllm_alerts    interval: 30s    rules:      - alert: HighRequestWaitTime        expr: vllm:num_requests_waiting &gt; 50        for: 2m        labels:          severity: warning        annotations:          summary: &quot;Too many requests waiting&quot;          description: &quot;&#123;&#123; $value &#125;&#125; requests waiting for processing&quot;            - alert: HighGPUMemoryUsage        expr: vllm:gpu_cache_usage_perc &gt; 95        for: 5m        labels:          severity: critical        annotations:          summary: &quot;GPU memory usage is very high&quot;          description: &quot;GPU cache usage at &#123;&#123; $value &#125;&#125;%&quot;            - alert: SlowTimeToFirstToken        expr: histogram_quantile(0.95, rate(vllm:time_to_first_token_seconds_bucket[5m])) &gt; 2        for: 5m        labels:          severity: warning        annotations:          summary: &quot;Slow time to first token&quot;          description: &quot;P95 TTFT is &#123;&#123; $value &#125;&#125;s&quot;

七、模型压测7.1 使用vLLM官方压测工具# 安装压测工具pip install aiohttp tqdm# 下载压测脚本wget https://raw.githubusercontent.com/vllm-project/vllm/main/benchmarks/benchmark_serving.py# 执行压测python benchmark_serving.py \    --host localhost \    --port 8000 \    --endpoint /v1/completions \    --model qwen3-80b \    --dataset-name random \    --request-rate 10 \    --num-prompts 500 \    --output-len 128 \    --seed 2024

压测参数说明


参数
说明
推荐值



--request-rate
每秒请求数(QPS)
1-50


--num-prompts
总请求数
100-1000


--input-len
输入长度
512-2048


--output-len
输出长度
128-512


--dataset-name
数据集
random&#x2F;sharegpt


7.2 使用Apache Bench进行简单压测# 安装absudo apt-get install apache2-utils# 创建测试请求文件cat &gt; test_request.json &lt;&lt; &#x27;EOF&#x27;&#123;  &quot;model&quot;: &quot;qwen3-80b&quot;,  &quot;messages&quot;: [    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请介绍一下人工智能的发展历史&quot;&#125;  ],  &quot;max_tokens&quot;: 200,  &quot;temperature&quot;: 0.7&#125;EOF# 执行压测ab -n 100 -c 10 \   -p test_request.json \   -T application/json \   -H &quot;Authorization: Bearer your-api-key&quot; \   http://localhost:8000/v1/chat/completions

7.3 使用Locust进行复杂压测安装Locustpip install locust

创建压测脚本# locustfile.pyfrom locust import HttpUser, task, betweenimport jsonimport randomclass VLLMUser(HttpUser):    wait_time = between(1, 3)        def on_start(self):        &quot;&quot;&quot;初始化时执行&quot;&quot;&quot;        self.headers = &#123;            &quot;Content-Type&quot;: &quot;application/json&quot;,            &quot;Authorization&quot;: &quot;Bearer your-api-key&quot;        &#125;                # 准备测试prompt        self.prompts = [            &quot;请解释一下什么是深度学习&quot;,            &quot;介绍一下自然语言处理的应用&quot;,            &quot;什么是Transformer架构&quot;,            &quot;如何优化大模型推理性能&quot;,            &quot;请写一个Python快速排序算法&quot;        ]        @task(3)    def chat_completion(self):        &quot;&quot;&quot;聊天补全压测&quot;&quot;&quot;        payload = &#123;            &quot;model&quot;: &quot;qwen3-80b&quot;,            &quot;messages&quot;: [                &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: random.choice(self.prompts)&#125;            ],            &quot;max_tokens&quot;: 200,            &quot;temperature&quot;: 0.7        &#125;                with self.client.post(            &quot;/v1/chat/completions&quot;,            json=payload,            headers=self.headers,            catch_response=True        ) as response:            if response.status_code == 200:                data = response.json()                # 记录首token时间等指标                response.success()            else:                response.failure(f&quot;Failed: &#123;response.status_code&#125;&quot;)        @task(1)    def streaming_completion(self):        &quot;&quot;&quot;流式补全压测&quot;&quot;&quot;        payload = &#123;            &quot;model&quot;: &quot;qwen3-80b&quot;,            &quot;messages&quot;: [                &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请详细介绍机器学习的基本概念&quot;&#125;            ],            &quot;max_tokens&quot;: 500,            &quot;stream&quot;: True        &#125;                with self.client.post(            &quot;/v1/chat/completions&quot;,            json=payload,            headers=self.headers,            stream=True,            catch_response=True        ) as response:            if response.status_code == 200:                # 读取流式响应                for line in response.iter_lines():                    if line:                        pass                response.success()            else:                response.failure(f&quot;Failed: &#123;response.status_code&#125;&quot;)

运行Locust压测# 启动Locustlocust -f locustfile.py --host=http://localhost:8000# 或使用命令行模式locust -f locustfile.py \    --host=http://localhost:8000 \    --users 50 \    --spawn-rate 5 \    --run-time 10m \    --headless \    --html=locust_report.html

7.4 压测结果分析关键性能指标


指标
说明
优秀标准
可接受标准



TTFT (Time To First Token)
首token延迟
&lt; 500ms
&lt; 1s


TPOT (Time Per Output Token)
每token生成时间
&lt; 50ms
&lt; 100ms


Throughput
吞吐量(tokens&#x2F;s)
&gt; 2000
&gt; 1000


QPS
每秒查询数
&gt; 20
&gt; 10


P99 Latency
99分位延迟
&lt; 3s
&lt; 5s


Error Rate
错误率
&lt; 0.1%
&lt; 1%


生成压测报告# analyze_benchmark.pyimport jsonimport pandas as pdimport matplotlib.pyplot as pltdef analyze_results(results_file):    &quot;&quot;&quot;分析压测结果&quot;&quot;&quot;    with open(results_file, &#x27;r&#x27;) as f:        results = json.load(f)        # 提取关键指标    ttft = results[&#x27;time_to_first_token&#x27;]    tpot = results[&#x27;time_per_output_token&#x27;]    throughput = results[&#x27;throughput&#x27;]        # 统计分析    stats = &#123;        &#x27;TTFT_mean&#x27;: sum(ttft) / len(ttft),        &#x27;TTFT_p95&#x27;: sorted(ttft)[int(len(ttft) * 0.95)],        &#x27;TTFT_p99&#x27;: sorted(ttft)[int(len(ttft) * 0.99)],        &#x27;TPOT_mean&#x27;: sum(tpot) / len(tpot),        &#x27;Throughput&#x27;: throughput    &#125;        print(&quot;=== 压测结果分析 ===&quot;)    print(f&quot;平均首token延迟: &#123;stats[&#x27;TTFT_mean&#x27;]:.2f&#125;ms&quot;)    print(f&quot;P95首token延迟: &#123;stats[&#x27;TTFT_p95&#x27;]:.2f&#125;ms&quot;)    print(f&quot;P99首token延迟: &#123;stats[&#x27;TTFT_p99&#x27;]:.2f&#125;ms&quot;)    print(f&quot;平均每token时间: &#123;stats[&#x27;TPOT_mean&#x27;]:.2f&#125;ms&quot;)    print(f&quot;吞吐量: &#123;stats[&#x27;Throughput&#x27;]:.2f&#125; tokens/s&quot;)        # 绘制图表    fig, axes = plt.subplots(2, 2, figsize=(12, 10))        axes[0, 0].hist(ttft, bins=50)    axes[0, 0].set_title(&#x27;Time to First Token Distribution&#x27;)    axes[0, 0].set_xlabel(&#x27;TTFT (ms)&#x27;)        axes[0, 1].hist(tpot, bins=50)    axes[0, 1].set_title(&#x27;Time per Output Token Distribution&#x27;)    axes[0, 1].set_xlabel(&#x27;TPOT (ms)&#x27;)        axes[1, 0].plot(ttft)    axes[1, 0].set_title(&#x27;TTFT Over Time&#x27;)    axes[1, 0].set_xlabel(&#x27;Request Number&#x27;)        axes[1, 1].plot(tpot)    axes[1, 1].set_title(&#x27;TPOT Over Time&#x27;)    axes[1, 1].set_xlabel(&#x27;Token Number&#x27;)        plt.tight_layout()    plt.savefig(&#x27;benchmark_analysis.png&#x27;)    print(&quot;图表已保存至 benchmark_analysis.png&quot;)if __name__ == &quot;__main__&quot;:    analyze_results(&#x27;benchmark_results.json&#x27;)

八、模型推理Trace追踪8.1 启用OpenTelemetry追踪安装依赖pip install opentelemetry-api opentelemetry-sdkpip install opentelemetry-instrumentation-requestspip install opentelemetry-exporter-jaeger

配置OpenTelemetry# tracing_config.pyfrom opentelemetry import tracefrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.jaeger.thrift import JaegerExporterfrom opentelemetry.sdk.resources import Resourcedef setup_tracing():    &quot;&quot;&quot;配置分布式追踪&quot;&quot;&quot;    resource = Resource.create(&#123;        &quot;service.name&quot;: &quot;vllm-qwen3-80b&quot;,        &quot;service.version&quot;: &quot;1.0.0&quot;    &#125;)        tracer_provider = TracerProvider(resource=resource)        # 配置Jaeger导出器    jaeger_exporter = JaegerExporter(        agent_host_name=&quot;localhost&quot;,        agent_port=6831,    )        span_processor = BatchSpanProcessor(jaeger_exporter)    tracer_provider.add_span_processor(span_processor)        trace.set_tracer_provider(tracer_provider)        return trace.get_tracer(__name__)tracer = setup_tracing()

8.2 部署Jaeger追踪系统# 使用Docker快速部署Jaegerdocker run -d \    --name jaeger \    -p 5775:5775/udp \    -p 6831:6831/udp \    -p 6832:6832/udp \    -p 5778:5778 \    -p 16686:16686 \    -p 14268:14268 \    -p 14250:14250 \    -p 9411:9411 \    jaegertracing/all-in-one:latest# 访问Jaeger UI: http://localhost:16686

8.3 客户端追踪示例# client_with_tracing.pyfrom openai import OpenAIfrom opentelemetry import tracefrom opentelemetry.trace import SpanKindimport time# 获取tracertracer = trace.get_tracer(__name__)client = OpenAI(    api_key=&quot;your-api-key&quot;,    base_url=&quot;http://localhost:8000/v1&quot;)def traced_chat_completion(messages):    &quot;&quot;&quot;带追踪的聊天补全&quot;&quot;&quot;    with tracer.start_as_current_span(        &quot;chat_completion&quot;,        kind=SpanKind.CLIENT    ) as span:        # 记录输入        span.set_attribute(&quot;model&quot;, &quot;qwen3-80b&quot;)        span.set_attribute(&quot;num_messages&quot;, len(messages))        span.set_attribute(&quot;input_text&quot;, messages[-1][&quot;content&quot;])                start_time = time.time()                try:            response = client.chat.completions.create(                model=&quot;qwen3-80b&quot;,                messages=messages,                max_tokens=200,                temperature=0.7            )                        # 记录输出            span.set_attribute(&quot;completion_tokens&quot;, response.usage.completion_tokens)            span.set_attribute(&quot;prompt_tokens&quot;, response.usage.prompt_tokens)            span.set_attribute(&quot;total_tokens&quot;, response.usage.total_tokens)                        elapsed = time.time() - start_time            span.set_attribute(&quot;latency_seconds&quot;, elapsed)                        return response                except Exception as e:            span.record_exception(e)            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))            raise# 使用示例messages = [    &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个有用的AI助手&quot;&#125;,    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请解释什么是深度学习&quot;&#125;]response = traced_chat_completion(messages)print(response.choices[0].message.content)

8.4 vLLM内部追踪启用vLLM详细日志：
# 启动时设置环境变量export VLLM_LOGGING_LEVEL=DEBUGexport VLLM_TRACE_FUNCTION=1vllm serve /data/models/Qwen3-Next-80B-A3B-Instruct \    --tensor-parallel-size 4 \    --disable-log-requests false


九、生产环境最佳实践9.1 安全加固# 1. 使用强API密钥openssl rand -hex 32 &gt; /etc/vllm/api_key.txtchmod 600 /etc/vllm/api_key.txt# 2. 配置防火墙sudo ufw allow from 10.0.0.0/8 to any port 8000sudo ufw deny 8000# 3. 启用HTTPS# 使用Nginx反向代理配置SSL

9.2 高可用部署# 使用HAProxy进行负载均衡cat &gt; /etc/haproxy/haproxy.cfg &lt;&lt; &#x27;EOF&#x27;global    maxconn 4096defaults    mode http    timeout connect 10s    timeout client 300s    timeout server 300sfrontend vllm_frontend    bind *:80    default_backend vllm_backendbackend vllm_backend    balance roundrobin    option httpchk GET /health    server vllm1 10.0.0.1:8000 check    server vllm2 10.0.0.2:8000 check    server vllm3 10.0.0.3:8000 checkEOFsudo systemctl restart haproxy
也可使用litellm-proxy来实现负载均衡
9.3 故障排查常见问题


问题
原因
解决方案



CUDA OOM
显存不足
降低--gpu-memory-utilization或增加GPU数量


请求超时
并发过高
调整--max-num-seqs和--max-num-batched-tokens


启动失败
模型文件损坏
重新下载模型文件


推理慢
配置不当
启用--enable-prefix-caching


诊断脚本# diagnose.sh#!/bin/bashecho &quot;=== vLLM诊断脚本 ===&quot;# 检查GPUecho &quot;1. 检查GPU状态&quot;nvidia-smi# 检查服务状态echo &quot;2. 检查vLLM服务&quot;curl -s http://localhost:8000/health || echo &quot;服务未响应&quot;# 检查模型文件echo &quot;3. 检查模型文件&quot;ls -lh /data/models/Qwen3-Next-80B-A3B-Instruct/# 检查日志echo &quot;4. 最近的错误日志&quot;tail -100 /var/log/vllm-qwen.log | grep -i error# 检查端口占用echo &quot;5. 检查端口&quot;netstat -tlnp | grep 8000echo &quot;=== 诊断完成 ===&quot;

十、总结通过本文，我们完整介绍了使用vLLM部署Qwen3-Next-80B-A3B-Instruct（MoE架构）大模型的全流程，包括：
核心要点
模型理解

掌握MoE架构特点：80B总参数，3B激活参数
理解MoE模型的显存和计算优势
认识MoE模型适用场景和限制


模型准备

从HuggingFace或ModelScope查找和下载模型
验证模型文件完整性
合理规划存储空间（MoE模型约160GB）


资源规划

理解MoE模型的特殊显存需求（权重显存高，计算显存低）
选择合适的GPU配置（2卡A100-80GB即可运行）
使用显存预估工具辅助决策


部署配置

多卡张量并行部署提升性能
合理设置关键参数优化资源利用
使用Systemd或Docker管理服务


监控运维

配置Prometheus+Grafana监控系统
实时追踪GPU和模型性能指标
设置合理的告警规则


性能优化

通过压测找到最优配置
启用前缀缓存和分块预填充
分析trace数据优化推理链路


生产实践

实施安全加固措施
部署高可用架构
建立完善的故障排查流程



MoE模型特殊优势Qwen3-Next-80B-A3B-Instruct作为MoE模型，相比传统Dense模型具有显著优势：
✅ 成本优势

仅需2卡A100-80GB即可运行（Dense 80B通常需要4-8卡）
推理速度快20-25倍，相同时间内可处理更多请求
功耗更低，运营成本降低60%以上

✅ 性能优势

相同显存下可支持更大batch size（提升吞吐量）
长上下文处理更高效（KV缓存占用更少）
冷启动快，首token延迟低

✅ 灵活性优势

不同专家可处理不同类型任务
便于针对特定领域fine-tune单个专家
支持专家级别的A&#x2F;B测试

后续优化方向
模型量化：使用FP8量化进一步降低显存（可在单卡A100-80GB运行）
专家调优：针对特定任务fine-tune相关专家网络
专家路由优化：研究专家选择策略，提升特定任务性能
混合部署：结合专家卸载技术，在更低成本硬件上运行
推理优化：探索Flash Attention、分块预填充等加速技术
成本优化：使用Spot实例或混合云降低成本
功能扩展：集成RAG、Function Calling等高级功能

本指南为您提供了完整的MoE大模型部署和运维知识，通过实践这些步骤，您将能够快速搭建一个高性能、低成本的生产级别大语言模型推理服务。MoE架构的优势使得80B级模型的部署和运营变得更加可行和经济。
相关资源
vLLM官方文档
Qwen模型主页
vLLM GitHub
Prometheus文档
Grafana文档
OpenTelemetry文档
NVIDIA DCGM
Locust压测工具


本文由 AI 辅助生成，如有错误或建议,欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>vLLM</tag>
        <tag>Qwen</tag>
        <tag>大模型部署</tag>
        <tag>模型推理</tag>
        <tag>GPU</tag>
        <tag>性能优化</tag>
        <tag>模型监控</tag>
      </tags>
  </entry>
  <entry>
    <title>生产级大语言模型平台系统设计：多期落地方案与实践</title>
    <url>/posts/b0a1603977e7.html</url>
    <content><![CDATA[背景与目标随着大语言模型在企业内的应用场景不断扩展，单一模型服务或简单的 API + 网关 架构已经难以满足生产环境下的多租户管理、资源隔离、安全合规、可观测性以及快速迭代等要求。企业需要一套生产级别的大语言模型平台系统，以平台化的方式统一承载模型推理、Agent 编排、MCP 工具生态及 RAG 检索能力。
本文面向有一定 DevOps&#x2F;平台工程基础的读者，设计一套可生产落地的大语言模型平台，从整体架构到关键模块拆解，涵盖：

模型部署与运行时管理
多集群 &#x2F; 多云资源管理与调度
监控、日志、链路追踪与容量管理
安全与访问控制
RAG 平台
Agent 平台
MCP（Model Context Protocol）生态集成
平台运维与发布管理

并按照优先级划分为多期落地路线，便于企业按阶段实施。

本文更偏向平台架构设计与关键实现要点，不绑定某个具体云厂商，可结合 Kubernetes、Service Mesh、向量数据库等基础设施实施。

多期落地规划概览为了降低一次性建设的复杂度，建议将大模型平台拆分为多期，逐步演进：

一期（核心推理与基础运维能力，必须上线）
核心推理服务（单&#x2F;多模型）
模型镜像与模型仓库管理
API 网关与统一鉴权
基础监控（CPU&#x2F;GPU、内存、QPS、延迟、错误率）
基础日志采集
灰度发布与限流熔断


二期（RAG + 向量检索 + 观测性完善）
通用 RAG 服务层（检索、重写、召回、重排）
向量数据库与多数据源接入
全链路追踪（TraceID 贯穿）
SLA&#x2F;SLO、告警策略、容量规划
成本与计费（按租户&#x2F;项目维度）


三期（Agent 平台 + MCP 生态）
Agent 运行时与编排引擎
工具（Tool）与 MCP 协议适配
工作流级别的可视化编排与版本管理
权限与审计（操作审计、数据访问审计）


四期（多模型、多云、精细化治理）
多云&#x2F;多集群调度与就近访问
模型 A&#x2F;B 测试、自动化评估（Eval）
模型市场（Model Catalog）与自助申请
更精细的治理：安全、合规、隐私、数据脱敏



下文按模块详细展开设计与实践要点。
一期：基础推理平台与运维能力核心架构组件一期的目标是搭建一个稳定可用的推理平台，支撑生产流量，并具备基础的运维与观测能力。典型组件包括：

API 网关层
负责统一入口、鉴权、流量控制、路由到对应模型服务或 RAG&#x2F;Agent 服务。
可选技术：Kong、APISIX、Envoy、Nginx + Lua 等。


推理服务层（模型运行时）
封装不同大模型框架（vLLM、TensorRT-LLM、TGI、OpenAI-compatible 等）。
通过统一的内部 API 规格暴露推理接口（例如 OpenAI API 兼容规范）。


控制平面（Control Plane）
模型元数据管理（版本、参数、资源需求、路由权重）。
实例编排：与 Kubernetes 交互，部署&#x2F;扩缩实例。
统一配置（如 context length、并发限制、max tokens 等）。


数据平面（Data Plane）
实际承载请求流量的推理 Pods&#x2F;容器。
通过 sidecar 或 SDK 接入日志、指标、追踪。



推荐技术选型（一期）
基础编排 &#x2F; 集群
首选：Kubernetes（自建 K8s 或云厂商托管版：EKS、GKE、ACK、TKE 等）。
GPU 节点：使用带 GPU 的节点池，配合 NVIDIA GPU Operator 管理驱动与 runtime。


API 网关层
开源自建：Kong、APISIX、Envoy Gateway。
云托管：云厂商 API Gateway &#x2F; Ingress Controller（配合 Nginx Ingress Controller 或 Traefik）。


模型推理服务层
开源推理框架：
文本类：vLLM、TGI (Text Generation Inference)、TensorRT-LLM。
多模态：基于 vLLM + 自行适配，或 OpenVINO &#x2F; DeepSpeed-MII 等。


OpenAI 兼容中间层（可选）：LiteLLM、OpenAI-compatible 自研网关。


控制平面
容器部署：Helm + Kustomize（推荐组合），或 Argo CD 进行 GitOps 管理。
配置管理：ConfigMap + Secret，复杂场景可使用 HashiCorp Vault、External Secrets。


数据平面 Sidecar &#x2F; SDK
观测：OpenTelemetry Collector sidecar 或 DaemonSet。
日志：Fluent Bit &#x2F; Vector 作为日志收集 Agent。



基于 LiteLLM 的统一 LLM 网关方案（推荐）
定位
作为统一的 LLM 代理层，对上暴露兼容 OpenAI 的标准接口，对下对接自建模型（vLLM&#x2F;TGI 等）和外部云模型（如 OpenAI、Anthropic 等）。
统一做鉴权、路由、限流、fallback 与审计，不需要为每个上游模型单独编写网关逻辑。


优势
开源、可自托管，支持多种上游提供商和路由策略（按租户、按成本、按延迟等）。
结合网关（如 APISIX&#x2F;Kong）和 Kubernetes，可以作为“逻辑控制平面”的一部分，集中管理模型 Key、配额与重试策略。
易于与后文的 Langfuse、RAG、Agent 平台集成，作为统一的调用入口。



模型部署与运行时管理模型镜像与模型仓库
模型镜像
将推理框架、依赖库和基础逻辑打包为镜像（例如基于 vLLM 或 llama.cpp）。
模型权重可以：
打包在镜像中（启动快，镜像大，更新成本高）。
通过模型仓库（对象存储、专用模型仓库）在启动时拉取（更灵活，启动慢）。




模型仓库
技术选型：S3&#x2F;MinIO、OSS、GCS 等对象存储，或专用模型仓库（如 Hugging Face Hub 镜像）。
需设计：
模型版本目录结构（/org/model/version/）。
校验机制（hash、签名）。
权限控制（哪些租户可访问哪些模型）。





模型规格与调度策略
模型规格定义（写入控制平面）：
模型名称、版本、大小、精度（fp16&#x2F;bf16&#x2F;INT4 等）。
所需 GPU 类型与数量（A100&#x2F;H100&#x2F;L40S 等）。
最大并发、最大 context length、吞吐预估。


调度策略：
按模型规格与租户优先级调度到对应 GPU 节点池。
支持：
基于 nodeSelector&#x2F;taints-tolerations 的 GPU 池隔离。
基于调度器插件（如 kube-scheduler 插件或 Karmada）实现多集群调度。





API 设计与网关层对外接口规范推荐采用兼容 OpenAI API 规范的对外接口，以简化生态集成：

/v1/chat/completions
/v1/completions
/v1/embeddings
/v1/models

并增加平台特有能力：

请求头 &#x2F; 参数：
X-Tenant-ID &#x2F; X-Project-ID：租户&#x2F;项目标识。
X-Trace-ID：链路追踪 ID。
x-routing-strategy：模型路由策略（如 canary, stable, ab_test）。



网关能力
鉴权
支持多种鉴权方式：API Key、OAuth2、OIDC、内部服务 Token。
与企业 IAM 集成（如 Keycloak、企业自建 SSO）。


限流与配额
按租户&#x2F;项目&#x2F;用户维度限流。
支持速率限制（QPS）、并发限制、每日调用量限制。


熔断与重试
下游模型服务异常时，触发熔断，返回降级结果或快速失败。
支持可配置的重试策略（幂等请求）。



可观测性：监控、日志与追踪（一期基础版）指标监控建议基于 Prometheus 体系，指标设计：

平台级指标
总 QPS、成功率、P95&#x2F;P99 延迟。
按租户、按模型的维度聚合。


模型级指标
每个模型实例的吞吐量、平均 tokens&#x2F;s。
GPU&#x2F;CPU&#x2F;内存利用率。
加载时间、模型重启次数。



可通过 Grafana 构建统一看板：

总览看板（平台运营视角）
模型运维看板（模型 SRE 视角）
GPU 节点资源看板（集群运维视角）

推荐监控 &#x2F; 日志技术栈
监控
指标采集：Prometheus（或兼容实现 Thanos &#x2F; VictoriaMetrics 做长时存储）。
看板展示：Grafana。


日志
日志收集：Fluent Bit、Vector。
日志存储与检索：Loki（与 Grafana 集成好）或 ElasticSearch。


告警
告警引擎：Alertmanager。
通知渠道：企业微信、钉钉、Slack、邮件等 Webhook 集成。



基于 Langfuse 的 LLM 应用观测与调试（推荐）
定位
面向 LLM 应用层的专用可观测平台，用于记录每次 LLM 调用的 Prompt、参数、输出、错误信息以及用户反馈。
与底层 Prometheus &#x2F; OTel 形成互补：Prometheus 关注基础设施指标，Langfuse 关注“每次模型调用和会话”的语义级信息。


集成方式
在 LiteLLM 网关、RAG 服务、Agent 平台中统一接入 Langfuse SDK，将每次模型调用（包括上游模型和自建模型）打点到 Langfuse。
为每个 TraceID &#x2F; 会话在 Langfuse 中形成完整视图，支持搜索、过滤和回放。


收益
方便排查“回答质量问题”：能够看到当时的 Prompt、上下文、使用的模型版本以及工具调用链。
提供反馈与评分通道，可用于后续 Prompt 优化、RAG 策略调整或离线评估数据构建。



日志采集
统一使用 stdout&#x2F;stderr 输出 JSON 日志，包含：
时间、请求 ID、Trace ID。
模型名称&#x2F;版本、租户 ID。
请求 token 数、输出 token 数、耗时。
错误码&#x2F;错误信息。


使用 Fluent Bit&#x2F;Vector 收集至集中日志系统（如 Loki&#x2F;ElasticSearch）。
提供基于 TraceID&#x2F;RequestID 的检索能力，便于问题定位。

链路追踪（基础）
一期可先引入简化版 Trace：
网关生成 TraceID，通过 HTTP 头透传到各服务。
模型服务在日志中打印 TraceID，便于关联。


二期再升级为完整 OpenTelemetry 方案（下文详细）。

发布管理与灰度能力
模型版本管理：
控制平面记录当前 stable 版本与 canary 版本。
通过路由权重控制流量比例（例如 90% stable &#x2F; 10% canary）。


发布流程：
新模型镜像 + 模型权重入库。
控制平面创建新版本元数据。
部署新版本实例，进行健康检查与内部验证。
调整权重，进行线上小流量试运行。
无异常后切换为 stable。



一期实施重点与注意事项
优先保证稳定性
优先选择社区成熟度高、团队已有经验的组件（如 K8s、Prometheus、Grafana、Fluent Bit）。
推理框架建议先从 1~2 个主力模型开始，避免一上来支持过多模型类型导致运维复杂度过高。


配额与隔离
一期必须实现租户级限流和配额管理，避免单租户打满整个平台资源。
GPU 节点与普通节点分池管理，防止非推理负载抢占 GPU。


观测从 Day 1 开始
一期就要将请求 ID &#x2F; Trace ID 规范写死在接口与日志中，后续扩展 OTel 才不需要大改协议。
所有模型实例必须纳入统一监控与告警，否则排障成本很高。


发布与回滚策略
规定统一的发布流程：灰度比例调整、关键指标回归检查、自动&#x2F;手工回滚条件。
镜像与模型权重版本要有统一命名规范，避免回滚时“找不到对应版本”。



二期：RAG 平台与完善的可观测性二期在推理平台之上，构建通用的 RAG 能力与完整可观测性，支撑更复杂的业务场景。
通用 RAG 架构RAG 服务层角色RAG 服务负责将“原生大模型”能力与“企业知识”结合，一般包含以下能力：

文档接入与切分（Ingestion）
向量化（Embedding）
存储与索引（Vector Store）
检索（Search&#x2F;Retrieval）
重写与重排（Rewriting&#x2F;Reranking）
上下文构造与 Prompt 编排
调用底层模型完成最终生成

典型组件划分
Ingestion 服务
支持多数据源（文件、数据库、Confluence、Git、S3 等）。
负责文档解析、清洗、分段、元数据提取。
通过异步任务（队列&#x2F;批处理）完成 embedding 写入向量库。


Embedding 服务
暴露统一 embeddings API。
底层可使用开源 embedding 模型或商用 embedding API。
与模型平台共享基础设施（也可独立部署）。


向量数据库
技术选型：Milvus、Qdrant、Weaviate、PGVector 等。
要求：多租户支持、分片扩展、备份恢复、权限控制。


检索编排服务（RAG Orchestrator）
负责根据场景（问答、文档助手、代码助手等）执行：
Query 分析
检索策略选择（多路向量检索、BM25 + 向量、Hybrid）
上下文过滤与拼装
调用底层 LLM 完成回答





推荐技术选型（RAG 平台）
Ingestion &#x2F; ETL
作业编排：Airflow、Argo Workflows。
文档解析：Apache Tika、unstructured、textract 等。
队列：Kafka、RabbitMQ、Redis Stream。


Embedding 服务
自建模型：开源 embedding 模型（如 bge-*、e5-*）+ vLLM &#x2F; Triton 推理。
托管服务：各云厂商向量服务、OpenAI&#x2F;Claude 等外部 API（注意数据合规）。


向量数据库
高性能专用向量库：Milvus、Qdrant（推荐优先评估）。
轻量内嵌：PGVector（适合已有 PostgreSQL 基础、规模中小场景）。


检索编排 &#x2F; 服务框架
统一编排层：可基于 FastAPI &#x2F; Spring Boot 自建 RAG 服务。
RAG SDK：LangChain、LlamaIndex（适合快速验证与部分生产场景，平台层可做二次封装）。



推荐整体 RAG 系统：Haystack（端到端开源方案）
定位
由 deepset 开源的端到端 RAG 框架，包含文档导入、索引、检索、生成、评估等完整能力。
提供“管道（Pipeline）”抽象，将 Reader、Retriever、重排、生成等步骤以节点形式串联，便于在平台中统一管理。


集成方式
将 Haystack 作为内部 RAG Orchestrator 的默认实现，使用其 Pipeline 作为具体 RAG 流程；底层向量库可选 Milvus、Qdrant、Elasticsearch 等。
将 LLM 调用接入前文的 LiteLLM 网关，使 Haystack 只关注检索与流程编排，而不感知具体模型提供商。


适用场景与优势
适合希望快速落地一个“通用企业知识问答 &#x2F; 文档助手”的团队，减少大量自建 RAG 基础设施的工作。
社区活跃、文档完善，支持多种检索策略与评估工具，便于后续按业务需要扩展或替换局部组件。



RAG 多租户与安全隔离
每个租户&#x2F;项目拥有独立的索引空间（collection&#x2F;namespace）。
元数据中记录文档来源、可见范围、标签等。
RAG 服务层通过 TenantID + 请求人身份控制检索范围，防止越权访问。

完整可观测性：OpenTelemetry + SLO全链路追踪
技术栈：OpenTelemetry + Tempo/Jaeger。
在以下关键点埋点：
网关接入层
RAG 编排层（包括每个检索、重写、重排、调用模型的子 span）
模型服务层（推理耗时、token 数）
向量数据库查询（检索耗时、召回数量）


Trace 中关键标签：
tenant_id, project_id, user_id
model_name, model_version
rag_pipeline, retriever, reranker
input_tokens, output_tokens, latency_ms



推荐技术选型（可观测性二期）
Trace
标准：OpenTelemetry（SDK + Collector）。
后端存储：Grafana Tempo、Jaeger（可用 Jaeger + Elasticsearch 或 Jaeger + ClickHouse）。


度量 + 日志 + Trace 一体化（可选）
一体化方案：Grafana stack (Prometheus + Loki + Tempo + Grafana)。



SLO&#x2F;SLA 与告警
指标定义：
Core API Availability：例如 99.9%
Latency：P95 延迟 &lt; 某阈值（区分在线&#x2F;离线场景）
错误率：非用户输入错误（5xx&#x2F;平台错误）


告警策略：
短时间突增（瞬时问题）与持续超标（容量问题）分级处理。
通知渠道：邮件、IM（企业微信&#x2F;Slack）、值班系统（PagerDuty 等）。



成本与计费
生成以下维度的计费&#x2F;成本数据：
按租户&#x2F;项目：
调用次数
输入&#x2F;输出 token 数
平均响应时间
所在集群、使用 GPU 规格


结合单位 GPU 小时成本，估算租户级成本。


为产品&#x2F;财务系统提供对接能力：
导出到数据仓库
提供成本查询 API 或报表。



二期实施重点与注意事项
RAG 能力要先收敛场景
优先选 1~2 个高价值、知识相对集中的业务场景做 RAG 试点（如内部知识库问答、运维手册问答）。
避免一开始接太多异构数据源，导致数据清洗和权限模型难以收敛。


向量库与数据安全
清晰划分向量库的命名空间与租户边界，避免“共享索引”带来的越权风险。
在数据入库前做脱敏和字段级过滤，明确哪些字段可以被检索到上下文中。


观测维度扩展
对 RAG 流水线单独埋点：检索耗时、召回条数、命中率、重排耗时等。
对每个 RAG 场景维护独立看板，便于判断问题是出在检索还是模型本身。


成本可见性
二期一定要把“按租户&#x2F;场景的成本视图”做清楚，否则后面很难推动内部收费或成本优化。
对高成本场景设置预警阈值（如单次请求 token 超过上限、单租户月度成本异常增长）。



三期：Agent 平台与 MCP 工具生态在稳定的推理与 RAG 能力之上，三期重点建设Agent 平台，支持多工具调用、复杂任务编排与企业系统集成，同时接入 MCP 生态。
Agent 平台总体设计核心目标
为业务方提供一个低代码 &#x2F; 可视化的多 Agent 编排与任务执行平台。
支持：
单 Agent、Multi-Agent 协作
工具调用（Tool&#x2F;MCP）
长&#x2F;短期记忆
工作流编排与版本管理



关键组件
Agent Runtime
承载 Agent 状态机、对话管理、工具调用调度。
支持同步&#x2F;异步任务。


Tool Registry &#x2F; MCP Adapter
用于注册、管理和调用各种工具。
提供 MCP 协议适配层，与外部 MCP 服务对接。


Workflow Orchestrator
支持通过 DSL 或可视化界面定义多步骤流程：
条件分支
并行执行
重试&#x2F;补偿策略




配置与版本管理
Agent 模板（prompt、角色、工具列表）版本化。
场景发布、回滚。



推荐技术选型（Agent &amp; Workflow）
Agent Runtime 与服务框架
语言栈：Python (FastAPI)、Node.js (NestJS) 或 Java (Spring Boot)。
Agent SDK &#x2F; 框架：LangChain、LlamaIndex（平台层需做二次封装以统一标准）。


工作流编排
可视化工作流：Temporal、Camunda、Argo Workflows。
轻量任务队列：Celery（Python）、RQ、Sidekiq（Ruby）等。


配置与版本管理
GitOps：Git + Argo CD 管理 Agent 场景配置（YAML&#x2F;JSON）。
在线配置：配合 ConfigMap、Consul、Nacos 等配置中心。



基于 Latitude 的 Agent 平台方案（推荐）
定位
作为开源的 LLM 应用与 Agent 管理平台，提供界面化的 Prompt 管理、场景配置、评估与调试能力。
可将内部的 Agent Runtime、RAG 服务、LiteLLM 网关等编排在一起，对业务团队暴露为“可配置的 AI 应用”。


集成方式
将 Latitude 接入 LiteLLM 作为统一模型后端，使 Latitude 专注于应用层编排，而非底层模型细节。
将 Agent 的调用数据和结果同步到 Langfuse，形成从“应用 → 调用 → 基础设施”的完整观测链路。


优势
为产品经理和业务开发提供低代码入口，不需要直接修改后端代码即可快速迭代 Agent 场景。
利用现有开源能力（界面、配置、评估），避免从零自研一套 Agent 控制台。



Tool 与 MCP 设计要点Tool 抽象
定义统一的 Tool 接口规范：
输入&#x2F;输出 schema（JSON Schema）
调用方式（HTTP、gRPC、本地函数等）
超时&#x2F;重试&#x2F;幂等等运行时配置


Tool 类型举例：
内部业务系统查询&#x2F;写入
RAG 查询
工作流触发（例如创建 Jira 任务、触发 Jenkins 构建）
数据分析与可视化任务



MCP 集成
MCP（Model Context Protocol）提供一种通用方式，让 LLM 与外部工具和数据源交互。
平台作为 MCP 客户端：
支持注册 MCP 服务器（工具集）。
为 Agent 提供统一的 Tool 列表。
根据权限控制不同租户&#x2F;场景可使用的 MCP 工具。


运行时：
Agent 通过 MCP 调用外部资源（文件系统、数据库、服务 API）。
日志中记录 MCP 调用详情，便于审计与调试。



推荐技术选型（MCP 与工具生态）
MCP 客户端 &#x2F; 服务器实现
语言 SDK：优先选择官方或社区活跃的 MCP SDK（例如 TypeScript&#x2F;Python 实现）。
运行方式：MCP Server 以独立进程或容器部署，通过标准协议与平台通信。


工具类型示例
DevOps 工具：Jenkins、GitLab、ArgoCD、Kubernetes API 等。
协作工具：Jira、Confluence、飞书&#x2F;企业微信机器人。
数据工具：内部 HTTP&#x2F;gRPC API、数据库读写工具（注意权限与脱敏）。



Agent 运行时与状态管理
对话状态：
存储于键值存储或专用会话存储（如 Redis&#x2F;数据库）。
记录对话历史、上下文摘要、工具调用结果。


长期记忆：
通过 RAG 或专用记忆向量库实现。
Agent 根据场景选择是否查询长期记忆。


任务编排：
使用队列&#x2F;任务系统（如 Celery&#x2F;Argo Workflows）承载长时任务。
Agent 负责“决定做什么”，任务系统负责“怎么调度执行”。



安全与审计
权限模型：
谁可以创建&#x2F;发布 Agent 场景。
Agent 场景可以使用哪些工具&#x2F;资源。
工具访问底层系统时的身份代理（Impersonation）。


审计：
记录每次 Agent 调用工具的参数（敏感信息脱敏后三脱敏存储）。
记录结果摘要与调用耗时。
支持按用户&#x2F;租户&#x2F;Agent 场景查询审计记录。



三期实施重点与注意事项
控制好 Agent 能力边界
对 Agent 可用的工具进行白名单管理，不允许默认访问所有内部系统。
为高危操作（如删除资源、变更配置）设计“二次确认”机制，必要时由人工复核。


工具接口规范化
Tool&#x2F;MCP 的输入输出要严格用 JSON Schema 描述，并固化日志格式，便于审计与回放。
工具实现要幂等，遇到网络抖动或重试时不会重复执行危险操作。


状态与长任务管理
将长时间运行的任务交给工作流&#x2F;任务系统，不要在 Agent Runtime 内阻塞长连接。
清晰定义会话过期时间与上下文截断策略，避免状态无限增长导致存储压力。


调试与回放能力
三期上线前，就要设计好“Agent 调用轨迹回放”能力，便于排查错误决策和工具调用问题。
对关键业务场景，可以保留脱敏后的对话 + 工具调用序列，用于后续优化。



四期：多模型、多云与平台治理四期关注平台的规模化与治理问题，包括多模型管理、多云环境支持以及细粒度的安全合规。
多模型与模型市场
模型目录（Model Catalog）：
列出所有可用模型（内部&#x2F;外部 API）。
标注能力标签（通用对话、代码助手、搜索增强等）。
提供性能&#x2F;成本评估数据。


模型申请流程：
租户可在控制台自助选择模型并申请使用。
审批流程：安全&#x2F;合规&#x2F;成本评估。
自动生成对应的调用凭证与配额策略。



多云&#x2F;多集群调度
架构原则：
控制平面可以集中或多集群联邦化。
数据平面可以分布在不同云、不同区域。


流量路由：
根据用户地理位置、租户策略、成本情况选择目标集群。
支持跨区域容灾与就近访问。


数据合规：
欧盟&#x2F;境内数据隔离。
按区域存储日志、向量数据与审计记录。



推荐技术选型（多云多集群）
多集群管理
联邦方案：KubeFed、Karmada、Cluster API 生态。
统一入口：利用 Global Load Balancer（DNS 线路、GSLB）按区域&#x2F;就近路由。


服务发现与流量治理
Service Mesh：Istio、Linkerd，用于跨集群服务发现、流量拆分与 mTLS。
API 入口：全球 Anycast &#x2F; CDN + 边缘网关（如 Cloudflare、GTM + 自建网关）。



模型评估与 A&#x2F;B 测试
Eval 平台：
支持离线评估：基于标注数据集的自动或半自动打分。
在线评估：通过 A&#x2F;B 测试，将用户流量拆分到不同模型或 Prompt 配置。


指标：
任务完成率、用户满意度、幻觉率。
业务特定指标（如转化率、留存等）。



推荐技术选型（评估与 A&#x2F;B 测试）
离线评估
数据存储：数据仓库（Hive、ClickHouse、BigQuery 等）。
评估框架：自研 Eval 服务（基于 Python + FastAPI 或 Jupyter + Papermill 等），也可参考开源 Eval 工具。


在线 A&#x2F;B 测试
流量分配：在 API 网关或 Service Mesh 层实现按权重路由。
实验平台：可复用现有 ABTest 平台（如内部实验平台）、或使用 GrowthBook 等开源方案。



安全、合规与隐私
数据分类与脱敏：
明确哪些数据可以进入模型上下文。
对日志和向量数据进行脱敏（如手机号、邮箱、身份信息）。


隐私保护：
对于外部模型服务，必须控制数据传出范围（如只发送必要字段）。
明确第三方模型服务的数据存储与使用条款。


合规：
符合公司内部与当地法规的数据合规要求（如数据留存周期、审计要求）。



四期实施重点与注意事项
控制复杂度螺旋
多云&#x2F;多集群意味着运维复杂度显著增加，应优先在单云&#x2F;双集群验证平台稳定性后再扩展。
明确哪些能力必须多云（如跨地域容灾），哪些可以保持“单云优先”以减负。


合规优先级提升
四期开始，需将数据主权、跨境传输、隐私保护等法规要求作为架构评审的必选项。
不同区域的日志、向量数据、审计记录要严格按区域存储和访问控制。


成本与资源调度策略
多云场景下要引入“成本感知”的调度策略，例如低峰期迁移部分负载到低成本区域或云厂商。
定期审计“闲置 GPU&#x2F;集群”，通过关停、缩容或任务迁移降低浪费。


标准化接口与抽象
对外接口（API、SDK）要保持与底层云&#x2F;集群解耦，避免绑定某一云厂商的专有特性。
内部尽量通过标准的 Service Mesh、OpenTelemetry、容器编排抽象，以便迁移和扩展。



平台运维与团队协作组织角色与职责
平台团队（Platform &#x2F; SRE）
负责基础设施、模型集群与平台的稳定运行。


模型团队（ML&#x2F;AI）
负责模型选择、微调、评估与上线。


业务团队（产品&#x2F;业务开发）
使用平台能力构建业务 Agent&#x2F;RAG 应用。



运维与应急
建议建立标准化的：
故障分级与响应流程。
压测与容量规划流程。
变更评审流程（Change Review）。



最佳实践与常见问题
建议维护平台文档中心：
使用指南、最佳实践。
常见错误码与排查流程。
典型架构案例。



总结与后续扩展本文从整体架构与多期路线的角度，设计了一套可生产落地的大语言模型平台系统，涵盖：

一期：稳定的推理平台与基础运维能力。
二期：通用 RAG 平台与完善的可观测性、成本管理。
三期：Agent 平台与 MCP 工具生态集成。
四期：多模型、多云环境下的平台治理与精细化管理。

在实际落地过程中，可以根据企业现有基础设施（Kubernetes、Service Mesh、对象存储、向量数据库等）的成熟度做取舍与调整。后续可进一步扩展的方向包括：更智能的自动扩缩容策略、更丰富的 Agent 编排能力、端到端安全合规方案等。

本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>MCP</tag>
        <tag>RAG</tag>
        <tag>Agent</tag>
        <tag>运维</tag>
        <tag>可观测性</tag>
        <tag>大语言模型</tag>
        <tag>AI平台</tag>
      </tags>
  </entry>
  <entry>
    <title>Python包开发与发布：使用 build 与 twine（含 project.scripts 示例）</title>
    <url>/posts/6aa04c23d95b.html</url>
    <content><![CDATA[本文面向有一定 Python 基础、希望将代码规范化为可安装包并发布到 PyPI 的工程师。你将学会：

如何创建标准的 Python 包工程骨架（src 布局）
在 pyproject.toml 中使用 PEP 621 声明元数据与 project.scripts 生成命令行脚本
使用 build 本地构建分发产物（sdist&#x2F;wheel）
使用 twine 校验并上传到 TestPyPI 与 PyPI
常见问题与排错要点

参考标准：PEP 517&#x2F;518（构建系统），PEP 621（项目元数据）。
适用环境
Python ≥ 3.8（推荐 3.10+）
macOS&#x2F;Linux&#x2F;Windows
包管理：pip 或 pipx


一、项目骨架（src 布局）推荐使用「src 布局」以避免导入歧义，目录结构如下：
mycli/├─ pyproject.toml├─ README.md├─ LICENSE├─ src/│  └─ mycli/│     ├─ __init__.py│     ├─ __main__.py│     └─ cli.py└─ tests/   └─ test_basic.py


src/mycli：包代码根目录
cli.py：命令行入口逻辑（将通过 project.scripts 暴露为命令）
__main__.py：支持 python -m mycli 直接运行
pyproject.toml：声明构建系统与项目元数据（PEP 621）


二、编写包代码（示例：命令行工具）所有代码与注释统一使用英文（便于社区协作与审核）。
src/mycli/cli.py
import argparsedef main() -&gt; None:    &quot;&quot;&quot;Entry point for the mycli command line tool.&quot;&quot;&quot;    parser = argparse.ArgumentParser(        prog=&quot;mycli&quot;,        description=&quot;A tiny example CLI that greets the user.&quot;    )    parser.add_argument(        &quot;-n&quot;, &quot;--name&quot;,        default=&quot;World&quot;,        help=&quot;Name to greet&quot;    )    args = parser.parse_args()    print(f&quot;Hello, &#123;args.name&#125;!&quot;)

src/mycli/__init__.py
__all__ = [&quot;__version__&quot;]__version__ = &quot;0.1.0&quot;

src/mycli/__main__.py
from .cli import mainif __name__ == &quot;__main__&quot;:    main()


三、pyproject.toml（PEP 621 + project.scripts）示例使用 setuptools 作为构建后端，启用 PEP 621 元数据与 project.scripts：
[build-system]requires = [&quot;setuptools&gt;=69&quot;, &quot;wheel&quot;]build-backend = &quot;setuptools.build_meta&quot;[project]name = &quot;mycli-example&quot;version = &quot;0.1.0&quot;description = &quot;A tiny example CLI published to PyPI.&quot;readme = &quot;README.md&quot;requires-python = &quot;&gt;=3.8&quot;license = &#123; text = &quot;MIT&quot; &#125;authors = [&#123; name = &quot;Your Name&quot;, email = &quot;you@example.com&quot; &#125;]keywords = [&quot;cli&quot;, &quot;example&quot;, &quot;greeting&quot;]classifiers = [  &quot;Programming Language :: Python :: 3&quot;,  &quot;Programming Language :: Python :: 3 :: Only&quot;,  &quot;License :: OSI Approved :: MIT License&quot;,  &quot;Operating System :: OS Independent&quot;,]dependencies = []# This creates an executable &#x27;mycli&#x27; in the user&#x27;s PATH after installation.[project.scripts]mycli = &quot;mycli.cli:main&quot;[tool.setuptools]package-dir = &#123;&quot;&quot; = &quot;src&quot;&#125;[tool.setuptools.packages.find]where = [&quot;src&quot;]

要点：

project.scripts 将 mycli 命令映射到 mycli.cli:main
采用 src 布局需设置 package-dir 与 packages.find.where
设置 requires-python 与 classifiers 便于用户检索与兼容性声明


四、README 与 LICENSE
README.md 作为包主页与长描述，需简要说明安装与用法
许可证建议选择常见的开源协议（如 MIT&#x2F;Apache-2.0）

README.md 示例：
# mycli-exampleSimple CLI that greets you.## Install```bashpip install mycli-example

Usagemycli --name Alice
---## 五、安装工具并本地构建推荐使用 `pipx` 安装构建与发布工具，避免污染项目虚拟环境：```bash# Install toolspipx install buildpipx install twine# Or use pip (in a temporary/clean venv)python -m pip install --upgrade build twine

执行构建：
python -m build# 输出产物位于 dist/ 目录，包含 .whl 与 .tar.gz

构建检查：
twine check dist/*


六、发布到 TestPyPI 与 PyPI强烈建议先在 TestPyPI 验证再发布至 PyPI。
方式 A：使用 .pypirc 管理凭据（推荐）创建 ~/.pypirc（macOS&#x2F;Linux）：
[distutils]index-servers =    pypi    testpypi[pypi]repository = https://upload.pypi.org/legacy/username = __token__password = pypi-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX[testpypi]repository = https://test.pypi.org/legacy/username = __token__password = pypi-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

上传到 TestPyPI：
twine upload -r testpypi dist/*

验证安装（注意使用 TestPyPI 索引）：
python -m pip install --index-url https://test.pypi.org/simple --extra-index-url https://pypi.org/simple mycli-example# 验证命令是否可用mycli --name Bob

确认无误后，上传至 PyPI：
twine upload -r pypi dist/*

方式 B：直接指定仓库 URL（无需 .pypirc）twine upload --repository-url https://test.pypi.org/legacy/ dist/*twine upload --repository-url https://upload.pypi.org/legacy/ dist/*

凭据将通过交互提示或环境变量读取（CI 中建议使用环境变量&#x2F;密文）。

七、使用与验证安装后应具备两种使用方式：
# 1) 通过脚本（来自 project.scripts）mycli --name Carol# 2) 通过模块运行（__main__.py）python -m mycli --name Carol


八、常见问题与排错
File already exists（重复版本）：PyPI 不允许覆盖同版本；升级 version（如 0.1.1）并重新构建上传
Invalid long_description：twine check 失败，多为 README 渲染问题；将 readme = &quot;README.md&quot; 并确保 Markdown 合规
Script not found：检查 project.scripts 的目标路径是否正确（mycli.cli:main）且包已包含在构建中（packages.find 设置）
ImportError after install：多因未使用 src 布局或测试时在项目根运行导致导入到源码；建议在全新 venv 中验证
401&#x2F;403 授权失败：确认使用 __token__ 用户名与有效 PyPI Token；避免泄露令牌（使用 CI 密文）
Wheel 缺少文件：确认未被 MANIFEST.in&#x2F;忽略规则误排除；或切换到现代 PEP 621 配置并避免遗留 setup.cfg 混用


九、在 CI 中自动发布（可选思路）
在 GitHub Actions&#x2F;其他 CI 中使用发布工作流
构建步骤同本地：python -m build → twine check → twine upload
凭据使用仓库密文（如 PYPI_API_TOKEN），并在 CI 步骤设置为环境变量

官方参考：

Packaging User Guide（构建&#x2F;发布）：Packaging Guide
PyPI（上传与凭据）：Publishing packages


十、总结
使用 PEP 621 在 pyproject.toml 声明元数据，结合 project.scripts 生成命令行
用 build 统一构建 sdist&#x2F;wheel，twine 校验与上传
先 TestPyPI 验证，再发布至 PyPI
采用 src 布局、明确 Python 版本与分类器，提升可维护性与可发现性


本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Packaging</tag>
        <tag>PyPI</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title>LiteLLM Proxy 使用指南：Docker 部署、vLLM 代理</title>
    <url>/posts/48e8b33bb2e1.html</url>
    <content><![CDATA[背景与目标LiteLLM Proxy 是一个 OpenAI API 兼容的模型网关，支持将来自 OpenAI、Azure OpenAI、Bedrock、Vertex AI 以及本地&#x2F;自建的 OpenAI 兼容推理服务（如 vLLM）统一到一套接口之下，并提供虚拟 API Key、用量与预算、速率限制、缓存、日志&#x2F;指标、路由、负载均衡与回退等能力。本文将演示：

如何用 Docker 快速部署 LiteLLM Proxy（含最小可用与带数据库的完整模式）
如何把 vLLM 暴露的 OpenAI 兼容接口接入到 LiteLLM Proxy 进行统一代理
如何生成虚拟 Key、设置每分钟请求数（RPM）限速
如何查询模型列表等常用“免费”功能

参考与更多细节请见官方文档：

LiteLLM Proxy Docker 快速上手
vLLM Provider 文档

你将学到什么
用 Docker 启动 LiteLLM Proxy，并验证 /chat/completions
将本地 vLLM（OpenAI 兼容接口）纳入代理，统一用 OpenAI 协议调用
配置同名模型多后端负载均衡，实现流量分发与高可用
在不接数据库的前提下，如何给”不同团队”分发可控的 Key（网关方案）
接入数据库后，如何生成”虚拟 Key”、设置限速并进行用量治理
查询模型列表、排错以及生产级监控与高可用实践

核心功能速览
OpenAI 兼容：统一 /chat/completions、/embeddings 等接口
多模型路由：一个代理前面挂多家模型与自建 vLLM
负载均衡：同名模型多后端分发，支持 simple-shuffle、usage-based-routing、latency-based-routing 等策略
密钥治理：
无数据库：用网关做静态 Key 白名单 + 限速 + 审计
带数据库：原生虚拟 Key、预算&#x2F;配额、团队&#x2F;用户可视化管理


可观测性：日志、指标、追踪，支持缓存与回退策略

如何阅读这篇文章
想快速跑通：直接看”快速开始”和”对接 vLLM”。
需要负载均衡：看”负载均衡：同名模型多后端分发”。
无数据库就要分发团队 Key：看”无数据库的密钥治理（最简部署）”。
需要 RPM&#x2F;预算&#x2F;面板：看”使用 Docker Compose（Proxy + Postgres）”与”生成虚拟 Key 并设置限速（RPM）”。
线上实践：看”生产部署实践（监控与高可用）”。

快速开始（最小可用）你可以直接拉取官方镜像并用一份 config.yaml 启动 Proxy。下面示例展示最小可用部署（未接数据库、仅用于功能验证）。
# Pull imagedocker pull ghcr.io/berriai/litellm:main-latest# Start proxy with your config (created in next section)docker run \  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  -e AZURE_API_KEY=demo \  -e AZURE_API_BASE=https://example.openai.azure.com/ \  -p 4000:4000 \  ghcr.io/berriai/litellm:main-latest \  --config /app/config.yaml --detailed_debug# Proxy up at http://0.0.0.0:4000

最简单的 litellm_config.yaml（示例使用 Azure OpenAI，仅用于快速跑通 Proxy）：
model_list:  - model_name: gpt-4o    litellm_params:      model: azure/my_azure_deployment      api_base: os.environ/AZURE_API_BASE      api_key: os.environ/AZURE_API_KEY      api_version: &quot;2025-01-01-preview&quot; # optional


提示：启动后可用 POST /chat/completions 进行测试，详见下文调用示例。

使用 Docker Compose（Proxy + Postgres）如果需要启用虚拟 Key、用户&#x2F;团队管理、预算与用量统计等高级功能，建议配合 Postgres 使用。官方提供了现成的 docker-compose.yml 与 .env 用法：
# Get the compose filecurl -O https://raw.githubusercontent.com/BerriAI/litellm/main/docker-compose.yml# Set master key (admin) and salt key (for encrypting provider keys)echo &#x27;LITELLM_MASTER_KEY=&quot;sk-1234&quot;&#x27; &gt; .envecho &#x27;LITELLM_SALT_KEY=&quot;sk-1234&quot;&#x27; &gt;&gt; .envsource .env# Start servicesdocker-compose up

将数据库接入到 config.yaml：
model_list:  - model_name: gpt-4o    litellm_params:      model: azure/my_azure_deployment      api_base: os.environ/AZURE_API_BASE      api_key: os.environ/AZURE_API_KEY      api_version: &quot;2025-01-01-preview&quot;general_settings:  master_key: sk-1234  database_url: &quot;postgresql://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;dbname&gt;&quot;


启用 Admin UI：Admin UI（管理界面）必须连接 Postgres 数据库才能工作。你需要同时配置 master_key 和 database_url。配置完成后，访问 /ui 路径并使用 master_key 登录。

  
对接 vLLM：把自建模型纳入统一代理vLLM 可以以 OpenAI 兼容方式暴露推理服务（/v1 路径）。我们先启动 vLLM，再在 LiteLLM 里将其注册为一个“模型”。
1) 启动 vLLM（OpenAI 兼容 API）# Example: start vLLM OpenAI-compatible serverpython -m vllm.entrypoints.openai.api_server \  --model meta-llama/Llama-3.1-8B-Instruct \  --host 0.0.0.0 \  --port 8000# vLLM OpenAI API at http://&lt;host&gt;:8000/v1


如果使用容器化 vLLM，请确保 LiteLLM Proxy 容器能够访问到 vLLM 的 http://&lt;host&gt;:8000/v1（可用 host.docker.internal 或桥接网络）。

2) 在 LiteLLM 中注册 vLLM 模型LiteLLM 对“OpenAI 兼容端点”的使用建议前缀为 hosted_vllm/。将 litellm_params.model 设为 hosted_vllm/&lt;model-identifier&gt;，并把 api_base 指向你 vLLM 的 /v1 即可。
# file: litellm_config.yamlmodel_list:  - model_name: llama-3.1-8b-instruct    litellm_params:      model: hosted_vllm/llama-3.1-8b-instruct  # route to OpenAI-compatible vLLM      api_base: os.environ/HOSTED_VLLM_API_BASE # e.g. http://host.docker.internal:8000/v1      api_key: os.environ/HOSTED_VLLM_API_KEY   # optional if your vLLM server doesn&#x27;t require itgeneral_settings:  master_key: sk-1234                           # required if you want admin/virtual keys

然后启动 LiteLLM Proxy：
docker run \  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  -e HOSTED_VLLM_API_BASE=http://host.docker.internal:8000/v1 \  -e HOSTED_VLLM_API_KEY=EMPTY \  -p 4000:4000 \  ghcr.io/berriai/litellm:main-latest \  --config /app/config.yaml --detailed_debug

现在，Proxy 会以 model: &quot;llama-3.1-8b-instruct&quot; 的名义，将请求转发到你本地&#x2F;私有的 vLLM 服务上。
3) 通过 Admin UI 添加 vLLM 模型（可选）如果你启用了 Admin UI（配置了数据库与 master_key），也可以通过图形界面添加模型：

访问 /ui 并使用 master_key 登录。
进入 Models 页面，点击 “Add Model”。
在配置表单中填写关键信息：
Public Model Name: 对外暴露的模型名称（客户端请求时使用的名字，如 llama-3.1-8b-instruct）。
LiteLLM Model Name(s): 重要！ 这里必须添加 hosted_vllm/ 前缀，例如 hosted_vllm/llama-3.1-8b-instruct。这是告诉 LiteLLM 使用 vLLM 适配器进行转发的关键。
API Base: vLLM 的 API 地址（如 http://host.docker.internal:8000/v1）。
API Key: 根据 vLLM 配置填写（如无鉴权可填 EMPTY 或任意字符）。



常见坑速查（vLLM 对接）
404/connection error：api_base 必须指向 vLLM 的 /v1 根，如 http://host:8000/v1。
容器网络：Proxy 与 vLLM 不在同一网络时，使用 host.docker.internal 或配置自定义 bridge 网络。
鉴权：若 vLLM 无鉴权，api_key 可留空；若自定义鉴权，请按 vLLM 的要求设置。
模型名不一致：model_name（对外名）可以自定义，但 litellm_params.model 需符合前缀与路由规则（hosted_vllm/&lt;name&gt;）。

负载均衡：同名模型多后端分发在 model_list 中设置同名的 model_name，可实现流量负载到不同的后端。这种情况下 master_key 是必须的。同时路由策略默认为：simple-shuffle，其它策略可在 LiteLLM 负载均衡文档 中查看。
负载均衡配置示例# file: litellm_config.yamlmodel_list:  # 第一个后端实例  - model_name: llama-3.1-8b-instruct    litellm_params:      model: hosted_vllm/llama-3.1-8b-instruct      api_base: http://vllm-backend-1:8000/v1      api_key: &quot;&quot;    # 第二个后端实例（同名模型）  - model_name: llama-3.1-8b-instruct    litellm_params:      model: hosted_vllm/llama-3.1-8b-instruct      api_base: http://vllm-backend-2:8000/v1      api_key: &quot;&quot;    # 第三个后端实例（同名模型）  - model_name: llama-3.1-8b-instruct    litellm_params:      model: hosted_vllm/llama-3.1-8b-instruct      api_base: http://vllm-backend-3:8000/v1      api_key: &quot;&quot;general_settings:  master_key: sk-1234  # 负载均衡功能需要 master_keyrouter_settings:  routing_strategy: simple-shuffle  # 默认策略，可选：usage-based-routing, latency-based-routing

负载均衡测试启动多个 vLLM 后端实例：
# 启动第一个 vLLM 实例python -m vllm.entrypoints.openai.api_server \  --model meta-llama/Llama-3.1-8B-Instruct \  --host 0.0.0.0 \  --port 8001# 启动第二个 vLLM 实例python -m vllm.entrypoints.openai.api_server \  --model meta-llama/Llama-3.1-8B-Instruct \  --host 0.0.0.0 \  --port 8002# 启动第三个 vLLM 实例python -m vllm.entrypoints.openai.api_server \  --model meta-llama/Llama-3.1-8B-Instruct \  --host 0.0.0.0 \  --port 8003

启动 LiteLLM Proxy：
docker run \  -v $(pwd)/litellm_config.yaml:/app/config.yaml \  -p 4000:4000 \  ghcr.io/berriai/litellm:main-latest \  --config /app/config.yaml --detailed_debug

验证负载均衡效果发送多个请求，观察流量分发：
# 发送 10 个请求，观察负载分发for i in &#123;1..10&#125;; do  echo &quot;Request $i:&quot;  curl -X POST &#x27;http://0.0.0.0:4000/chat/completions&#x27; \    -H &#x27;Content-Type: application/json&#x27; \    -H &#x27;Authorization: Bearer sk-1234&#x27; \    -d &#x27;&#123;      &quot;model&quot;: &quot;llama-3.1-8b-instruct&quot;,      &quot;messages&quot;: [        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello, which backend am I talking to?&quot;&#125;      ],      &quot;max_tokens&quot;: 50    &#125;&#x27; | jq &#x27;.choices[0].message.content&#x27;  echo &quot;---&quot;done

其他路由策略除了默认的 simple-shuffle，还可以使用以下策略：
general_settings:  master_key: sk-1234  routing_strategy: usage-based-routing  # 基于使用量的路由  # 或者  routing_strategy: latency-based-routing  # 基于延迟的路由


注意：负载均衡功能需要 master_key 支持，确保在 general_settings 中正确配置。

通过 Proxy 发起聊天请求LiteLLM Proxy 兼容 OpenAI SDK&#x2F;接口。以下用 curl 演示：
curl -X POST &#x27;http://0.0.0.0:4000/chat/completions&#x27; \  -H &#x27;Content-Type: application/json&#x27; \  -H &#x27;Authorization: Bearer sk-1234&#x27; \  -d &#x27;&#123;    &quot;model&quot;: &quot;llama-3.1-8b-instruct&quot;,    &quot;messages&quot;: [      &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,      &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is LiteLLM Proxy?&quot;&#125;    ]  &#125;&#x27;


如果未使用数据库，你也可以把 Authorization 设置为你在 general_settings.master_key 中配置的主密钥；若启用了虚拟 Key（见下一节），则推荐使用虚拟 Key 访问。

生成虚拟 Key 并设置限速（RPM）启用了数据库后，可使用主密钥（master_key）来创建受控的虚拟 Key。例如限制每分钟 1 次请求：
curl -L -X POST &#x27;http://0.0.0.0:4000/key/generate&#x27; \  -H &#x27;Authorization: Bearer sk-1234&#x27; \  -H &#x27;Content-Type: application/json&#x27; \  -d &#x27;&#123;&quot;rpm_limit&quot;: 1&#125;&#x27;

成功将返回：
&#123; &quot;key&quot;: &quot;sk-12...&quot; &#125;

使用该虚拟 Key 调用模型：
# 1st call - should succeedcurl -X POST &#x27;http://0.0.0.0:4000/chat/completions&#x27; \  -H &#x27;Content-Type: application/json&#x27; \  -H &#x27;Authorization: Bearer sk-12...&#x27; \  -d &#x27;&#123;    &quot;model&quot;: &quot;llama-3.1-8b-instruct&quot;,    &quot;messages&quot;: [      &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;&#125;,      &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How to solve 8x + 7 = -23?&quot;&#125;    ]  &#125;&#x27;# 2nd call (within one minute) - should fail with 429curl -X POST &#x27;http://0.0.0.0:4000/chat/completions&#x27; \  -H &#x27;Content-Type: application/json&#x27; \  -H &#x27;Authorization: Bearer sk-12...&#x27; \  -d &#x27;&#123;    &quot;model&quot;: &quot;llama-3.1-8b-instruct&quot;,    &quot;messages&quot;: [      &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;&#125;,      &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How to solve 8x + 7 = -23?&quot;&#125;    ]  &#125;&#x27;

查询模型列表（&#x2F;models）LiteLLM Proxy 暴露 OpenAI 兼容的模型列表接口：
curl -s &#x27;http://0.0.0.0:4000/models&#x27; \  -H &#x27;Authorization: Bearer sk-1234&#x27;

返回的列表中将包含你在 model_list 中注册的模型（例如 llama-3.1-8b-instruct）。
常见问题与故障排除
SSL 校验失败 &#x2F; 连接错误：可在 config.yaml 中关闭 SSL 校验（仅在受信环境调试时使用）。
litellm_settings:  ssl_verify: false

数据库连接失败 &#x2F; 权限不足：确认数据库用户有建库&#x2F;建表权限。必要时在数据库中执行授权（云厂商如 CloudSQL 语法略有差异）。

非 root 场景：官方提供了非 root 镜像使用说明，按需选择。

vLLM 访问不到：检查容器网络连通性，api_base 是否指向了可达的 http://&lt;host&gt;:8000/v1，以及是否通过 host.docker.internal 或自定义 bridge 网络打通。


安全与最佳实践
生产环境务必使用强随机的 LITELLM_SALT_KEY 与 master_key，切勿在版本库中明文提交。
对外暴露的 Proxy 建议置于零信任&#x2F;网关之后，搭配 WAF&#x2F;速率限制&#x2F;鉴权策略。
针对不同团队与应用使用不同的虚拟 Key，并配置预算、RPM&#x2F;TPM、可访问模型白名单。
定期审计访问日志与开销，启用缓存与模型路由策略以优化成本与稳定性。

生产部署实践（监控与高可用）架构与高可用
多副本部署：将 LiteLLM Proxy 以无状态方式水平扩展，多副本 + 负载均衡（Nginx&#x2F;Ingress&#x2F;Gateway）。
数据库高可用：Postgres 采用主从&#x2F;托管服务（如 RDS&#x2F;CloudSQL&#x2F;Aurora），开启自动备份与只读副本；连接池建议使用 PgBouncer。
缓存层：开启 Proxy 的缓存能力，或引入 Redis 作为响应缓存（对热门提示&#x2F;嵌入有效）。
故障回退：在 model_list 中配置路由回退与多提供商冗余，优先同类开源模型或公有云模型作为备份。
弹性伸缩：以 QPS&#x2F;RPM、CPU、延迟 P95&#x2F;P99 为指标触发 HPA（K8s 水平自动扩缩容）。

监控与日志
核心指标（Proxy）：
请求量：requests_total（按模型、调用方、状态码分维度）
成功率：success_rate（2xx 比例）
延迟：latency_ms_p50/p95/p99
速率限制触发：rate_limit_hits
Token&#x2F;成本：input_tokens_total、output_tokens_total、cost_total


核心指标（vLLM）：
并发&#x2F;排队：requests_in_flight、queue_depth
显存与负载：gpu_memory_used、gpu_utilization、cpu_utilization
拒绝&#x2F;超时：backend_errors、timeout_errors


日志与追踪：
结构化访问日志（含 request_id、调用方、模型、用量、耗时、状态码）
结合 OpenTelemetry &#x2F; Jaeger 进行链路追踪，串联网关→Proxy→vLLM→存储


可观测性栈：Prometheus + Grafana（Dashboard：Proxy 指标 + vLLM&#x2F;GPU 指标），Loki&#x2F;ELK 做日志聚合。

与 Prometheus 集成：暴露 /metricsLiteLLM Proxy 内置 Prometheus 集成能力，只需启用 prometheus 回调即可在 /metrics 暴露关键指标（如请求量、失败率、延迟、Token 用量、预算等），便于被 Prometheus 抓取，再用 Grafana 做可视化（参考：Prometheus metrics 官方文档）。如果你使用官方 Docker 镜像，所需的 prometheus_client 已经内置。
最小配置示例（放入 config.yaml）：
model_list:  - model_name: gpt-4o    litellm_params:      model: gpt-4olitellm_settings:  callbacks:    - prometheus

按本文前文方式启动 Proxy 后，即可在浏览器或 Prometheus 中访问：
curl http://localhost:4000/metrics# or open in browser: http://localhost:4000/metrics

在 Prometheus 的 scrape_configs 中将 http://&lt;proxy-host&gt;:4000/metrics 作为抓取目标，即可采集合适的请求与 Token 相关指标，然后结合 Grafana 使用官方维护的 Dashboard 模板或自定义仪表盘。
与 Langfuse 集成：OpenTelemetry 追踪在生产环境中，如果你希望对每次 Prompt &#x2F; Completion 进行更细粒度的链路追踪、提示调优与质量分析，可以通过 LiteLLM Proxy 将调用数据推送到 Langfuse。Langfuse 提供了基于 OpenTelemetry 的接入方式，只需设置环境变量并启用 langfuse_otel 回调即可。
导出 Langfuse 凭据与 OTEL 端点（以 US 云服务区为例）：
export LANGFUSE_PUBLIC_KEY=&quot;pk-lf-...&quot;          # your Langfuse public keyexport LANGFUSE_SECRET_KEY=&quot;sk-lf-...&quot;          # your Langfuse secret keyexport LANGFUSE_OTEL_HOST=&quot;https://us.cloud.langfuse.com&quot;  # default US region# export LANGFUSE_OTEL_HOST=&quot;https://otel.my-langfuse.company.com&quot;  # custom OTEL endpoint

在 config.yaml 中启用 Langfuse OTEL 回调：
# config.yamllitellm_settings:  callbacks:    - langfuse_otel

随后按常规方式运行 Proxy 即可（Docker 或本地 CLI 均可）：
litellm --config /path/to/config.yaml

一旦集成成功，你可以在 Langfuse 的 Web 控制台中查看通过 LiteLLM Proxy 发出的请求轨迹、提示&#x2F;回复内容、Token 用量与延迟分布等信息，并结合项目中的其它 OTEL 数据进行统一分析与调优。
告警阈值示例
成功率低于 98%（5 分钟窗口）
P95 延迟高于 2s（连续 10 分钟）
429 速率限制命中率 &gt; 5%（提示需要扩容或调参）
vLLM queue_depth &gt; 100 且持续 5 分钟（推理拥塞）
GPU 显存利用率 &gt; 95% 且持续 10 分钟（需要缩短 max tokens&#x2F;增加副本）

安全与密钥治理
密钥管理：LITELLM_SALT_KEY 与 master_key 存放于 KMS&#x2F;Secret Manager；定期轮换，最小权限访问。
虚拟 Key 策略：为每个应用&#x2F;团队单独发放虚拟 Key，设置 RPM/TPM/并发、预算与模型白名单，超限自动告警。
网络与访问控制：将 Proxy 置于私有网络内，通过 API 网关暴露；开启 WAF 与 IP 访问控制；对外只暴露必要端口。
数据与合规：记录审计日志，敏感数据脱敏；对模型消息体避免长期持久化，仅保留元数据。

灰度与流量治理
版本灰度：新模型&#x2F;新参数走 1%-5% 灰度；监控指标稳定后逐步扩大权重。
路由策略：按租户&#x2F;区域&#x2F;延迟&#x2F;成本做智能路由与回退；对低优先级任务使用更低成本模型。
缓存与重放：对可缓存请求开启缓存；对失败请求采用指数退避与幂等重试（结合 request_id）。

无数据库的密钥治理（最简部署）
前提说明：官方 E2E 教程中，虚拟 Key（/key/generate）功能需要接入 Postgres 数据库方可启用与持久化管理（参考：LiteLLM Proxy Docker 快速上手 的“Generate a virtual key”章节）。在“最简部署（无数据库）”下，内建的虚拟 Key、预算、配额面板不可用。以下提供两种在无数据库前提下的可操作密钥治理方案。

方案 A：API 网关静态 Key 白名单（推荐）思路：

为每个团队生成一个“外发 Key”（例如 sk-teamA-...、sk-teamB-...），仅对团队公开。
在 API 网关（Nginx&#x2F;Kong&#x2F;Traefik 等）校验来访 Key；校验通过后，将请求头 Authorization 改写为 LiteLLM 的 master_key，并注入 X-Team 头标识团队，LiteLLM Proxy 端只需维护 master_key 即可。
限速与访问控制在网关实现（按团队维度），日志也在网关聚合，LiteLLM 只做推理转发。

操作步骤：

配置 LiteLLM 最简 config.yaml（仅 master_key）：
model_list:  - model_name: llama-3.1-8b-instruct    litellm_params:      model: hosted_vllm/llama-3.1-8b-instruct      api_base: http://vllm:8000/v1      api_key: &quot;&quot;general_settings:  master_key: sk-master-very-strong

启动 LiteLLM Proxy（略，见前文）。

在 Nginx 上实现静态 Key 白名单 + 改写：
# 假设放在 stream 或 http 反向代理前置，以下为 http 片段示例map $http_authorization $team_name &#123;  default &quot;&quot;;  &quot;Bearer sk-teamA-123&quot; teamA;  &quot;Bearer sk-teamB-456&quot; teamB;&#125;map $http_authorization $is_valid_key &#123;  default 0;  &quot;Bearer sk-teamA-123&quot; 1;  &quot;Bearer sk-teamB-456&quot; 1;&#125;# 可选：每团队限速桶，1 分钟 60 次limit_req_zone $team_name zone=per_team:10m rate=60r/m;server &#123;  listen 80;  server_name litellm.example.com;  location / &#123;    if ($is_valid_key = 0) &#123; return 401; &#125;    # 限速（命中返回 429）    limit_req zone=per_team burst=30 nodelay;    # 注入团队头，改写为 master_key 转发到 LiteLLM Proxy    proxy_set_header Authorization &quot;Bearer sk-master-very-strong&quot;;    proxy_set_header X-Team $team_name;    proxy_pass http://litellm-proxy:4000;  &#125;&#125;

团队使用各自外发 Key 调用：
# Team Acurl -X POST &#x27;https://litellm.example.com/chat/completions&#x27; \  -H &#x27;Content-Type: application/json&#x27; \  -H &#x27;Authorization: Bearer sk-teamA-123&#x27; \  -d &#x27;&#123;    &quot;model&quot;: &quot;llama-3.1-8b-instruct&quot;,    &quot;messages&quot;: [      &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;hi&quot;&#125;    ]  &#125;&#x27;

优点：

无需数据库即可做到“多团队 Key、限速、隔离与审计”。
Key 轮换在网关层进行，LiteLLM 无感。

注意：

外发 Key 白名单应存放于 Secret 管理，启用审计与告警；网关日志中保留 X-Team 以便统计。
若需更精细的模型白名单、预算、用量统计面板，请切换到“带数据库”模式，使用虚拟 Key 原生能力。

方案 B：多实例多主密钥（隔离更强）思路：

为每个团队部署一个 LiteLLM Proxy 实例，分别设置不同的 master_key 与域名（如 teamA-llm.example.com）。
通过上游 LB&#x2F;Ingress 做域名或路径路由，实例间资源独立，便于限速与版本隔离。

优点：

实例层面的强隔离，团队级资源&#x2F;风控策略互不影响。

成本：

需要多实例运维与配置同步（可用 IaC&#x2F;Helm 统一下发）。

无数据库下的常见诉求替代
限速：用网关 limit_req（Nginx）或 API 网关内置策略按团队维度限速。
预算&#x2F;用量：解析网关日志与 LiteLLM 访问日志，按 X-Team 汇总请求数与 token 用量（可在边车或日志管道中解析响应 usage 字段）。
模型白名单：按路由规则控制允许访问的路径和模型名（例如基于 model 字段的 WAF&#x2F;自定义网关插件）。


若后续需要虚拟 Key、预算、速率限制、团队&#x2F;用户&#x2F;密钥可视化管理，建议切换带数据库的部署方式；参考前文与官方教程：LiteLLM Proxy Docker 快速上手。

参考资料
官方教程：LiteLLM Proxy E2E 与 Docker 快速启动（包含虚拟 Key、速率限制、数据库接入等）
https://docs.litellm.ai/docs/proxy/docker_quick_start


vLLM Provider 集成说明（如何让 LiteLLM 代理 vLLM 暴露的 OpenAI 兼容服务）
https://docs.litellm.ai/docs/providers/vllm




本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>LiteLLM</tag>
        <tag>vLLM</tag>
        <tag>LLM Proxy</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>LightRAG：轻量级检索增强生成系统详解</title>
    <url>/posts/3ce7fb915f3c.html</url>
    <content><![CDATA[随着大语言模型（LLM）的快速发展，如何让AI系统能够访问和处理大量外部知识成为了一个关键挑战。检索增强生成（Retrieval-Augmented Generation，RAG）技术应运而生，而LightRAG作为一个轻量级且高效的RAG系统，通过结合知识图谱和向量检索技术，为企业级知识管理和智能问答提供了优秀的解决方案。
LightRAG 简介LightRAG是一个现代化的检索增强生成系统，专注于提供高质量的问答和知识管理功能。该系统最大的特点是将传统的向量检索与知识图谱技术相结合，实现了更精准和上下文相关的信息检索。
核心特性
轻量级设计：优化的架构设计，降低资源消耗
多模态支持：同时支持向量检索和图谱检索
多存储后端：兼容Neo4j、PostgreSQL、Faiss等多种存储系统
多模型支持：支持OpenAI、Hugging Face、Ollama等主流LLM
生产就绪：提供完整的API接口和Web UI界面
高并发处理：支持并发索引和查询操作

系统架构设计LightRAG采用分层模块化架构，确保了系统的可扩展性和维护性。
整体架构LightRAG的架构分为索引（Index）和检索（Retrieve）两个核心流程，采用双重存储策略实现知识图谱和向量检索的完美结合。
LightRAG索引架构流程图  
从索引流程图可以看到，LightRAG的索引过程包含以下关键步骤：

文档输入与分块

Input Documents → Text Chunks
使用嵌入模型进行文本分块处理


并行处理管道

实体提取路径：Extract Entities &amp; Relations → Entities Data → Deduped Entities → Update Description → Embedding
关系提取路径：Relations Data → Deduped Relations → Update Description → Embedding
向量嵌入路径：Text Chunks → Embedding


双重存储

知识图谱存储：Store in Knowledge Graph
向量数据库存储：Store in Vector DB Storage (Naive Vector DB Storage)



LightRAG检索与生成架构流程图  
检索流程图展示了LightRAG如何处理查询并生成回答：

查询输入

Query → 进入检索系统


双路径检索

左侧路径：Vector DB Storage → TopK Entities Results → Related text_units → Local Query Context
右侧路径：Knowledge Graph → TopK Relations Results → Related Entities → Related text_units → Global Query Context


上下文融合与关键词提取

Local Query Context + Global Query Context → keywords_extraction
生成 low_level_keywords 和 high_level_keywords
使用嵌入模型进行关键词处理


最终生成

combined context → System Template Prompt → System Prompt
使用LLM生成最终响应（Response）



这种双重检索架构确保了：

精确性：通过实体和关系检索获得准确信息
全面性：通过向量检索捕获语义相关内容
智能性：自动融合本地和全局上下文

核心模块1. 核心后端（/lightrag）包含LightRAG的核心逻辑，负责：

文档处理和分块
实体和关系提取
向量嵌入生成
知识图谱构建
查询处理和响应生成

2. API层（/lightrag-api）基于FastAPI构建的Web服务层，提供：

RESTful API接口
文档上传和管理
查询接口
系统配置和监控

3. Web UI（/lightrag_webui）基于React的前端界面，支持：

直观的文档管理界面
知识图谱可视化
交互式查询测试
系统状态监控

4. 工具与插件（/tools）提供额外功能扩展：

知识图谱可视化工具
数据导入导出工具
性能分析工具

索引与查询流程详解索引流程详解（Index Pipeline）根据LightRAG索引架构图，索引流程采用高效的并行处理设计：
核心索引流程# LightRAG索引流程实现def lightrag_indexing_pipeline(documents):    # 1. 文档预处理和分块    text_chunks = document_chunker.split(documents)        # 2. 三路并行处理    with concurrent.futures.ThreadPoolExecutor() as executor:        # 路径1：向量嵌入处理        vector_future = executor.submit(generate_text_embeddings, text_chunks)                # 路径2：实体提取和处理        entities_future = executor.submit(extract_and_process_entities, text_chunks)                # 路径3：关系提取和处理        relations_future = executor.submit(extract_and_process_relations, text_chunks)        # 3. 双重存储策略    # 向量存储：支持naive模式查询    store_to_vector_db_storage(vector_future.result())        # 知识图谱存储：支持实体关系查询    store_to_knowledge_graph(        entities_future.result(),         relations_future.result()    )def extract_and_process_entities(text_chunks):    &quot;&quot;&quot;实体提取和处理管道&quot;&quot;&quot;    # 使用LLM提取实体和关系    raw_entities = extract_entities_relations(text_chunks)        # 实体数据处理    entities_data = process_entities_data(raw_entities)        # 去重处理    deduped_entities = deduplicate_entities(entities_data)        # 更新描述信息    updated_entities = update_entity_descriptions(deduped_entities)        # 生成实体嵌入    entity_embeddings = generate_embeddings(updated_entities)        return entity_embeddingsdef extract_and_process_relations(text_chunks):    &quot;&quot;&quot;关系提取和处理管道&quot;&quot;&quot;    # 提取关系数据    relations_data = extract_relations_data(text_chunks)        # 去重处理    deduped_relations = deduplicate_relations(relations_data)        # 更新描述信息    updated_relations = update_relation_descriptions(deduped_relations)        # 生成关系嵌入    relation_embeddings = generate_embeddings(updated_relations)        return relation_embeddings

详细处理步骤阶段1：文档预处理

Input Documents → Text Chunks
智能文档分割，保持语义完整性
支持多种文档格式（PDF、Word、Markdown等）
可配置的分块大小和重叠策略

阶段2：三路并行提取

实体提取路径
Text Chunks → Extract Entities &amp; Relations → Entities Data (name, type, description, chunk_id)→ Deduped Entities → Update Description → Embedding

关系提取路径
Text Chunks → Extract Entities &amp; Relations→ Relations Data (source, target, description, strength, keywords, chunk_id)→ Deduped Relations → Update Description → Embedding

向量嵌入路径
Text Chunks → Embedding → Store in Vector DB Storage

阶段3：智能存储

向量数据库：存储文本块嵌入，支持语义相似性检索
知识图谱：存储实体关系网络，支持结构化查询
KV存储：缓存中间结果，提高查询效率

查询流程详解（Query Pipeline）根据LightRAG检索生成架构图，查询流程采用双路径检索和智能融合策略：
核心查询流程# LightRAG查询流程实现def lightrag_query_pipeline(query, mode=&quot;mix&quot;):    # 1. 查询预处理    processed_query = preprocess_query(query)        # 2. 双路径并行检索    with concurrent.futures.ThreadPoolExecutor() as executor:        # 左侧路径：向量检索 → 实体检索        local_future = executor.submit(local_retrieval_path, processed_query)                # 右侧路径：知识图谱检索 → 关系检索        global_future = executor.submit(global_retrieval_path, processed_query)        # 3. 上下文融合和关键词提取    local_context = local_future.result()    global_context = global_future.result()        # 关键词提取和分层处理    keywords_data = extract_keywords(local_context, global_context)        # 4. 上下文组合和生成    combined_context = combine_contexts(        local_context,         global_context,         keywords_data    )        # 5. 系统提示词生成和LLM调用    system_prompt = generate_system_prompt(combined_context, query)    response = llm_generate(system_prompt)        return responsedef local_retrieval_path(query):    &quot;&quot;&quot;本地检索路径：向量DB → TopK实体 → 相关文本单元&quot;&quot;&quot;    # 从向量数据库检索    vector_results = vector_db_search(query)        # 获取TopK实体结果    topk_entities = get_topk_entities(vector_results)        # 获取相关文本单元    related_text_units = get_related_text_units(topk_entities)        # 构建本地查询上下文    local_query_context = build_local_context(related_text_units)        return local_query_contextdef global_retrieval_path(query):    &quot;&quot;&quot;全局检索路径：知识图谱 → TopK关系 → 相关实体和文本&quot;&quot;&quot;    # 从知识图谱检索    graph_results = knowledge_graph_search(query)        # 获取TopK关系结果    topk_relations = get_topk_relations(graph_results)        # 获取相关实体    related_entities = get_related_entities(topk_relations)        # 获取相关文本单元    related_text_units = get_related_text_units(related_entities)        # 构建全局查询上下文    global_query_context = build_global_context(related_text_units)        return global_query_contextdef extract_keywords(local_context, global_context):    &quot;&quot;&quot;关键词提取和分层处理&quot;&quot;&quot;    # 使用嵌入模型进行关键词提取    combined_text = local_context + global_context        # 分层关键词提取    keywords_extraction_result = embedding_model.extract_keywords(combined_text)        return &#123;        &#x27;low_level_keywords&#x27;: keywords_extraction_result[&#x27;low_level&#x27;],        &#x27;high_level_keywords&#x27;: keywords_extraction_result[&#x27;high_level&#x27;]    &#125;

详细检索步骤阶段1：查询输入处理

Query → 查询预处理和意图分析

阶段2：双路径并行检索

本地检索路径（左侧）
Vector DB Storage → TopK Entities Results → Related text_units → Local Query Context

基于向量相似性检索最相关的实体
获取实体关联的文本单元
构建本地化的查询上下文


全局检索路径（右侧）
Knowledge Graph → TopK Relations Results → Related Entities → Related text_units → Global Query Context

基于图结构检索最相关的关系
获取关系涉及的实体和文本单元
构建全局化的查询上下文



阶段3：智能上下文融合

Local Query Context + Global Query Context → keywords_extraction
生成 low_level_keywords 和 high_level_keywords
使用嵌入模型进行语义理解和关键词提取

阶段4：生成与输出

combined context → System Template Prompt → System Prompt
使用LLM生成最终的智能回答（Response）

查询模式路由策略def route_query_mode(query, user_mode=None):    &quot;&quot;&quot;根据查询特征自动选择或验证查询模式&quot;&quot;&quot;    if user_mode:        return user_mode        # 自动模式选择逻辑    query_features = analyze_query_features(query)        if query_features[&#x27;entity_focused&#x27;]:        return &#x27;local&#x27;    elif query_features[&#x27;relationship_focused&#x27;]:        return &#x27;global&#x27;    elif query_features[&#x27;semantic_similarity&#x27;]:        return &#x27;naive&#x27;    elif query_features[&#x27;creative_task&#x27;]:        return &#x27;bypass&#x27;    elif query_features[&#x27;complex_reasoning&#x27;]:        return &#x27;mix&#x27;    else:        return &#x27;hybrid&#x27;  # 默认模式

查询模式深度解析LightRAG提供了六种不同的查询模式，每种模式针对不同的使用场景进行了优化。下表展示了各种查询模式的特征对比：
查询模式特征对比表


Query mode
entity
relationship
vector
Description



mix
✅
✅
✅
Default mode - 默认模式，综合使用所有检索方式


hybrid
✅
✅
❌
graph - 图谱模式，结合实体和关系检索


local
✅
✅
❌
Focus on entity - 专注于实体检索


global
✅
✅
❌
Focus on relationship - 专注于关系检索


naive
❌
❌
✅
Vector only - 纯向量检索


bypass
❌
❌
❌
LLM only - 直接使用大语言模型，无检索


1. Mix模式（融合模式）适用场景：最复杂的查询，需要全面的信息检索
工作原理：

综合使用实体、关系和向量检索
深度融合图结构和语义表示
提供最全面的信息覆盖

技术实现：
def mix_search(query):    # 实体检索    entity_results = entity_search(query)    # 关系检索    relation_results = relationship_search(query)    # 向量检索    vector_results = semantic_search(query)        # 深度融合所有结果    fused_results = deep_fusion(entity_results, relation_results, vector_results)    return fused_results

2. Hybrid模式（混合模式）适用场景：需要结构化知识和关系推理的查询
工作原理：

结合实体和关系检索
专注于图谱结构信息
适合复杂的知识推理

示例查询：
问题：苹果公司与特斯拉公司有什么关联？检索策略：找到&quot;苹果&quot;和&quot;特斯拉&quot;实体 → 查询两者间的关系路径 → 分析关联性

3. Local模式（本地模式）适用场景：需要精确信息的查询，如特定实体的属性查询
工作原理：

专注于检索特定实体及其直接关系
利用知识图谱的局部结构
提供高精度的事实性回答

示例查询：
问题：张三的工作单位是什么？检索策略：找到&quot;张三&quot;实体 → 查询&quot;工作于&quot;关系 → 返回关联实体

4. Global模式（全局模式）适用场景：需要综合理解的广泛主题查询
工作原理：

专注于关系检索和推理
处理更广泛的主题和概念
提供全面的背景信息

示例查询：
问题：人工智能在医疗领域的应用趋势如何？检索策略：收集AI、医疗相关的关系网络 → 分析关系模式 → 生成趋势报告

5. Naive模式（纯向量检索）适用场景：简单的语义相似性查询
工作原理：

仅使用向量检索
基于语义相似性匹配
适合快速检索和模糊查询

技术实现：
def naive_search(query):    # 将查询转换为向量    query_vector = embed_query(query)        # 在向量数据库中检索相似文档    similar_docs = vector_db.similarity_search(query_vector, k=10)        return similar_docs

示例查询：
问题：什么是机器学习？检索策略：查询向量 → 匹配相似文档 → 返回语义相关内容

6. Bypass模式（直接LLM）适用场景：不需要外部知识的通用性查询
工作原理：

完全跳过检索步骤
直接使用LLM的内置知识
适合常识性问题和创意性任务

技术实现：
def bypass_search(query):    # 直接使用LLM生成回答，不进行任何检索    response = llm.generate(query)    return response

示例查询：
问题：请写一首关于春天的诗检索策略：无检索 → 直接LLM创作 → 返回原创内容

查询模式选择策略根据不同的查询类型，系统可以智能选择最适合的查询模式：
def auto_select_mode(query):    query_type = analyze_query_type(query)        if query_type == &quot;factual_entity&quot;:        return &quot;local&quot;    elif query_type == &quot;relationship_analysis&quot;:        return &quot;global&quot;    elif query_type == &quot;complex_reasoning&quot;:        return &quot;mix&quot;    elif query_type == &quot;semantic_similarity&quot;:        return &quot;naive&quot;    elif query_type == &quot;creative_task&quot;:        return &quot;bypass&quot;    else:        return &quot;hybrid&quot;  # 默认选择

性能特征对比


模式
检索复杂度
响应速度
准确性
资源消耗
适用场景



mix
最高
较慢
最高
最高
复杂推理查询


hybrid
高
中等
高
高
知识推理查询


local
中等
快
高
中等
实体属性查询


global
中等
中等
中高
中等
关系分析查询


naive
低
最快
中等
低
语义检索查询


bypass
无
快
中等
最低
通用知识查询


组件选项与配置存储后端选择1. 向量数据库选项Faiss
# 轻量级本地向量存储vector_config = &#123;    &quot;type&quot;: &quot;faiss&quot;,    &quot;dimension&quot;: 1536,    &quot;index_type&quot;: &quot;IVF&quot;,    &quot;nlist&quot;: 100&#125;

Chroma
# 易于使用的向量数据库vector_config = &#123;    &quot;type&quot;: &quot;chroma&quot;,    &quot;persist_directory&quot;: &quot;./chroma_db&quot;,    &quot;collection_name&quot;: &quot;documents&quot;&#125;

Milvus
# 企业级向量数据库vector_config = &#123;    &quot;type&quot;: &quot;milvus&quot;,    &quot;host&quot;: &quot;localhost&quot;,    &quot;port&quot;: 19530,    &quot;collection_name&quot;: &quot;lightrag_vectors&quot;&#125;

2. 图数据库选项Neo4j
# 功能最全面的图数据库graph_config = &#123;    &quot;type&quot;: &quot;neo4j&quot;,    &quot;uri&quot;: &quot;bolt://localhost:7687&quot;,    &quot;username&quot;: &quot;neo4j&quot;,    &quot;password&quot;: &quot;password&quot;&#125;

NetworkX
# 轻量级图存储（适合开发测试）graph_config = &#123;    &quot;type&quot;: &quot;networkx&quot;,    &quot;persist_path&quot;: &quot;./graph_data.pkl&quot;&#125;

LLM模型选择OpenAI模型llm_config = &#123;    &quot;type&quot;: &quot;openai&quot;,    &quot;model&quot;: &quot;gpt-4-turbo&quot;,    &quot;api_key&quot;: &quot;your-api-key&quot;,    &quot;temperature&quot;: 0.1&#125;

本地模型（Ollama）llm_config = &#123;    &quot;type&quot;: &quot;ollama&quot;,    &quot;model&quot;: &quot;qwen2.5:7b&quot;,    &quot;base_url&quot;: &quot;http://localhost:11434&quot;&#125;

Hugging Face模型llm_config = &#123;    &quot;type&quot;: &quot;huggingface&quot;,    &quot;model&quot;: &quot;microsoft/DialoGPT-medium&quot;,    &quot;device&quot;: &quot;cuda:0&quot;&#125;

部署配置设置参考根据不同的硬件配置，以下是推荐的参数设置：



硬件配置
MAX_PARALLEL_INSERT
MAX_ASYNC
EMBEDDING_FUNC_MAX_ASYNC
CHUNK_SIZE



4core 8GB
2
6
12
600


8core 16GB
4
12
24
800


16core 32GB
8
20
40
1000


32core 64GB
12
32
64
1200


配置说明：

MAX_PARALLEL_INSERT: 并行插入的最大数量，影响数据导入速度
MAX_ASYNC: 异步操作的最大并发数，控制系统并发能力
EMBEDDING_FUNC_MAX_ASYNC: 向量化函数的最大异步数，影响向量生成效率
CHUNK_SIZE: 文本分块大小，影响检索精度和性能平衡

性能优化建议：

内存充足时可适当增加CHUNK_SIZE提高检索精度
CPU核心多时可增加并行插入数量加速数据导入
网络带宽充足时可增加异步并发数提升响应速度
建议根据实际业务场景进行性能测试和参数调优

使用场景与应用示例1. 企业知识管理场景描述：构建企业内部知识库，支持员工快速检索公司政策、技术文档、项目信息等。
实现方案：
# 企业知识库配置config = &#123;    &quot;storage&quot;: &#123;        &quot;vector_db&quot;: &quot;milvus&quot;,  # 企业级向量存储        &quot;graph_db&quot;: &quot;neo4j&quot;     # 复杂关系存储    &#125;,    &quot;llm&quot;: &#123;        &quot;type&quot;: &quot;openai&quot;,        &quot;model&quot;: &quot;gpt-4&quot;    &#125;,    &quot;query_modes&quot;: [&quot;hybrid&quot;, &quot;mix&quot;]  # 支持复杂查询&#125;# 使用示例rag = LightRAG(config)response = rag.query(&quot;公司的远程工作政策是什么？&quot;, mode=&quot;hybrid&quot;)

2. 学术研究助手场景描述：处理大量学术论文，帮助研究人员快速找到相关研究、理解技术脉络。
技术特点：

支持论文PDF解析
构建学术概念知识图谱
提供研究趋势分析

3. 客户服务智能问答场景描述：基于产品文档和FAQ构建智能客服系统。
优势特点：

多轮对话支持
上下文感知回答
实时知识更新

4. 法律文档分析场景描述：处理复杂的法律条文，提供法条查询和案例分析。
实现要点：

精确的实体识别（法条、案例、当事人）
复杂的法律关系建模
高精度的检索要求

部署与集成Docker部署LightRAG提供了完整的Docker部署方案：
# 基础镜像FROM python:3.10-slim# 安装依赖COPY requirements.txt .RUN pip install -r requirements.txt# 复制应用代码COPY . /appWORKDIR /app# 启动服务CMD [&quot;python&quot;, &quot;-m&quot;, &quot;lightrag_api.main&quot;]

部署命令：
# 构建镜像docker build -t lightrag:latest .# 启动服务docker run -d \  --name lightrag \  -p 8000:8000 \  -v ./data:/app/data \  lightrag:latest

生产环境配置# docker-compose.ymlversion: &#x27;3.8&#x27;services:  lightrag:    image: lightrag:latest    ports:      - &quot;8000:8000&quot;    environment:      - NEO4J_URI=bolt://neo4j:7687      - VECTOR_DB_TYPE=milvus    depends_on:      - neo4j      - milvus    neo4j:    image: neo4j:5.0    environment:      - NEO4J_AUTH=neo4j/password    ports:      - &quot;7474:7474&quot;      - &quot;7687:7687&quot;    milvus:    image: milvusdb/milvus:latest    ports:      - &quot;19530:19530&quot;

性能优化与最佳实践1. 并发处理优化LightRAG支持并发索引和查询处理：
# 并发索引配置indexing_config = &#123;    &quot;concurrent_workers&quot;: 4,    &quot;batch_size&quot;: 100,    &quot;chunk_overlap&quot;: 50&#125;# 查询缓存配置cache_config = &#123;    &quot;enable_query_cache&quot;: True,    &quot;cache_size&quot;: 1000,    &quot;cache_ttl&quot;: 3600  # 1小时&#125;

2. 重排序集成通过集成重排序模型提高检索精度：
# 重排序配置rerank_config = &#123;    &quot;enable_rerank&quot;: True,    &quot;rerank_model&quot;: &quot;BAAI/bge-reranker-large&quot;,    &quot;top_k&quot;: 10,    &quot;rerank_top_k&quot;: 3&#125;

3. 性能监控# 性能监控指标metrics = &#123;    &quot;indexing_speed&quot;: &quot;documents/second&quot;,    &quot;query_latency&quot;: &quot;milliseconds&quot;,    &quot;memory_usage&quot;: &quot;MB&quot;,    &quot;cache_hit_rate&quot;: &quot;percentage&quot;&#125;

常见问题与解决方案1. 内存使用优化问题：大规模文档处理时内存占用过高
解决方案：
# 启用流式处理config = &#123;    &quot;streaming_mode&quot;: True,    &quot;batch_processing&quot;: True,    &quot;max_memory_usage&quot;: &quot;4GB&quot;&#125;

2. 查询性能优化问题：复杂查询响应时间过长
解决方案：

启用查询缓存
优化索引结构
使用更快的嵌入模型

3. 多语言支持问题：处理中文等非英语文档
解决方案：
# 多语言配置config = &#123;    &quot;language&quot;: &quot;zh-CN&quot;,    &quot;embedding_model&quot;: &quot;BAAI/bge-large-zh-v1.5&quot;,    &quot;text_splitter&quot;: &quot;chinese_text_splitter&quot;&#125;

LightRAG与其他RAG系统对比在RAG技术生态中，除了LightRAG，还有多个优秀的解决方案。下面我们将LightRAG与两个主要竞品进行详细对比。
与GraphRAG对比GraphRAG 是微软推出的基于知识图谱的RAG系统，专注于图结构化知识表示。
架构对比


对比维度
LightRAG
GraphRAG



核心理念
图谱+向量双重检索
纯图谱检索


存储架构
向量DB + 图DB并行
主要依赖图数据库


查询模式
4种模式灵活切换
基于图遍历


部署复杂度
轻量级，易部署
相对复杂


技术特点对比LightRAG优势：
# LightRAG的混合检索def hybrid_search(query):    # 同时利用向量相似性和图结构    vector_results = semantic_search(query)    graph_results = graph_traversal(query)    return fuse_results(vector_results, graph_results)

GraphRAG优势：
# GraphRAG的深度图推理def graph_reasoning(query):    # 多跳图推理，发现复杂关系    entities = extract_entities(query)    paths = multi_hop_traversal(entities, max_hops=3)    return synthesize_from_paths(paths)

适用场景对比LightRAG更适合：

需要快速语义检索的场景
混合查询需求（精确+模糊）
资源受限的环境
快速原型开发

GraphRAG更适合：

复杂关系推理需求
多跳查询场景
结构化知识密集的领域
深度分析应用

与RAG Everything对比RAG Everything 是一个全功能的RAG平台，强调”everything”的理念，支持多种数据源和检索方式。
功能覆盖对比


功能模块
LightRAG
RAG Everything



数据源支持
文档为主
全数据源（DB、API、文件等）


检索方式
向量+图谱
多种检索器组合


系统复杂度
专注核心功能
功能全面但复杂


学习成本
较低
较高


定制能力
高度可配置
极高的灵活性


技术架构对比LightRAG架构：
# 精简但高效的架构LightRAG:  - Core Engine (轻量级)  - Dual Storage (向量+图谱)  - Multi Query Modes  - Simple API Layer

RAG Everything架构：
# 全功能平台架构RAG_Everything:  - Multiple Data Connectors  - Various Retrieval Engines  - Complex Orchestration Layer  - Extensive Plugin System  - Advanced Analytics

性能对比响应速度：

LightRAG：优化的双重检索，平均响应&lt;2秒
RAG Everything：功能全面但响应时间较长，3-5秒
GraphRAG：图遍历计算复杂，响应时间2-4秒

资源消耗：
# 典型资源使用对比resource_usage = &#123;    &quot;LightRAG&quot;: &#123;        &quot;memory&quot;: &quot;2-4GB&quot;,        &quot;cpu&quot;: &quot;2-4 cores&quot;,        &quot;storage&quot;: &quot;适中&quot;    &#125;,    &quot;RAG_Everything&quot;: &#123;        &quot;memory&quot;: &quot;4-8GB&quot;,         &quot;cpu&quot;: &quot;4-8 cores&quot;,        &quot;storage&quot;: &quot;较大&quot;    &#125;,    &quot;GraphRAG&quot;: &#123;        &quot;memory&quot;: &quot;3-6GB&quot;,        &quot;cpu&quot;: &quot;2-6 cores&quot;,         &quot;storage&quot;: &quot;适中到大&quot;    &#125;&#125;

开发体验对比LightRAG开发体验：
# 简单直接的使用方式from lightrag import LightRAGrag = LightRAG(config)rag.insert_documents(docs)response = rag.query(&quot;问题&quot;, mode=&quot;hybrid&quot;)

RAG Everything开发体验：
# 功能丰富但配置复杂from rag_everything import RAGPlatformplatform = RAGPlatform()platform.add_data_source(&quot;database&quot;, db_config)platform.add_data_source(&quot;files&quot;, file_config)platform.configure_retrievers(retriever_configs)platform.setup_pipeline(pipeline_config)response = platform.query(&quot;问题&quot;)

GraphRAG开发体验：
# 专注于图谱的使用方式from graphrag import GraphRAGgraph_rag = GraphRAG(graph_config)graph_rag.build_knowledge_graph(documents)response = graph_rag.query(&quot;问题&quot;, reasoning_depth=2)

三者详细对比矩阵


对比维度
LightRAG
GraphRAG
RAG Everything



技术门槛
⭐⭐ 中等
⭐⭐⭐ 较高
⭐⭐⭐⭐ 高


部署难度
⭐⭐ 简单
⭐⭐⭐ 中等
⭐⭐⭐⭐ 复杂


查询精度
⭐⭐⭐⭐ 高
⭐⭐⭐⭐⭐ 很高
⭐⭐⭐ 中高


查询速度
⭐⭐⭐⭐ 快
⭐⭐⭐ 中等
⭐⭐ 较慢


扩展性
⭐⭐⭐ 良好
⭐⭐⭐ 良好
⭐⭐⭐⭐⭐ 优秀


资源消耗
⭐⭐⭐⭐ 低
⭐⭐⭐ 中等
⭐⭐ 较高


社区生态
⭐⭐⭐ 发展中
⭐⭐⭐⭐ 活跃
⭐⭐⭐ 中等


选择建议选择LightRAG的情况
✅ 需要快速搭建RAG系统
✅ 兼顾语义检索和关系查询
✅ 资源受限的环境
✅ 注重系统稳定性和可维护性
✅ 中小型团队或项目

选择GraphRAG的情况
✅ 复杂知识推理需求
✅ 深度关系分析场景
✅ 结构化数据为主
✅ 对查询精度要求极高
✅ 有足够的图数据库运维能力

选择RAG Everything的情况
✅ 需要处理多种异构数据源
✅ 复杂的企业级集成需求
✅ 高度定制化要求
✅ 大型团队和充足资源
✅ 需要全方位的RAG能力

技术演进趋势graph LR    A[传统RAG] --&gt; B[LightRAG&lt;br/&gt;轻量级+双重检索]    A --&gt; C[GraphRAG&lt;br/&gt;深度图推理]     A --&gt; D[RAG Everything&lt;br/&gt;全功能平台]        B --&gt; E[未来融合&lt;br/&gt;最佳实践结合]    C --&gt; E    D --&gt; E

未来发展趋势：

技术融合：各系统优势互补，形成更完善的解决方案
标准化：RAG接口和协议标准化
智能化：自动选择最优检索策略
边缘化：支持边缘计算和离线部署

总结LightRAG作为一个现代化的检索增强生成系统，通过创新性地结合知识图谱和向量检索技术，为企业级AI应用提供了强大的知识管理能力。其轻量级的设计、灵活的架构和丰富的功能特性，使其成为构建智能问答系统的理想选择。
核心优势
技术先进性：融合多种检索模式，提供精准的知识获取
架构灵活性：模块化设计，支持多种存储和模型选择
生产就绪：完整的部署方案和监控能力
易于集成：丰富的API接口和配置选项

适用场景
企业知识管理平台
智能客服系统
学术研究工具
法律文档分析
技术文档问答

随着RAG技术的不断发展，LightRAG将继续演进，为更多场景提供高效的知识检索和生成能力。
相关资源
LightRAG GitHub仓库
算法详解文档
并发处理说明
重排序集成指南
Docker部署文档


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>LLM</tag>
        <tag>RAG</tag>
        <tag>知识图谱</tag>
        <tag>向量检索</tag>
      </tags>
  </entry>
  <entry>
    <title>从零构建RAG文档问答系统：技术栈与实现方案详解</title>
    <url>/posts/05696bb73c9b.html</url>
    <content><![CDATA[从零构建RAG文档问答系统：技术栈与实现方案详解引言在人工智能快速发展的今天，如何让AI模型基于特定文档内容进行准确回答，成为了一个重要的技术挑战。传统的问答系统往往存在”幻觉”问题，即模型会生成看似合理但实际不准确的信息。为了解决这个问题，我们构建了一个基于RAG（Retrieval-Augmented Generation）技术的文档问答系统。
本文将详细介绍这个项目的技术栈选择、架构设计、实现方案以及开发过程中的关键决策。
项目概述项目源代码: https://github.com/xhuaustc/rag-qa-system
  
我们的RAG文档问答系统具有以下核心特性：

🔍 多格式文档支持: PDF、DOCX、Markdown、TXT等
🤖 多LLM后端: Ollama、OpenAI、Azure OpenAI
📝 智能文档分块: 支持中英文混合文本的智能分块
🔗 向量检索: 基于ChromaDB的高效向量检索
💬 智能问答: 基于文档内容的智能问答
⚙️ 灵活配置: 支持环境变量和代码配置
🛠️ 模块化设计: 清晰的模块分离和扩展性

技术栈选择核心框架LangChain: 作为我们的核心框架，LangChain提供了丰富的LLM集成、文档处理和向量存储功能。它支持多种LLM提供商，并且有良好的扩展性。
# 核心依赖langchain&gt;=0.1.0langchain-community&gt;=0.0.10langchain-openai&gt;=0.0.5langchain-ollama&gt;=0.1.0

ChromaDB: 选择ChromaDB作为向量数据库，主要考虑其以下优势：

轻量级，易于部署
支持本地存储，无需额外服务
良好的Python集成
支持元数据过滤

LLM提供商我们支持三种主要的LLM提供商：

Ollama: 本地部署，隐私保护，成本可控
OpenAI: 云端服务，性能稳定，功能丰富
Azure OpenAI: 企业级服务，合规性好

文档处理# 文档处理依赖markdown&gt;=3.5.0pypdf&gt;=3.17.0python-docx&gt;=1.1.0docx2txt&gt;=0.8unstructured&gt;=0.11.0

系统架构设计整体架构我们的系统采用模块化设计，主要包含以下几个核心模块：
ollama/├── config/                 # 配置管理├── doc_proc/              # 文档处理├── embed/                  # 嵌入处理├── llm/                    # LLM客户端├── utils/                  # 工具模块└── main.py                 # 主程序

核心模块详解1. 配置管理模块 (config/)使用Python的dataclasses模块构建了层次化的配置系统：
@dataclassclass LLMConfig:    &quot;&quot;&quot;LLM客户端配置&quot;&quot;&quot;    provider: str = field(default_factory=lambda: os.getenv(&quot;LLM_PROVIDER&quot;, &quot;ollama&quot;))    model: str = field(default_factory=lambda: os.getenv(&quot;LLM_MODEL&quot;, &quot;qwen3:8b&quot;))    embedding_model: str = field(default_factory=lambda: os.getenv(&quot;LLM_EMBEDDING_MODEL&quot;, &quot;shaw/dmeta-embedding-zh&quot;))    api_key: Optional[str] = field(default_factory=lambda: os.getenv(&quot;LLM_API_KEY&quot;))    base_url: str = field(default_factory=lambda: os.getenv(&quot;LLM_BASE_URL&quot;, &quot;http://localhost:11434&quot;))@dataclassclass AppSettings:    &quot;&quot;&quot;应用程序设置&quot;&quot;&quot;    llm: LLMConfig = field(default_factory=LLMConfig)    storage: StorageConfig = field(default_factory=StorageConfig)    document_processing: DocumentProcessingConfig = field(default_factory=DocumentProcessingConfig)

这种设计的优势：

类型安全: 使用dataclass提供类型检查
环境变量集成: 自动从环境变量读取配置
默认值: 提供合理的默认配置
验证: 支持配置验证逻辑

2. LLM客户端模块 (llm/)采用工厂模式和策略模式设计LLM客户端：
class BaseLLMClient(ABC):    &quot;&quot;&quot;LLM客户端基类&quot;&quot;&quot;        @abstractmethod    def generate_text(self, prompt: str, **kwargs) -&gt; Dict[str, Any]:        &quot;&quot;&quot;生成文本&quot;&quot;&quot;        pass        @abstractmethod    def generate_embeddings(self, texts: List[str]) -&gt; List[List[float]]:        &quot;&quot;&quot;生成嵌入向量&quot;&quot;&quot;        passclass OllamaLLMClient(BaseLLMClient):    &quot;&quot;&quot;Ollama LLM客户端&quot;&quot;&quot;        def __init__(self, config: LLMConfig):        self.config = config        self._initialize_components()        def _initialize_components(self) -&gt; None:        &quot;&quot;&quot;初始化Ollama组件&quot;&quot;&quot;        self.llm = OllamaLLM(model=self.config.model, base_url=self.config.base_url)        self.embeddings = OllamaEmbeddings(model=self.config.embedding_model, base_url=self.config.base_url)class LLMClient:    &quot;&quot;&quot;LLM客户端工厂&quot;&quot;&quot;        def __init__(self, provider: str = &quot;ollama&quot;, **kwargs):        self.provider = provider        self.config = LLMConfig(provider=provider, **kwargs)        self._client = self._create_client()        def _create_client(self) -&gt; BaseLLMClient:        &quot;&quot;&quot;创建具体的LLM客户端&quot;&quot;&quot;        if self.provider == &quot;ollama&quot;:            return OllamaLLMClient(self.config)        elif self.provider == &quot;openai&quot;:            return OpenAILLMClient(self.config)        else:            raise ValueError(f&quot;不支持的LLM提供商: &#123;self.provider&#125;&quot;)

3. 文档处理模块 (doc_proc/)文档处理模块负责文档的加载、分块和预处理：
class MultilingualChunker:    &quot;&quot;&quot;多语言文档分块器&quot;&quot;&quot;        def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):        self.chunk_size = chunk_size        self.chunk_overlap = chunk_overlap        self.text_splitter = RecursiveCharacterTextSplitter(            chunk_size=chunk_size,            chunk_overlap=chunk_overlap,            separators=[&quot;\n\n&quot;, &quot;\n&quot;, &quot;。&quot;, &quot;！&quot;, &quot;？&quot;, &quot;.&quot;, &quot;!&quot;, &quot;?&quot;, &quot; &quot;, &quot;&quot;]        )        def clean_base64_data(self, text: str) -&gt; str:        &quot;&quot;&quot;清理文本中的Base64数据&quot;&quot;&quot;        # 移除Base64编码的图片数据        base64_patterns = [            r&#x27;data:image/[^;]+;base64,[A-Za-z0-9+/=]+&#x27;,            r&#x27;[A-Za-z0-9+/]&#123;50,&#125;=&#123;0,2&#125;&#x27;  # 长Base64字符串        ]                cleaned_text = text        for pattern in base64_patterns:            cleaned_text = re.sub(pattern, &#x27;[图片数据]&#x27;, cleaned_text, flags=re.IGNORECASE)                return cleaned_text.strip()        def split_documents(self, documents: List[Document]) -&gt; List[Document]:        &quot;&quot;&quot;分割文档&quot;&quot;&quot;        processed_docs = []                for doc in documents:            # 清理Base64数据            cleaned_content = self.clean_base64_data(doc.page_content)            doc.page_content = cleaned_content                        # 分割文档            chunks = self.text_splitter.split_text(cleaned_content)                        for i, chunk in enumerate(chunks):                chunk_doc = Document(                    page_content=chunk,                    metadata=&#123;                        **doc.metadata,                        &quot;chunk_id&quot;: i,                        &quot;total_chunks&quot;: len(chunks)                    &#125;                )                processed_docs.append(chunk_doc)                return processed_docs

4. 嵌入处理模块 (embed/)嵌入处理模块负责文档的向量化和存储：
class EmbeddingProcessor:    &quot;&quot;&quot;嵌入处理器&quot;&quot;&quot;        def __init__(self, collection_name: str, llm_provider: str = &quot;ollama&quot;, **kwargs):        self.collection_name = collection_name        self.llm_client = LLMClient(provider=llm_provider, **kwargs)        self.chroma_client = chromadb.PersistentClient(path=settings.storage.chroma_db_path)        self._setup_collection()        def _setup_collection(self) -&gt; None:        &quot;&quot;&quot;设置ChromaDB集合&quot;&quot;&quot;        try:            # 尝试获取现有集合            try:                self.collection = self.chroma_client.get_collection(name=self.collection_name)                logger.info(f&quot;已获取现有集合: &#123;self.collection_name&#125;&quot;)            except:                # 如果集合不存在，创建新集合                self.collection = self.chroma_client.create_collection(name=self.collection_name)                logger.info(f&quot;已创建新集合: &#123;self.collection_name&#125;&quot;)        except Exception as e:            logger.error(f&quot;集合设置失败: &#123;e&#125;&quot;)            raise        def process_documents(self, documents: List[Document]) -&gt; None:        &quot;&quot;&quot;处理文档并存储到向量数据库&quot;&quot;&quot;        # 生成嵌入向量        texts = [doc.page_content for doc in documents]        embeddings = self.llm_client.generate_embeddings(texts)                # 准备元数据        metadatas = []        ids = []                for i, doc in enumerate(documents):            metadata = &#123;                **doc.metadata,                &quot;embedding_model&quot;: self.llm_client.config.embedding_model,                &quot;processed_at&quot;: datetime.now().isoformat()            &#125;            metadatas.append(metadata)            ids.append(f&quot;doc_&#123;i&#125;_&#123;hash(doc.page_content) % 1000000&#125;&quot;)                # 存储到ChromaDB        self.collection.add(            embeddings=embeddings,            documents=texts,            metadatas=metadatas,            ids=ids        )                logger.info(f&quot;成功处理并存储 &#123;len(documents)&#125; 个文档块&quot;)        def query_similar(self, query: str, n_results: int = 5) -&gt; List[Dict]:        &quot;&quot;&quot;查询相似文档&quot;&quot;&quot;        # 生成查询嵌入        query_embedding = self.llm_client.generate_embeddings([query])[0]                # 向量检索        results = self.collection.query(            query_embeddings=[query_embedding],            n_results=n_results        )                # 格式化结果        formatted_results = []        for i in range(len(results[&#x27;documents&#x27;][0])):            formatted_results.append(&#123;                &#x27;content&#x27;: results[&#x27;documents&#x27;][0][i],                &#x27;metadata&#x27;: results[&#x27;metadatas&#x27;][0][i],                &#x27;distance&#x27;: results[&#x27;distances&#x27;][0][i] if &#x27;distances&#x27; in results else None            &#125;)                return formatted_results

关键技术实现1. 智能文档分块策略文档分块是RAG系统的关键环节。我们采用了以下策略：

递归字符分割: 使用RecursiveCharacterTextSplitter，支持多种分隔符
多语言支持: 针对中英文混合文本优化分隔符
元数据保持: 在分块过程中保持原始文档的元数据
Base64清理: 自动清理文档中的Base64编码图片数据

2. 向量检索优化
相似度计算: 使用余弦相似度进行向量匹配
元数据过滤: 支持基于文档类型、来源等元数据进行过滤
结果排序: 按相似度降序排列检索结果

3. 错误处理机制我们实现了分层的错误处理系统：
class BaseError(Exception):    &quot;&quot;&quot;基础异常类&quot;&quot;&quot;    passclass LLMClientError(BaseError):    &quot;&quot;&quot;LLM客户端错误&quot;&quot;&quot;    passclass DocumentProcessingError(BaseError):    &quot;&quot;&quot;文档处理错误&quot;&quot;&quot;    passclass EmbeddingError(BaseError):    &quot;&quot;&quot;嵌入处理错误&quot;&quot;&quot;    pass

4. 配置管理支持多种配置方式：

环境变量配置
代码配置
配置文件

开发过程中的关键决策1. 模块化设计决策: 将系统拆分为多个独立模块理由: 

提高代码可维护性
便于单元测试
支持功能扩展
降低模块间耦合

2. 抽象基类设计决策: 为LLM客户端和文档加载器设计抽象基类理由:

统一接口规范
便于添加新的提供商
提高代码复用性

3. 配置验证决策: 在配置类中添加验证逻辑理由:

及早发现配置错误
提供清晰的错误信息
避免运行时错误

4. 日志系统决策: 实现统一的日志系统理由:

便于调试和监控
记录关键操作
支持不同日志级别

性能优化1. 批量处理文档处理和嵌入生成采用批量处理方式，减少API调用次数：
def process_documents(self, documents: List[Document]) -&gt; None:    # 批量生成嵌入向量    texts = [doc.page_content for doc in documents]    embeddings = self.llm_client.generate_embeddings(texts)

2. 向量数据库优化
使用持久化存储，避免重复计算
支持增量更新
元数据索引优化

3. 内存管理
流式处理大文档
及时释放不需要的对象
控制批处理大小

部署和使用环境要求# Python 3.8+# 内存: 至少4GB# 存储: 根据文档数量确定

快速开始# 1. 安装依赖pip install -r requirements.txt# 2. 配置环境变量export LLM_PROVIDER=ollamaexport LLM_MODEL=qwen3:8bexport LLM_BASE_URL=http://localhost:11434# 3. 运行系统python main.py

使用示例from embed.embedding import EmbeddingProcessor# 初始化处理器processor = EmbeddingProcessor(    collection_name=&quot;my_docs&quot;,    llm_provider=&quot;ollama&quot;,    llm_model=&quot;qwen3:8b&quot;)# 处理文档processor.process_directory(&quot;docs/&quot;)# 问答query = &quot;如何配置系统？&quot;results = processor.query_similar(query, n_results=3)

未来扩展方向1. 功能扩展
多模态支持: 支持图片、音频等多媒体文档
实时更新: 支持文档的实时更新和增量处理
用户界面: 开发Web界面，提升用户体验
API服务: 提供RESTful API接口

2. 性能优化
分布式处理: 支持大规模文档的分布式处理
缓存机制: 实现智能缓存，提高查询速度
索引优化: 优化向量索引结构

3. 智能化提升
自动分块优化: 基于内容语义的智能分块
查询理解: 改进查询理解和重写
答案生成: 优化答案生成的质量和准确性

总结通过这个RAG文档问答系统的开发，我们深入理解了RAG技术的核心原理和实现细节。项目的成功关键在于：

合理的架构设计: 模块化、可扩展的架构
技术栈选择: 成熟稳定的技术栈
错误处理: 完善的错误处理和日志系统
性能优化: 针对性的性能优化措施
用户体验: 简单易用的接口设计

这个项目不仅解决了实际的文档问答需求，也为后续的AI应用开发提供了良好的基础架构。通过持续优化和扩展，我们可以构建更加智能和强大的AI应用系统。
参考资料
LangChain官方文档
ChromaDB文档
Ollama文档
RAG技术论文



本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>LangChain</tag>
        <tag>RAG</tag>
        <tag>ChromaDB</tag>
        <tag>Ollama</tag>
        <tag>文档问答</tag>
      </tags>
  </entry>
  <entry>
    <title>LangChain框架入门与实践：组件详解、使用场景与示例</title>
    <url>/posts/475a94dfb51a.html</url>
    <content><![CDATA[背景与目标读者LangChain 是一个面向大型语言模型（Large Language Models, LLM）应用开发的开源框架，由 Harrison Chase 于 2022 年发布，并在 2023 年成立公司后快速发展。它通过统一的抽象与模块化组件，帮助开发者高效构建复杂的 AI 应用，如聊天机器人、文档问答（RAG）、智能代理（Agent）与自动摘要等。
本文面向有一定 Python 基础、希望系统了解并快速上手 LangChain 的工程师与技术爱好者，覆盖核心组件、常见应用场景与可运行示例代码。

LangChain 是什么，为什么需要它？
统一接口：屏蔽不同模型与服务的差异（如 OpenAI、Hugging Face、本地模型等），提供一致的调用方式。
组件化设计：围绕模型、提示（Prompt）、链（Chain）、代理（Agent）、记忆（Memory）、索引（Indexes&#x2F;Retriever）等模块化组合，便于扩展与维护。
工程化能力：提供可观测（Callbacks）、持久化（Checkpointers&#x2F;Message History）、工具接入（Tools）、生态集成（VectorStore&#x2F;Embeddings&#x2F;Loaders）等工程能力。


核心组件与用法速览1) 模型（Models）对接各类聊天&#x2F;补全模型，常见通过 langchain-openai、langchain-community、langchain-anthropic 等包集成。
# Pythonfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0.2)resp = model.invoke(&quot;用一句话解释什么是RAG？&quot;)print(resp.content)

2) 提示（Prompts）将系统指令、示例与用户输入组织为模板，支持变量插值与角色化消息。
from langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages([    (&quot;system&quot;, &quot;你是一名资深AI助教，输出要简洁准确。&quot;),    (&quot;human&quot;, &quot;请用要点列举回答：&#123;question&#125;&quot;)])

3) 链（Chains &#x2F; LCEL）使用 LangChain Expression Language（LCEL）以“管道”方式组合组件。
from langchain_core.output_parsers import StrOutputParserchain = prompt | model | StrOutputParser()print(chain.invoke(&#123;&quot;question&quot;: &quot;LangChain 的核心组件有哪些？&quot;&#125;))

4) 代理（Agents）让 LLM 作为决策引擎，基于工具（Tools）动态选择行动，如搜索、计算或代码执行。
from langchain.tools import toolfrom langchain.agents import create_react_agent, AgentExecutor@tooldef add(a: float, b: float) -&gt; float:    &quot;&quot;&quot;Return a + b&quot;&quot;&quot;    return a + btools = [add]agent = create_react_agent(model, tools)executor = AgentExecutor(agent=agent, tools=tools, verbose=True)print(executor.invoke(&#123;&quot;input&quot;: &quot;请计算 12.5 与 7.5 的和，然后用一句中文描述结果。&quot;&#125;)[&quot;output&quot;])

5) 记忆（Memory）存储会话历史，使多轮对话更连贯。推荐以 RunnableWithMessageHistory 使用。
from langchain_core.chat_history import InMemoryChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistoryhistory_store = &#123;&#125;def get_history(session_id: str):    return history_store.setdefault(session_id, InMemoryChatMessageHistory())conversational_chain = RunnableWithMessageHistory(    chain,    get_history,    input_messages_key=&quot;question&quot;,    history_messages_key=&quot;history&quot;,)print(conversational_chain.invoke(    &#123;&quot;question&quot;: &quot;记住我喜欢Python。&quot;&#125;,    config=&#123;&quot;configurable&quot;: &#123;&quot;session_id&quot;: &quot;u1&quot;&#125;&#125;))print(conversational_chain.invoke(    &#123;&quot;question&quot;: &quot;我刚才说我喜欢哪种语言？&quot;&#125;,    config=&#123;&quot;configurable&quot;: &#123;&quot;session_id&quot;: &quot;u1&quot;&#125;&#125;))

6) 索引 &#x2F; 检索（Indexes &#x2F; VectorStores &#x2F; Retrievers）将文档切分、向量化并索引，供模型检索以提升事实性（RAG）。
from langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterdocs = [    &quot;LangChain 提供统一的模型接口与组件化能力。&quot;,    &quot;RAG 将检索与生成结合，提升回答的准确性。&quot;]splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)splits = splitter.create_documents(docs)embeddings = OpenAIEmbeddings()vectordb = FAISS.from_documents(splits, embeddings)retriever = vectordb.as_retriever(k=2)context_docs = retriever.invoke(&quot;什么是RAG？&quot;)print([d.page_content for d in context_docs])


快速开始：环境与第一个程序安装与准备pip install -U &quot;langchain&gt;=0.2&quot; langchain-openai langchain-community langchain-text-splitters faiss-cpu python-dotenv

配置密钥（以 OpenAI 为例），建议使用环境变量或 .env：
setx OPENAI_API_KEY &quot;sk-xxxxxxxx&quot;   # Windows PowerShell 可使用 $env:OPENAI_API_KEY=&quot;...&quot;

Hello, LangChain（LCEL 版）import osfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParseros.environ[&quot;OPENAI_API_KEY&quot;] = os.getenv(&quot;OPENAI_API_KEY&quot;, &quot;&lt;YOUR_KEY&gt;&quot;)model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0.2)prompt = ChatPromptTemplate.from_messages([    (&quot;system&quot;, &quot;你是专业的技术助理，回答务必准确且简洁。&quot;),    (&quot;human&quot;, &quot;&#123;question&#125;&quot;)])chain = prompt | model | StrOutputParser()print(chain.invoke(&#123;&quot;question&quot;: &quot;一句话介绍LangChain。&quot;&#125;))


常见使用场景与示例场景一：知识库问答（RAG）将企业文档做切分与向量化，检索相关片段并与提示拼接后交给模型生成答案。
import osfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_community.vectorstores import FAISSfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserdocs = [    &quot;LangChain 是用于构建由大型语言模型驱动应用的框架。&quot;,    &quot;它包含模型、提示、链、代理、记忆与索引等组件。&quot;,]splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=20)splits = splitter.create_documents(docs)emb = OpenAIEmbeddings()vectordb = FAISS.from_documents(splits, emb)retriever = vectordb.as_retriever(k=2)prompt = ChatPromptTemplate.from_messages([    (&quot;system&quot;, &quot;请基于给定上下文回答用户问题，若无法从上下文得到答案，请明确说明。上下文：\n&#123;context&#125;&quot;),    (&quot;human&quot;, &quot;问题：&#123;question&#125;&quot;)])model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0)parser = StrOutputParser()def rag_answer(question: str) -&gt; str:    context_docs = retriever.invoke(question)    context = &quot;\n\n&quot;.join(d.page_content for d in context_docs)    chain = prompt | model | parser    return chain.invoke(&#123;&quot;context&quot;: context, &quot;question&quot;: question&#125;)print(rag_answer(&quot;LangChain 的核心组件是什么？&quot;))

场景二：多轮聊天机器人（会话记忆）使用 RunnableWithMessageHistory 追踪对话上下文，使回答更连贯。
from langchain_core.chat_history import InMemoryChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistoryfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (&quot;system&quot;, &quot;你是友好的中文助理，记住用户偏好。&quot;),    (&quot;placeholder&quot;, &quot;&#123;history&#125;&quot;),    (&quot;human&quot;, &quot;&#123;question&#125;&quot;)])chain = prompt | ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0.3) | StrOutputParser()store = &#123;&#125;def get_history(sid):    return store.setdefault(sid, InMemoryChatMessageHistory())conv = RunnableWithMessageHistory(chain, get_history, input_messages_key=&quot;question&quot;, history_messages_key=&quot;history&quot;)sid = &quot;user-42&quot;print(conv.invoke(&#123;&quot;question&quot;: &quot;我喜欢Go语言，请记住。&quot;&#125;, config=&#123;&quot;configurable&quot;: &#123;&quot;session_id&quot;: sid&#125;&#125;))print(conv.invoke(&#123;&quot;question&quot;: &quot;我喜欢哪种语言？&quot;&#125;, config=&#123;&quot;configurable&quot;: &#123;&quot;session_id&quot;: sid&#125;&#125;))

场景三：长文档摘要将长文切分后分别摘要，再做聚合（Map-Reduce 思路）。
from langchain_text_splitters import RecursiveCharacterTextSplitterfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParsertext = &quot;&quot;&quot;LangChain 通过统一抽象与模块化组件，帮助开发者构建LLM应用……（长文略）&quot;&quot;&quot;splits = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50).split_text(text)model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0)parser = StrOutputParser()map_prompt = ChatPromptTemplate.from_template(&quot;请用中文简洁总结：&#123;chunk&#125;&quot;)map_chain = map_prompt | model | parsersummaries = [map_chain.invoke(&#123;&quot;chunk&quot;: c&#125;) for c in splits]reduce_prompt = ChatPromptTemplate.from_template(&quot;将以下摘要合并为一段清晰总结：\n&#123;points&#125;&quot;)reduce_chain = reduce_prompt | model | parserfinal_summary = reduce_chain.invoke(&#123;&quot;points&quot;: &quot;\n&quot;.join(summaries)&#125;)print(final_summary)

场景四：工具调用&#x2F;数据分析代理通过 Agent 使用工具（如计算、搜索、数据库查询）来完成复杂任务。
from langchain.tools import toolfrom langchain.agents import create_react_agent, AgentExecutorfrom langchain_openai import ChatOpenAI@tooldef multiply(a: float, b: float) -&gt; float:    &quot;&quot;&quot;Multiply two numbers&quot;&quot;&quot;    return a * btools = [multiply]agent = create_react_agent(ChatOpenAI(model=&quot;gpt-4o-mini&quot;), tools)executor = AgentExecutor(agent=agent, tools=tools)print(executor.invoke(&#123;&quot;input&quot;: &quot;请计算 3.2 乘以 8，并给出中文说明。&quot;&#125;)[&quot;output&quot;])


进阶与工程化建议
版本与依赖：
推荐使用 langchain&gt;=0.2 与分包（langchain-openai、langchain-community 等）；不同版本 API 可能有变更。
向量库可选 Lancedb、FAISS、Chroma、Milvus、PGVector 等；生产建议持久化与备份策略。


可观测与调试：
使用 Callbacks 记录 Token 成本与时延；保留提示与上下文以便复现。


安全与隐私：
使用环境变量存储 API Key，不要硬编码；
对数据做脱敏，遵守隐私合规；
明确第三方服务的数据保留政策。


部署建议：
后端服务化（FastAPI 等），前后端分离；
缓存热点向量检索与响应；
结合队列与异步提高吞吐。




常见问题（FAQ）
输出偶尔不稳定？
降低 temperature，并在提示中明确约束输出格式；
为 RAG 增强检索质量（更好的切分、合适的 k 值与重排序）。


检索不相关？
优化切分策略与嵌入模型；尝试添加领域示例与更明确的系统指令。


Agent 行为“走偏”？
限制可用工具、添加防护指令与超时；在日志中审计推理轨迹。




参考资料
LangChain 官方文档（入门&#x2F;指南&#x2F;组件）：https://python.langchain.com/docs/
OpenAI 接入（langchain-openai）：https://python.langchain.com/docs/integrations/chat/openai
向量数据库（FAISS&#x2F;Chroma&#x2F;Milvus）：https://python.langchain.com/docs/integrations/vectorstores/



本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>LangChain</tag>
        <tag>RAG</tag>
        <tag>Agent</tag>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>vLLM高性能大模型推理引擎使用指南</title>
    <url>/posts/c076d9f3f05d.html</url>
    <content><![CDATA[在当今AI快速发展的时代，大模型推理性能直接影响着应用的用户体验和成本效益。vLLM作为一个高性能的大模型推理引擎，为开发者提供了快速、高效的模型服务解决方案。本文将详细介绍如何使用vLLM进行离线推理和在线服务部署，特别是如何利用uv工具进行快速环境管理，以及如何部署兼容OpenAI API的模型服务。
什么是vLLMvLLM（Very Large Language Model）是由UC Berkeley开发的高性能大语言模型推理和服务引擎。它具有以下特点：

高吞吐量：通过PagedAttention等技术优化，显著提升推理速度
内存效率：动态内存管理，减少显存占用
易于使用：提供简洁的Python API和OpenAI兼容接口
灵活部署：支持批量推理和在线服务两种模式

环境准备与安装系统要求
操作系统：Linux
Python版本：3.9 - 3.12
硬件：NVIDIA GPU（推荐）

使用uv工具快速安装uv是一个超快的Python环境管理器，可以显著加速环境创建和包安装过程。
1. 安装uv工具# 在Linux/macOS上安装uvcurl -LsSf https://astral.sh/uv/install.sh | sh# 或使用pip安装pip install uv

2. 创建Python环境并安装vLLMuv python list #查看 python 可用版本# 创建Python 3.12环境并安装vLLMuv venv myenv --python 3.12 --seedsource myenv/bin/activateuv pip install vllm

3. 一键运行方式（推荐）如果您只是想快速测试vLLM，可以使用uv的--with选项直接运行，无需创建虚拟环境：
# 直接运行vLLM命令uv run --with vllm vllm --help# 启动vLLM服务uv run --with vllm vllm serve Qwen/Qwen2.5-1.5B-Instruct

传统conda安装方式如果您更习惯使用conda管理环境：
conda create -n vllm-env python=3.12 -yconda activate vllm-envpip install vllm

常用模型下载与本地存放位置说明在使用vLLM进行推理或服务部署前，需先准备好大模型权重文件。vLLM支持直接加载HuggingFace Hub上的主流模型，常见模型包括：

Qwen&#x2F;Qwen2.5-1.5B-Instruct
meta-llama&#x2F;Llama-2-7b-chat-hf
baichuan-inc&#x2F;Baichuan2-7B-Chat
internlm&#x2F;internlm2-chat-1_8b
deepseek-ai&#x2F;deepseek-llm-7b-chat

下载模型命令示例以Qwen2.5-1.5B-Instruct为例，可使用如下命令提前下载模型：
# 使用huggingface-cli下载模型pip install huggingface_hubhuggingface-cli download Qwen/Qwen2.5-1.5B-Instruct --local-dir ./models/Qwen2.5-1.5B-Instruct# 删除模型huggingface-cli delete-cache

或直接在vLLM加载时自动下载：
vllm serve Qwen/Qwen2.5-1.5B-Instruct

模型本地存放路径
默认情况下，HuggingFace Hub会将模型文件缓存到 ~/.cache/huggingface/hub 目录。
你也可以通过 HUGGINGFACE_HUB_CACHE 环境变量自定义缓存路径，例如：

export HUGGINGFACE_HUB_CACHE=/data/models/hf_cache


若使用huggingface-cli download，可通过--local-dir参数指定下载目录。

使用ModelScope下载模型ModelScope: https://www.modelscope.cn/models设置环境变量，优先使用 ModelScope 下载：
export VLLM_USE_MODELSCOPE=True
安装ModelScope 
uv pip install modelscope
下载模型
modelscope download qwen/Qwen2.5-1.5B-Instruct
ModelScope 默认缓存在 ~&#x2F;.cache&#x2F;modelscope&#x2F;hub可通过环境变量 TRANSFORMERS_CACHE&#x2F;MODELSCOPE_CACHE自定义目录。
注意事项
注意：首次加载大模型时需保证磁盘空间充足，部分模型体积较大（数GB至数十GB不等）。
安全建议：请从官方或权威渠道下载模型，避免使用来历不明的第三方模型文件。

离线批量推理vLLM的离线推理功能适用于批量处理大量文本的场景，如数据分析、内容生成等。
基础使用示例创建一个Python脚本offline_inference.py：
from vllm import LLM, SamplingParams# 定义输入提示列表prompts = [    &quot;你好，我的名字是&quot;,    &quot;人工智能的未来是&quot;,    &quot;深度学习最重要的突破是&quot;,    &quot;请解释一下什么是Transformer架构&quot;,]# 配置采样参数# temperature: 控制生成文本的随机性，值越高越随机# top_p: 核采样参数，控制候选词汇的范围# max_tokens: 生成文本的最大长度sampling_params = SamplingParams(    temperature=0.8,     top_p=0.95,    max_tokens=256)# 初始化vLLM引擎# 这里使用较小的模型作为示例，您可以替换为其他支持的模型llm = LLM(model=&quot;facebook/opt-125m&quot;)# 执行批量推理outputs = llm.generate(prompts, sampling_params)# 处理并显示结果for output in outputs:    prompt = output.prompt    generated_text = output.outputs[0].text    print(f&quot;输入: &#123;prompt&#125;&quot;)    print(f&quot;输出: &#123;generated_text&#125;&quot;)    print(&quot;-&quot; * 50)

高级配置选项from vllm import LLM, SamplingParams# 创建更详细的采样配置sampling_params = SamplingParams(    temperature=0.7,           # 生成温度    top_p=0.9,                # 核采样    top_k=40,                 # Top-K采样    repetition_penalty=1.1,    # 重复惩罚    max_tokens=512,           # 最大生成长度    stop=[&quot;&lt;/s&gt;&quot;, &quot;\n\n&quot;]     # 停止符号)# 配置模型加载参数llm = LLM(    model=&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;,    tensor_parallel_size=1,    # 张量并行大小    dtype=&quot;float16&quot;,           # 数据类型    max_model_len=4096,       # 最大模型长度    gpu_memory_utilization=0.8 # GPU内存使用率)

部署OpenAI兼容API服务vLLM最强大的功能之一是能够部署为兼容OpenAI API的服务，让您可以无缝替换OpenAI的API端点。
启动vLLM服务# 启动基础服务vllm serve Qwen/Qwen2.5-1.5B-Instruct# 启动带自定义配置的服务vllm serve Qwen/Qwen2.5-1.5B-Instruct \    --host 0.0.0.0 \    --port 8000 \    --api-key your-api-key \    --max-model-len 4096 \    --tensor-parallel-size 1

使用curl测试API1. 测试模型列表curl http://localhost:8000/v1/models

2. 文本补全APIcurl http://localhost:8000/v1/completions \    -H &quot;Content-Type: application/json&quot; \    -d &#x27;&#123;        &quot;model&quot;: &quot;Qwen/Qwen2.5-1.5B-Instruct&quot;,        &quot;prompt&quot;: &quot;人工智能的发展趋势是&quot;,        &quot;max_tokens&quot;: 100,        &quot;temperature&quot;: 0.7    &#125;&#x27;

3. 聊天补全APIcurl http://localhost:8000/v1/chat/completions \    -H &quot;Content-Type: application/json&quot; \    -d &#x27;&#123;        &quot;model&quot;: &quot;Qwen/Qwen2.5-1.5B-Instruct&quot;,        &quot;messages&quot;: [            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个有用的AI助手。&quot;&#125;,            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请解释一下什么是机器学习？&quot;&#125;        ],        &quot;max_tokens&quot;: 200,        &quot;temperature&quot;: 0.7    &#125;&#x27;

Python客户端使用1. 使用OpenAI Python库from openai import OpenAI# 配置客户端连接到vLLM服务client = OpenAI(    api_key=&quot;your-api-key&quot;,  # 如果设置了API密钥    base_url=&quot;http://localhost:8000/v1&quot;)# 文本补全completion = client.completions.create(    model=&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;,    prompt=&quot;深度学习的核心概念包括&quot;,    max_tokens=150,    temperature=0.7)print(&quot;补全结果:&quot;, completion.choices[0].text)# 聊天补全chat_response = client.chat.completions.create(    model=&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;,    messages=[        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个专业的技术顾问。&quot;&#125;,        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请推荐一个适合初学者的机器学习框架。&quot;&#125;    ],    max_tokens=200,    temperature=0.7)print(&quot;聊天回复:&quot;, chat_response.choices[0].message.content)

2. 流式响应处理from openai import OpenAIclient = OpenAI(    api_key=&quot;your-api-key&quot;,    base_url=&quot;http://localhost:8000/v1&quot;)# 流式聊天补全stream = client.chat.completions.create(    model=&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;,    messages=[        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请详细介绍一下Transformer架构的工作原理。&quot;&#125;    ],    stream=True,    max_tokens=500)print(&quot;流式响应:&quot;)for chunk in stream:    if chunk.choices[0].delta.content is not None:        print(chunk.choices[0].delta.content, end=&quot;&quot;, flush=True)print()

生产环境部署建议1. 服务配置优化# 生产环境启动脚本vllm serve Qwen/Qwen2.5-7B-Instruct \    --host 0.0.0.0 \    --port 8000 \    --api-key $(cat /path/to/api-key) \    --tensor-parallel-size 2 \    --max-model-len 8192 \    --gpu-memory-utilization 0.85 \    --disable-log-requests \    --max-num-seqs 64

2. Docker部署创建Dockerfile：
FROM nvidia/cuda:12.1-devel-ubuntu20.04# 安装系统依赖RUN apt-get update &amp;&amp; apt-get install -y \    python3 python3-pip curl \    &amp;&amp; rm -rf /var/lib/apt/lists/*# 安装uv和vLLMRUN curl -LsSf https://astral.sh/uv/install.sh | shENV PATH=&quot;/root/.cargo/bin:$PATH&quot;RUN uv pip install --system vllm# 设置工作目录WORKDIR /app# 暴露端口EXPOSE 8000# 启动命令CMD [&quot;vllm&quot;, &quot;serve&quot;, &quot;Qwen/Qwen2.5-1.5B-Instruct&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8000&quot;]

构建并运行：
# 构建镜像docker build -t vllm-server .# 运行容器docker run --gpus all -p 8000:8000 vllm-server

3. 负载均衡和监控使用Nginx进行负载均衡：
upstream vllm_backend &#123;    server 127.0.0.1:8000;    server 127.0.0.1:8001;    server 127.0.0.1:8002;&#125;server &#123;    listen 80;    server_name your-domain.com;    location / &#123;        proxy_pass http://vllm_backend;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_read_timeout 300s;        proxy_connect_timeout 30s;    &#125;&#125;

性能优化技巧1. 显存优化# 启用KV缓存量化llm = LLM(    model=&quot;your-model&quot;,    kv_cache_dtype=&quot;fp8&quot;,  # 使用FP8量化KV缓存    quantization=&quot;awq&quot;      # 使用AWQ量化)

2. 并行策略# 多GPU张量并行vllm serve your-model --tensor-parallel-size 4# 流水线并行（适用于超大模型）vllm serve your-model --pipeline-parallel-size 2

3. 批处理优化# 动态批处理配置sampling_params = SamplingParams(    temperature=0.7,    max_tokens=256,    # 启用连续批处理    use_beam_search=False)

常见问题和解决方案1. 内存不足错误# 降低GPU内存使用率vllm serve your-model --gpu-memory-utilization 0.7# 启用CPU卸载vllm serve your-model --cpu-offload-gb 4

2. 模型加载失败# 使用ModelScope镜像（国内用户）import osos.environ[&quot;VLLM_USE_MODELSCOPE&quot;] = &quot;true&quot;llm = LLM(model=&quot;qwen/Qwen2.5-1.5B-Instruct&quot;)

3. API响应超时# 增加超时时间curl -X POST http://localhost:8000/v1/chat/completions \    --max-time 300 \    -H &quot;Content-Type: application/json&quot; \    -d &#x27;&#123;&quot;model&quot;: &quot;your-model&quot;, &quot;messages&quot;: [...]&#125;&#x27;

Windows 下安装vllm安装WSL2及NVIDIA驱动
启动wsl2

打开“控制面板”，进入“程序和功能”→“启用或关闭 Windows 功能”。勾选“适用于Linux的Windows子系统”和“虚拟机平台”。点击“确定”后重启电脑


安装Ubuntu系统

wsl.exe --list --onlinewsl --install Ubuntu --location E:/WSL2/Ubuntuwsl -s Ubuntu # 将Ubuntu设置为默认的系统

安装nvidia 驱动： https://www.nvidia.cn/drivers/lookup/

安装编译器wsl --cd ~sudo apt updatesudo apt install -y build-essential # 安装编译器工具sudo apt install -y python3-dev
安装vllmcurl -LsSf https://astral.sh/uv/install.sh | shmkdir vllm &amp;&amp; cd vllmuv venv myenv --python 3.12 --seedsource myenv/bin/activateuv pip install vllm
运行模型CUDA_VISIBLE_DEVICES=0 uv run --with vllm vllm serve Qwen/Qwen2.5-1.5B-Instruct# 或CUDA_VISIBLE_DEVICES=0 vllm serve Qwen/Qwen2.5-1.5B-Instruct
如果有两张显卡，可设置 CUDA_VISIBLE_DEVICES&#x3D;0,1 并加上–tensor-parallel-size 2 开启并行运算–port 指定端口， 默认为8000
CUDA OOM如果遇到 CUDA OOM错误，则设置DGX参数
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:TrueCUDA_VISIBLE_DEVICES=0 uv run --with vllm vllm serve Qwen/Qwen2.5-1.5B-Instruct \    --gpu-memory-utilization 0.4

总结vLLM作为一个高性能的大模型推理引擎，为开发者提供了强大的工具来部署和使用大语言模型。通过本文的介绍，您应该能够：

快速环境搭建：使用uv工具高效管理Python环境和依赖
离线批量推理：处理大规模文本生成任务
在线API服务：部署兼容OpenAI API的模型服务
生产环境优化：通过各种配置提升性能和稳定性

vLLM的优势在于其出色的性能表现和易用性，特别是在需要高吞吐量推理的场景下。随着大模型技术的不断发展，vLLM将继续是一个值得关注和使用的重要工具。
相关资源
vLLM官方文档
vLLM GitHub仓库
uv工具官方文档
OpenAI API文档
vLLM中文站


本文由 AI 辅助生成，如有错误或建议，欢迎指出。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>vLLM</tag>
        <tag>大模型推理</tag>
        <tag>OpenAI API</tag>
        <tag>uv</tag>
      </tags>
  </entry>
  <entry>
    <title>Cursor创建一个python项目的所有交互</title>
    <url>/posts/b9e5a0c5a4ad.html</url>
    <content><![CDATA[本文内容主要参考自《用cursor玩转AI辅助编程》，将详细记录如何借助 Cursor AI 高效开发一个基于 FastAPI 和 Vue.js 的销售数据分析系统。适合希望了解 AI 辅助编程实践的 Python 全栈开发者。文章涵盖了从项目初始化、依赖配置、数据库设计、后端与前端开发、数据处理、API 实现、测试到前后端联调的完整流程。通过真实的对话与操作示例，帮助读者掌握在实际开发场景下如何高效利用 Cursor 进行协作，显著提升开发效率与代码质量。
后端开发需求分析
chat

我需要开发一个销售数据分析系统，主要功能包括数据处理、统计分析和API。 请推荐合适的Python框架和技术栈，并帮助我搭建基础项目结构。 系统需要处理CSV格式的销售数据，进行数据清洗和分析，最后通过Web API框架提供查询API。 请帮我细化开发步骤，以便后续和你沟通时，能够一步一步实现。

chat

感谢你的建议！不过我想做一些调整： 1. 我希望使用MySQL而不是PostgreSQL，因为我在本地电脑已经安装了MySQL 2. 暂时不需要Docker，我们先在本地开发运行即可 3. 其他的技术选型都很好，我们继续使用FastAPI和Pandas 请帮我调整项目结构和依赖配置来适配这些变更内容。

chat

现在的技术选型我很满意。 请帮我把实现细节细化一下，并生成COMPOSER 可以理解的提示词，以便后续项目实现 
生成结果如下：
我来帮你把项目实现细节拆分成更具体的步骤，并按照实现顺序组织。 1. 基础框架搭建 步骤1.1：创建项目结构 mkdir sales_analysis cd sales_analysis # 创建所需目录 mkdir -p app/api/endpoints app/core app/models app/schemas app/services tests data 步骤1.2：创建虚拟环境和安装依赖 python -m venv venv source venv/bin/activate  # Linux/Mac pip install -r requirements.txt 2. 数据模型设计 步骤2.1：创建基础数据模型 from sqlalchemy import Column, Integer, String, Float, DateTime, ForeignKey from sqlalchemy.orm import relationship from app.core.database import Base class Product(Base):     __tablename__ = &quot;products&quot;     id = Column(Integer, primary_key=True, index=True)     name = Column(String(100), nullable=False)     category = Column(String(50))     price = Column(Float, nullable=False) class Sale(Base):     __tablename__ = &quot;sales&quot;      id = Column(Integer, primary_key=True, index=True)     product_id = Column(Integer, ForeignKey(&quot;products.id&quot;))     quantity = Column(Integer, nullable=False)     sale_date = Column(DateTime, nullable=False)     total_amount = Column(Float, nullable=False) 3. 数据处理服务 步骤3.1：CSV数据处理服务 import pandas as pd from sqlalchemy.orm import Session from app.models.sales import Product, Sale from datetime import datetime  class DataProcessor:     @staticmethod     def process_csv(file_path: str, db: Session):         df = pd.read_csv(file_path)         # 数据清洗和转换逻辑         # 将处理后的数据保存到数据库 4.  API开发 步骤4.1：创建API路由 from fastapi import APIRouter, Depends, HTTPException from sqlalchemy.orm import Session from app.core.database import get_db from app.services.data_processor import DataProcessor  router = APIRouter()  @router.get(&quot;/sales/summary&quot;) def get_sales_summary(db: Session = Depends(get_db)):     # 实现销售汇总统计     pass  @router.get(&quot;/sales/trend&quot;) def get_sales_trend(start_date: str, end_date: str, db: Session = Depends(get_db)):     # 实现销售趋势分析     pass 5. 数据分析服务 步骤5.1：创建分析服务 import pandas as pd from sqlalchemy.orm import Session from app.models.sales import Sale, Product  class SalesAnalytics:     @staticmethod     def calculate_summary(db: Session):         # 实现销售汇总统计逻辑         pass      @staticmethod     def analyze_trends(db: Session, start_date: str, end_date: str):         # 实现销售趋势分析逻辑         pass 实现顺序和提示词 1. 设置项目基础结构 请帮我创建sales_analysis项目的基础目录结构，包括所有必要的空文件。 2. 创建数据库配置 请帮我实现MySQL数据库配置文件，包括database.py和config.py，需要支持环境变量配置。 3. 实现数据模型 请帮我实现销售数据分析系统的SQLAlchemy数据模型，包括产品表和销售记录表。 4. 实现数据处理服务 请帮我实现CSV数据处理服务，包括数据清洗和导入数据库的功能。 5. 实现API路由 请帮我实现销售数据分析系统的FastAPI路由，包括数据导入、查询和分析API。 6. 实现数据分析服务 请帮我实现销售数据分析服务，包括销售汇总、趋势分析等功能。 测试数据示例 创建示例CSV文件 `data/sample_sales.csv`: product_id,product_name,category,price,quantity,sale_date 1,产品A,电子,99.9,2,2024-01-01 2,产品B,服装,199.9,1,2024-01-02 下一步建议 1. 我们可以从创建项目基础结构开始 2. 然后实现数据库配置 3. 接着是数据模型的实现 你想从哪个部分开始？每个步骤我都可以提供详细的代码实现。

将返回结果保存在notepads中

基础配置&#x2F;环境搭建
agent

请帮我创建sales_analysis项目的基础目录结构，包括所有必要的空文件。 目录是这样的： sales_analysis/ ├── app/ 第5章  Cursor项目进阶：销售数据分析（后端Python部分）  |  97  │   ├── __init__.py │   ├── main.py           # FastAPI应用入口 │   ├── api/ │   │   ├── __init__.py │   │   └── endpoints/ │   ├── core/ │   │   ├── config.py │   │   └── database.py │   ├── models/ │   ├── schemas/ │   └── services/ ├── tests/ ├── data/ └── requirements.txt 其中 requirements.txt 内部的依赖是这样的： fastapi&gt;=0.104.0 uvicorn&gt;=0.24.0 pandas&gt;=2.1.0 numpy&gt;=1.24.0 sqlalchemy&gt;=2.0.0 mysqlclient&gt;=2.2.0 python-dotenv&gt;=1.0.0 pydantic&gt;=2.4.0 pytest&gt;=7.4.0 

agent

以下是我的真实的MySQL连接信息，请帮我做对应文件的修改 localhost:3306 用户名：root 密码：root 

agent

帮我创建虚拟环境和安装依赖 
功能实现
agent

@notepads 请帮我实现销售数据分析系统的SQLAlchemy数据模型，包括产品表和销售记录表。 

agent

@Codebase 请帮我在database.py中添加创建表的代码

agent

我现在想用运行方法的形式启动整个项目，这样后续我做代码调试时会更方便。 请你帮我修改必要的代码 

agent

@notepads 请帮我实现CSV文件的数据处理服务，包括数据清洗和导入数据库的功能。

agent

@notepads 请帮我实现销售数据分析系统的FastAPI路由，包括数据导入、查询和分析API。 

agent

@notepads 请帮我实现销售数据分析服务，包括销售汇总、趋势分析等功能。
测试
chat

@notepads @Codebase  我现在已经写完代码了，需要测试，请你帮我生成一份测试计划 

将生成内容保存到 notepads中
agent

@notepads 帮我生成单元测试的部分

chat

我需要在cursor中对这个文件做Debug，请告诉我如何配置

在notepads中添加以下内容

## 异常处理 在写代码时，如果你捕获了异常，就需要打印异常堆栈信息和当时请求的参数，方便我后续排查问题。

前端开发准备
启动后端项目，获取API文档
添加doc 索引, https://vuejs.**g/guide/introduction.html,名字为 Vue.js
复制http://localhost:8000/openapi.json，内容保存到文件api.md

# 文档以下是 API 文档的JSON 定义&#123;    ...&#125;
需求分析
chat

@api.md @Vue.js我希望你基于api.md中的API内容，开发一个前端项目。 项目需要用到Vue.js和其他一些配套的前端组件。 整体页面风格希望可以做得简洁、大方。 对于一些数据展示的业务模块，希望可以增加图表组件的支持，看上去更加直观。 我希望你可以清晰罗列每个步骤，方便我后续在“COMPOSER”面板中与Cursor进行沟通 

复制输出到新的nodepads 中
chat

需要运行前端项目，在电脑上需要安装哪些软件？
后端开发
agent

@notepads 现在帮我创建Vue项目 

agent

帮我生成一份测试CSV文件，数据尽可能多一些，方便我后续在前端用图表展示。 

agent

@notepads @Codebase 在前端单击“点击上传”选项时，希望可以加一个进度条，上传成功后希望可以有一个提示消息，而且现在上传文件没有请求后端接口，你来修复一下 

agent

@Codebase 我在数据库中查看sale_records表数据时，发现有一些数据的total_amount算得不对，大部分是对的。请你分析可能的原因并给出解决方案。 

agent

@notepads 帮我实现销售数据管理模块的其他功能 

后端更新了接口后，前端需要对应适配agent

@notepads @Codebase 请基于最新的API对现有的代码做出修改与适配

数据分析模块 agent

@notepads 帮我生成数据分析模块的代码 

agent

@Codebase 我希望光标悬浮在品类分析图表上可以显示数字
附录提示词参考@note @Codebase 我需要实现一个文章管理模块，要求如下： 1. 功能需求：    - 文章的增删改查操作    - 支持分页列表展示    - 包含标题、内容、发布时间等字段 2. 技术要求：    - 使用SpringBoot框架    - MyBatis 作为ORM框架    - 遵循RESTful API设计规范 3. 预期输出：    - 后端API代码    - 数据库表设计    - 前端页面代码（Vue.js） 4. 质量规范：    - 需要添加适当的注释    - 包含异常处理    - 遵循项目现有的代码风格 
使用上下文，@notepads @Codebase @docs @file @git @cursor rules @Folders @Chat @Link @Web @Recent changes  @Code


本文由AI生成，内容仅供参考。在实际部署前，请根据具体环境进行测试和验证。

]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>Cursor</tag>
        <tag>Python</tag>
        <tag>FastAPI</tag>
        <tag>Vue.js</tag>
        <tag>全栈开发</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT-4.1 提示指南（翻译）</title>
    <url>/posts/7ef80ced6f51.html</url>
    <content><![CDATA[原文 GPT-4.1 Prompting Guide
GPT-4.1 模型系列在编码、指令遵循和长上下文处理能力方面相比 GPT-4o 有了显著提升。在本提示指南中，我们整理了从大量内部测试中得出的重要提示技巧，以帮助开发者充分利用这个新模型系列的改进能力。
许多典型的最佳实践仍然适用于 GPT-4.1，例如提供上下文示例、使指令尽可能具体和清晰，以及通过提示诱导规划以最大化模型智能。然而，我们预计充分利用这个模型需要一些提示迁移。GPT-4.1 经过训练，比其前身更严格、更字面地遵循指令，而前身倾向于更自由地从用户和系统提示中推断意图。这也意味着，GPT-4.1 具有高度的可引导性，对明确指定的提示反应灵敏——如果模型行为与您期望的不同，一个坚定且明确澄清您期望行为的单句几乎总是足以引导模型回到正轨。
请继续阅读可用作参考的提示示例，并记住虽然这些指导广泛适用，但没有建议是万能的。AI 工程本质上是一门经验性学科，大型语言模型本质上是非确定性的；除了遵循本指南外，我们建议构建信息丰富的评估并经常迭代，以确保您的提示工程变更为您的用例带来好处。
1. 代理工作流GPT-4.1 是构建代理工作流的绝佳选择。在模型训练中，我们强调提供多样化的代理问题解决轨迹，我们的模型代理框架在 SWE-bench Verified 上实现了非推理模型的最先进性能，解决了 55% 的问题。
系统提示提醒为了充分利用 GPT-4.1 的代理能力，我们建议在所有代理提示中包含三种关键类型的提醒。以下提示专门针对代理编码工作流进行了优化，但可以轻松修改用于一般代理用例。

持久性：这确保模型理解它正在进入多消息轮次，并防止它过早地将控制权交还给用户。我们的示例如下：

你是一个代理 - 请继续直到用户的查询完全解决，然后结束你的轮次并交还给用户。只有在确定问题已解决时才终止你的轮次。


工具调用：这鼓励模型充分利用其工具，并减少其产生幻觉或猜测答案的可能性。我们的示例如下：

如果你不确定与用户请求相关的文件内容或代码库结构，请使用你的工具读取文件并收集相关信息：不要猜测或编造答案。


规划（可选）：如果需要，这确保模型在文本中明确规划和反思每个工具调用，而不是仅通过链接一系列工具调用来完成任务。我们的示例如下：

你必须在每个函数调用之前广泛规划，并广泛反思之前函数调用的结果。不要仅通过函数调用来完成整个过程，因为这可能会损害你解决问题和深入思考的能力。

GPT-4.1 经过训练，在代理环境中对用户指令和系统提示都非常严格地响应。模型严格遵循这三个简单指令，我们的内部 SWE-bench Verified 分数提高了近 20%——因此我们强烈建议从涵盖上述三个类别的明确提醒开始任何代理提示。总的来说，我们发现这三个指令将模型从类似聊天机器人的状态转变为更加”渴望”的代理，自主且独立地推动交互向前发展。
工具调用与之前的模型相比，GPT-4.1 在有效利用作为 OpenAI API 请求参数传递的工具方面接受了更多训练。我们鼓励开发者专门使用 tools 字段来传递工具，而不是手动将工具描述注入到提示中并为工具调用编写单独的解析器，正如一些人过去报告的那样。这是最小化错误并确保模型在工具调用轨迹期间保持分布的最佳方式——在我们自己的实验中，我们观察到使用 API 解析的工具描述与手动将模式注入系统提示相比，SWE-bench Verified 通过率提高了 2%。
开发者应该清楚地命名工具以表明其目的，并在工具的”description”字段中添加清晰、详细的描述。同样，对于每个工具参数，依靠良好的命名和描述来确保适当的使用。如果你的工具特别复杂，你想提供工具使用示例，我们建议你在系统提示中创建一个 # Examples 部分并将示例放在那里，而不是将它们添加到”description”字段中，该字段应该保持全面但相对简洁。提供示例有助于指示何时使用工具、是否在工具调用中包含用户文本，以及哪些参数适合不同的输入。记住，你可以在 Prompt Playground 中使用”Generate Anything”来为新工具定义获得良好的起点。
提示诱导规划和思维链如前所述，开发者可以选择性地提示使用 GPT-4.1 构建的代理在工具调用之间进行规划和反思，而不是在无间断序列中静默调用工具。GPT-4.1 不是一个推理模型——意味着它在回答之前不会产生内部思维链——但在提示中，开发者可以通过使用上面显示的规划提示组件的任何变体来诱导模型产生明确的、逐步的计划。这可以被认为是模型”大声思考”。在我们对 SWE-bench Verified 代理任务的实验中，诱导明确规划使通过率提高了 4%。
示例提示：SWE-bench Verified下面，我们分享用于在 SWE-bench Verified 上获得最高分数的代理提示，该提示包含关于工作流和问题解决策略的详细指令。这种通用模式可用于任何代理任务。
from openai import OpenAIimport osclient = OpenAI(    api_key=os.environ.get(        &quot;OPENAI_API_KEY&quot;, &quot;&lt;your OpenAI API key if not set as env var&gt;&quot;    ))SYS_PROMPT_SWEBENCH = &quot;&quot;&quot;你将被要求修复开源仓库中的问题。你的思考应该彻底，所以如果很长也没关系。你可以在决定采取每个行动之前和之后逐步思考。你必须迭代并继续直到问题解决。你已经在 /testbed 文件夹中拥有解决这个问题所需的一切，即使没有互联网连接。我希望你在回到我之前完全自主地解决这个问题。只有在确定问题已解决时才终止你的轮次。逐步解决问题，并确保验证你的更改是正确的。永远不要在没有解决问题的情况下结束你的轮次，当你说你要进行工具调用时，确保你实际上进行了工具调用，而不是结束你的轮次。问题绝对可以在没有互联网的情况下解决。花时间思考每一步 - 记住严格检查你的解决方案并注意边界情况，特别是你做的更改。你的解决方案必须是完美的。如果不是，继续努力。最后，你必须使用提供的工具严格测试你的代码，并多次测试，以捕获所有边界情况。如果它不够健壮，继续迭代并使其完美。未能充分严格地测试代码是这类任务的头号失败模式；确保你处理所有边界情况，如果提供了现有测试则运行它们。你必须在每个函数调用之前广泛规划，并广泛反思之前函数调用的结果。不要仅通过函数调用来完成整个过程，因为这可能会损害你解决问题和深入思考的能力。# 工作流## 高级问题解决策略1. 深入理解问题。仔细阅读问题并批判性地思考需要什么。2. 调查代码库。探索相关文件，搜索关键函数，并收集上下文。3. 制定清晰、逐步的计划。将修复分解为可管理的、增量步骤。4. 增量实施修复。进行小的、可测试的代码更改。5. 根据需要调试。使用调试技术隔离和解决问题。6. 频繁测试。每次更改后运行测试以验证正确性。7. 迭代直到根本原因修复且所有测试通过。8. 全面反思和验证。测试通过后，思考原始意图，编写额外测试以确保正确性，并记住还有隐藏测试也必须通过，解决方案才真正完成。请参考下面的详细部分以获取每个步骤的更多信息。## 1. 深入理解问题仔细阅读问题并在编码前认真思考解决计划。## 2. 代码库调查- 探索相关文件和目录。- 搜索与问题相关的关键函数、类或变量。- 阅读和理解相关代码片段。- 识别问题的根本原因。- 在收集更多上下文时持续验证和更新你的理解。## 3. 制定详细计划- 概述修复问题的具体、简单和可验证的步骤序列。- 将修复分解为小的、增量更改。## 4. 进行代码更改- 在编辑之前，始终读取相关文件内容或部分以确保完整上下文。- 如果补丁未正确应用，尝试重新应用。- 进行小的、可测试的、增量更改，这些更改逻辑上来自你的调查和计划。## 5. 调试- 仅在你高度确信它们可以解决问题时进行代码更改- 调试时，尝试确定根本原因而不是解决症状- 调试所需时间以识别根本原因并确定修复- 使用打印语句、日志或临时代码检查程序状态，包括描述性语句或错误消息以了解发生了什么- 为了测试假设，你还可以添加测试语句或函数- 如果发生意外行为，重新审视你的假设。## 6. 测试- 使用 `!python3 run_tests.py`（或等效）频繁运行测试。- 每次更改后，通过运行相关测试验证正确性。- 如果测试失败，分析失败并修改你的补丁。- 如果需要，编写额外测试以捕获重要行为或边界情况。- 在最终确定之前确保所有测试通过。## 7. 最终验证- 确认根本原因已修复。- 审查你的解决方案的逻辑正确性和健壮性。- 迭代直到你极其确信修复完成且所有测试通过。## 8. 最终反思和额外测试- 仔细反思用户的原始意图和问题陈述。- 考虑可能未被现有测试覆盖的潜在边界情况或场景。- 编写需要通过的额外测试以完全验证你的解决方案的正确性。- 运行这些新测试并确保它们都通过。- 注意还有额外的隐藏测试也必须通过，解决方案才能成功。- 不要仅仅因为可见测试通过就假设任务完成；继续完善直到你确信修复是健壮和全面的。&quot;&quot;&quot;PYTHON_TOOL_DESCRIPTION = &quot;&quot;&quot;此函数用于在有状态的 Jupyter notebook 环境中执行 Python 代码或终端命令。python 将响应执行输出或在 60.0 秒后超时。此会话的互联网访问已禁用。不要进行外部网络请求或 API 调用，因为它们会失败。就像在 Jupyter notebook 中一样，你也可以通过调用此函数并带有以感叹号开头的终端命令来执行终端命令。此外，出于此任务的目的，你可以使用 `apply_patch` 命令作为输入调用此函数。`apply_patch` 有效地允许你对文件执行 diff/patch，但 diff 规范的格式对此任务是唯一的，所以请仔细注意这些指令。要使用 `apply_patch` 命令，你应该将以下结构的消息作为&quot;input&quot;传递：%%bashapply_patch &lt;&lt;&quot;EOF&quot;*** Begin Patch[YOUR_PATCH]*** End PatchEOF其中 [YOUR_PATCH] 是你的补丁的实际内容，以以下 V4A diff 格式指定。*** [ACTION] File: [path/to/file] -&gt; ACTION 可以是 Add、Update 或 Delete 之一。对于需要更改的每个代码片段，重复以下内容：[context_before] -&gt; 参见下面关于上下文的进一步说明。- [old_code] -&gt; 在旧代码前加减号。+ [new_code] -&gt; 在新替换代码前加加号。[context_after] -&gt; 参见下面关于上下文的进一步说明。关于 [context_before] 和 [context_after] 的说明：- 默认情况下，显示每个更改上方和下方的 3 行代码。如果一个更改在另一个更改的 3 行内，不要在第二个更改的 [context_before] 行中重复第一个更改的 [context_after] 行。- 如果 3 行上下文不足以唯一标识文件中的代码片段，使用 @@ 操作符指示代码片段所属的类或函数。例如，我们可能有：@@ class BaseClass[3 行预上下文]- [old_code]+ [new_code][3 行后上下文]- 如果一个代码块在类或函数中重复如此多次，以至于即使单个 @@ 语句和 3 行上下文也无法唯一标识代码片段，你可以使用多个 `@@` 语句跳转到正确的上下文。例如：@@ class BaseClass@@ 	def method():[3 行预上下文]- [old_code]+ [new_code][3 行后上下文]注意，我们不在此 diff 格式中使用行号，因为上下文足以唯一标识代码。你可能作为&quot;input&quot;传递给此函数以应用补丁的消息示例如下所示。%%bashapply_patch &lt;&lt;&quot;EOF&quot;*** Begin Patch*** Update File: pygorithm/searching/binary_search.py@@ class BaseClass@@     def search():-        pass+        raise NotImplementedError()@@ class Subclass@@     def search():-        pass+        raise NotImplementedError()*** End PatchEOF文件引用只能是相对的，永远不要是绝对的。运行 apply_patch 命令后，python 总是会说&quot;Done!&quot;，无论补丁是否成功应用。但是，你可以通过查看在&quot;Done!&quot;输出之前打印的任何警告或日志行来确定是否有问题和错误。&quot;&quot;&quot;python_bash_patch_tool = &#123;  &quot;type&quot;: &quot;function&quot;,  &quot;name&quot;: &quot;python&quot;,  &quot;description&quot;: PYTHON_TOOL_DESCRIPTION,  &quot;parameters&quot;: &#123;      &quot;strict&quot;: True,      &quot;type&quot;: &quot;object&quot;,      &quot;properties&quot;: &#123;          &quot;input&quot;: &#123;              &quot;type&quot;: &quot;string&quot;,              &quot;description&quot;: &quot; 你希望执行的 Python 代码、终端命令（以感叹号开头）或 apply_patch 命令。&quot;,          &#125;      &#125;,      &quot;required&quot;: [&quot;input&quot;],  &#125;,&#125;# 额外的框架设置：# - 将你的仓库添加到 /testbed# - 将你的问题添加到第一个用户消息# - 注意：尽管我们为 python、bash 和 apply_patch 使用了单个工具，但我们通常建议定义更细粒度的工具，专注于单个功能response = client.responses.create(    instructions=SYS_PROMPT_SWEBENCH,    model=&quot;gpt-4.1-2025-04-14&quot;,    tools=[python_bash_patch_tool],    input=f&quot;请回答以下问题：\nBug: Typerror...&quot;)response.to_dict()[&quot;output&quot;]

2. 长上下文GPT-4.1 具有高性能的 100 万 token 输入上下文窗口，适用于各种长上下文任务，包括结构化文档解析、重新排序、在忽略无关上下文的同时选择相关信息，以及使用上下文执行多跳推理。
最佳上下文大小我们观察到在针在干草堆评估中，直到我们完整的 100 万 token 上下文都有很好的性能，我们观察到在具有相关和无关代码以及其他文档混合的复杂任务中有很强的性能。然而，长上下文性能可能会随着需要检索更多项目或执行需要了解整个上下文状态的复杂推理（例如执行图搜索）而降低。
调整上下文依赖考虑回答你的问题可能需要的内部与外部世界知识的混合。有时模型使用一些自己的知识来连接概念或进行逻辑跳跃很重要，而在其他情况下，只使用提供的上下文是可取的。
# 指令// 用于内部知识- 仅使用提供的外部上下文中的文档来回答用户查询。如果你基于此上下文不知道答案，你必须回应&quot;我没有回答该问题所需的信息&quot;，即使用户坚持要你回答问题。// 用于内部和外部知识- 默认情况下，使用提供的外部上下文来回答用户查询，但如果需要其他基本知识来回答，并且你对答案有信心，你可以使用一些自己的知识来帮助回答问题。

提示组织特别是在长上下文使用中，指令和上下文的放置会影响性能。如果你的提示中有长上下文，理想情况下将你的指令放在提供上下文的开始和结束，因为我们发现这比仅在上下或下方表现更好。如果你希望只将指令放在一次，那么在提供的上下文上方比下方效果更好。
3. 思维链如上所述，GPT-4.1 不是一个推理模型，但提示模型逐步思考（称为”思维链”）可以是模型将问题分解为更易管理的部分、解决它们并提高整体输出质量的有效方式，但代价是使用更多输出 token 的更高成本和延迟。该模型经过训练，在代理推理和现实世界问题解决方面表现良好，因此它不应该需要太多提示就能表现良好。
我们建议从提示末尾的这个基本思维链指令开始：
...首先，仔细逐步思考回答查询需要哪些文档。然后，打印出每个文档的标题和 ID。然后，将 ID 格式化为列表。

从那里，你应该通过审计你特定示例和评估中的失败，并用更明确的指令解决系统性规划和推理错误来改进你的思维链（CoT）提示。在无约束的 CoT 提示中，它尝试的策略可能有差异，如果你观察到一种效果很好的方法，你可以在提示中编码该策略。一般来说，错误往往来自误解用户意图、上下文收集或分析不足，或逐步思考不足或不正确，所以注意这些并尝试用更有主见的指令解决它们。
这是一个示例提示，指示模型更系统地专注于分析用户意图并在继续回答之前考虑相关上下文。
# 推理策略1. 查询分析：分解和分析查询，直到你对其可能询问的内容有信心。考虑提供的上下文以帮助澄清任何模糊或令人困惑的信息。2. 上下文分析：仔细选择和分析大量潜在相关文档。优化召回 - 如果有些无关紧要也没关系，但正确的文档必须在此列表中，否则你的最终答案将是错误的。每个的分析步骤：	a. 分析：分析它可能与回答查询相关或不相关的分析。	b. 相关性评级：[高、中、低、无]3. 综合：总结哪些文档最相关以及原因，包括相关性评级为中等或更高的所有文档。# 用户问题&#123;user_question&#125;# 外部上下文&#123;external_context&#125;首先，仔细逐步思考回答查询需要哪些文档，严格遵循提供的推理策略。然后，打印出每个文档的标题和 ID。然后，将 ID 格式化为列表。

4. 指令遵循GPT-4.1 表现出出色的指令遵循性能，开发者可以利用这一点来精确塑造和控制其特定用例的输出。开发者经常广泛提示代理推理步骤、响应语气和声音、工具调用信息、输出格式、要避免的主题等。然而，由于模型更字面地遵循指令，开发者可能需要包含关于做什么或不做什么的明确规范。此外，为其他模型优化的现有提示可能不会立即与此模型一起工作，因为现有指令被更严格地遵循。
这演示了虚构客户服务代理的最佳实践。观察规则的多样性、具体性、使用额外部分获得更多细节，以及一个示例来演示整合所有先前规则的精确行为。
尝试运行以下 notebook 单元格 - 你应该看到用户消息和工具调用，用户消息应该以问候开始，然后回显他们的答案，然后提到他们即将调用工具。尝试更改指令来塑造模型行为，或尝试其他用户消息，以测试指令遵循性能。
SYS_PROMPT_CUSTOMER_SERVICE = &quot;&quot;&quot;你是 NewTelco 的有用客户服务代理，帮助用户高效地满足他们的请求，同时严格遵守提供的指导方针。# 指令- 始终用&quot;你好，你已联系到 NewTelco，我能帮你什么？&quot;问候用户- 在回答关于公司、其产品或服务或用户账户的事实性问题之前，始终调用工具。仅使用检索的上下文，永远不要依赖你自己的知识来回答这些问题。    - 但是，如果你没有足够的信息来正确调用工具，请向用户询问你需要的信息。- 如果用户请求，升级到人工。- 不要讨论禁止的主题（政治、宗教、有争议的时事、医疗、法律或财务建议、个人对话、内部公司运营，或对任何人员或公司的批评）。- 在适当时依赖示例短语，但永远不要在同一对话中重复示例短语。随意变化示例短语以避免听起来重复，并使其更适合用户。- 始终遵循新消息的提供输出格式，包括对检索的政策文档中任何事实陈述的引用。- 如果你要调用工具，始终在调用工具之前和之后向用户发送适当的消息。- 在所有回复中保持专业和简洁的语气，并在句子之间使用表情符号。- 如果你已解决用户的请求，询问是否还有其他可以帮忙的# 精确响应步骤（每次响应）1. 如有必要，调用工具来满足用户期望的行动。始终在调用工具之前和之后向用户发送消息，让他们了解情况。2. 在你的用户回复中    a. 使用积极倾听并回显你听到用户要求的内容。    b. 根据上述指导方针适当回应。# 示例短语## 转移禁止主题- &quot;对不起，但我无法讨论该主题。还有其他我可以帮助你的吗？&quot;- &quot;这不是我能够提供信息的内容，但我很乐意帮助你解决其他问题。&quot;## 调用工具前- &quot;为了帮助你，我只需要验证你的信息。&quot;- &quot;让我为你检查一下——请稍等。&quot;- &quot;我现在为你检索最新详情。&quot;## 调用工具后- &quot;好的，这是我找到的：[回复]&quot;- &quot;所以我找到的是：[回复]&quot;# 输出格式- 始终包含你对用户的最终回复。- 在提供来自检索上下文的事实信息时，始终在相关陈述后立即包含引用。使用以下引用格式：    - 对于单个来源：[NAME](ID)    - 对于多个来源：[NAME](ID), [NAME](ID)- 仅提供关于此公司、其政策、其产品或客户账户的信息，并且仅基于上下文中提供的信息。不要回答此范围之外的问题。# 示例## 用户你能告诉我你们的家庭计划选项吗？## 助手回复 1### 消息&quot;你好，你已联系到 NewTelco，我能帮你什么？ 😊🎉你想了解我们的家庭计划选项。 🤝 让我为你检查一下——请稍等。 🚀&quot;### 工具调用lookup_policy_document(topic=&quot;family plan options&quot;)// 工具调用后，助手会跟进：## 助手回复 2（工具调用后）### 消息&quot;好的，这是我找到的： 🎉 我们的家庭计划允许最多 5 条线路共享数据，每条额外线路享受 10% 折扣 [家庭计划政策](ID-010)。 📱 今天还有其他我可以帮助你的吗？ 😊&quot;&quot;&quot;&quot;get_policy_doc = &#123;    &quot;type&quot;: &quot;function&quot;,    &quot;name&quot;: &quot;lookup_policy_document&quot;,    &quot;description&quot;: &quot;按主题或关键词查找内部文档和政策的工具。&quot;,    &quot;parameters&quot;: &#123;        &quot;strict&quot;: True,        &quot;type&quot;: &quot;object&quot;,        &quot;properties&quot;: &#123;            &quot;topic&quot;: &#123;                &quot;type&quot;: &quot;string&quot;,                &quot;description&quot;: &quot;在公司政策或文档中搜索的主题或关键词。&quot;,            &#125;,        &#125;,        &quot;required&quot;: [&quot;topic&quot;],        &quot;additionalProperties&quot;: False,    &#125;,&#125;get_user_acct = &#123;    &quot;type&quot;: &quot;function&quot;,    &quot;name&quot;: &quot;get_user_account_info&quot;,    &quot;description&quot;: &quot;获取用户账户信息的工具&quot;,    &quot;parameters&quot;: &#123;        &quot;strict&quot;: True,        &quot;type&quot;: &quot;object&quot;,        &quot;properties&quot;: &#123;            &quot;phone_number&quot;: &#123;                &quot;type&quot;: &quot;string&quot;,                &quot;description&quot;: &quot;格式为 &#x27;(xxx) xxx-xxxx&#x27;&quot;,            &#125;,        &#125;,        &quot;required&quot;: [&quot;phone_number&quot;],        &quot;additionalProperties&quot;: False,    &#125;,&#125;response = client.responses.create(    instructions=SYS_PROMPT_CUSTOMER_SERVICE,    model=&quot;gpt-4.1-2025-04-14&quot;,    tools=[get_policy_doc, get_user_acct],    input=&quot;国际服务要多少钱？我要去法国。&quot;,    # input=&quot;为什么我上个月的账单这么高？&quot;)response.to_dict()[&quot;output&quot;]

5. 一般建议提示结构作为参考，这是构建提示的良好起点。
# 角色和目标# 指令## 更详细指令的子类别# 推理步骤# 输出格式# 示例## 示例 1# 上下文# 最终指令和逐步思考的提示

添加或删除部分以满足你的需求，并进行实验以确定对你的使用最优的内容。
分隔符以下是为提示选择最佳分隔符的一些一般指导原则。请参考长上下文部分以获取该上下文类型的特殊考虑。

Markdown：我们建议从这里开始，使用 markdown 标题作为主要部分和子部分（包括更深的层次结构，到 H4+）。使用内联反引号或反引号块来精确包装代码，并根据需要使用标准编号或项目符号列表。

XML：这些也表现良好，我们改进了此模型对 XML 中信息的遵循。XML 便于精确包装包括开始和结束的部分，为标签添加元数据以获得额外上下文，并启用嵌套。以下是使用 XML 标签在示例部分中嵌套示例的示例，每个都有输入和输出：


&lt;examples&gt;&lt;example1 type=&quot;Abbreviate&quot;&gt;&lt;input&gt;San Francisco&lt;/input&gt;&lt;output&gt;- SF&lt;/output&gt;&lt;/example1&gt;&lt;/examples&gt;


JSON 高度结构化，模型在编码上下文中特别理解良好。但它可能更冗长，并且需要字符转义，这会增加开销。

专门用于向输入上下文添加大量文档或文件的指导：

XML 在我们的长上下文测试中表现良好。
示例：&lt;doc id=&#39;1&#39; title=&#39;The Fox&#39;&gt;The quick brown fox jumps over the lazy dog&lt;/doc&gt;


这种格式由 Lee 等人提出（ref），在我们的长上下文测试中也表现良好。
示例：ID: 1 | TITLE: The Fox | CONTENT: The quick brown fox jumps over the lazy dog


JSON 表现特别差。
示例：[{&#39;id&#39;: 1, &#39;title&#39;: &#39;The Fox&#39;, &#39;content&#39;: &#39;The quick brown fox jumped over the lazy dog&#39;}]



模型经过训练，能够稳健地理解各种格式的结构。一般来说，使用你的判断并思考什么将提供清晰的信息并”突出”给模型。例如，如果你检索的文档包含大量 XML，基于 XML 的分隔符可能效果较差。
注意事项
在一些孤立的情况下，我们观察到模型抵制产生很长、重复的输出，例如，逐个分析数百个项目。如果这对你的用例是必要的，强烈指示模型完整输出此信息，并考虑分解问题或使用更简洁的方法。
我们看到一些罕见的并行工具调用不正确的情况。我们建议测试这个，并考虑将 parallel_tool_calls 参数设置为 false，如果你看到问题。

附录：生成和应用文件差异开发者向我们提供反馈，准确和格式良好的差异生成是为编码相关任务提供动力的关键能力。为此，GPT-4.1 系列相对于之前的 GPT 模型具有显著改进的差异功能。此外，虽然 GPT-4.1 在给定清晰指令和示例的情况下在生成任何格式的差异方面都有很强的性能，但我们在这里开源一个推荐的差异格式，模型在这方面接受了大量训练。我们特别希望对于刚开始的开发者，这将消除你自己创建差异的大部分猜测工作。
应用补丁请参阅下面的示例，了解正确应用我们推荐的工具调用的提示。
APPLY_PATCH_TOOL_DESC = &quot;&quot;&quot;这是一个自定义实用程序，使添加、删除、移动或编辑代码文件更加方便。`apply_patch` 有效地允许你对文件执行 diff/patch，但 diff 规范的格式对此任务是唯一的，所以请仔细注意这些指令。要使用 `apply_patch` 命令，你应该将以下结构的消息作为&quot;input&quot;传递：%%bashapply_patch &lt;&lt;&quot;EOF&quot;*** Begin Patch[YOUR_PATCH]*** End PatchEOF其中 [YOUR_PATCH] 是你的补丁的实际内容，以以下 V4A diff 格式指定。*** [ACTION] File: [path/to/file] -&gt; ACTION 可以是 Add、Update 或 Delete 之一。对于需要更改的每个代码片段，重复以下内容：[context_before] -&gt; 参见下面关于上下文的进一步说明。- [old_code] -&gt; 在旧代码前加减号。+ [new_code] -&gt; 在新替换代码前加加号。[context_after] -&gt; 参见下面关于上下文的进一步说明。关于 [context_before] 和 [context_after] 的说明：- 默认情况下，显示每个更改上方和下方的 3 行代码。如果一个更改在另一个更改的 3 行内，不要在第二个更改的 [context_before] 行中重复第一个更改的 [context_after] 行。- 如果 3 行上下文不足以唯一标识文件中的代码片段，使用 @@ 操作符指示代码片段所属的类或函数。例如，我们可能有：@@ class BaseClass[3 行预上下文]- [old_code]+ [new_code][3 行后上下文]- 如果一个代码块在类或函数中重复如此多次，以至于即使单个 @@ 语句和 3 行上下文也无法唯一标识代码片段，你可以使用多个 `@@` 语句跳转到正确的上下文。例如：@@ class BaseClass@@ 	def method():[3 行预上下文]- [old_code]+ [new_code][3 行后上下文]注意，我们不在此 diff 格式中使用行号，因为上下文足以唯一标识代码。你可能作为&quot;input&quot;传递给此函数以应用补丁的消息示例如下所示。%%bashapply_patch &lt;&lt;&quot;EOF&quot;*** Begin Patch*** Update File: pygorithm/searching/binary_search.py@@ class BaseClass@@     def search():-        pass+        raise NotImplementedError()@@ class Subclass@@     def search():-        pass+        raise NotImplementedError()*** End PatchEOF文件引用只能是相对的，永远不要是绝对的。运行 apply_patch 命令后，python 总是会说&quot;Done!&quot;，无论补丁是否成功应用。但是，你可以通过查看在&quot;Done!&quot;输出之前打印的任何警告或日志行来确定是否有问题和错误。&quot;&quot;&quot;APPLY_PATCH_TOOL = &#123;    &quot;name&quot;: &quot;apply_patch&quot;,    &quot;description&quot;: APPLY_PATCH_TOOL_DESC,    &quot;parameters&quot;: &#123;        &quot;type&quot;: &quot;object&quot;,        &quot;properties&quot;: &#123;            &quot;input&quot;: &#123;                &quot;type&quot;: &quot;string&quot;,                &quot;description&quot;: &quot; 你希望执行的 apply_patch 命令。&quot;,            &#125;        &#125;,        &quot;required&quot;: [&quot;input&quot;],    &#125;,&#125;

参考实现：apply_patch.py这是我们作为模型训练一部分使用的 apply_patch 工具的参考实现。你需要使其可执行并作为 apply_patch 从模型将执行命令的 shell 中可用：
#!/usr/bin/env python3&quot;&quot;&quot;A self-contained **pure-Python 3.9+** utility for applying human-readable“pseudo-diff” patch files to a collection of text files.&quot;&quot;&quot;from __future__ import annotationsimport pathlibfrom dataclasses import dataclass, fieldfrom enum import Enumfrom typing import (    Callable,    Dict,    List,    Optional,    Tuple,    Union,)# --------------------------------------------------------------------------- ##  Domain objects# --------------------------------------------------------------------------- #class ActionType(str, Enum):    ADD = &quot;add&quot;    DELETE = &quot;delete&quot;    UPDATE = &quot;update&quot;@dataclassclass FileChange:    type: ActionType    old_content: Optional[str] = None    new_content: Optional[str] = None    move_path: Optional[str] = None@dataclassclass Commit:    changes: Dict[str, FileChange] = field(default_factory=dict)# --------------------------------------------------------------------------- ##  Exceptions# --------------------------------------------------------------------------- #class DiffError(ValueError):    &quot;&quot;&quot;Any problem detected while parsing or applying a patch.&quot;&quot;&quot;# --------------------------------------------------------------------------- ##  Helper dataclasses used while parsing patches# --------------------------------------------------------------------------- #@dataclassclass Chunk:    orig_index: int = -1    del_lines: List[str] = field(default_factory=list)    ins_lines: List[str] = field(default_factory=list)@dataclassclass PatchAction:    type: ActionType    new_file: Optional[str] = None    chunks: List[Chunk] = field(default_factory=list)    move_path: Optional[str] = None@dataclassclass Patch:    actions: Dict[str, PatchAction] = field(default_factory=dict)# --------------------------------------------------------------------------- ##  Patch text parser# --------------------------------------------------------------------------- #@dataclassclass Parser:    current_files: Dict[str, str]    lines: List[str]    index: int = 0    patch: Patch = field(default_factory=Patch)    fuzz: int = 0    # ------------- low-level helpers -------------------------------------- #    def _cur_line(self) -&gt; str:        if self.index &gt;= len(self.lines):            raise DiffError(&quot;Unexpected end of input while parsing patch&quot;)        return self.lines[self.index]    @staticmethod    def _norm(line: str) -&gt; str:        &quot;&quot;&quot;Strip CR so comparisons work for both LF and CRLF input.&quot;&quot;&quot;        return line.rstrip(&quot;\r&quot;)    # ------------- scanning convenience ----------------------------------- #    def is_done(self, prefixes: Optional[Tuple[str, ...]] = None) -&gt; bool:        if self.index &gt;= len(self.lines):            return True        if (            prefixes            and len(prefixes) &gt; 0            and self._norm(self._cur_line()).startswith(prefixes)        ):            return True        return False    def startswith(self, prefix: Union[str, Tuple[str, ...]]) -&gt; bool:        return self._norm(self._cur_line()).startswith(prefix)    def read_str(self, prefix: str) -&gt; str:        &quot;&quot;&quot;        Consume the current line if it starts with *prefix* and return the text        **after** the prefix.  Raises if prefix is empty.        &quot;&quot;&quot;        if prefix == &quot;&quot;:            raise ValueError(&quot;read_str() requires a non-empty prefix&quot;)        if self._norm(self._cur_line()).startswith(prefix):            text = self._cur_line()[len(prefix) :]            self.index += 1            return text        return &quot;&quot;    def read_line(self) -&gt; str:        &quot;&quot;&quot;Return the current raw line and advance.&quot;&quot;&quot;        line = self._cur_line()        self.index += 1        return line    # ------------- public entry point -------------------------------------- #    def parse(self) -&gt; None:        while not self.is_done((&quot;*** End Patch&quot;,)):            # ---------- UPDATE ---------- #            path = self.read_str(&quot;*** Update File: &quot;)            if path:                if path in self.patch.actions:                    raise DiffError(f&quot;Duplicate update for file: &#123;path&#125;&quot;)                move_to = self.read_str(&quot;*** Move to: &quot;)                if path not in self.current_files:                    raise DiffError(f&quot;Update File Error - missing file: &#123;path&#125;&quot;)                text = self.current_files[path]                action = self._parse_update_file(text)                action.move_path = move_to or None                self.patch.actions[path] = action                continue            # ---------- DELETE ---------- #            path = self.read_str(&quot;*** Delete File: &quot;)            if path:                if path in self.patch.actions:                    raise DiffError(f&quot;Duplicate delete for file: &#123;path&#125;&quot;)                if path not in self.current_files:                    raise DiffError(f&quot;Delete File Error - missing file: &#123;path&#125;&quot;)                self.patch.actions[path] = PatchAction(type=ActionType.DELETE)                continue            # ---------- ADD ---------- #            path = self.read_str(&quot;*** Add File: &quot;)            if path:                if path in self.patch.actions:                    raise DiffError(f&quot;Duplicate add for file: &#123;path&#125;&quot;)                if path in self.current_files:                    raise DiffError(f&quot;Add File Error - file already exists: &#123;path&#125;&quot;)                self.patch.actions[path] = self._parse_add_file()                continue            raise DiffError(f&quot;Unknown line while parsing: &#123;self._cur_line()&#125;&quot;)        if not self.startswith(&quot;*** End Patch&quot;):            raise DiffError(&quot;Missing *** End Patch sentinel&quot;)        self.index += 1  # consume sentinel    # ------------- section parsers ---------------------------------------- #    def _parse_update_file(self, text: str) -&gt; PatchAction:        action = PatchAction(type=ActionType.UPDATE)        lines = text.split(&quot;\n&quot;)        index = 0        while not self.is_done(            (                &quot;*** End Patch&quot;,                &quot;*** Update File:&quot;,                &quot;*** Delete File:&quot;,                &quot;*** Add File:&quot;,                &quot;*** End of File&quot;,            )        ):            def_str = self.read_str(&quot;@@ &quot;)            section_str = &quot;&quot;            if not def_str and self._norm(self._cur_line()) == &quot;@@&quot;:                section_str = self.read_line()            if not (def_str or section_str or index == 0):                raise DiffError(f&quot;Invalid line in update section:\n&#123;self._cur_line()&#125;&quot;)            if def_str.strip():                found = False                if def_str not in lines[:index]:                    for i, s in enumerate(lines[index:], index):                        if s == def_str:                            index = i + 1                            found = True                            break                if not found and def_str.strip() not in [                    s.strip() for s in lines[:index]                ]:                    for i, s in enumerate(lines[index:], index):                        if s.strip() == def_str.strip():                            index = i + 1                            self.fuzz += 1                            found = True                            break            next_ctx, chunks, end_idx, eof = peek_next_section(self.lines, self.index)            new_index, fuzz = find_context(lines, next_ctx, index, eof)            if new_index == -1:                ctx_txt = &quot;\n&quot;.join(next_ctx)                raise DiffError(                    f&quot;Invalid &#123;&#x27;EOF &#x27; if eof else &#x27;&#x27;&#125;context at &#123;index&#125;:\n&#123;ctx_txt&#125;&quot;                )            self.fuzz += fuzz            for ch in chunks:                ch.orig_index += new_index                action.chunks.append(ch)            index = new_index + len(next_ctx)            self.index = end_idx        return action    def _parse_add_file(self) -&gt; PatchAction:        lines: List[str] = []        while not self.is_done(            (&quot;*** End Patch&quot;, &quot;*** Update File:&quot;, &quot;*** Delete File:&quot;, &quot;*** Add File:&quot;)        ):            s = self.read_line()            if not s.startswith(&quot;+&quot;):                raise DiffError(f&quot;Invalid Add File line (missing &#x27;+&#x27;): &#123;s&#125;&quot;)            lines.append(s[1:])  # strip leading &#x27;+&#x27;        return PatchAction(type=ActionType.ADD, new_file=&quot;\n&quot;.join(lines))# --------------------------------------------------------------------------- ##  Helper functions# --------------------------------------------------------------------------- #def find_context_core(    lines: List[str], context: List[str], start: int) -&gt; Tuple[int, int]:    if not context:        return start, 0    for i in range(start, len(lines)):        if lines[i : i + len(context)] == context:            return i, 0    for i in range(start, len(lines)):        if [s.rstrip() for s in lines[i : i + len(context)]] == [            s.rstrip() for s in context        ]:            return i, 1    for i in range(start, len(lines)):        if [s.strip() for s in lines[i : i + len(context)]] == [            s.strip() for s in context        ]:            return i, 100    return -1, 0def find_context(    lines: List[str], context: List[str], start: int, eof: bool) -&gt; Tuple[int, int]:    if eof:        new_index, fuzz = find_context_core(lines, context, len(lines) - len(context))        if new_index != -1:            return new_index, fuzz        new_index, fuzz = find_context_core(lines, context, start)        return new_index, fuzz + 10_000    return find_context_core(lines, context, start)def peek_next_section(    lines: List[str], index: int) -&gt; Tuple[List[str], List[Chunk], int, bool]:    old: List[str] = []    del_lines: List[str] = []    ins_lines: List[str] = []    chunks: List[Chunk] = []    mode = &quot;keep&quot;    orig_index = index    while index &lt; len(lines):        s = lines[index]        if s.startswith(            (                &quot;@@&quot;,                &quot;*** End Patch&quot;,                &quot;*** Update File:&quot;,                &quot;*** Delete File:&quot;,                &quot;*** Add File:&quot;,                &quot;*** End of File&quot;,            )        ):            break        if s == &quot;***&quot;:            break        if s.startswith(&quot;***&quot;):            raise DiffError(f&quot;Invalid Line: &#123;s&#125;&quot;)        index += 1        last_mode = mode        if s == &quot;&quot;:            s = &quot; &quot;        if s[0] == &quot;+&quot;:            mode = &quot;add&quot;        elif s[0] == &quot;-&quot;:            mode = &quot;delete&quot;        elif s[0] == &quot; &quot;:            mode = &quot;keep&quot;        else:            raise DiffError(f&quot;Invalid Line: &#123;s&#125;&quot;)        s = s[1:]        if mode == &quot;keep&quot; and last_mode != mode:            if ins_lines or del_lines:                chunks.append(                    Chunk(                        orig_index=len(old) - len(del_lines),                        del_lines=del_lines,                        ins_lines=ins_lines,                    )                )            del_lines, ins_lines = [], []        if mode == &quot;delete&quot;:            del_lines.append(s)            old.append(s)        elif mode == &quot;add&quot;:            ins_lines.append(s)        elif mode == &quot;keep&quot;:            old.append(s)    if ins_lines or del_lines:        chunks.append(            Chunk(                orig_index=len(old) - len(del_lines),                del_lines=del_lines,                ins_lines=ins_lines,            )        )    if index &lt; len(lines) and lines[index] == &quot;*** End of File&quot;:        index += 1        return old, chunks, index, True    if index == orig_index:        raise DiffError(&quot;Nothing in this section&quot;)    return old, chunks, index, False# --------------------------------------------------------------------------- ##  Patch → Commit and Commit application# --------------------------------------------------------------------------- #def _get_updated_file(text: str, action: PatchAction, path: str) -&gt; str:    if action.type is not ActionType.UPDATE:        raise DiffError(&quot;_get_updated_file called with non-update action&quot;)    orig_lines = text.split(&quot;\n&quot;)    dest_lines: List[str] = []    orig_index = 0    for chunk in action.chunks:        if chunk.orig_index &gt; len(orig_lines):            raise DiffError(                f&quot;&#123;path&#125;: chunk.orig_index &#123;chunk.orig_index&#125; exceeds file length&quot;            )        if orig_index &gt; chunk.orig_index:            raise DiffError(                f&quot;&#123;path&#125;: overlapping chunks at &#123;orig_index&#125; &gt; &#123;chunk.orig_index&#125;&quot;            )        dest_lines.extend(orig_lines[orig_index : chunk.orig_index])        orig_index = chunk.orig_index        dest_lines.extend(chunk.ins_lines)        orig_index += len(chunk.del_lines)    dest_lines.extend(orig_lines[orig_index:])    return &quot;\n&quot;.join(dest_lines)def patch_to_commit(patch: Patch, orig: Dict[str, str]) -&gt; Commit:    commit = Commit()    for path, action in patch.actions.items():        if action.type is ActionType.DELETE:            commit.changes[path] = FileChange(                type=ActionType.DELETE, old_content=orig[path]            )        elif action.type is ActionType.ADD:            if action.new_file is None:                raise DiffError(&quot;ADD action without file content&quot;)            commit.changes[path] = FileChange(                type=ActionType.ADD, new_content=action.new_file            )        elif action.type is ActionType.UPDATE:            new_content = _get_updated_file(orig[path], action, path)            commit.changes[path] = FileChange(                type=ActionType.UPDATE,                old_content=orig[path],                new_content=new_content,                move_path=action.move_path,            )    return commit# --------------------------------------------------------------------------- ##  User-facing helpers# --------------------------------------------------------------------------- #def text_to_patch(text: str, orig: Dict[str, str]) -&gt; Tuple[Patch, int]:    lines = text.splitlines()  # preserves blank lines, no strip()    if (        len(lines) &lt; 2        or not Parser._norm(lines[0]).startswith(&quot;*** Begin Patch&quot;)        or Parser._norm(lines[-1]) != &quot;*** End Patch&quot;    ):        raise DiffError(&quot;Invalid patch text - missing sentinels&quot;)    parser = Parser(current_files=orig, lines=lines, index=1)    parser.parse()    return parser.patch, parser.fuzzdef identify_files_needed(text: str) -&gt; List[str]:    lines = text.splitlines()    return [        line[len(&quot;*** Update File: &quot;) :]        for line in lines        if line.startswith(&quot;*** Update File: &quot;)    ] + [        line[len(&quot;*** Delete File: &quot;) :]        for line in lines        if line.startswith(&quot;*** Delete File: &quot;)    ]def identify_files_added(text: str) -&gt; List[str]:    lines = text.splitlines()    return [        line[len(&quot;*** Add File: &quot;) :]        for line in lines        if line.startswith(&quot;*** Add File: &quot;)    ]# --------------------------------------------------------------------------- ##  File-system helpers# --------------------------------------------------------------------------- #def load_files(paths: List[str], open_fn: Callable[[str], str]) -&gt; Dict[str, str]:    return &#123;path: open_fn(path) for path in paths&#125;def apply_commit(    commit: Commit,    write_fn: Callable[[str, str], None],    remove_fn: Callable[[str], None],) -&gt; None:    for path, change in commit.changes.items():        if change.type is ActionType.DELETE:            remove_fn(path)        elif change.type is ActionType.ADD:            if change.new_content is None:                raise DiffError(f&quot;ADD change for &#123;path&#125; has no content&quot;)            write_fn(path, change.new_content)        elif change.type is ActionType.UPDATE:            if change.new_content is None:                raise DiffError(f&quot;UPDATE change for &#123;path&#125; has no new content&quot;)            target = change.move_path or path            write_fn(target, change.new_content)            if change.move_path:                remove_fn(path)def process_patch(    text: str,    open_fn: Callable[[str], str],    write_fn: Callable[[str, str], None],    remove_fn: Callable[[str], None],) -&gt; str:    if not text.startswith(&quot;*** Begin Patch&quot;):        raise DiffError(&quot;Patch text must start with *** Begin Patch&quot;)    paths = identify_files_needed(text)    orig = load_files(paths, open_fn)    patch, _fuzz = text_to_patch(text, orig)    commit = patch_to_commit(patch, orig)    apply_commit(commit, write_fn, remove_fn)    return &quot;Done!&quot;# --------------------------------------------------------------------------- ##  Default FS helpers# --------------------------------------------------------------------------- #def open_file(path: str) -&gt; str:    with open(path, &quot;rt&quot;, encoding=&quot;utf-8&quot;) as fh:        return fh.read()def write_file(path: str, content: str) -&gt; None:    target = pathlib.Path(path)    target.parent.mkdir(parents=True, exist_ok=True)    with target.open(&quot;wt&quot;, encoding=&quot;utf-8&quot;) as fh:        fh.write(content)def remove_file(path: str) -&gt; None:    pathlib.Path(path).unlink(missing_ok=True)# --------------------------------------------------------------------------- ##  CLI entry-point# --------------------------------------------------------------------------- #def main() -&gt; None:    import sys    patch_text = sys.stdin.read()    if not patch_text:        print(&quot;Please pass patch text through stdin&quot;, file=sys.stderr)        return    try:        result = process_patch(patch_text, open_file, write_file, remove_file)    except DiffError as exc:        print(exc, file=sys.stderr)        return    print(result)if __name__ == &quot;__main__&quot;:    main()

其他有效的 Diff 格式如果你想尝试使用不同的 diff 格式，我们在测试中发现，Aider 的 polyglot 基准测试中使用的 SEARCH&#x2F;REPLACE（查找&#x2F;替换）diff 格式，以及一种没有内部转义的伪 XML 格式，都有很高的成功率。
这些 diff 格式有两个关键共同点：（1）它们不使用行号；（2）它们都明确提供了需要被替换的精确代码和用于替换的新代码，并且两者之间有清晰的分隔符。
SEARCH_REPLACE_DIFF_EXAMPLE = &quot;&quot;&quot;path/to/file.py```&gt;&gt;&gt;&gt;&gt;&gt;&gt; SEARCHdef search():    pass=======def search():   raise NotImplementedError()&lt;&lt;&lt;&lt;&lt;&lt;&lt; REPLACE&quot;&quot;&quot;PSEUDO_XML_DIFF_EXAMPLE = &quot;&quot;&quot;&lt;edit&gt;&lt;file&gt;path/to/file.py&lt;/file&gt;&lt;old_code&gt;def search():    pass&lt;/old_code&gt;&lt;new_code&gt;def search():   raise NotImplementedError()&lt;/new_code&gt;&lt;/edit&gt;&quot;&quot;&quot;


本文由AI生成，内容仅供参考。在实际部署前，请根据具体环境进行测试和验证。

]]></content>
      <categories>
        <category>AI</category>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>MCP</tag>
      </tags>
  </entry>
  <entry>
    <title>Argo Rollouts使用指南：Kubernetes高级部署策略详解</title>
    <url>/posts/3634f808528d.html</url>
    <content><![CDATA[引言在现代云原生应用部署中，传统的Kubernetes Deployment虽然简单易用，但在生产环境中往往需要更精细的部署控制和更安全的发布策略。Argo Rollouts作为Kubernetes的高级部署控制器，提供了蓝绿部署、金丝雀部署、渐进式发布等多种高级部署策略，让应用发布变得更加安全、可控和自动化。
本文将为初次接触Argo Rollouts的读者提供全面的使用指南，包括核心概念、功能特性、实际案例和最佳实践，帮助您快速掌握这一强大的部署工具。
Argo Rollouts简介什么是Argo RolloutsArgo Rollouts是Argo项目的一部分，它是一个Kubernetes控制器，用于提供更高级的部署策略。与标准的Kubernetes Deployment不同，Argo Rollouts支持：

蓝绿部署（Blue-Green Deployment）：零停机时间部署
金丝雀部署（Canary Deployment）：渐进式流量切换
渐进式发布（Progressive Delivery）：基于指标的自动发布
回滚策略：快速回滚到之前的版本
暂停和恢复：手动控制发布过程

核心优势
零停机部署：通过蓝绿部署策略实现真正的零停机时间
风险控制：金丝雀部署可以逐步验证新版本
自动化：基于Prometheus指标的自动发布决策
可视化：提供Web UI和CLI工具进行部署管理
与现有生态集成：与Istio、Linkerd、NGINX Ingress等无缝集成

安装和配置前置条件
Kubernetes集群（1.16+）
kubectl已配置
可选：Prometheus（用于指标分析）

安装Argo Rollouts# 安装Argo Rollouts控制器kubectl create namespace argo-rolloutskubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml# 验证安装kubectl get pods -n argo-rollouts

安装Argo Rollouts CLI工具# 下载kubectl-argo-rollouts插件curl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-linux-amd64chmod +x kubectl-argo-rollouts-linux-amd64sudo mv kubectl-argo-rollouts-linux-amd64 /usr/local/bin/kubectl-argo-rollouts# 验证安装kubectl argo rollouts version

核心概念Rollout资源Argo Rollout使用自定义资源Rollout来替代标准的Deployment：
apiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata:  name: example-rolloutspec:  replicas: 5  strategy:    blueGreen:      activeService: active-service      previewService: preview-service  selector:    matchLabels:      app: example-app  template:    metadata:      labels:        app: example-app    spec:      containers:      - name: example-app        image: nginx:1.19

部署策略类型
BlueGreen策略：创建两个完全相同的环境，快速切换
Canary策略：逐步将流量从旧版本转移到新版本
Mixed策略：结合蓝绿和金丝雀的混合策略

蓝绿部署实战场景介绍蓝绿部署是最安全的部署策略之一，特别适合对可用性要求极高的生产环境。它通过创建两个完全相同的环境（蓝色和绿色），在新版本部署完成后快速切换流量。
完整示例1. 创建Service资源# active-service.yamlapiVersion: v1kind: Servicemetadata:  name: active-servicespec:  ports:  - port: 80    targetPort: 8080  selector:    app: example-app---apiVersion: v1kind: Servicemetadata:  name: preview-servicespec:  ports:  - port: 80    targetPort: 8080  selector:    app: example-app

2. 创建Rollout资源# rollout-bluegreen.yamlapiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata:  name: example-rolloutspec:  replicas: 3  strategy:    blueGreen:      activeService: active-service      previewService: preview-service      autoPromotionEnabled: false  # 禁用自动提升      scaleDownDelaySeconds: 30    # 延迟删除旧版本      prePromotionAnalysis:        templates:        - templateName: success-rate        args:        - name: service-name          value: preview-service.default.svc.cluster.local:80  selector:    matchLabels:      app: example-app  template:    metadata:      labels:        app: example-app    spec:      containers:      - name: example-app        image: nginx:1.19        ports:        - containerPort: 8080        readinessProbe:          httpGet:            path: /            port: 8080          initialDelaySeconds: 10          periodSeconds: 5

3. 部署应用# 应用配置kubectl apply -f active-service.yamlkubectl apply -f rollout-bluegreen.yaml# 查看部署状态kubectl argo rollouts get rollout example-rollout

4. 更新镜像版本# 更新到新版本kubectl argo rollouts set image example-rollout example-app=nginx:1.20# 查看部署进度kubectl argo rollouts get rollout example-rollout

5. 手动提升到生产环境# 手动提升（将流量切换到新版本）kubectl argo rollouts promote example-rollout

蓝绿部署的优势
零停机时间：新版本完全部署后才切换流量
快速回滚：只需切换Service选择器即可回滚
完全隔离：新旧版本完全隔离，互不影响
易于验证：可以在切换前对新版本进行充分测试

金丝雀部署实战场景介绍金丝雀部署适合需要逐步验证新版本的场景，通过逐步增加新版本的流量比例，可以及时发现和解决问题。
完整示例1. 创建Ingress资源（使用NGINX Ingress）# ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: example-ingress  annotations:    nginx.ingress.kubernetes.io/canary: &quot;true&quot;    nginx.ingress.kubernetes.io/canary-weight: &quot;0&quot;spec:  rules:  - host: example.com    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: example-service            port:              number: 80

2. 创建Rollout资源# rollout-canary.yamlapiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata:  name: example-rolloutspec:  replicas: 5  strategy:    canary:      canaryService: canary-service  # required      stableService: stable-service  # required      steps:      - setWeight: 20      - pause: &#123;duration: 60s&#125;      - setWeight: 40      - pause: &#123;duration: 60s&#125;      - setWeight: 60      - pause: &#123;duration: 60s&#125;      - setWeight: 80      - pause: &#123;duration: 60s&#125;      - setWeight: 100      trafficRouting:        nginx:          stableIngress: example-ingress  selector:    matchLabels:      app: example-app  template:    metadata:      labels:        app: example-app    spec:      containers:      - name: example-app        image: nginx:1.19        ports:        - containerPort: 8080

3. 部署和更新# 应用配置kubectl apply -f ingress.yamlkubectl apply -f rollout-canary.yaml# 更新镜像kubectl argo rollouts set image example-rollout example-app=nginx:1.20# 查看部署状态kubectl argo rollouts get rollout example-rollout

4. 手动控制部署过程# 暂停部署kubectl argo rollouts pause example-rollout# 继续部署kubectl argo rollouts resume example-rollout# 回滚到上一个版本kubectl argo rollouts undo example-rollout

基于指标的渐进式发布场景介绍基于指标的渐进式发布是Argo Rollout的高级功能，它可以根据Prometheus指标自动决定是否继续发布或回滚。
配置示例# rollout-with-analysis.yamlapiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata:  name: example-rolloutspec:  replicas: 5  strategy:    canary:      steps:      - setWeight: 20      - pause: &#123;duration: 60s&#125;      - analysis:          templates:          - templateName: success-rate          args:          - name: service-name            value: example-service.default.svc.cluster.local:80      - setWeight: 40      - pause: &#123;duration: 60s&#125;      - analysis:          templates:          - templateName: success-rate          args:          - name: service-name            value: example-service.default.svc.cluster.local:80      - setWeight: 100      trafficRouting:        nginx:          stableIngress: example-ingress  selector:    matchLabels:      app: example-app  template:    metadata:      labels:        app: example-app    spec:      containers:      - name: example-app        image: nginx:1.19        ports:        - containerPort: 8080---apiVersion: argoproj.io/v1alpha1kind: AnalysisTemplatemetadata:  name: success-ratespec:  args:  - name: service-name  metrics:  - name: success-rate    interval: 60s    count: 5    successCondition: result[0] &gt;= 0.95    provider:      prometheus:        address: http://prometheus.monitoring.svc.cluster.local:9090        query: |          sum(rate(http_requests_total&#123;service=&quot;&#123;&#123;args.service-name&#125;&#125;&quot;,status!~&quot;5..&quot;&#125;[5m])) /          sum(rate(http_requests_total&#123;service=&quot;&#123;&#123;args.service-name&#125;&#125;&quot;&#125;[5m]))

高级功能1. 自动回滚# 配置自动回滚spec:  strategy:    canary:      steps:      - analysis:          templates:          - templateName: success-rate          args:          - name: service-name            value: example-service.default.svc.cluster.local:80          rollbackOnFailure: true  # 分析失败时自动回滚

2. 资源限制和HPA集成# 与HPA集成的RolloutapiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata:  name: hpa-rolloutspec:  replicas: 5  strategy:    canary:      steps:      - setWeight: 20      - pause: &#123;duration: 60s&#125;      - setWeight: 100  selector:    matchLabels:      app: example-app  template:    metadata:      labels:        app: example-app    spec:      containers:      - name: example-app        image: nginx:1.19        ports:        - containerPort: 8080        resources:          requests:            memory: &quot;64Mi&quot;            cpu: &quot;250m&quot;          limits:            memory: &quot;128Mi&quot;            cpu: &quot;500m&quot;---apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata:  name: example-hpaspec:  scaleTargetRef:    apiVersion: argoproj.io/v1alpha1    kind: Rollout    name: hpa-rollout  minReplicas: 3  maxReplicas: 10  metrics:  - type: Resource    resource:      name: cpu      target:        type: Utilization        averageUtilization: 70

最佳实践1. 部署策略选择
蓝绿部署：适合对可用性要求极高的生产环境
金丝雀部署：适合需要逐步验证的场景
基于指标的发布：适合有完善监控体系的环境

2. 监控和告警# 配置监控指标spec:  strategy:    canary:      steps:      - analysis:          templates:          - templateName: error-rate          - templateName: response-time          args:          - name: service-name            value: example-service.default.svc.cluster.local:80

3. 回滚策略# 配置回滚策略spec:  strategy:    canary:      steps:      - analysis:          templates:          - templateName: success-rate          rollbackOnFailure: true          rollbackOnError: true

4. 资源管理
合理设置副本数，避免资源浪费
使用资源限制确保应用稳定性
配置HPA实现自动扩缩容

5. 安全考虑
使用RBAC控制访问权限
配置网络策略限制Pod间通信
定期更新镜像版本修复安全漏洞

故障排除常见问题
部署卡住

# 检查Pod状态kubectl get pods -l app=example-app# 查看Rollout状态kubectl argo rollouts get rollout example-rollout# 查看事件kubectl describe rollout example-rollout


流量路由问题

# 检查Service配置kubectl get svc -o wide# 检查Ingress配置kubectl describe ingress example-ingress


指标分析失败

# 检查Prometheus连接kubectl get pods -n monitoring# 查看分析模板kubectl get analysistemplate

调试技巧# 启用详细日志kubectl logs -n argo-rollouts deployment/argo-rollouts-controller -f# 查看Rollout详细状态kubectl argo rollouts get rollout example-rollout --watch# 导出Rollout配置kubectl argo rollouts get rollout example-rollout -o yaml

总结Argo Rollouts为Kubernetes提供了强大的高级部署策略，通过蓝绿部署、金丝雀部署和基于指标的渐进式发布，可以显著提高应用部署的安全性和可靠性。
关键要点
选择合适的部署策略：根据业务需求选择蓝绿、金丝雀或混合策略
配置完善的监控：使用Prometheus指标进行自动化决策
实施最佳实践：合理配置资源、设置回滚策略、管理权限
持续优化：根据实际运行情况调整部署参数

相关资源
Argo Rollouts官方文档
Argo Rollouts GitHub仓库
Kubernetes部署策略最佳实践
Prometheus监控指南

通过本文的学习，您应该能够熟练使用Argo Rollouts进行高级部署管理，为您的Kubernetes应用提供更安全、更可靠的发布策略。

本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <categories>
        <category>openshift</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>DevOps</tag>
        <tag>Argo Rollouts</tag>
      </tags>
  </entry>
  <entry>
    <title>SRE MCP Tools：运维工程师的AI助手工具箱</title>
    <url>/posts/e0a2af663124.html</url>
    <content><![CDATA[引言在现代互联网时代，站点可靠性工程（SRE）已成为确保服务稳定运行的核心实践。随着系统复杂性的不断增加，运维工程师需要管理的工具和平台也越来越多。从监控告警到事件响应，从部署管理到成本优化，每个环节都需要专业的工具支持。
但是，在面对海量的监控数据、复杂的告警规则、频繁的部署需求时，传统的手工操作已经无法满足现代运维的需求。这时，MCP（Model Context Protocol）工具的出现，为SRE工程师提供了全新的解决方案。
今天，我将为大家介绍一套完整的SRE MCP Tools工具箱，涵盖监控可观测性、部署编排、事件响应、数据库管理等多个方面，帮助运维工程师构建智能化的运维体系。
什么是SRE MCP Tools？SRE MCP Tools是一套基于模型上下文协议（MCP）的运维工具集合，它允许AI助手直接与各种运维工具和平台进行集成，实现自动化的运维操作。通过这些工具，运维工程师可以使用自然语言与AI助手交互，让AI帮助完成复杂的运维任务。
核心优势
🚨 快速发现问题：自动化监控和告警，第一时间发现系统异常
🔧 快速解决问题：自动化故障响应和修复，减少人工干预
📈 预防问题：趋势分析和容量规划，防患于未然
💼 提高效率：减少手动操作，标准化运维流程
💰 降低成本：优化资源使用，避免过度配置

🔍 监控与可观测性 MCPPrometheus&#x2F;Grafana MCP作为现代监控体系的核心，Prometheus&#x2F;Grafana MCP提供了强大的监控数据访问和可视化能力。
主要功能：

自动化指标查询：通过自然语言查询Prometheus指标
告警规则管理：智能创建和管理告警规则
仪表板创建：自动生成Grafana仪表板

使用场景：
# 示例：查询CPU使用率query_cpu_usage = &quot;&quot;&quot;请帮我查询过去1小时内CPU使用率超过80%的服务器，并创建一个告警规则当CPU使用率连续5分钟超过90%时发送告警。&quot;&quot;&quot;

Datadog&#x2F;New Relic MCP商业化监控平台的强大功能，提供统一的监控数据访问接口。
主要功能：

统一监控数据访问：跨平台监控数据查询
异常检测：基于机器学习的异常检测
性能分析：应用性能监控和分析

Jaeger&#x2F;Zipkin MCP分布式系统的链路追踪分析工具，帮助快速定位性能瓶颈。
主要功能：

分布式链路追踪分析：可视化请求在系统中的传播路径
性能瓶颈定位：识别慢查询和性能热点
依赖关系映射：生成服务依赖图

ELK Stack MCP强大的日志分析平台，提供智能化的日志处理能力。
主要功能：

日志聚合查询：跨多个数据源的日志查询
错误模式识别：自动识别错误模式和异常
日志分析自动化：智能化的日志分析和报告

🚀 部署与编排 MCPArgoCD MCPGitOps部署管理的最佳实践，实现声明式的应用部署。
主要功能：

GitOps部署管理：基于Git的自动化部署
应用状态监控：实时监控应用部署状态
回滚操作：快速回滚到上一个稳定版本

Kubernetes MCP容器编排平台的核心管理工具，提供集群资源的全面管理。
主要功能：

集群资源管理：Pod、Service、ConfigMap等资源管理
Pod状态检查：实时监控Pod健康状态
自动扩缩容：基于指标的自动扩缩容

使用示例：
# 示例：检查集群状态kubectl_status = &quot;&quot;&quot;请帮我检查生产环境k8s集群的状态，包括：- 节点资源使用情况- 异常Pod列表- 最近的事件日志&quot;&quot;&quot;

Helm MCPKubernetes应用包管理工具，简化应用部署和管理。
主要功能：

Chart管理：Helm Chart的创建和管理
版本控制：应用版本的管理和回滚
批量部署：多环境的批量部署

Terraform MCP基础设施即代码的实践工具，管理云基础设施。
主要功能：

基础设施即代码：声明式的基础设施管理
资源变更管理：安全的基础设施变更
多云支持：支持多个云平台

🔧 事件响应与自动化 MCPPagerDuty MCP专业的事件响应管理平台，提供完整的事件响应流程。
主要功能：

告警管理：智能告警聚合和去重
事件响应自动化：自动化的事件响应流程
值班调度：智能的值班人员调度

Slack&#x2F;Teams MCP团队协作平台的集成，实现ChatOps的最佳实践。
主要功能：

自动化通知：重要事件的自动通知
ChatOps集成：在聊天中执行运维命令
团队协作：团队间的协作和信息共享

Jira&#x2F;ServiceNow MCP事件管理和工作流自动化平台。
主要功能：

事件单管理：故障单的创建和跟踪
工作流自动化：自动化的工作流程
问题跟踪：问题的全生命周期管理

📊 数据库与存储 MCPTrino&#x2F;Presto MCP大数据查询引擎，提供跨数据源的统一查询能力。
主要功能：

大数据查询：跨多个数据源的统一查询
性能分析：查询性能的分析和优化
数据洞察：业务数据的深度分析

Redis&#x2F;MongoDB MCPNoSQL数据库的管理工具，提供缓存和数据库的健康监控。
主要功能：

缓存管理：Redis缓存的管理和监控
数据库健康检查：数据库性能和健康状态监控
数据备份：自动化的数据备份和恢复

S3&#x2F;对象存储 MCP云存储服务的管理工具，提供存储监控和成本优化。
主要功能：

存储监控：存储使用情况和性能监控
成本优化：存储成本的分析和优化建议
数据备份验证：备份数据的完整性验证

🛡️ 安全与合规 MCPVault MCP企业级密钥管理解决方案，提供安全的密钥管理。
主要功能：

密钥管理：集中化的密钥管理
证书轮换：自动化的证书轮换
安全策略：细粒度的安全策略管理

Falco MCP运行时安全监控工具，提供容器和云原生的安全监控。
主要功能：

运行时安全监控：实时的安全事件监控
异常行为检测：基于规则的异常行为检测
威胁响应：自动化的威胁响应

Compliance MCP合规性管理工具，确保系统符合各种合规要求。
主要功能：

合规性检查：自动化的合规性检查
审计日志分析：审计日志的分析和报告
合规报告：自动生成合规报告

🔄 CI&#x2F;CD与版本管理 MCPJenkins&#x2F;GitHub Actions MCP持续集成和持续部署平台的管理工具。
主要功能：

构建管道管理：CI&#x2F;CD管道的创建和管理
部署自动化：自动化的部署流程
构建监控：构建状态的监控和告警

Git MCP版本控制系统的管理工具，提供代码管理的自动化。
主要功能：

代码变更分析：代码变更的影响分析
发布管理：版本发布的管理和跟踪
回滚操作：快速的代码回滚

Docker Registry MCP容器镜像管理工具，提供镜像的全生命周期管理。
主要功能：

镜像管理：容器镜像的管理和分发
漏洞扫描：镜像安全漏洞扫描
清理策略：自动化的镜像清理

💰 成本优化 MCPAWS&#x2F;GCP&#x2F;Azure Cost MCP云服务成本管理工具，提供成本分析和优化建议。
主要功能：

成本分析：云服务成本的详细分析
资源优化建议：基于使用情况的优化建议
预算管理：成本预算的管理和告警

Kubernetes Cost MCPKubernetes集群成本管理工具。
主要功能：

集群成本分配：应用和团队的成本分配
资源使用优化：资源使用效率的优化
成本预测：基于历史数据的成本预测

🎯 实施建议与最佳实践优先级实施顺序
监控可观测性 - 建立完整的监控体系

首先部署Prometheus&#x2F;Grafana MCP
配置基础的系统监控指标
建立告警规则和通知机制


事件响应自动化 - 减少MTTR，提高响应速度

集成PagerDuty MCP或类似工具
建立自动化的事件响应流程
配置ChatOps集成


部署自动化 - 标准化部署流程，减少人为错误

实施GitOps部署管理
建立CI&#x2F;CD管道
配置自动化测试


数据库监控 - 确保数据层稳定性

监控数据库性能指标
建立数据备份和恢复机制
配置数据库健康检查


成本优化 - 持续优化资源使用效率

分析云服务成本
优化资源配置
建立成本控制机制



配置示例以下是一个完整的SRE MCP配置示例：
&#123;    &quot;mcpServers&quot;: &#123;        &quot;prometheus&quot;: &#123;            &quot;command&quot;: &quot;mcp-prometheus-server&quot;,            &quot;args&quot;: [&quot;--url&quot;, &quot;http://prometheus:9090&quot;],            &quot;env&quot;: &#123;                &quot;PROMETHEUS_URL&quot;: &quot;http://prometheus:9090&quot;            &#125;        &#125;,        &quot;kubernetes&quot;: &#123;            &quot;command&quot;: &quot;mcp-kubernetes-server&quot;,            &quot;args&quot;: [&quot;--kubeconfig&quot;, &quot;/path/to/kubeconfig&quot;],            &quot;env&quot;: &#123;                &quot;KUBECONFIG&quot;: &quot;/path/to/kubeconfig&quot;            &#125;        &#125;,        &quot;pagerduty&quot;: &#123;            &quot;command&quot;: &quot;mcp-pagerduty-server&quot;,            &quot;env&quot;: &#123;                &quot;PAGERDUTY_API_KEY&quot;: &quot;your-api-key&quot;            &#125;        &#125;,        &quot;slack&quot;: &#123;            &quot;command&quot;: &quot;mcp-slack-server&quot;,            &quot;env&quot;: &#123;                &quot;SLACK_BOT_TOKEN&quot;: &quot;your-bot-token&quot;,                &quot;SLACK_CHANNEL&quot;: &quot;#ops&quot;            &#125;        &#125;    &#125;&#125;

最佳实践
渐进式实施：不要一次性部署所有工具，建议分阶段实施
团队培训：确保团队成员了解MCP工具的使用方法
监控覆盖：确保关键服务都有相应的监控和告警
文档维护：维护详细的运维文档和操作手册
安全考虑：确保API密钥和敏感信息的安全管理

实际应用场景故障响应场景当生产环境出现故障时，SRE工程师可以通过AI助手快速响应：
用户：生产环境API响应时间异常，请帮我排查问题AI助手：1. 正在查询Prometheus监控数据...2. 发现API响应时间在15:30开始异常增长3. 检查Kubernetes集群状态...4. 发现数据库连接池使用率达到95%5. 建议：增加数据库连接池大小或扩容数据库实例6. 已创建PagerDuty事件并通知相关人员7. 是否需要我执行自动扩容？

日常运维场景在日常运维中，AI助手可以帮助完成各种例行任务：
用户：请帮我检查今天的系统健康状态AI助手：📊 系统健康状态报告 (2025-01-14)🔍 监控指标：- CPU使用率：平均65%，峰值82%- 内存使用率：平均71%，峰值89%- 磁盘使用率：平均45%，峰值67%- 网络流量：入流量2.3GB/s，出流量1.8GB/s🚨 告警情况：- 当前活跃告警：3个- 最近24小时告警：15个- 已解决告警：12个📈 应用性能：- 平均响应时间：245ms- 错误率：0.02%- 吞吐量：1200 req/s💰 成本分析：- 今日成本：$1,234- 较昨日变化：+2.3%- 本月预计：$35,678

总结SRE MCP Tools代表了运维自动化的新趋势，它将AI技术与传统运维工具完美结合，为运维工程师提供了智能化的解决方案。通过这套工具箱，运维团队可以：

提升效率：自动化重复性任务，释放人力资源
降低风险：标准化操作流程，减少人为错误
快速响应：智能化的故障检测和响应
持续优化：基于数据的持续改进

作为互联网工程师，掌握这些工具不仅能提升个人技能，更能为团队和企业创造价值。建议大家根据自己的实际需求，选择合适的工具进行试用和部署。
未来，随着AI技术的不断发展，我们相信SRE MCP Tools将会变得更加智能和强大，为运维工程师带来更多的可能性。让我们一起拥抱这个充满机遇的时代！

参考资料
MCP官方文档
Prometheus监控最佳实践
Kubernetes运维指南
SRE工程实践
GitOps最佳实践
Awesome DevOps MCP Servers


本文由AI生成，内容仅供参考。在实际部署前，请根据具体环境进行测试和验证。

]]></content>
      <categories>
        <category>AI</category>
        <category>SRE</category>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>MCP</tag>
        <tag>自动化</tag>
        <tag>运维</tag>
        <tag>SRE</tag>
        <tag>可观测性</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac配置Windows键盘与鼠标：让你的Mac体验更像Windows</title>
    <url>/posts/820f6c3acb0b.html</url>
    <content><![CDATA[背景为什么需要这样的配置？Mac 电脑的鼠标滚轮方向、键盘快捷键以及操作逻辑与Windows系统存在显著差异。对于长期使用Windows系统的用户来说，突然转换到Mac可能会遇到以下问题：

鼠标滚轮方向相反：Mac的”自然滚动”与Windows相反
Alt+Tab切换逻辑不同：Mac的Command+Tab与Windows的Alt+Tab行为差异很大
剪贴板功能缺失：Mac缺少类似Windows的剪贴板历史功能
修饰键位置不同：Command、Option、Control键的位置和功能与Windows的Ctrl、Alt键不同

通过合理的软件配置和系统设置，我们可以让Mac的使用体验更接近Windows，减少学习成本，提高工作效率。
核心软件解决方案1. Scroll Reverser - 解决鼠标滚轮方向问题软件介绍Scroll Reverser 是一款免费的开源软件，专门用于调整Mac的滚轮滚动方向。
主要功能
独立控制鼠标和触控板的滚动方向
支持水平和垂直滚动的独立设置
可以针对不同应用程序设置不同的滚动行为
菜单栏快速切换功能

详细配置步骤
下载安装：访问官网下载最新版本
启动设置：
打开Scroll Reverser
在”Scrolling”选项卡中勾选”Reverse Vertical”
如果需要，也可以勾选”Reverse Horizontal”



2. AltTab - 实现Windows式窗口切换软件介绍AltTab 让Mac的窗口切换体验更接近Windows系统。
核心优势
显示所有窗口：不只是应用程序图标，而是实际窗口预览
包含最小化窗口：可以直接切换到最小化的窗口
更快的响应速度：相比系统自带的切换更流畅
高度可自定义：支持多种显示样式和快捷键组合

详细配置指南
基础设置：
下载安装AltTab
在系统偏好设置中禁用原生的Command+Tab
启动AltTab并设置开机自启


快捷键配置：
将主快捷键设置为Option+Tab（更接近Windows的Alt+Tab）
或保持Command+Tab但享受增强功能


外观定制：
选择窗口显示大小
设置背景透明度
调整窗口间距和布局



  
3. Maccy - 强大的剪贴板管理器软件特色Maccy 是一款轻量级的剪贴板历史管理器，类似Windows的剪贴板功能。
主要功能
历史记录：自动保存复制的文本、图片等内容
快速搜索：支持模糊搜索历史剪贴内容
智能分类：自动识别文本、图片、文件等不同类型
隐私安全：支持忽略敏感应用（如密码管理器）

使用技巧
快捷键设置：
默认快捷键：Shift+Command+C
可自定义为更符合个人习惯的组合


高级功能：
设置历史记录数量限制
配置忽略列表（银行、密码等敏感应用）
启用&#x2F;禁用图片和文件的历史记录



  
4. 系统修饰键配置修改键盘映射 Modifier Keys通过Mac的系统偏好设置，我们可以重新映射修饰键：

进入设置：
打开”系统偏好设置” → “键盘”
点击右下角的”修饰键…”按钮


推荐配置：
Command键 → 映射为Control
Control键 → 映射为Command
Option键 → 保持不变（作为Alt键使用）


针对外接键盘：
如果使用Windows键盘，需要单独为外接键盘设置映射
Windows键 → Command键
Alt键 → Option键



  
进阶配置与替代方案其他实用工具推荐Karabiner-Elements - 高级键盘定制
更精细的键盘映射控制
支持复杂的快捷键组合
可以创建自定义的键盘行为

BetterTouchTool - 触控板和鼠标增强
自定义触控板手势
鼠标按键重新映射
窗口管理快捷操作

Magnet - 窗口管理
类似Windows的窗口贴边功能
快捷键快速调整窗口大小和位置

常见问题与解决方案问题1：软件冲突现象：多个键盘工具同时运行导致快捷键失效解决方案：

优先使用系统原生设置
按需启用第三方工具
避免功能重复的软件同时运行

问题2：外接设备兼容性现象：某些Windows键鼠在Mac上功能异常解决方案：

检查设备驱动是否支持Mac
使用USB Overdrive等通用驱动
考虑购买Mac兼容的设备

问题3：系统更新后配置丢失现象：macOS更新后需要重新配置解决方案：

定期备份配置文件
使用支持配置同步的工具
记录个人配置清单

性能优化建议系统资源管理
开机启动项管理：只保留必要的工具自启
内存占用监控：定期检查各工具的资源占用
定期清理：清除不需要的历史数据和缓存

工作流程优化
快捷键标准化：在不同设备间保持一致的快捷键设置
场景化配置：为不同工作场景创建配置文件
定期评估：根据使用习惯调整配置

总结与建议核心要点通过合理配置上述工具，可以显著改善Mac的Windows用户体验：

Scroll Reverser 解决滚轮方向问题
AltTab 提供熟悉的窗口切换体验  
Maccy 增强剪贴板功能
系统修饰键配置 让键盘操作更符合Windows习惯

实施建议
循序渐进：不要一次性安装所有工具，先从最需要的开始
个性化调整：根据个人使用习惯微调各项设置
保持更新：定期检查软件更新，确保兼容性和安全性
备份配置：重要的配置文件要做好备份

注意事项
某些软件可能需要系统权限，请谨慎授权
部分功能可能影响系统性能，请根据实际需求启用
定期检查软件的安全性和隐私政策

通过这些配置，Mac用户可以在保持系统稳定性的同时，享受到更符合Windows使用习惯的操作体验，从而提高日常工作效率。

本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <tags>
        <tag>devops</tag>
        <tag>mac</tag>
        <tag>windows</tag>
        <tag>键鼠配置</tag>
        <tag>效率工具</tag>
      </tags>
  </entry>
  <entry>
    <title>rclone使用教程</title>
    <url>/posts/10b4145e2f83.html</url>
    <content><![CDATA[rclone 是一款强大的命令行工具，支持在本地与多种云存储之间进行高效的数据同步和迁移。本文将介绍 rclone 的安装、配置及常用操作方法，帮助你快速上手。
一、安装 rclone1. 公网环境安装（以 Ubuntu 为例）curl https://downloads.rclone.org/v1.67.0/rclone-v1.67.0-linux-amd64.deb -o rclone-v1.67.0-linux-amd64.debsudo dpkg -i rclone-v1.67.0-linux-amd64.deb

2. Mac 安装brew install rclone

更多平台和安装方式可参考 rclone 官方下载页面。
二、配置 rclonerclone 的配置文件默认位于 ~/.config/rclone/rclone.conf。你可以手动创建和编辑该文件，或通过 rclone config 命令进行交互式配置。
1. 创建配置目录mkdir -p ~/.config/rclone

2. 编辑配置文件以下是一个典型的配置示例，支持 swift 和 s3 两种后端：
cat &gt; ~/.config/rclone/rclone.conf &lt;&lt; EOF[ss-data]type = swiftenv_auth = falseuser = $&#123;STORAGE_USER&#125;key = $&#123;STORAGE_PASSWORD&#125;auth = https://&#123;SS_ENDPOINT&#125;/auth/v1.0auth_version = 1[s3-data]type = s3env_auth = trueaccess_key_id = $&#123;S3_ACCESS_ID&#125;secret_access_key = $&#123;S3_SECRET_KEY&#125;region = us-east-1endpoint = https://$&#123;S3_ENDPOINT&#125;EOF

请根据实际账号和密钥替换 ${STORAGE_USER}, ${STORAGE_PASSWORD},${S3_ACCESS_ID},${S3_SECRET_KEY} 等变量。
三、常用命令1. 查看目录rclone lsd ss-data:bucketname

2. 查看文件rclone ls ss-data:bucketname/path/file.txt

3. 拷贝文件将文件从一个存储拷贝到另一个存储：
rclone copy ss-data:bucketname/path/file.txt s3-data:bucketname2/path/ -P

4. 目录同步rclone copy 拷贝文件rclone copy 只会把源目录的内容复制到目标目录，不会删除目标目录中多余的文件。
rclone copy ./local_dir ss-data:bucketname/remote_dir -P --transfers=32 --checkers=32 --retries=8 --log-level INFO

-P显示进度条（progress），包括传输速度、已完成&#x2F;剩余数据量、预计剩余时间等信息，便于实时观察任务进度。

–transfers&#x3D;32同时进行的文件传输任务数，默认值通常为4。设置为32可以大幅提升并发传输能力，适合大量小文件或高带宽场景，但过高可能导致资源争用或被服务端限流。官方认为最大不要超过128。

–multi-thread-streams&#x3D;16启用单个大文件的多线程分块并发传输，N 表示每个大文件使用的并发线程数（如 4、8、16 等）。

–buffer-size&#x3D;128M主要作用于单个文件的流式传输，即每个并发传输任务分配的内存缓冲区。对于大文件，较大的 buffer 可以减少磁盘 I&#x2F;O 等待、提升吞吐量。对于小文件，单个文件本身很小，往往一次就能读&#x2F;写完，缓冲区很难被充分利用。

–checkers&#x3D;32并发校验文件（如比对源和目标文件是否一致）的线程数。默认值通常为8。提高该值可以加快大批量文件的校验速度，适合大规模同步。官方认为最大不要超过64.



场景
关键参数
说明



大文件
–multi-thread-streams&#x3D;8
单文件多线程分块传输



–buffer-size&#x3D;128M
提高单线程缓冲区



–transfers&#x3D;4
并发文件数适中


小文件
–transfers&#x3D;32 –checkers&#x3D;32
提高并发数



–buffer-size&#x3D;默认
无需调大缓冲区



–multi-thread-streams&#x3D;0（默认）
无需多线程分块



–retries&#x3D;8单个文件传输失败时的重试次数。默认值为3。设置为8可以增强在网络不稳定或偶发错误情况下的容错能力，减少因偶发错误导致的整体失败。

–log-level INFO设置日志输出级别为 INFO，显示一般性的信息、进度和警告。常用级别有 DEBUG、INFO、NOTICE、ERROR。INFO 适合日常使用，既能看到关键日志，又不会太冗杂。

–bwlimit 参数用于限制 rclone 的传输带宽速率，例如 –bwlimit&#x3D;20M 表示将传输速度限制为每秒 20MB。该参数的作用：控制网络使用率：防止 rclone 占用全部可用带宽，确保其他网络应用仍能正常工作避免被服务商限流：某些云存储提供商会对高速持续传输进行限流，设置合理的带宽限制可以避免这种情况适应网络环境：在网络不稳定的情况下，适当限制带宽可能会提高整体传输的可靠性支持的单位：K 或 k：KB&#x2F;s (千字节&#x2F;秒)M 或 m：MB&#x2F;s (兆字节&#x2F;秒)G 或 g：GB&#x2F;s (吉字节&#x2F;秒)也可以设置时间段带宽限制，例如：


--bwlimit &quot;08:00,10M 12:00,1M 18:00,5M 23:00,off&quot;
这会在不同时间段自动应用不同的带宽限制。

–s3-chunk-size：设置 S3 分片上传时每个分块的大小（默认通常为 5MB）

–s3-upload-concurrency：控制单个文件上传时并发上传的分块数量（默认通常为 4）当上传大文件到 S3 时，rclone 会：将文件分割成多个大小为 –s3-chunk-size 的块同时上传 –s3-upload-concurrency 个块增大 –s3-chunk-size 减少了分块数量，但每个块需要更多内存增大 –s3-upload-concurrency 提高并行度，但会增加内存和网络资源消耗



场景
关键参数
说明



快速网络环境
–s3-chunk-size&#x3D;64M –s3-upload-concurrency&#x3D;8
增大两个值可提高吞吐量


内存受限环境
–s3-chunk-size&#x3D;16M –s3-upload-concurrency&#x3D;6
保持适中的块大小，适当增加并发数


不稳定网络
–s3-chunk-size&#x3D;8M –s3-upload-concurrency&#x3D;4
较小的块大小和适中的并发数有助于错误恢复




rclone rsync 同步目录rclone rsync 会让目标目录和源目录完全一致，即：目标中有但源中没有的文件会被删除，行为更接近 rsync 的 –delete。
rclone sync ./local_dir ss-data:bucketname/remote_dir -P

适用场景
rclone copy 适合做备份，不会误删目标端已有文件。
rclone sync 适合做镜像同步，但要小心目标端数据被删除。
文件特别多的场景，应该将需要上传的文件切分成多个任务进行


获取需要上传的所有文件列表

rclone lsf --files-only -R src:path | sort &gt; all_files.txt

将文件进行切分，每份不超过10w个文件

split -l 100000 --numeric-suffixes all_files.txt chunk_
以上是将源文件每100000个文件拆成一份3. 指定切分的文件列表，进行上传
rclone copy src:path dst:path --files-from chunk_00 --no-traverse --transfers=64 --checkers=32 --retries=8 --progress
这里是上传chunk_00 列表。–no-traverse: rclone在复制文件时是不先列出目标目录下的所有文件，适用于目标目录文件数特别多的场景
四、更多参考
rclone 官方文档
HCC: Using Rclone for File Transfer

rclone 支持丰富的云存储后端和灵活的参数配置，适合多种数据迁移和备份场景。建议结合实际需求，查阅官方文档，发挥其最大效能。

本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>rclone</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes云原生存储解决方案 - Longhorn安装指南</title>
    <url>/posts/ae11adc18d45.html</url>
    <content><![CDATA[简介Longhorn 是一个轻量级、可靠且功能强大的分布式块存储系统，专为 Kubernetes 而设计。它实现了可靠的持久化存储，支持快照、备份和跨集群灾难恢复等企业级功能。
主要特性
企业级分布式块存储
跨节点数据复制和自动重建
无中心化架构
非侵入式架构
图形化管理界面
快照和备份支持
跨集群灾备能力

环境要求在安装 Longhorn 之前，请确保您的环境满足以下要求：
环境检查在部署 Longhorn 之前，强烈建议运行环境检查脚本来验证您的环境是否满足所有必要条件：
# 下载环境检查脚本curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.8.1/scripts/environment_check.sh | bash

这个脚本会检查：

操作系统依赖
内核模块
挂载点
系统工具
网络配置
SELinux 状态
iscsi 服务状态

硬件要求
CPU: 最低 1 核
内存: 最低 1GB
磁盘: 每个节点至少 20GB 可用空间

软件要求
Kubernetes v1.21+
所有节点已安装 open-iscsi
所有节点已安装 curl、findmnt、grep、awk、blkid、lsblk 等工具

节点要求# 在所有节点执行以下命令安装必要组件apt-get install open-iscsi curl util-linux grep awk -ysystemctl enable iscsidsystemctl start iscsid

安装方法方法一：使用 Helm 安装（推荐）
添加 Longhorn 的 Helm 仓库：

helm repo add longhorn https://charts.longhorn.iohelm repo update


创建 longhorn-system 命名空间：

kubectl create namespace longhorn-system


安装 Longhorn：

helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --version 1.8.1
默认数据目录是&#x2F;var&#x2F;lib&#x2F;longhorn, 可以通过配置values.yaml按照自己的存储规划要求设置自定义的目录。 如: –set defaultSettings.defaultDataPath&#x3D;”&#x2F;data&#x2F;longhorn”
方法二：使用 kubectl 安装
直接应用官方的安装清单：

kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/deploy/longhorn.yaml

方法三：使用 ArgoCD 安装如果您的集群使用 ArgoCD 进行 GitOps 管理，可以通过以下方式部署 Longhorn：

创建 Application 配置文件 longhorn-application.yaml：

apiVersion: argoproj.io/v1alpha1kind: Applicationmetadata:  name: longhorn  namespace: argocdspec:  project: default  source:    chart: longhorn    repoURL: https://charts.longhorn.io    targetRevision: 1.8.1    helm:      values: |        defaultSettings:          defaultDataPath: /data/longhorn          # 可选：配置备份存储位置          backupTarget: s3://backups@us-east-1/          backupTargetCredentialSecret: longhorn-backup-secret        persistence:          defaultClass: true          defaultClassReplicaCount: 3  destination:    server: https://kubernetes.default.svc    namespace: longhorn-system  syncPolicy:    automated:      prune: true      selfHeal: true    syncOptions:      - CreateNamespace=true


如果需要配置备份到 S3，创建密钥文件 backup-secret.yaml：

apiVersion: v1kind: Secretmetadata:  name: longhorn-backup-secret  namespace: longhorn-systemtype: OpaquestringData:  AWS_ACCESS_KEY_ID: your-access-key  AWS_SECRET_ACCESS_KEY: your-secret-key  AWS_ENDPOINTS: https://s3.us-east-1.amazonaws.com/


应用配置：

# 创建 namespace（如果不存在）kubectl create namespace longhorn-system# 如果配置了备份，先创建 secretkubectl apply -f backup-secret.yaml# 创建 ArgoCD Applicationkubectl apply -f longhorn-application.yaml


验证部署状态：
在 ArgoCD UI 中查看应用同步状态
等待所有资源创建完成
检查 Pod 状态：

kubectl get pods -n longhorn-system

验证安装
检查所有 Pod 是否正常运行：

kubectl get pods -n longhorn-system

预期输出应该显示所有 Pod 都处于 Running 状态：
NAME                                        READY   STATUS    RESTARTS   AGElonghorn-manager-pzqsp                     1/1     Running   0          5mlonghorn-driver-deployer-7fd7f8b5c-shpkj   1/1     Running   0          5mlonghorn-ui-78f64844b9-mb7t2              1/1     Running   0          5mcsi-attacher-7bf4b7f996-df8v6             1/1     Running   0          4mcsi-provisioner-869bdc4b79-mzrwf          1/1     Running   0          4mcsi-resizer-6d8cf5f99f-fd2ck              1/1     Running   0          4mcsi-snapshotter-588457fcdf-22bqp          1/1     Running   0          4m


验证 StorageClass 是否创建：

kubectl get sc

应该能看到 longhorn StorageClass：
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGElonghorn (default)   driver.longhorn.io   Delete          Immediate           true                   5m

访问 Longhorn UI
创建 Ingress（可选）：

apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: longhorn-ingress  namespace: longhorn-systemspec:  rules:  - host: longhorn.example.com  # 替换为您的域名    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: longhorn-frontend            port:              number: 80


或者使用端口转发：

kubectl port-forward service/longhorn-frontend 8000:80 -n longhorn-system

然后访问 http://localhost:8000
使用示例创建一个使用 Longhorn 存储的示例 Pod：
apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: longhorn-test-pvcspec:  accessModes:    - ReadWriteOnce  storageClassName: longhorn  resources:    requests:      storage: 2Gi---apiVersion: v1kind: Podmetadata:  name: test-podspec:  containers:  - name: test-pod    image: nginx:stable-alpine    volumeMounts:    - name: longhorn-test-volume      mountPath: /data    ports:    - containerPort: 80  volumes:  - name: longhorn-test-volume    persistentVolumeClaim:      claimName: longhorn-test-pvc

常见问题处理
Pod 无法调度到节点


检查节点是否满足 Longhorn 的系统要求
确认 iscsid 服务是否正常运行
查看 Longhorn Manager 日志


存储卷创建失败


检查存储空间是否充足
确认网络连接是否正常
查看 Longhorn UI 中的事件日志

最佳实践
存储配置


建议使用 SSD 作为存储介质
为不同类型的工作负载创建不同的 StorageClass
根据实际需求配置副本数量


备份策略


定期创建数据快照
配置远程备份到对象存储
制定灾难恢复计划

存储配置指定节点参与 Longhorn 存储
将需要作为存储的节点添加 label, storage&#x3D;longhorn

kubectl label node &lt;node-1&gt; storage=longhorn

在1步中的节点对应的位置创建好目录

mkdir -p /data/longhorn

通过 Helm 安装时指定node

helm install longhorn longhorn/longhorn \    --namespace longhorn-system \    --set longhornManager.nodeSelector.storage=longhorn \    --set longhornDriver.nodeSelector.storage=longhorn \    --set longhornUI.nodeSelector.storage=longhorn \    --set defaultSettings.defaultDataPath=&quot;/data/longhorn&quot;

配置节点默认存储路径Longhorn 默认使用 /var/lib/longhorn 作为存储路径。您可以通过以下方式修改默认存储路径：

在 Longhorn UI 中修改：

访问 Longhorn UI
点击 “Setting” -&gt; “General” -&gt; “Default Data Path”
修改为您想要的路径，如 /data/longhorn


通过 Helm 安装时指定：


helm install longhorn longhorn/longhorn \  --namespace longhorn-system \  --create-namespace \  --version 1.8.1 \  --set defaultSettings.defaultDataPath=&quot;/data/longhorn&quot;

配置额外的存储目录您可以为节点添加多个存储目录或磁盘：

准备存储目录：

# 在节点上创建目录mkdir -p /data/longhorn-1mkdir -p /data/longhorn-2# 确保目录权限正确chmod 700 /data/longhorn-*chown root:root /data/longhorn-*


在 Longhorn UI 中添加磁盘：
进入 “Node” 页面
选择要配置的节点
点击 “Edit Node and Disks”
在 “Add Disk” 部分：
输入磁盘路径（如 /data/longhorn-1）
设置磁盘调度选项
设置存储预留空间
点击 “Save” 保存配置





使用原始磁盘设备如果要使用原始磁盘设备作为存储：

准备磁盘：

# 查看可用磁盘lsblk# 格式化磁盘（假设使用 /dev/sdb）mkfs.ext4 /dev/sdb# 创建挂载点mkdir -p /data/longhorn-disk# 挂载磁盘mount /dev/sdb /data/longhorn-disk# 添加到 fstab 以确保重启后自动挂载echo &quot;/dev/sdb /data/longhorn-disk ext4 defaults 0 0&quot; &gt;&gt; /etc/fstab


在 Longhorn UI 中添加磁盘：
路径填写磁盘的挂载点（如 /data/longhorn-disk）
其他步骤同上



存储标签（Storage Tags）您可以使用存储标签来管理不同性能的存储：

添加标签：

在 Longhorn UI 中选择节点和磁盘
添加标签，如：
ssd=true 用于 SSD 存储
hdd=true 用于 HDD 存储




在创建卷时使用标签：


apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: longhorn-ssdprovisioner: driver.longhorn.ioparameters:  numberOfReplicas: &quot;3&quot;  diskSelector: &quot;ssd&quot;---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: high-perf-pvcspec:  accessModes:    - ReadWriteOnce  storageClassName: longhorn-ssd  resources:    requests:      storage: 10Gi

存储最佳实践
容量规划

为系统预留足够空间（建议 30% 以上）
考虑副本数量对存储空间的影响
定期监控存储使用情况


性能优化

使用 SSD 作为主存储
合理设置副本数量（通常为 3）
避免在系统盘上存储数据


可靠性保障

使用 RAID 配置提高可靠性
配置定期备份
启用磁盘健康检查


监控告警

配置存储空间告警阈值
监控磁盘 I&#x2F;O 性能
设置节点状态监控



参考资料
Longhorn 官方文档
Longhorn GitHub
Kubernetes 存储文档


本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <categories>
        <category>DevOps</category>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>存储</tag>
        <tag>Longhorn</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Python开发自己的MCP服务：AI能力扩展入门指南</title>
    <url>/posts/1d209e12ddec.html</url>
    <content><![CDATA[引言随着人工智能技术的快速发展，大语言模型（LLM）如ChatGPT、Claude等已经成为了改变我们工作和生活方式的强大工具。但你是否想过，如何让这些AI模型具备访问外部工具和数据的能力，从而解决更复杂的问题？今天，我将向大家介绍一项令人兴奋的技术——模型上下文协议（Model Context Protocol，简称MCP），并教你如何使用Python开发自己的MCP服务，为AI模型赋予更强大的能力。
什么是MCP？基本概念模型上下文协议（MCP）是一个开放标准，用于AI应用程序与大型语言模型之间的通信。它定义了一套标准接口，使应用程序能够向模型提供上下文信息，并允许模型调用应用程序暴露的工具。
简单来说，MCP就像是AI模型和外部世界之间的一座桥梁，让模型能够”看见”和”操作”外部的数据和功能。
为什么需要MCP？想象一下，如果你正在与ChatGPT聊天，希望它能够：

查询你的个人日历
分析你的Excel数据
控制你的智能家居设备
从你的私有数据库中获取信息

这些功能都需要AI模型能够访问外部系统和数据，而MCP正是为解决这一需求而生的。
MCP的核心组件MCP协议定义了三个核心原语，每一个都扮演着重要的角色：



原语
控制权
描述
示例应用



Prompts（提示）
用户控制
用户可以选择调用的交互式模板
快捷命令、应用菜单


Resources（资源）
应用程序控制
应用程序管理的上下文数据
文件内容、数据库记录


Tools（工具）
模型控制
AI模型可调用的函数
API请求、数据处理


这三种原语共同构成了MCP的基础，使模型能够以结构化的方式与外部世界交互。
准备工作：安装MCP Python SDK在开始开发MCP服务之前，我们需要安装MCP的Python SDK。打开你的终端并运行：
pip install mcp

这个简单的命令将安装所有必要的依赖，让你能够立即开始开发。
创建你的第一个MCP服务让我们从一个简单的例子开始，创建一个能够执行基本数学运算的MCP服务。
基本加法计算器创建一个名为calculator.py的文件，并添加以下代码：
from mcp.server.fastmcp import FastMCP# 创建一个名为&quot;简易计算器&quot;的MCP服务器mcp = FastMCP(&quot;简易计算器&quot;)# 定义一个加法工具@mcp.tool()def add(a: int, b: int) -&gt; int:    &quot;&quot;&quot;Add two numbers together&quot;&quot;&quot;    return a + b# 启动服务器if __name__ == &quot;__main__&quot;:    mcp.run()

这段代码创建了一个简单的MCP服务，它提供了一个add工具，可以将两个数字相加。
运行你的MCP服务现在，你可以在终端中运行这个服务：
python calculator.py# 或者mcp run calculator.py

一旦服务启动，它就可以被支持MCP的AI模型或应用程序使用了。
进阶：创建功能丰富的MCP服务在了解了基础知识后，让我们创建一个更复杂的MCP服务，它包含了所有三种核心原语：提示、资源和工具。
多功能助手服务创建一个名为assistant.py的文件，并添加以下代码：
from mcp.server.fastmcp import FastMCPimport datetimeimport random# 创建一个名为&quot;多功能助手&quot;的MCP服务器mcp = FastMCP(&quot;多功能助手&quot;)# 定义一个资源：当前时间@mcp.resource(&quot;time://current&quot;)def current_time() -&gt; str:    &quot;&quot;&quot;Return the current time as a resource&quot;&quot;&quot;    return datetime.datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)# 定义一个工具：掷骰子@mcp.tool()def roll_dice(sides: int = 6) -&gt; int:    &quot;&quot;&quot;Roll a dice with the specified number of sides&quot;&quot;&quot;    return random.randint(1, sides)# 定义一个工具：计算BMI@mcp.tool()def calculate_bmi(weight_kg: float, height_m: float) -&gt; float:    &quot;&quot;&quot;Calculate BMI given weight in kg and height in meters&quot;&quot;&quot;    return weight_kg / (height_m * height_m)# 定义一个提示：创建健康建议@mcp.prompt()def health_advice(user_name: str) -&gt; str:    &quot;&quot;&quot;Generate a prompt for health advice&quot;&quot;&quot;    return f&quot;请为&#123;user_name&#125;提供一些基于科学的健康生活方式建议。考虑饮食、运动和睡眠方面的建议。&quot;# 启动服务器if __name__ == &quot;__main__&quot;:    mcp.run()

这个多功能助手服务提供了:

一个资源：获取当前时间
两个工具：掷骰子和计算BMI
一个提示模板：生成健康建议

高级功能：整合外部APIMCP服务的真正力量在于它能够将AI模型与外部系统和API集成。下面是一个示例，展示如何创建一个能够获取天气信息的MCP服务：
from mcp.server.fastmcp import FastMCPimport requests# 创建一个名为&quot;天气助手&quot;的MCP服务器mcp = FastMCP(&quot;天气助手&quot;)API_KEY = &quot;your_api_key_here&quot;  # 替换为你的API密钥@mcp.tool()def get_weather(city: str) -&gt; str:    &quot;&quot;&quot;Get current weather for a specified city&quot;&quot;&quot;    url = f&quot;https://api.weatherapi.com/v1/current.json?key=&#123;API_KEY&#125;&amp;q=&#123;city&#125;&quot;    response = requests.get(url)    if response.status_code == 200:        data = response.json()        temp_c = data[&quot;current&quot;][&quot;temp_c&quot;]        condition = data[&quot;current&quot;][&quot;condition&quot;][&quot;text&quot;]        humidity = data[&quot;current&quot;][&quot;humidity&quot;]        return f&quot;City: &#123;city&#125;, Temperature: &#123;temp_c&#125;°C, Condition: &#123;condition&#125;, Humidity: &#123;humidity&#125;%&quot;    else:        return f&quot;Error fetching weather data: &#123;response.status_code&#125;&quot;# 启动服务器if __name__ == &quot;__main__&quot;:    mcp.run()

这个服务通过调用外部的天气API，为模型提供了获取实时天气信息的能力。
将MCP服务与现有应用集成如果你已经有一个现有的Web应用程序，你可以将MCP服务集成到其中。以下是如何将MCP服务与Starlette（一个轻量级的ASGI框架）集成的示例：
from starlette.applications import Starlettefrom starlette.routing import Mountfrom mcp.server.fastmcp import FastMCP# 创建MCP服务mcp = FastMCP(&quot;集成服务&quot;)@mcp.tool()def greet(name: str) -&gt; str:    &quot;&quot;&quot;Generate a greeting message&quot;&quot;&quot;    return f&quot;Hello, &#123;name&#125;! Welcome to our integrated service.&quot;# 将MCP服务挂载到Starlette应用app = Starlette(    routes=[        Mount(&#x27;/mcp&#x27;, app=mcp.sse_app()),    ])# 启动服务器if __name__ == &quot;__main__&quot;:    import uvicorn    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000)

通过这种方式，你可以在不改变现有应用架构的情况下，为它添加AI能力。
高级概念：状态管理有时，你需要在不同的请求之间保持状态。下面是一个简单的待办事项管理器，它展示了如何在MCP服务中管理状态：
from mcp.server.fastmcp import FastMCP# 创建一个名为&quot;待办事项管理器&quot;的MCP服务器mcp = FastMCP(&quot;待办事项管理器&quot;)# 使用一个简单的列表来存储待办事项todos = []@mcp.tool()def add_todo(task: str) -&gt; str:    &quot;&quot;&quot;Add a new task to the todo list&quot;&quot;&quot;    todos.append(task)    return f&quot;Task added: &#123;task&#125;&quot;@mcp.tool()def list_todos() -&gt; str:    &quot;&quot;&quot;List all tasks in the todo list&quot;&quot;&quot;    if not todos:        return &quot;No tasks in the todo list.&quot;    return &quot;\n&quot;.join([f&quot;&#123;i+1&#125;. &#123;task&#125;&quot; for i, task in enumerate(todos)])@mcp.tool()def complete_todo(task_number: int) -&gt; str:    &quot;&quot;&quot;Mark a task as completed and remove it from the list&quot;&quot;&quot;    if not todos or task_number &lt;= 0 or task_number &gt; len(todos):        return &quot;Invalid task number.&quot;    task = todos.pop(task_number - 1)    return f&quot;Completed task: &#123;task&#125;&quot;# 启动服务器if __name__ == &quot;__main__&quot;:    mcp.run()

这个示例展示了如何使用全局变量来保存状态。在实际应用中，你可能需要使用数据库或其他持久化存储方案。
客户端与MCP服务的交互MCP不仅定义了服务器端的接口，还定义了客户端如何与这些服务交互。下面是一个简单的客户端示例：
from mcp import ClientSession, StdioServerParametersfrom mcp.client.stdio import stdio_client# 创建服务器参数server_params = StdioServerParameters(    command=&quot;python&quot;,    args=[&quot;calculator.py&quot;],)async def run():    async with stdio_client(server_params) as (read, write):        async with ClientSession(read, write) as session:            # 初始化连接            await session.initialize()                        # 列出可用的工具            tools = await session.list_tools()            print(f&quot;Available tools: &#123;tools&#125;&quot;)                        # 调用add工具            result = await session.call_tool(&quot;add&quot;, arguments=&#123;&quot;a&quot;: 5, &quot;b&quot;: 3&#125;)            print(f&quot;5 + 3 = &#123;result&#125;&quot;)if __name__ == &quot;__main__&quot;:    import asyncio    asyncio.run(run())

这个客户端脚本连接到我们之前创建的计算器服务，列出可用的工具，然后调用add工具计算5+3的结果。
cursor中配置自己开发的MCP在mcp.json中添加mcp server
&#123;    &quot;mcpServers&quot;: &#123;        &quot;server&quot;: &#123;            &quot;command&quot;: &quot;python&quot;,            &quot;args&quot;: [                &quot;/User/admin/workspace/mcp-server/main.py&quot;            ]        &#125;    &#125;&#125;

  
  
实际应用场景MCP服务的应用场景非常广泛，以下是一些可能的使用案例：

个人助手：创建一个能够访问你的日历、邮件和文档的个人助手

数据分析工具：开发一个能够处理和分析数据集的服务，让AI模型可以直接操作数据

内容管理系统：构建一个服务，允许AI模型创建、编辑和发布内容到你的网站或博客

企业知识库：开发一个能够安全访问企业内部文档和知识库的服务

智能家居控制：创建一个服务，让AI能够控制你的智能家居设备


调试技巧在开发MCP服务时，调试是一个重要的环节。以下是一些有用的调试技巧：

启用详细日志：在创建FastMCP实例时，可以启用详细日志：

mcp = FastMCP(&quot;调试服务&quot;, verbose=True)


添加自定义日志：在工具和资源处理函数中，添加打印语句：

@mcp.tool()def my_tool(param: str) -&gt; str:    print(f&quot;my_tool called with param: &#123;param&#125;&quot;)    # 处理逻辑    return result


使用调试客户端：创建一个简单的客户端脚本，专门用于测试你的服务

总结MCP为开发者提供了一种强大的方式，让AI模型能够与外部世界交互，从而扩展其能力范围。通过使用Python和MCP SDK，即使是初学者也能轻松地开发出功能丰富的MCP服务，为AI模型赋予更强大的能力。
无论你是想创建个人助手、企业工具，还是创新的AI应用，MCP都为你提供了所需的基础设施。随着对MCP的深入学习和实践，你将能够构建出更加复杂和强大的系统，充分发挥AI技术的潜力。
学习资源
MCP Python SDK GitHub 仓库
Model Context Protocol 官方文档
Python 官方文档
Anthropic Claude 开发者文档

开始你的MCP开发之旅吧，相信不久的将来，你将能够创建出令人惊叹的AI驱动应用！

本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <categories>
        <category>AI</category>
        <category>开发</category>
      </categories>
      <tags>
        <tag>MCP</tag>
        <tag>Python</tag>
        <tag>AI开发</tag>
        <tag>大语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>使用NSSM将程序注册为Windows服务</title>
    <url>/posts/d5f034c0aad2.html</url>
    <content><![CDATA[NSSM (Non-Sucking Service Manager) 是一个非常实用的Windows服务管理工具，它可以将任何应用程序注册为Windows服务，支持开机自启动、失败自动重启等功能，本文将介绍如何使用NSSM，并以Nginx为例进行实践。
NSSM简介NSSM的主要特点：

可以将任何可执行程序注册为Windows服务
支持服务失败自动重启
支持开机自动启动
可以捕获程序的标准输出和错误输出到日志文件
提供图形界面和命令行两种操作方式
完全免费开源

安装NSSM
下载NSSM访问 NSSM官网 下载最新版本或使用 Chocolatey 包管理器安装：

choco install nssm


解压下载的zip文件（如果是手动下载）将win64目录下的nssm.exe复制到一个固定目录，比如：C:\Program Files\nssm

添加环境变量（可选）将nssm.exe所在目录添加到系统PATH环境变量中


使用NSSM注册Nginx服务图形界面方式
打开命令提示符，输入：

nssm install nginx


在弹出的图形界面中配置：


Path：填写nginx.exe的完整路径，如：C:\nginx\nginx.exe
Startup directory：填写nginx的安装目录，如：C:\nginx
Service name：服务名称，默认为”nginx”


切换到”Details”标签：


Display name：显示名称，如：”Nginx Web Server”
Description：描述信息，如：”Nginx HTTP and reverse proxy server”


切换到”Log on”标签：


选择”Local System account”（推荐）


切换到”I&#x2F;O”标签：


Output (stdout)：设置标准输出日志路径，如：C:\nginx\logs\service-stdout.log
Error (stderr)：设置错误输出日志路径，如：C:\nginx\logs\service-error.log


切换到”Exit actions”标签：


Restart：选择”Restart application”
Delay：设置重启延迟，如：30000（30秒）

命令行方式也可以使用命令行一次性完成配置：
nssm install nginx &quot;C:\nginx\nginx.exe&quot;nssm set nginx AppDirectory &quot;C:\nginx&quot;nssm set nginx DisplayName &quot;Nginx Web Server&quot;nssm set nginx Description &quot;Nginx HTTP and reverse proxy server&quot;nssm set nginx AppStdout &quot;C:\nginx\logs\service-stdout.log&quot;nssm set nginx AppStderr &quot;C:\nginx\logs\service-error.log&quot;nssm set nginx AppRestartDelay 30000

服务管理命令
启动服务

nssm start nginx


停止服务

nssm stop nginx


重启服务

nssm restart nginx


查看服务状态

nssm status nginx


移除服务

nssm remove nginx

常见问题处理
服务启动失败


检查程序路径是否正确
检查程序所需的依赖文件是否完整
查看Windows事件查看器中的错误日志
检查服务账户是否有足够权限


日志文件无法创建


确保日志目录存在
确保服务账户对日志目录有写入权限


程序异常退出不重启


检查Exit actions标签中的重启设置
适当调整重启延迟时间

最佳实践
路径设置


使用绝对路径
避免路径中包含空格，如果无法避免，请使用引号包围
建议将程序安装在系统盘之外的分区


日志管理


设置日志轮转，避免日志文件过大
定期清理旧日志
建议使用专门的日志目录


权限控制


使用最小权限原则
必要时创建专门的服务账户
定期检查和更新权限设置


监控告警


配置服务监控
设置异常重启告警
定期检查服务状态

总结NSSM是一个强大的Windows服务管理工具，通过它可以轻松地将普通程序转换为Windows服务，实现自动化运维。本文以Nginx为例介绍了NSSM的基本使用方法，希望对大家有所帮助。在实际应用中，还可以根据具体需求进行更多个性化配置。

本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>windows</tag>
        <tag>nssm</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>在Cursor中配置MCP</title>
    <url>/posts/0d8ec3487c1f.html</url>
    <content><![CDATA[什么是Cursor MCPMCP (Model Context Protocol) 是由Anthropic公司开发的协议，旨在让大型语言模型(LLM)能够安全地与外部工具和服务交互。Cursor作为先进的AI编程工具，集成了MCP功能，允许Cursor连接自定义AI模型服务器，扩展AI助手能力，连接第三方服务（GitHub、Jira等）。通过MCP，AI助手能够获取实时数据、执行特定操作，大大增强了其实用性和功能范围。
配置步骤1. 打开MCP配置界面
打开Cursor
点击左下角设置图标或使用快捷键 Ctrl+Shift+P（Windows）&#x2F;Cmd+Shift+P（Mac）
搜索并选择”Cursor Settings”
在左侧边栏选择”MCP”

2. 添加MCP服务器在MCP Servers部分，点击”Add new MCP server”，推荐配置以下三个服务器：
2.1 GitHub MCP Server
Name: GitHub
Type: command
Command: npx -y @modelcontextprotocol/server-github

主要功能：

文件操作：创建、更新文件，提交多个文件
仓库管理：搜索、创建仓库，管理分支
Issue和PR：创建、更新、评论Issue，管理PR
代码搜索：按语言、路径、文件类型搜索代码

配置GitHub令牌：

在GitHub创建个人访问令牌（需要repo权限）
使用环境变量： GITHUB_PERSONAL_ACCESS_TOKEN=你的令牌

2.2 Atlassian MCP Server
Name: Atlassian
Type: command
Command: uvx mcp-atlassian --confluence-url=https://your-domain.atlassian.net/wiki --confluence-username=你的邮箱 --confluence-token=你的令牌 --jira-url=https://your-domain.atlassian.net --jira-username=你的邮箱 --jira-token=你的令牌

先安装uvx：pip install uv
主要功能：

Confluence：搜索内容，获取&#x2F;创建&#x2F;更新&#x2F;删除页面
Jira：查询issue，创建&#x2F;更新任务，状态转换，添加工作日志
项目管理：链接issue到Epic，获取项目的所有issue

配置Atlassian令牌：

在 https://id.atlassian.com/manage/api-tokens 创建API令牌

2.3 Time MCP Server
Name: Time
Type: command
Command: npx -y @modelcontextprotocol/server-time

也可使用：uvx mcp-server-time
主要功能：

获取当前时间：获取特定时区或系统时区的当前时间
时区转换：在不同时区之间转换时间
自定义时区：支持设置默认时区（如--local-timezone=Asia/Shanghai）

3. 完整配置示例以下是三个MCP服务器的完整配置示例：
&#123;    &quot;mcpServers&quot;: &#123;        &quot;github&quot;: &#123;            &quot;command&quot;: &quot;npx&quot;,            &quot;args&quot;: [                &quot;-y&quot;,                &quot;@modelcontextprotocol/server-github&quot;            ],            &quot;env&quot;: &#123;                &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;: &quot;github token&quot;,                &quot;GITHUB_USERNAME&quot;: &quot;github username&quot;            &#125;        &#125;,        &quot;atlassian&quot;: &#123;            &quot;command&quot;: &quot;uvx&quot;,            &quot;args&quot;: [                &quot;mcp-atlassian&quot;            ],            &quot;env&quot;: &#123;                &quot;JIRA_URL&quot;: &quot;https://&lt;jira url&gt;&quot;,                &quot;JIRA_USERNAME&quot;: &quot;jira username&quot;,                &quot;JIRA_PERSONAL_TOKEN&quot;: &quot;jira token&quot;            &#125;        &#125;,        &quot;time&quot;: &#123;            &quot;command&quot;: &quot;uvx&quot;,            &quot;args&quot;: [                &quot;mcp-server-time&quot;,                &quot;--local-timezone=Asia/Shanghai&quot;            ]        &#125;    &#125;&#125;
windows下配置需要特别注意command需要全路径，如
&#123;    &quot;mcpServers&quot;: &#123;        &quot;github&quot;: &#123;            &quot;command&quot;: &quot;cmd&quot;,            &quot;args&quot;: [                &quot;/c&quot;,                &quot;npx -y @modelcontextprotocol/server-github&quot;            ],            &quot;env&quot;: &#123;                &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;: &quot;github token&quot;,                &quot;GITHUB_USERNAME&quot;: &quot;github username&quot;            &#125;        &#125;,        &quot;filesystem&quot;: &#123;            &quot;command&quot;: &quot;C:\\nodejs\\node.exe&quot;,            &quot;args&quot;: [                &quot;C:\\Users\\admin\\AppData\\Roaming\\npm\\node_modules\\@modelcontextprotocol\\server-filesystem\\dist\\index.js&quot;,                &quot;D:\\&quot;            ]        &#125;,        &quot;time&quot;: &#123;            &quot;command&quot;: &quot;C:\\Users\\admin\\.local\\bin\\uvx.exe&quot;,            &quot;args&quot;: [                &quot;mcp-server-time&quot;,                &quot;--local-timezone=Asia/Shanghai&quot;            ]        &#125;    &#125;&#125;

4. 验证配置添加服务器后，服务器状态应显示为绿色，表示连接成功。黄色状态时，尝试刷新或重启Cursor。
  
5. 使用MCP功能配置完成后：

在Agent模式下使用MCP功能
在聊天窗口指示AI使用MCP工具，例如”使用GitHub创建一个新仓库”
在提示时选择接受

常见问题
服务器显示黄色状态：尝试刷新或重启Cursor
命令执行失败：确认已安装Node.js，检查网络连接
工具未显示：确认使用Agent模式，重启Cursor
权限问题：确保API令牌正确且有足够权限

总结通过配置MCP，可以显著扩展Cursor功能，提高开发效率。推荐的三个服务器（GitHub、Atlassian和Time）满足开发者日常多种需求。
参考资料：

Cursor MCP官方文档
GitHub MCP Server文档
sooperset&#x2F;mcp-atlassian
Time MCP Server文档



本文由 AI 辅助生成，如有错误或建议，欢迎指出。 

]]></content>
      <tags>
        <tag>ai</tag>
        <tag>cursor</tag>
        <tag>mcp</tag>
      </tags>
  </entry>
  <entry>
    <title>在本地电脑部署AI大模型</title>
    <url>/posts/b7d35640b4b3.html</url>
    <content><![CDATA[软件：

ollama: https://ollama.com/
docker: https://www.docker.com/
open-webui: https://github.com/open-webui/open-webui

1. 下载安装ollama. https://ollama.com/download安装时，ollama默认安装在C盘，如果希望安装在其它盘，安装时可指定安装路径
.\OllamaSetup.exe /DIR=&#x27;D:\Program Files\Ollama&#x27;
ollama 使用
# ollama -hLarge language model runnerUsage:  ollama [flags]  ollama [command]Available Commands:  serve       Start ollama  create      Create a model from a Modelfile  show        Show information for a model  run         Run a model  stop        Stop a running model  pull        Pull a model from a registry  push        Push a model to a registry  list        List models  ps          List running models  cp          Copy a model  rm          Remove a model  help        Help about any command
ollama 也支持 Hugging Face 模型
# ollama run hf.co/&#123;用户名&#125;/&#123;模型仓库&#125;:&#123;标签&#125;# #如# ollama run hf.co/inclusionAI/Ming-Lite-Omni:latest
Ming-lite-omni 是 Ming-omni 的轻量版本，源自 Ling-lite，具有 28 亿个激活参数。它是一个统一的多模态模型，能够处理图像、文本、音频和视频，并在语音和图像生成方面表现出色。
2. 安装本地模型. https://ollama.com/search
PetrosStav&#x2F;gemma3-tools: 基于google gemma3模型，同时支持图片识别与tools。

deepseek-r1: 一个开放式推理模型家族，其性能接近O3和Gemini 2.5 Pro等领先模型。

qwen3: 来自Alibaba的千问模型，对中文支持比较好,支持tools, thinking。

qwen2.5-coder: 用于代码生成，代码推理及代码修复。

llama4: Meta公司最新推出的大型语言模型, 擅长文本摘要、情感分析、代码生成等任务，但有限支持中文。

gemma3: google开源的大语言模型，当前单卡最强的模型根据需求下载特定模型，及参数版本。参数量越大对电脑配置要求越好，也越聪明。普通7B参数量即可。



本地终端运行命令下载及运行大模型
# ollama run qwen2.5pulling manifestpulling 2bada8a74506... 100% ▕████████████████████████████████████████████████████████▏ 4.7 GBpulling 66b9ea09bd5b... 100% ▕████████████████████████████████████████████████████████▏   68 Bpulling eb4402837c78... 100% ▕████████████████████████████████████████████████████████▏ 1.5 KBpulling 832dd9e00a68... 100% ▕████████████████████████████████████████████████████████▏  11 KBpulling 2f15b3218f05... 100% ▕████████████████████████████████████████████████████████▏  487 Bverifying sha256 digestwriting manifestsuccess&gt;&gt;&gt; 中国有几个省截至2023年，中国的省级行政区划包括34个省份。具体划分如下：- 23个省：如广东、江苏、山东等；- 5个自治区：内蒙古自治区、广西壮族自治区、西藏自治区、宁夏回族自治区和新疆维吾尔自治区；- 4个直辖市：北京、天津、上海、重庆；- 2个特别行政区：香港特别行政区、澳门特别行政区；- 1个省级行政单位：海南省，其中三沙市为地级市。这种划分是中华人民共和国现行的省份及行政区划形式。
运行完成后，即可与模型对话。模型本地位置：默认保存在用户Home目录&#x2F;.ollama&#x2F;models(windows 为C:\Users&lt;用户名&gt;.ollama\models)，可通过设置环境变量 OLLAMA_MODELS更改模型的保存路径。
3. docker部署open web ui提供聊天界面，体验类似chat gpt.https://github.com/open-webui/open-webui
3.1 如果ollama 运行在本机：docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
3.2 如果ollama 运行在不同机器：docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=http://&lt;remote-server-ip&gt;:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
其中OLLAMA_BASE_URL为ollama server地址，本地默认为：127.0.0.1:11434
3.3 访问open-webui，通过浏览器与模型对话访问地址： http://localhost:3000 
  

open-webui中可以管理用户，同时注意将更新模型的可见性，默认是不可见的，即只有管理员可以看到。

4. 补充：open-webui库中支持一键运行ollama和open-webuihttps://raw.githubusercontent.com/open-webui/open-webui/refs/heads/main/docker-compose.yaml
docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama

实战MiGPT: 将小爱同学接入ollama本地大模型（也支持接入线上模型ChatGPT,豆包等），变得更智能。https://github.com/idootop/mi-gpt?tab=readme-ov-file
]]></content>
      <tags>
        <tag>ai</tag>
      </tags>
  </entry>
  <entry>
    <title>Sunshine &amp; Moonlight实现串流传输</title>
    <url>/posts/9dc2bd153b4d.html</url>
    <content><![CDATA[有时希望将家里的投影仪作为电脑显示器及音响，但是重新布线却很麻烦，这时使用串流传输就可以轻松解决。开源方案就是：Sunshine &amp; Moonlight，它可以实现低延迟、高画质的跨平台串流体验。
简介Sunshine简介Sunshine是一个开源的串流服务器软件,可以安装在Windows、Linux等系统上。它的主要功能包括:

捕获屏幕画面和音频
对画面进行编码压缩
通过网络传输给客户端
Sunshine支持多种编码格式,如H.264、HEVC等,可以根据网络状况自动调整画质。

Moonlight客户端Moonlight是与Sunshine配套的客户端软件,支持Windows、Android、iOS等多个平台。它可以接收Sunshine传输的画面,并进行解码显示。Moonlight的主要特点包括:

低延迟:延迟可低至20ms
高画质:支持4K 120FPS串流
跨平台:支持PC、手机、平板等设备
手柄支持:可使用手柄控制远程设备

部署与配置SunShine安装SunShine部署在远程被控制电脑上，下载对应系统的安装包，直接安装运行即可。Sunshine下载：https://github.com/LizardByte/Sunshine/releases  
配置
Configuration中可进行配置：如同时连接client数， 性能设置等。一般默认配置即可。  

Application中设置进入控制电脑后执行的命令，如设置Ryujinx.exe启动Switch模拟器。  

Pin页面，在控制端首次连接时进行认证校验。


Moonlight安装在控制端安装Moonlight App. 支持多种系统，ios&#x2F;Android&#x2F;Windows&#x2F;Linux&#x2F;Mac.Moonlight下载：https://moonlight-stream.org
配置
确保设备与被控制端在同一个局域网下（这是SunShine的默认配置，也可以将SunShine配置为公网可访问，需要先开通相关网络）。
打开Moonlight App，主机列表中会默认搜索到相关的主机，如果没有可通过输入IP添加。
第一次点击主机后，提示PIN码，进行SunShine管理界面，输入PIN码，匹配完成后，完成配对连接。  
点击主机，进入远程桌面。

]]></content>
      <tags>
        <tag>tool</tag>
        <tag>infra</tag>
      </tags>
  </entry>
  <entry>
    <title>Vault常见操作</title>
    <url>/posts/c21413747f24.html</url>
    <content><![CDATA[Hashicorp Vault 是一个基于身份的秘密和加密管理系统，用于严格控制访问各种机密数据，如 API 加密密钥、密码和证书。以下是 Vault 的一些常见使用操作。
Login$ export VAULT_ADDR=&#x27;http://your-vault-address:8200&#x27;$ export VAULT_TOKEN=&#x27;your-vault-token&#x27;


Secret操作
secret引擎KV
Pathregion&#x2F;cluster&#x2F;project&#x2F;application
Keydevops-api
fieldk-name
valuek-value


查看权限

$ vault token lookup

新建kv

$ vault secrets enable \    -path=/git-av \    -description &quot;k/v engine for the quickstart guide&quot; \    -version=1 \    kv

查看所有secrets

$ vault secrets list

创建或者更新key

$ vault kv put git-av/devops-api/k8s-test/test a=b


list

$ vault kv list git-av/devops-api/k8s-test/


添加字段

$ vault kv patch git-av/devops-api/k8s-test/test d=b

删除字段

$ vault kv patch git-av/devops-api/k8s-test/test d=null

查看key的内容

$ vault kv get git-av/devops-api/k8s-test/test

删除key

$ vault kv delete git-av/devops-api/k8s-test/test$ vault kv metadata delete git-av/kratos-api/k8s-test/test

删除kv secret

$ vault secrets disable git-av

查看版本列表

$ vault kv metadata get  git-av/devops-api/k8s-test/test

回滚到指定版本

$ vault kv rollback -version 6 git-av/devops-api/k8s-test/test

权限管理
创建policy

$ vault policy write jenkins-policy - &lt;&lt;EOFpath &quot;global/services/app/prod/kv/*&quot; &#123;  capabilities = [&quot;read&quot;]&#125;path &quot;global/services/app/stage/kv/*&quot; &#123;  capabilities = [&quot;read&quot;]&#125;EOF
policy中的path指的是key路径

查看 policy

$ vault policy read jenkins-policy

查看role

$ vault auth list$ vault list auth/approle/role$ vault read auth/approle/role/jenkins$ vault read auth/approle/role/jenkins/role-id

更新role key

$ vault write -f auth/approle/role/devops-readonly/secret-id


创建app role for jenkins5.1 enable approle

$ vault auth enable approle

5.2 policy
$ vault policy write jenkins-policy - &lt;&lt;EOFpath &quot;devops/data/jenkins/*&quot; &#123;  capabilities = [&quot;read&quot;]&#125;EOF

5.3 add approle
$vault write auth/approle/role/jenkins \    secret_id_ttl=0s \    token_num_uses=10 \    token_ttl=20m \    token_max_ttl=30m \    secret_id_num_uses=0 \    token_policies=&quot;jenkins-policy&quot;

当 secret_id_ttl&#x3D;0s，secret_id_num_uses&#x3D;0 时表示secret 永不过期5.4 更新secret_id_num_uses字段
vault write auth/approle/role/jenkins \    secret_id_num_uses=0
5.5 get key&#x2F;secretvault read auth&#x2F;approle&#x2F;role&#x2F;jenkins&#x2F;role-idvault write -f auth&#x2F;approle&#x2F;role&#x2F;jenkins&#x2F;secret-id
5.6 login with approle
vault write auth/approle/login \    role_id=&lt;role_id&gt; \    secret_id=&lt;secret_id&gt;export VAULT_TOKEN=&lt;vault_token&gt;

服务管理
vault初始化

$ vault operator init

封锁

vault operator seal

解封

vault operator unseal &lt;key1&gt;vault operator unseal &lt;key2&gt;vault operator unseal &lt;key3&gt;

生成新的解封key

vault operator rekey
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>vault</tag>
      </tags>
  </entry>
  <entry>
    <title>Lego签SSL证书</title>
    <url>/posts/f8f8c2afa37e.html</url>
    <content><![CDATA[HTTPS已经成为当今网站的标准配置,为网站提供加密和身份验证功能。但是申请和维护SSL证书一直是一个繁琐的过程。幸运的是,Lego这个强大的工具可以帮助我们轻松地自动化获取和更新Let’s Encrypt免费SSL证书。
Lego 简介Lego是一个用Go语言编写的Let’s Encrypt客户端,它可以帮助我们自动化完成SSL证书的申请、验证和更新过程。相比于其他工具,Lego的优势在于:

支持多种验证方式,包括HTTP和DNS验证
集成了众多DNS服务商的API,可以自动完成DNS验证
使用简单,只需几个命令即可完成证书申请
支持通配符证书

安装 Lego
mac

brew install lego

docker

docker run goacme/lego -h

签发rout53证书
配置AWS密钥

export AWS_ACCESS_KEY_ID=xxxexport AWS_SECRET_ACCESS_KEY=xxxexport AWS_REGION=us-east-1

生成ssl证书

lego --email xhuaustc@gmail.com --domains jenkins.douhua.com --dns route53 --accept-tos --dns.disable-cp  --path ~/.lego/jenkins run
证书将保存在 ~&#x2F;.lego&#x2F;jenkins 目录下。

其中email用来接收证书快过期时的提示
lego支持多种dns provider, 如：阿里云，腾讯云，Route53等
对于不同的证书，需要指定不同的path目录


可生成泛域名证书，以腾讯云为例

export TENCENTCLOUD_SECRET_ID=your_secret_idexport TENCENTCLOUD_SECRET_KEY=your_secret_keylego --email xhuaustc@gmail.com --domains douhua.com --domains *.douhua.com --dns tencentcloud --accept-tos --dns.disable-cp  --path ~/.lego/tencent/jenkins run

对于一些java应用需要jks证书
将ssl证书转为jks

openssl pkcs12 -export -in domain.crt -inkey domain.key -out certificate.p12 -name jenkinskeytool -importkeystore -srckeystore certificate.p12 -srcstoretype PKCS12 -destkeystore jenkins.douhua.com.jks -deststoretype JKS
过程中需要输入密码。密码也可以通过命令行指定
openssl pkcs12 -export -in domain.crt -inkey domain.key -out certificate.p12 -name &quot;jenkins&quot; -password pass:yourpasswordkeytool -importkeystore -srckeystore certificate.p12 -srcstoretype PKCS12 -destkeystore jenkins.douhua.com.jks -deststoretype JKS -srcstorepass yourpassword -deststorepass yourpassword

验证jks

keytool -list -v -keystore jenkins.douhua.com.jks

jenkins中配置证书

$ cat /etc/jenkins/jenkins.confJENKINS_WAR=/opt/jenkins/jenkins.warJENKINS_HOME=/opt/jenkins/dataJAVA_ARGS=&quot;-Djava.awt.headless=true \  -Djava.net.preferIPv4Stack=true \  -XX:+AlwaysPreTouch \  -XX:+UseG1GC \  -Xms8192m \  -Xmx8192m \  -Xss127m \  -Xlog:gc*=debug:file=gclog.log:utctime,level,tags:filecount=9,filesize=1M&quot;JENKINS_ARGS=&quot;--httpPort=-1 --httpsPort=8800  --logfile=/var/log/jenkins/jenkins.log \  --httpsKeyStore=/opt/jenkins/jenkins.douhua.com.jks \  --httpsKeyStorePassword=xxxxxx \  --useJmx&quot;$ /usr/bin/java $JAVA_ARGS -jar $JENKINS_WAR $JENKINS_ARGS

更新route53证书当证书有效期在30天以内，可以使用renew更新证书
lego --email xhuaustc@gmail.com --domains jenkins.douhua.com --dns route53 --accept-tos --dns.disable-cp  --path ~/.lego/jenkins renew
]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>selenium+Xvfb+ffmpeg实现服务器录屏</title>
    <url>/posts/0b2e9fb9bf3f.html</url>
    <content><![CDATA[在现代软件开发中，自动化测试变得越来越重要，而录屏功能则为测试结果的验证提供了直观的方式。通过结合Selenium、Xvfb和FFmpeg这三个强大的工具，我们可以在服务器上实现无头(headless)的录屏环境，这对于没有图形界面的服务器尤其有用。
Selenium是一个自动化测试工具，它可以模拟用户对网页的操作，如点击、输入文本等。Xvfb（X虚拟帧缓冲）提供了一个虚拟的显示环境，使得我们可以在不需要实际显示器的情况下运行图形应用程序。FFmpeg是一个强大的多媒体框架，能够处理视频和音频的录制、转换和流式传输。
将这三个工具结合起来，我们可以创建一个自动化的录屏流程，这个流程可以在后台运行，录制Selenium驱动的浏览器会话。这样，无论是进行自动化测试验证，还是生成用户操作教程，都可以通过这种方式来实现。
例如，我们可以使用Selenium启动一个浏览器会话，然后用Xvfb创建一个虚拟的显示环境来捕获这个会话的屏幕。接着，使用FFmpeg开始录制屏幕内容，直到测试完成。这个过程完全自动化，可以集成到持续集成&#x2F;持续部署（CI&#x2F;CD）的流程中，提高开发效率和质量。
环境准备
安装软件

# 安装浏览器wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.debdpkg -i google-chrome-stable_current_amd64.deb# 下载 chromedrive，选择合适的版本# https://googlechromelabs.github.io/chrome-for-testing/wget https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.105/linux64/chromedriver-linux64.zipunzip chromedriver-linux64.zipmv chromedriver-linux64/chromedriver /usr/bin/# 安装xvfb, ffmpegsudo apt install  -y xvfb ffmpeg xrandr# 安装selenium packagepip3 install selenium screeninfo # screeninfo 可以获取屏幕分辨率，为录屏做准备
Xvfb开启虚拟桌面
后台运行Xvfb开启虚拟桌面，终端将会默认在该桌面运行。此时能成功使用selenium控制浏览器

Xvfb -ac :99 -screen 0 1024x768x16 &amp;export DISPLAY=:99xrandr

使用xvfb-run开户虚拟桌面

xvfb-run -a --server-args=&quot;-screen 0 1920x1080x24&quot; xrandr # 在虚拟桌面下运行xrandr,运行完毕后，自动关闭桌面。可通过添加server参数 --server-args=&quot; -displayID 99&quot;指定display id.
-a : 自动使用不重复的DisplayId, 在运行的脚本中，可使用环境变量$DISPLAY来获取桌面ID，如：”:99”
使用selenium 控制浏览器不同浏览器可设置不同的参数，以chrom浏览器为例：
# 初始化driverfrom selenium import webdriverdef local_chrome_driver():    option = webdriver.ChromeOptions()    option.add_experimental_option(&#x27;useAutomationExtension&#x27;, False)    option.add_experimental_option(        &quot;excludeSwitches&quot;, [&#x27;enable-automation&#x27;])    option.add_argument(&quot;--disable-infobars&quot;)    option.add_argument(&quot;--guest&quot;)    option.add_argument(&quot;--log-level=3&quot;)    driver = webdriver.Chrome(options=option)    driver.maximize_window()    return driver
以上参数设置将便得浏览器打开更干净。获取driver后，便可通过selenium代码对其进行控制了。
使用ffmpeg开启录屏不同的操作系统使用不同的参数,例如：开始录制20s视频

windows

ffmpeg -hide_banner -f gdigrab -i desktop -t 20 -f mp4 out.mp4

Mac

ffmpeg -hide_banner -f avfoundation -i &#x27;1:none&#x27; -t 20 -r 30 -y -f mp4 out.mp4
其中’1:none’为显示器与音频的编号。可通过命令ffmpeg -f avfoundation -list_devices true -i &quot;&quot;查看3. Linux
ffmpeg -hide_banner -video_size &#123;width&#125;x&#123;height&#125; -f x11grab -i &#123;os.getenv(&quot;DISPLAY&quot;)&#125; -t 20 -vcodec h264 -y -f mp4 out.mp4
录屏完整代码如下：
import subprocessimport platformfrom screeninfo import get_monitorsdef record_screen_with_ffmpeg(self, duration, output_path):    ffmpeg_cmd = &quot;ffmpeg&quot;    platform_os = platform.system().lower()    if platform_os == &#x27;windows&#x27;:        ffmpeg_format = &quot;-f gdigrab -i desktop -t &#123;duration&#125;&quot;    elif platform_os == &#x27;darwin&#x27;:        ffmpeg_format = f&quot;-f avfoundation -i &#x27;1:none&#x27; -t &#123;duration&#125; -r 30 -y &quot;    elif platform_os == &#x27;linux&#x27;:        monitors = get_monitors()        monitor = monitors[0]        width = monitor.width        height = monitor.height        display_number = os.getenv(&quot;DISPLAY&quot;)        ffmpeg_format = f&quot; -video_size &#123;width&#125;x&#123;height&#125; -f x11grab -i &#123;display_number&#125; -t &#123;duration&#125; -vcodec h264 -y &quot;    else:        raise Exception(&quot;Unsupported platform&quot;)    command = f&quot;&#123;ffmpeg_cmd&#125; -hide_banner &#123;ffmpeg_format&#125; -f mp4 &#123;output_path&#125;&quot;    print(f&quot;start record screen: &#123;command&#125;&quot;)    subprocess.call(command, shell=True, stdin=subprocess.PIPE)    print(f&quot;completed record. save mp4 file to &#123;output_path&#125;&quot;)
补充ffmpeg工具
ffmpeg 工具可以方便对视频进行压缩, vcodec可选择编码格式，其中编码名中有nv字符串，则会利用nvidia gpu加速压缩，

## cpu 编码ffmpeg -i ./source.mp4 -vcodec libx264 -acodec aac dest.mp4ffmpeg -i ./source.mp4 -vcodec libx265 -acodec aac dest.mp4## gpu 编码ffmpeg -i ./source.mp4 -vcodec h264_nvenc -acodec aac dest.mp4ffmpeg -i ./source.mp4 -vcodec hevc_nvenc -acodec aac dest.mp4



-vcodec 参数
含义说明



copy
复制原始视频流解码器，不重新编码。


libx264
将视频压缩为H.264格式。


libx265
将视频压缩为H.265&#x2F;HEVC格式。


h264_nvenc
使用NVIDIA GPU硬件加速进行H.264编码。


hevc_nvenc
使用NVIDIA GPU硬件加速进行H.265&#x2F;HEVC编码。


h264_qsv
使用Intel Quick Sync Video硬件加速进行H.264编码。


hevc_qsv
使用Intel Quick Sync Video硬件加速进行H.265&#x2F;HEVC编码。


libvpx
将视频压缩为VP8或VP9格式（通过-b:v指定压缩比特率）。


huffyuv
生成无损视频。



ffmpeg 可对mp4文件进行diff

## 生成一个差值视频ffmpeg -i video1.mp4 -i video2.mp4 -filter_complex &quot;[0:v][1:v]blend=all_mode=&#x27;difference&#x27;&quot; -y diff.mp4## 计算视频相似度，结果存放在diff_ssim.logffmpeg -i video1.mp4 -i video2.mp4 -filter_complex &quot;[0:v][1:v]ssim=diff_ssim.log&quot; -f null -
对比前需确保两个视频的分辨率和帧率相同，否则FFmpeg可能无法正确处理。如果两个视频的规格不一致，需要先对其进行转码，使它们匹配后再进行差异比较。
使用 x11vnc 远程访问服务器应用界面
安装x11vnc在服务器上安装 x11vnc。可以通过以下命令进行安装

sudo apt install x11vnc -y

启动 Xserver启动一个虚拟的 Xserver。使用 Xvfb 可以创建一个虚拟显示器

Xvfb -ac :99 -screen 0 1024x768x16

设置x11vnc密码为了确保连接的安全性，您需要为 x11vnc 设置一个访问密码。运行以下命令来设置密码

x11vnc -storepasswd
默认情况下，密码会保存在 ~&#x2F;.vnc&#x2F;passwd 文件中。4. 启动 x11vnc启动 x11vnc 并指定使用虚拟显示器 :99：
x11vnc -display :99 -rfbport 5900 -forever -shared -rfbauth ~/.vnc/passwd
这将使 x11vnc 在虚拟显示器 :99 上运行，并通过端口 5900 提供 VNC 服务。5. 在虚拟显示器中运行 Google Chrome为了在远程访问中看到浏览器界面，您需要在虚拟显示器 :99 中启动 Google Chrome
export DISPLAY=:99google-chrome

通过 VNC 客户端访问 VNC 服务器使用常规的 VNC 客户端连接到您的 VNC 服务器。输入服务器的 IP 地址和端口号（例如 192.168.1.100:5900），您将能够看到并控制服务器上的 Google Chrome 界面。

总结通过Selenium、Xvfb和FFmpeg的结合使用，我们可以在服务器上实现高效、灵活的自定义录屏解决方案，为软件开发和测试提供了极大的便利。
]]></content>
      <tags>
        <tag>devops</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title>常用的Tcpdump命令</title>
    <url>/posts/12c881755833.html</url>
    <content><![CDATA[
监控某一网络接口的数据包

tcpdump -i eth0


查看dns解析的包 port domain

tcpdump -i eth0 -nt  -s 500 port domain
-t 不显示时间-n 不会将ip显示为hostname

过滤IP

tcpdump -i eth0 host 202.22.22.22tcpdump -i eth0 src host 202.22.22.22  ## 指定源地址tcpdump -i eth0 dst host 202.22.22.22  ## 指定目的地址tcpdump -i eth0 host 202.22.22.22 and \(202.22.22.21 or 202.22.22.20 \)  ## 获取202.22.22.22 与 202.22.22.21或202.22.22.20之间的包tcpdump -i eth0 host 202.22.22.22 and !202.22.22.21  ## 获取202.22.22.22 与 除了202.22.22.21 之外的ip之间的包


过滤端口 port

tcpdump -i eth0 port 22tcpdump -i eth0 src port 22  ## 指定源端口tcpdump -i eth0 dst port 22  ## 指定目的端口

过滤协议

tcpdump -i eth0 arptcpdump -i eth0 iptcpdump -i eth0 tcptcpdump -i eth0 udptcpdump -i eth0 icmp

参考文章：
tcpdump高级过滤
]]></content>
      <tags>
        <tag>devops</tag>
        <tag>infra tools</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins Pipeline局部变量</title>
    <url>/posts/85b6f42d9df2.html</url>
    <content><![CDATA[变量未定义在编写Jenkinsfile时，经常会遇到变量问题，明明已经定义过了，但是函数是就是无法读取。例如：
def some_var = &quot;some value&quot;def pr() &#123;    def another_var = &quot;another &quot; + some_var    echo &quot;$&#123;another_var&#125;&quot;&#125;pipeline &#123;    agent any    stages &#123;        stage (&quot;Run&quot;) &#123;            steps &#123;                pr()            &#125;        &#125;    &#125;&#125;
将获得如下错误：
groovy.lang.MissingPropertyException: No such property: some_var for class: groovy.lang.Binding
在该例子中明明开始已经定义了dome_var变量，但是在pr函数中却无法引用。
原因
在main script中使用def定义的变量并不能被其它函数引用， def some_var &#x3D; “some value”
在main script中不使用def定义的变量，能够被其它函数引用, some_var &#x3D; “some value”
使用def定义的变量，添加@Field标注手，能够被同一个script中的其它函数引用，     import groovy.transform.Field@Fielddef some_var = &quot;some value&quot;
这是因为在groovy编译该script时，会将脚本中的内容包含在一个class中，内容大致如下：

class Script1 &#123;    def pr() &#123;        def another_var = &quot;another &quot; + some_var        echo &quot;$&#123;another_var&#125;&quot;    &#125;    def run() &#123;        def some_var = &quot;some value&quot;        pipeline &#123;            agent any            stages &#123;                stage (&quot;Run&quot;) &#123;                    steps &#123;                        pr()                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;
从以上脚本中看到，some_var 并不在pr函数的范围中，所以无法识别该变量。
如果在script中定义变量不带def的话，groovy编译时，会将该变量保存到groovy.lang.Binding中，这样的话，groovy的任意函数均可访问到该变量，但这里存在一个风险，就是会被覆盖，因为Binding是全局的。
最后使用@Field方式，将变量定义为该script下的变量，在该例子中，如果变量some_var添加了@Field标注，则script会被编译为：
class Script1 &#123;    def some_var = &quot;some value&quot;    def pr() &#123;        def another_var = &quot;another &quot; + some_var        echo &quot;$&#123;another_var&#125;&quot;    &#125;    def run() &#123;        pipeline &#123;            agent any            stages &#123;                stage (&quot;Run&quot;) &#123;                    steps &#123;                        pr()                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;
此时some_var变量做为该Script下的成员变量，该Script下的函数均可访问到。
参考资料Strange variable scoping behavior in Jenkinsfile
]]></content>
      <tags>
        <tag>devops</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx stream代理多个网站</title>
    <url>/posts/9c7df2b819b4.html</url>
    <content><![CDATA[在一些特殊场景下，尤其是多网络区的环境中，难免使用proxy来代理请求。其中HTTPS会提供更安全的数据传输，同时也添加了代理配置的复杂性（特别是证书的管理）。
通过nginx ngx_stream_ssl_preread_module模块，可以使用四层代理代理不同的服务。它允许从 ClientHello 消息中提取信息，而不会终止 SSL&#x2F;TLS，例如提取通过 SNI 请求的服务器名称并保存在变量$ssl_preread_server_name中，于是我们可以通过对该变量进行转发请求到对应的上游服务。
配置示例如下：
proxy 1配置：
stream &#123;    server &#123;        listen      443;        resolver 8.8.8.8;        proxy_pass  10.0.0.4:10443;        ssl_preread on;    &#125;&#125;


proxy 2 配置 
stream &#123;    map $ssl_preread_server_name $name &#123;        baidu.com     prod;        stg.baidu.com     stage;    &#125;    upstream prod &#123;        server baidu.com:443;    &#125;    upstream stage &#123;        server stg.baidu.com:443;    &#125;    server &#123;        listen      10443;        resolver 8.8.8.8;        proxy_pass  $name;        ssl_preread on;    &#125;&#125;
做好域名解析后，在本地curl&#x2F;浏览器访问资源，即可获得对应的资源。
curl https://baidu.com
参考文档http://nginx.org/en/docs/stream/ngx_stream_ssl_preread_module.html: ngx_stream_ssl_preread_module 模块（1.11.5）允许从 ClientHello 消息中提取信息，而不会终止 SSL&#x2F;TLS，例如提取通过 SNI 请求的服务器名称。默认情况下不构建此模块，您可以在构建时使用 –with-stream_ssl_preread_module 配置参数启用此模块。
]]></content>
      <tags>
        <tag>nginx</tag>
        <tag>infra</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible kubernetes.core 模块的使用</title>
    <url>/posts/8742f1edd9f5.html</url>
    <content><![CDATA[介绍Ansible Guide: https://docs.ansible.com/ansible/latest/collections/kubernetes/core/index.html#description
code: https://github.com/ansible-collections/kubernetes.core
模块列表：https://github.com/ansible-collections/kubernetes.core#modules
kubernetes.core可以自动化管理kubernetes或openshift集群的应用与资源对象，以及集群的运维管理。
该collection环境要求：kubernetes &gt;&#x3D; 1.19python &gt;&#x3D; 3.6ansible &gt;&#x3D; 2.9.17
Moduleskubernetes.core collection包含有多个模块插件：inventory plugin, lookup plugin, connection plugin, K8s filter plugin以及多个Model



Name
Description



kubernetes.core.helm
Manages Kubernetes packages with the Helm package manager


kubernetes.core.helm_info
Get information from Helm package deployed inside the cluster


kubernetes.core.helm_plugin
Manage Helm plugins


kubernetes.core.helm_plugin_info
Gather information about Helm plugins


kubernetes.core.helm_pull
download a chart from a repository and (optionally) unpack it in local directory.


kubernetes.core.helm_repository
Manage Helm repositories.


kubernetes.core.helm_template
Render chart templates


kubernetes.core.k8s
Manage Kubernetes (K8s) objects


kubernetes.core.k8s_cluster_info
Describe Kubernetes (K8s) cluster, APIs available and their respective versions


kubernetes.core.k8s_cp
Copy files and directories to and from pod.


kubernetes.core.k8s_drain
Drain, Cordon, or Uncordon node in k8s cluster


kubernetes.core.k8s_exec
Execute command in Pod


kubernetes.core.k8s_info
Describe Kubernetes (K8s) objects


kubernetes.core.k8s_json_patch
Apply JSON patch operations to existing objects


kubernetes.core.k8s_log
Fetch logs from Kubernetes resources


kubernetes.core.k8s_rollback
Rollback Kubernetes (K8S) Deployments and DaemonSets


kubernetes.core.k8s_scale
Set a new size for a Deployment, ReplicaSet, Replication Controller, or Job.


kubernetes.core.k8s_service
Manage Services on Kubernetes


kubernetes.core.k8s_taint
Taint a node in a Kubernetes&#x2F;OpenShift cluster


准备环境查看当前支持的inventory插件
❯ ansible-doc -t inventory -l | grep kuberneteskubernetes.core.k8s                                     Kubernetes (K8s) in...
判断是否安装了kubernetes.core collection
❯ ansible-galaxy  collection list | grep kuberneteskubernetes.core               2.4.0
如果没有的话，需要安装
❯ ansible-galaxy collection install kubernetes.core

配置inventorykubernetes.core.k8s inventory支持多种方式配置，如token, kubeconfig
❯ cat k8s.yamlplugin: kubernetes.core.k8sconnections:  - kubeconfig: /Users/mpan/.kube/config# Authenticate with token, and return all pods and services for all namespaces---plugin: kubernetes.core.k8sconnections:  - host: https://192.168.64.4:8443    api_key: xxxxxxxxxxxxxxxx    validate_certs: false# Use default config (~/.kube/config) file and active context, and return objects for a specific namespaceplugin: kubernetes.core.k8sconnections:  - namespaces:    - testing# Use a custom config file, and a specific context.plugin: kubernetes.core.k8sconnections:  - kubeconfig: /path/to/config    context: &#x27;awx/192-168-64-4:8443/developer&#x27;

配置完成inventory后，可以查看inventory内容
❯ ansible-inventory -i k8s.yaml --list &gt; out
该inventory将根据namespace, 及label信息等为k8s中的资源创建了很多组，方便在运行具体tasks时进行选择。同时为pod默认配置了连接方式 &quot;ansible_connection&quot;: &quot;kubernetes.core.kubectl&quot;,
运行ansible playbookansible将通过kubectl工具，在相关的pod中运行指定的模块，其中pod就相当于一台机器，它必须安装有python环境，才能正常运行模块。
---- hosts: namespace_blossom_pods  gather_facts: false  tasks:    - name: debug info      debug: msg=&quot;&#123;&#123; container_state &#125;&#125;&quot;      when: container_state == &quot;Running&quot;        - name: Execute a command      shell: echo &quot;Hello&quot;      when: container_state == &quot;Running&quot;
运行 playbook
ansible-playbook -i k8s.yaml a.yaml

使用kubernetes.core.k8s_exec模块在指定的pod中运行命令- hosts: localhost  gather_facts: false  tasks:  - name: Execute a command    kubernetes.core.k8s_exec:      namespace: myproject      pod: sample-pod      command: echo &quot;hello&quot;      kubeconfig: /path/to/config
运行Playbook
❯ ansible-playbook main.yaml[WARNING]: No inventory was parsed, only implicit localhost is available[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match&#x27;all&#x27;PLAY [localhost] *******************************************************************************************************TASK [Execute a command] ***********************************************************************************************[DEPRECATION WARNING]: The &#x27;return_code&#x27; return key is being renamed to &#x27;rc&#x27;. Both keys are being returned for now toallow users to migrate their automation. This feature will be removed from kubernetes.core in version 4.0.0.Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.changed: [localhost]PLAY RECAP *************************************************************************************************************localhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
对于其它模块可以参考它的说明，配置相关的参数。
]]></content>
      <tags>
        <tag>devops</tag>
        <tag>ansible</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Bash中的换行</title>
    <url>/posts/884e03ef9a57.html</url>
    <content><![CDATA[换行在平常的shell编程中是经常遇到的，但是有时也会忽略掉一些问题。本篇中将会介绍多种方式实现输出换行的方法。
使用 echoecho 自带换行echo 命令输出字符串，在最后后会添加一个换行
root@Michael:~# echo hello worldhello worldroot@Michael:~#
添加 -n 可以禁用echo最后的换行
root@Michael:~# echo -n hello worldhello worldroot@Michael:~#
换行符”\n”但是当我们使用bash执行以下命令时，发现它并没有换行
root@Michael:~#  bash -c &quot;echo \&quot;\n\&quot;&quot;\nroot@Michael:~#
而是需要加一个 -e
root@Michael:~# bash -c &quot;echo -e \&quot;\n\&quot;&quot;root@Michael:~#
从echo的说明中可以看到 -e 指的是让转义符生效，其中有：
If -e is in effect, the following sequences are recognized:     \\     backslash     \a     alert (BEL)     \b     backspace     \c     produce no further output     \e     escape     \f     form feed     \n     new line     \r     carriage return     \t     horizontal tab     \v     vertical tab     \0NNN  byte with octal value NNN (1 to 3 digits)     \xHH   byte with hexadecimal value HH (1 to 2 digits)
另外可以在字符串前加 $ 符号
root@Michael:~# echo $&#x27;hello\nworld&#x27;helloworldroot@Michael:~#
使用echo多行模式上例子
root@Michael:~# echo &quot;&quot;&quot;helloworld&quot;&quot;&quot;helloworldroot@Michael:~#
使用”””符号，包裹着多行字符串，实现多行输出。但是这种方式有个问题，即当字符串中存在变量时，会将变量先进行解析，如
echo &quot;&quot;&quot;hello$hello&quot;&quot;&quot;helloroot@Michael:~#
而有的时候我们希望是保留原始字符串$hello。这时可以在变量$前加上转义符$
root@Michael:~# echo &quot;&quot;&quot;hello\$hello&quot;&quot;&quot;hello$helloroot@Michael:~#
也可以使用三个单引号’’’替换三个双引号”””
root@Michael:~# echo &#x27;&#x27;&#x27;hello&gt; $hello&gt; &#x27;&#x27;&#x27;hello$helloroot@Michael:~#
使用cat命令输出多行root@Michael:~# cat &lt;&lt;EOF&gt; hello&gt; world&gt; EOFhelloworld
与echo一样，如果字符串中存在变量，该方式同样会解析变量。这时可以在第一个EOF两边添加引号，如
root@Michael:~# cat &lt;&lt;&#x27;EOF&#x27;$helloworldEOF$helloworldroot@Michael:~#
使用printf使用printf打印字符串中的\n换行符，单引号，双引号均可
root@Michael:~# printf &quot;hello\nworld\n&quot;helloworldroot@Michael:~# printf &#x27;hello\nworld\n&#x27;helloworldroot@Michael:~#
对于变量问题，与echo一样，可以为$添加转义符\$, 或者使用单引号。如：
root@Michael:~# printf &#x27;hello \n$hello\n&#x27;hello $helloroot@Michael:~#]]></content>
      <tags>
        <tag>devops</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenResty</title>
    <url>/posts/7a9b103dcaf6.html</url>
    <content><![CDATA[什么是OpenRestyOpenResty是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。可用于实现：

路由控制
高并发入口
动态服务降级
动态负载均衡
WAF应用防火墙

安装OpenResty
二进制安装

[root@localhost ~]# yum-config-manager --add-repo https://openresty.org/yum/cn/centos/OpenResty.repo[root@localhost ~]# yum install openresty -y[root@localhost ~]# systemctl start openresty


Docker 安装

docker run -e KEY_ENV --name proxy --rm -p 443:443 -p 80:80 -v ~/openresty/:/etc/nginx/ docker.io/openresty/openresty:alpine /etc/nginx/startup.sh

Openresty生命周期  

init_by_lua：master-initing阶段，初始化全局配置或模块
init_worker_by_lua：worker-initing阶段，初始化进程专用功能
ssl_certificate_by_lua：ssl阶段，在“握手”时设置安全证书
set_by_lua：rewrite阶段，改写Nginx变量
rewrite_by_lua：rewrite阶段，改写URI，实现跳转&#x2F;重定向
access_by_lua：access阶段，访问控制或限速
content_by_lua：content阶段，产生响应内容
balancer_by_lua：balancer阶段，反向代理时选择upstream
header_filter_by_lua：header_filter阶段，加工处理响应头,处理response header
body_filter_by_lua：body_filter阶段，加工处理响应体
log_by_lua：log阶段，记录日志或其他的收尾工作

通常指定有三种模式：

xx_by_lua: 执行字符串形式的Lua代码
xx_by_lua_block: 功能相同，但指令后是｛…｝的Lua代码块
xx_by_lua_file: 功能相同，但执行磁盘上的源码文件

Openresty 模块Openresty组件：http://openresty.org/cn/components.html社区模块：https://opm.openresty.org/packages可使用opm工具管理openresty模块。
# show usageopm --help# search package names and abstracts with the user pattern &quot;lock&quot;.opm search lock# search package names and abstracts with multiple patterns &quot;lru&quot; and &quot;cache&quot;.opm search lru cache# install a package named lua-resty-foo under the name of some_authoropm get some_author/lua-resty-foo# get a list of lua-resty-foo packages under all authors.opm get lua-resty-foo# show the details of the installed package specified by name.opm info lua-resty-foo# show all the installed packages.opm list# upgrade package lua-resty-foo to the latest version.opm upgrade lua-resty-foo# update all the installed packages to their latest version.opm update# uninstall the newly installed packageopm remove lua-resty-foo
例如，下载ledgetech&#x2F;lua-resty-http模块：
4576e3cdb5be:/usr/local/openresty/# opm get ledgetech/lua-resty-http* Fetching ledgetech/lua-resty-http  Downloading https://opm.openresty.org/api/pkg/tarball/ledgetech/lua-resty-http-0.17.1.opm.tar.gz  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed100 20622  100 20622    0     0  40595      0 --:--:-- --:--:-- --:--:-- 40594Package ledgetech/lua-resty-http 0.17.1 installed successfully under /usr/local/openresty/site/ .
Openresty 共享数据使用ngx.shared实现不同请求间的数据共享

在nginx.conf配置http模块下初始化共享块,并加载自定义lua模块

http &#123;    lua_shared_dict my_cache 1m;    lua_package_path &quot;/etc/nginx/lua/?.lua;;&quot;;&#125;

新建cache模块在&#x2F;etc&#x2F;nginx&#x2F;lua&#x2F;cache.lua中新建cache模块

local _M = &#123;&#125;function _M.get(key)    local cache_ngx = ngx.shared.my_cache    local value = cache_ngx:get(key)    return valueendfunction _M.set(key, value, exptime)    if not exptime then        exptime = 0    end    local cache_ngx = ngx.shared.my_cache    local succ, err, forcible = cache_ngx:set(key, value, exptime)    return succendreturn _M

在业务逻辑中使用cache模块

local cache = require(&quot;cache&quot;)local cacheExpireTime = 1800local maglevToken = &quot;&quot;cache.set(&quot;MaglevToken&quot;, maglevToken, cacheExpireTime)maglevToken = cache.get(&quot;MaglevToken&quot;)
Openresty 调用外部接口使用resty.http模块: https://github.com/ledgetech/lua-resty-http
local httpc = require(&quot;resty.http&quot;).new()    local uri = &quot;https://&quot; .. appHostIP .. &quot;/data.json&quot;    local res, err = httpc:request_uri(uri, &#123;        method = &quot;GET&quot;,        ssl_verify = false,        headers = &#123;            [&quot;Content-Type&quot;] = &quot;application/x-www-form-urlencoded&quot;,            [&quot;Host&quot;] = appHostName,        &#125;    &#125;)    data = json.decode(res.body)    print(data.data)

Lua 语法Lua 是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放， 其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。w3cschool lua基本语法lua参考手册
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Github使用技巧</title>
    <url>/posts/fad4bf87987c.html</url>
    <content><![CDATA[快捷键Github快捷键官方文档

S, 快速搜索
T, 搜索当前项目文件
L, 跳转文件行号
B, 查看文件历史改动记录
., 代码库将会在vscode online中打开，方便代码阅读
Ctrl + K, 命令面板

高级搜索
仓库搜索限定符Github上搜索仓库官方说明go in:name star:&gt;&#x3D;1000 pushed:&gt;2022-06-01 language:gojenkins in:readme 
in 限定符范围：
in:name, 	jquery in:name 匹配名称中带有“jquery”的存储库。
in:description, jquery in:name,description 匹配名称或说明中带有“jquery”的存储库。
in:topics, jquery in:topics 将带“jquery”标签的存储库匹配为主题。
in:readme, jquery in:readme 匹配自述文件中提及“jquery”的存储库。
repo:owner&#x2F;name, repo:octocat&#x2F;hello-world 匹配特定的存储库名称。

followers 关注度限定符
followers:&gt;&#x3D;n, node followers:&gt;&#x3D;10000 匹配有 10,000 名或更多关注者提及“node”一词的存储库。
followers:n..n, styleguide linter followers:1..10 匹配有 1 至 10 名关注者提及“styleguide linter”一词的存储库。

stars 按照标星限定符
stars:n, stars:500 匹配正好有 500 个星标的存储库。
stars:n..n size:&lt;n, stars:10..20 size:&lt;1000 匹配有 10 到 20 个星标、小于 1000 KB 的存储库。
stars:&gt;&#x3D;n fork:true language:LANGUAGE, stars:&gt;&#x3D;500 fork:true language:php 匹配至少有 500 个星标（包括用 PHP 编写的分支星标）的存储库。

created, pushed 时间限定符
created:&lt;YYYY-MM-DD,	webos created:&lt;2011-01-01 匹配具有 2011 年之前创建的“webos”一词的存储库。
pushed:&gt;YYYY-MM-DD, css pushed:&gt;2013-02-01 匹配具有在 2013 年 1 月之后推送到其中的“css”一词的存储库。
pushed:&gt;&#x3D;YYYY-MM-DD fork:only, case pushed:&gt;&#x3D;2013-03-06 fork:only 匹配在 2013 年 3 月 6 日或之后将“case”一词推送到其中的存储库（即分支）。

language 语言限定符
language:LANGUAGE, rails language:javascript 匹配具有以 JavaScript 编写的“rails”一词的存储库。

topics 主题限定符
topic:TOPIC, topic:jekyll 匹配按“Jekyll”主题分类的存储库。

代码搜索语法GitHub 代码搜索语法
一个没有限定符的裸词将与文件的内容或文件的路径匹配
http-push匹配文件名或文本内容中带有http-push的文件

输入多个用空格分隔的术语，以搜索满足这两个字词的文档
sparse index匹配文件或内容中包含有sparse和index两个词的文件，不管顺序
&#x2F;sparse.*index&#x2F;使用&#x2F;…&#x2F;支持正则表达式，如果正则表达式中&#x2F;需要用/表示。

完全匹配
“name &#x3D; &quot;tensorflow&quot;“完全匹配name&#x3D;”tensorfow”的文本

运算符 AND&#x2F;OR&#x2F;NOT
“fatal error” NOT path:testing文件或者内容中包含fatal error,同时排除__testing__目录中的文件 
(language:ruby OR language:python) AND NOT path:”&#x2F;tests&#x2F;“语言为ruby或者python,并且排除&#x2F;tests&#x2F;目录中的文件

path 路径限定符
path:src&#x2F;.js匹配src&#x2F;.js 路径下的文件，以上为src目录下的文件，如果需要包含其子目录，写法为 path:src&#x2F;**&#x2F;*.js
path:*.a?c?全局字符，匹配任意字符：file.aac, file.abc

codespaces每个月可免费使用15GB,120小时。
codespaces官方文档在项目的Code按键中选择codespaces,并创建codespace，将会打开vscode界面，在该codespace中不仅可以查看代码，还可以进行开发、调试，运行项目。
  
github actionaction官方文档action市场awesom actions
Github Action 是一种持续集成和持续交付 (CI&#x2F;CD) 平台，可用于自动执行生成、测试和部署管道，而且可以免费使用。GitHub 提供 Linux、Windows 和 macOS 虚拟机来运行工作流程，也可以在自己托管的机器运行。
在之前文章Hexo + Butterfly + Github Page + Github Action 配置自己的个人网站中就使用了Github Action 来实现hexo自动发布。
示例：
name: Deploy Blog siterun-name: $&#123;&#123; github.actor &#125;&#125; is deploy blog site 🚀on: [push]jobs:  Deploy-Actions:    runs-on: ubuntu-latest    container:      image: xhuaustc/hexo:latest      env:        TOKEN: $&#123;&#123; secrets.GIT_TOKEN &#125;&#125;    steps:      - uses: actions/checkout@v3        with:          fetch-depth: 0      - name: Deploy blog        run: |          npm install --force          hexo deploy

workflow中配置的说明：
工作流语法
workflow在具体执行中，可以全用 steps.uses 来调用Action，也可以使用 steps.run 来运行指定的 shell 命令，其中 shell 默认为运行器操作系统的默认shell，它还可以为 cmd, python, sh等。可参考：jobs..steps[*].shell
Github Action 基本概念
Workflow: 工作流，一个可配置的自动化过程，它将运行一个或多个作业。
on: 事件, 存储库中触发工作流程运行的特定活动。
job: 任务，工作流中在同一运行器上执行的一组步骤。 
step: 步骤, 要么是一个将要执行的 shell 脚本，要么是一个将要运行的动作。
action: 动作，用于 GitHub Actions 平台的自定义应用程序，它执行复杂但经常重复的任务。

Github Action的配置文件，会以yaml的形式定义，并保存在项目的.github/workflows/目录中。github将会自动检查该目录下的文件，触发相关的任务。
创建自定义ActionAction动作是工作流中具体执行的最小单元。一个Action单元，包括：name, description, inputs, outputs, runs其中runs为具体的执行操作。GitHub Actions 的元数据语法
创建 Docker 容器Action：使用 Docker 容器Action作为执行环境。
创建组合Action：使用steps进行组合Action
创建 JavaScript Action：使用Javascript 作为Action执行环境
查找和自定义ActionAction除了自己编写外，还可以包含社区创建的Action，也可以直接在应用程序的仓库中创建您自己的Action。

在Marketplace市场中查找并使用Action打开仓库中的具体workflow, 并点击编辑工作流按钮。

在编辑框的右侧，将会出现Marketplace,在其中搜索需要的action,并复制到workflow.yaml中。  

引入本仓库中的ActionAction可以在工作流文件同一仓库中定义，在工作流文件中通过 {owner}&#x2F;{repo}@{ref} 或 .&#x2F;path&#x2F;to&#x2F;dir 语法引用操作。示例仓库文件结构：

|-- hello-world (repository)|   |__ .github|       └── workflows|           └── my-first-workflow.yml|       └── actions|           |__ hello-world-action|               └── action.yml
示例工作流程文件：
jobs:  build:    runs-on: ubuntu-latest    steps:      # This step checks out a copy of your repository.      - uses: actions/checkout@v3      # This step references the directory that contains the action.      - uses: ./.github/actions/hello-world-action
action.yml 文件用于提供操作的元数据。如何将自己的Action发布到MarketPlace

引入外部仓库中的Action

如果操作在与工作流文件不同的仓库中定义，可在工作流文件中通过 {owner}&#x2F;{repo}@{ref} 语法引用该操作。该操作必须存储在公共仓库中。示例工作流文件：
jobs:  my_first_job:    steps:      - name: My first step        uses: actions/setup-node@v3

引入Docker镜像中的Action如果Action在 Docker Hub 上发布的 Docker 容器镜像中定义，必须在工作流文件中通过 docker:&#x2F;&#x2F;{image}:{tag} 语法引用该操作。示例工作流文件：

jobs:  my_first_job:    steps:      - name: My first step        uses: docker://alpine:3.8
如何构建Docker镜像类Action，请参考：创建 Docker 容器Action
示例构建go项目
name: Upload Go test resultson: [push]jobs:  build:    runs-on: ubuntu-latest    strategy:      matrix:        go-version: [ &#x27;1.14&#x27;, &#x27;1.15&#x27;, &#x27;1.16.x&#x27; ]    steps:      - uses: actions/checkout@v3      - name: Setup Go        uses: actions/setup-go@v4        with:          go-version: $&#123;&#123; matrix.go-version &#125;&#125;      - name: Install dependencies        run: go get .      - name: Test with Go        run: go test -json &gt; TestResults-$&#123;&#123; matrix.go-version &#125;&#125;.json      - name: Upload Go test results        uses: actions/upload-artifact@v3        with:          name: Go-results-$&#123;&#123; matrix.go-version &#125;&#125;          path: TestResults-$&#123;&#123; matrix.go-version &#125;&#125;.json
项目探索 ExploreExplore界面中，可以通过邮件订阅感兴趣的项目
LinksGithub 趋势：https://github.com/trending 
代码片段：https://gist.github.com
Github课堂：https://classroom.github.com
Git教程：https://try.github.io
Github项目榜单：https://hellogithub.com
]]></content>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>docker in docker测试docker的proxy功能</title>
    <url>/posts/0ceba1378452.html</url>
    <content><![CDATA[docker in docker是什么？顾名思义，docker in docker就是在docker容器中运行docker服务端。这种技术通常用于CI&#x2F;CD流水线中，因为它可以提供一个干净的环境来运行构建和测试。例如，如果您正在使用Jenkins作为CI&#x2F;CD工具，您可以使用DinD来运行构建和测试，而无需在Jenkins服务器上安装Docker。这样可以避免与Jenkins服务器上的其他应用程序发生冲突。使用DinD时，需要注意的是，它会增加容器的复杂性，并且可能会导致性能下降。此外，由于容器内部运行的是另一个容器，因此可能会出现一些安全问题。本篇中用DinD来完成对docker proxy功能的验证。
为什么docker要使用代理？Docker使用代理的原因有很多，其中一些原因包括：

访问公司私有化部署的镜像仓库，对网络环境有限制。
对于一些公共仓库，下载速度很慢，通过代理实现加速上传与下载。

在本测试环境中，docker容器所在环境与目标docker 镜像仓库是网络隔离的，同时两个网络之间可以通过一个http proxy来进行通信。
如何配置proxy代理?官方说明：https://docs.docker.com/config/daemon/systemd/#httphttps-proxy

运行dockerindocker

docker run -it --privileged --rm  --entrypoint=&quot;sh&quot; docker:24.0.1-dind

设置proxy, 有两种方法进行配置(docker 23.0以版本)


通过&#x2F;etc&#x2F;docker&#x2F;daemon.json

&#123;  &quot;proxies&quot;: &#123;    &quot;http-proxy&quot;: &quot;http://proxy.example.com:3128&quot;,    &quot;https-proxy&quot;: &quot;https://proxy.example.com:3129&quot;,    &quot;no-proxy&quot;: &quot;*.test.example.com,.example.org,127.0.0.0/8&quot;  &#125;&#125;

设置代理环境变量

/ # export http_proxy=http://proxy.example.com:3128/ # export https_proxy=https://proxy.example.com:3129


正常使用docker命令，docker pull&#x2F;push 均会使用该proxy与镜像仓库交互

/ # dockerd &amp;/ # docker pull docker:24.01-dind/ # docker tag docker:24.01-dind remote-repo.app/docker:24.01-dind/ # docker push remote-repo.app/docker:24.01-dind
]]></content>
      <tags>
        <tag>docker</tag>
        <tag>container</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo + Butterfly + Github Page + Github Action 配置自己的个人网站</title>
    <url>/posts/244103fb7421.html</url>
    <content><![CDATA[Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。Butterfly 是 Hexo 的一个流行的主题，它的用户量很大，使其扩展丰富，使用方便。Github Action 是一种持续集成和持续交付 (CI&#x2F;CD) 平台，可用于自动执行生成、测试和部署管道，而且可以免费使用。Github Page 是免费的静态站点，只需要一个代码库，就可以免费构建自己的网站。
本文将以上四者相结合，构建个人博客网站。部署流水线，让我们只需要关注内容的产出，而无需关心 hexo 的部署与博客的发布。Let’s go.
1. 预准备
github 上创建账号，并创建两个 git repo。
一个 repo 用来保存平时编辑的博客文件
一个用来保存 hexo 生成的静态文件，并为其设置为 github page.


本机安装 docker 与 vscode 软件。
docker 用来运行 hexor 容器，用来本地开发与调试
vscode 用来日常编写博客文本
安装 vscode 插件 docker, markdown images



2. 准备 hexo，使用 docker image&gt; docker run --name hexo -p 4000:4000 -it --rm --entrypoint=&quot;bash&quot; -v ~/blog/:/app xhuaustc/hexo:latestroot@291c4cec0506:/app/# hexo init myblogroot@291c4cec0506:/app/# cd myblogroot@291c4cec0506:/app/# cd myblogroot@a20365558032:/app/myblog# hexo sINFO  Validating configINFO  Start processingINFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.

该镜像已经准备好了 hexo 的编译与运行环境，其中 ~&#x2F;blog&#x2F;中存放第 1 步创建的 git 代码。
3. 安装 butterfly 主题
安装 butterfly

npm i hexo-theme-butterfly


: 将 butterfly 的默认配置拷贝到项目根目录_config.butterfly.yml

wget https://raw.githubusercontent.com/jerryc127/hexo-theme-butterfly/dev/_config.yml _config.butterfly.yml


: hexo 设置主题 buterfly

theme: butterfly

4. 配置本地搜索
安装 hexo-generator-search

npm install hexo-generator-search --save


hexo 配置中配置 search

search:  path: search.xml  field: post  content: true  format: html


butterfly 配置中打开 search

local_search:  enable: true

5. 配置 tags 页面
新建 tags page

$ hexo new page &quot;tags&quot;


更新 source&#x2F;tags 文件夹下 index.md 文件内容为

type: &quot;tags&quot;layout: &quot;tags&quot;comments: false


在_config.butterfly.yml 中 menue 打开 tags

# Menu 目錄menu:  主页: / || fas fa-home  归档: /archives/ || fas fa-archive  标签: /tags/ || fas fa-tags

6. 配置 categories 页面同tags页面

新建 categories page

$ hexo new page &quot;categories&quot;


更新 source&#x2F;tags 文件夹下 index.md 文件内容为

type: &quot;categories&quot;layout: &quot;tags&quot;comments: false


在_config.butterfly.yml 中 menue 打开 tags

# Menu 目錄menu:  主页: / || fas fa-home  归档: /archives/ || fas fa-archive  标签: /tags/ || fas fa-tags  分类: /categories/ || fas fa-folder-open

7. 添加置顶功能安装置顶插件
$ npm uninstall hexo-generator-index --save$ npm install hexo-generator-index-pin-top --save
在post的元数据描述中添加top标识即可：如
---title: 置顶配置date: 2021-09-08 12:00:25categories: 教程top: true # true 或者数字---

6. 配置 github page只需要将 repo 名字设置为.github.io,即可通过浏览器 https:&#x2F;&#x2F;.github.io 访问。
7. 使用 github action 实现自动部署
安装 hexo-deployer-git 插件并配置

npm install hexo-deployer-git --save

在 hexo 项目的配置中设置 deploy
deploy:  type: git  repository: https://&lt;username&gt;:GIT-TOKEN@github.com/&lt;username&gt;/&lt;username&gt;.github.io.git  branch: master  name: &lt;username&gt;  email: &lt;email&gt;

repository 便是静态文件 repo, 也是 Github Page 对应的 repo。 2. 在 github 中创建 token(Settings Developer &#x2F; settings &#x2F; personal access tokens &#x2F; tokens(classic)，并为其添加 repo、workflow 权限 3. 在代码 repo 中创建 secret，将 2 步创建的 token 保存为 secret,名为 GIT_TOKEN4. 在代码 repo 中设置 github action.
# .github/workflows/build.yamlname: Deploy Blog siterun-name: $&#123;&#123; github.actor &#125;&#125; is deploy blog site 🚀on: [push]jobs:  Deploy-Actions:    runs-on: ubuntu-latest    env:      TOKEN: $&#123;&#123; secrets.GIT_TOKEN &#125;&#125;    steps:      - uses: actions/checkout@v3        with:          fetch-depth: 0      - uses: actions/setup-node@v3        with:          node-version: &quot;20&quot;      - run: |          sed -i &quot;s/GIT-TOKEN/$&#123;TOKEN&#125;/&quot; _config.yml          npm install hexo-cli -g          npm install --force          hexo clean          hexo deploy

8. 加速 github page 网站访问 github page 速度很慢，可以通过Netlify对其进行免费加速，但是国内访问仍然不稳定。具体操作请参考：Github Pages 访问太慢？通过 Netlify 免费加速 
可以通过帽子云将应用在国内部署，从而加速网站。具体介绍参考：帽子云帮助手册
9. 提交文章以上就绪后，在_source&#x2F;_posts 中创建新的博客，并其提交到代码库。 Github Action 将自动触发，生成静态文件，并将其提交到静态文件 repo。通过 Github Page 便可访问最新的博客了。vscode插件Markdown Image可以方便（Alt+Shift+V）上传插件，并生成Markdown文本。  

设置 markdown-image.base.fileFormat 为 jpg，可以减小上传剪切板图片的大小。

10. 免费域名https://nic.eu.org/ 该网站可申请免费域名，并支持通过dnspod.cn进行域名管理。
##参考文章Butterfly 安裝文檔(三) 主題配置-1Butterfly 安裝文檔(四) 主題配置-2Github Pages 访问太慢？通过 Netlify 免费加速 
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>butterfly</tag>
      </tags>
  </entry>
  <entry>
    <title>晒一晒Jenkins那些常用插件</title>
    <url>/posts/e8b881018f34.html</url>
    <content><![CDATA[作为 CI&#x2F;CD 的调度中心，Jenkins 具有十八般武艺，目前已有 1700 多个插件，功能强大到似乎有点过分了。本文主要列出平时我们常用的插件。
以下这两个网站是 Jenkins 所有的插件及说明
Jenkins Plugins https://plugins.jenkins.ioJinkins Plugins Wiki https://wiki.jenkins.io

ConfigurationJenkins CASC: configuration-as-code将Jenkins的配置以Code的方式进行保存，方便Jenkins的迁移与重建。该插件对于敏捷管理Jenkins服务非常有用。
jobConfigHistory记录和跟踪Jenkins作业配置的历史变更，方便查看配置修改记录和回滚操作。
ParameterGit Parameter这是一个参数构建扩展，可以在构建的时候选择git的某一个分支来构建服务。
file-parameters这个让Jenkinsfile支持将文件作为参数。
Hidden Parameter参数化构建时，隐藏相关参数。可用于设置默认参数，同时通过API来传递参数。
Extended Choice Parameter支持更多类型的参数，如单选，多选，多级单选，多级多选类型
Build Name and Description Setter自定义每个Build名字
buildName &quot;$&#123;GERRIT_CHANGE_SUBJECT&#125;&quot;buildDescription &quot;Executed @ $&#123;NODE_NAME&#125;&quot;
build user vars为job注入用户信息环境变量



Variable
Description



BUILD_USER
Full name (first name + last name)


BUILD_USER_FIRST_NAME
First name


BUILD_USER_LAST_NAME
Last name


BUILD_USER_ID
Jenkins user ID


BUILD_USER_GROUPS
Jenkins user groups


BUILD_USER_EMAIL
Email address


  
Pipeline examples:
stages &#123;    stage(&#x27;create user&#x27;) &#123;      steps&#123;        wrap([$class: &#x27;BuildUser&#x27;]) &#123;          sh &#x27;&#x27;&#x27;echo $BUILD_USER          &#x27;&#x27;&#x27;        &#125;      &#125;    &#125;&#125;

Environment Injector在jenkins构建job时，可以自定义添加环境变量。  
HashiCorp Vault可以帮助Jenkins安全地管理和使用存储在Vault中的敏感信息。从Vault中获取密钥，并将其注入到Jenkins 构建环装中作为环境变量。这样可以安全地使用Vault中存储的敏感信息,而不需要将其直接写入Jenkins job中。
AgentDocker利用Docker容器动态创建Jenkins Slave。如果有了Kubernetes&#x2F;Openshift集群，就不需要这个插件了。直接使用下面的Kubernetes插件。jenkins非root启动的话，为了运行docker需要执行sudo chmod 777 /var/run/docker.sock
Kubernetes这个插件可以将Jenkins Slave Node动态配置为Kubernetes集群上的pod。
Openshift这个插件支持调度Openshift的对象，包括触发 BuildConfig、Deployment、Scale up a Deployment，给ImageStream打新的Tag，以及创建新的对象、删除已有对象等。
swarm这个插件可以方便地为jenkins master添加slave agent.
TriggerGitLab配置Gitlab的相关认证，同时也支持GitLab的Webhook触发。
GitLab Hook支持GitLab更好的触发。
Gogs WebHook支持Gogs代码仓库的触发。
ToolsMaven这个插件为Maven 2 &#x2F; 3项目提供了高级集成功能。
Pyenv Pipeline方便对python进行项目级别的环境隔离。jenkins机器上需要安装python、pip、virtualenv
Python这个插件支持在Jenkins的构建过程中执行Python脚本。
SonarQube Scanner支持SonarQube的代码扫描。
Ansible在构建任务中可以执行Ansible任务。
Publish Over SSH通过SSH拷贝文件到目标机器，同时可以在目标机器上执行脚本
  
事先要在设置中添加目标机器的访问方式。
Condition BuildStep可以使用 when 对 step 做判断
Http Request向用户发送一个 HTTP &#x2F; HTTPS 请求
Workspace Cleanup每次 build 之前删除 workspace 目录下指定的文件
  
Pipeline Utility Steps  
xvfb为应用程序提供虚拟的 X server，主要用于界面自动化测试。
Timestamper为构建日志的每一行添加时间戳，方便调试和分析构建过程的时间消耗。
Job ManagerJob Generator定义一个参数化的模板，通过这个模板快速的在 Jenkins 上创建出任务。
  
Parameterized Remote Trigger触发远程的Jenkins server的Job
Job DSL PluginJenkins Job DSL Plugin 可以让开发者通过 Groovy 脚本来定义和配置任务，随后插件会通过执行这些脚本来维护 Jenkins 任务。与 Job Generator 相比，它需要了解 Groovy。
Pipeline:Job添加一个新的 Job 类型：Pipeline。
  
Multijob把多个 Job 组织起来。
Parameterized Trigger这是一个扩展型的插件，使各个 job 连接的时候可以传递一些 job 相关的信息。
Parameterized Scheduler这个扩展是 Parameterized Trigger 的定时触发版
Join这也是一个触发 job 的插件，亮点在于它触发 job 的条件是等待所有当前 job 的下游的 job 都完成才会发生。
Build Pipeline这个插件提供一个构建流水线的视图。同时它提供了一个任务的手动触发器。
  
TestingJUnit展示JUnit单元测试报告。
TestNG Results导出TestNG的测试报告。
JaCoCo生成测试覆盖率的报告。
Performance生成性能测试报告需要在Jenkins机器上安装Taurus（开源负载测试工具和功能测试工具自动化框架）
  
Html Publisher生成报告文档。参考资料：https://www.jianshu.com/p/8fb776f83243
AuthorizationRole-based Authorization Strategy 用户角色给Jenkins用户权限管理添加了角色组。
Matrix Authorization Strategy Plugin为每个项目设置用户权限
MonitorBuild Monitor View将 Jenkins 项目以一块看板的形式呈现。
  
Disk Usage对Jenkins节点服务器磁盘的监控。
  
Monitoring 监控监控 Jenkins 节点的 CPU、系统负载、平均响应时间和内存使用。
  
Prometheus Metrics将Jenkins 服务及job监控指标统一由Prometheus采集。a Jenkins performance and health overview for jenkinsci&#x2F;prometheus-plugin
AlertEmail Extension扩展了发送告警邮件的控制力度。可以定义邮件触发器、邮件内容、收件人。
Mailer每次不稳定的构建都发送邮件通知。单独发送邮件给对构建造成不良影响的责任人，会从 SCM 提交者人的信息中，拼出邮箱。
Sounds这个插件能让 Jenkins 通过播放声音来发出通知。
Backup And RestoreBackup 备份 Jenkins自定义备份 Jenkins Home 目录。
  
ThinBackup 轻量备份 Jenkins轻量备份 Jenkins 上的配置与 Job
  
SCM Sync Configuration  
备份与恢复secrets备份
cd /var/lib/jenkinstar czvf /tmp/jenkins_secrets.tgz secret* credentials.xml
恢复
systemctl stop jenkinsrm /var/lib/jenkins/identity.key.enccd /var/lib/jenkinstar xzvf /tmp/jenkins_secrets.tgz -C ./chown jenkins:jenkins /var/lib/jenkins/credentials.xmlchown jenkins:jenkins /var/lib/jenkins/secret*systemctl start jenkins
通过这种方法，新的Jenkins实例将能够正确解密和使用credentials.xml中的加密凭证。这是因为Jenkins使用master.key和hudson.util.Secret文件来加密和解密凭证信息。通过复制这些文件，新的Jenkins实例将拥有相同的加密密钥，从而能够正确读取加密的凭证。
预先要在全局配置中设置代码版本控制库的配置，每次创建或更新 job 配置时都会提示是否同步配置
Job Import Plugin可方便地从一个Jenkins实例导入job到另一个实例。

参考文章
[jenkins常用插件汇总](http://www.cnblogs.com/honeybee/p/7877875.html)
[Jenkins插件大全](https://blog.csdn.net/pansaky/article/details/80755739)
]]></content>
      <tags>
        <tag>devops</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>Cue-Lang介绍</title>
    <url>/posts/3d81057c47a8.html</url>
    <content><![CDATA[Cue，是一种开源语言，用于定义，生成和验证各种数据：配置，API，数据库模式，代码……。它能够将数据的结构、约束、数值作为同一层级成员，从而简化配置文件的生成。Cue教程
Cue格式说明
使用//进行单行注释
对象被称为结构体
对象成员称为结构字段
对于没有特殊字符的字段名，可以省略引号
结构字段后面无需,
在列表中的最后一个元素后放置,
最外层的{}可省略

例子：
str: &quot;hello world&quot;num: 42flt: 3.14// Special field name (and a comment)&quot;k8s.io/annotation&quot;: &quot;secure-me&quot;// lists can have different element typeslist: [	&quot;a&quot;, &quot;b&quot;, &quot;c&quot;,	1,	2,	3,]obj: &#123;	foo: &quot;bar&quot;	// reuse another field?!	L: list&#125;
Cue 结构、约束、数据// 结构album: &#123;	title: string	year: int	live: bool&#125;
// 约束album: &#123;	title: string	year: &gt;1950	live: false&#125;
// 数据album: &#123;	title: &quot;Houses of the Holy&quot;	year: 1973	live: false&#125;
Cue的最佳实践：从开放的结构模式开始，限制上下文可能性，最终具体到数据实例。Cue哲学：为了保证唯一性，Cue的数据不会被覆盖。
Cue核心规则
数据可被重复定义，但必须值保持一致
结构字段可以被更强限制覆盖
结构的字段会被合并，如果是列表，必须严格匹配
规则可被递规应用

hello: &quot;world&quot;hello: &quot;world&quot;// set a types: &#123; a: int &#125;// set some datas: &#123; a: 1, b: 2 &#125;// set a nested field without curly bracess: c: d: 3// lists must have the same elements// and cannot change lengthl: [&quot;abc&quot;, &quot;123&quot;]l: [	&quot;abc&quot;,	&quot;123&quot;]
结构
结构并不会输出
它的值可能是不确认、不完整的
字段必须完全

使用#mydef来定义结构，使用...来定义一个开放的结构体
#Album: &#123;	artist: string	title: string	year: int	// ...  uncomment to open, must be last&#125;// This is a conjunction, it says &quot;album&quot; has to be &quot;#Album&quot;album: #Album &amp; &#123;	artist: &quot;Led Zeppelin&quot;	title: &quot;Led Zeppelin I&quot;	year: 1969	// studio: true  (uncomment to trigger error)&#125;
#Person: &#123; name: string... // open struct&#125;Jim: #Person &amp; &#123; name: &quot;jim&quot; age: 12&#125;

约束约束与数值使用&amp;字符进行连接时，会将值进行校验
// conjunctions on a fieldn: int &amp; &gt;0 &amp; &lt;100n: 23// conjuction of schemasval: #Def1 &amp; #Def2val: &#123; foo: &quot;bar&quot;, ans: 42 &#125;#Def1: &#123;	foo: string	ans: int&#125;#Def2: &#123;	foo: =~ &quot;[a-z]+&quot;	ans: &gt;0&#125;

替换使用|可以实现支持多种结构。同时它也可以为出错值设置替换值
// disjunction of values (like an enum)hello: &quot;world&quot; | &quot;bob&quot; | &quot;mary&quot;hello: &quot;world&quot;// disjunction of typesport: string | intport: 5432// disjunction of schemasval: #Def1 | #Def2val: &#123; foo: &quot;bar&quot;, ans: 42 &#125;#Def1: &#123;	foo: string	ans: int&#125;#Def2: &#123;	name: string	port: int&#125;

默认值与可选使用*来设置默认值， ?设置可选字段 
s: &#123;	// field with a default	hello: string | *&quot;world&quot; | &quot;apple&quot;	// an optional integer	count?: int&#125;
开放模式与封闭模式开放模式意味着结构可以扩展，关闭模式意味着不能扩展。 默认情况下，结构是开放模式，定义是封闭模式。 可以通过定义的最后添加...来申明开放模式定义；另外通过过close强制为结构体设置为关闭模式
// Open definition#d: &#123;	foo: &quot;bar&quot;	... // must be last&#125;// Closed structs: close(&#123;	foo: &quot;bar&quot;&#125;)jim: &#123;  name: &quot;Jim&quot;&#125;jim: &#123;  age: 12&#125;
推荐从基础定义开始，复用定义在编写Cue时，推荐从基础定义开始，这样能够有更好的复用能力。
#Base: &#123;	name: string	kind: string	... // so it can be extended&#125;#Meta: &#123;	// string and a semver regex	version: string &amp; =~&quot;^v[0-9]+\\.[0-9]+\\.[0-9]+$&quot;	// list of strings	labels: [...string]&#125;#Permissions: &#123;	role: string	public: bool | *false&#125;// Building up a schema using a conjunction and embedding#Schema: #Base &amp; &#123;	// &quot;embed&quot; meta and permissions	#Meta	#Permissions	// with no &#x27;...&#x27; this is final&#125;value: #Schema &amp; &#123;	name: &quot;app&quot;	kind: &quot;deploy&quot;	version: &quot;v1.0.42&quot;	labels: [&quot;server&quot;, &quot;prod&quot;]	role: &quot;backend&quot;	// public: false  (by default)&#125;
定义多行字符串
使用&quot;&quot;&quot;来定义多行字符串

str1: #&quot;avoid using \ to &quot;escape&quot;&quot;#str2: &quot;&quot;&quot;a nested multilinestring goes here&quot;&quot;&quot;

使用反引号(&#96;)定义原始字符串

multiline: `这是一个多行字符串保留了换行和空格`

使用#”…”# 定义原始字符串,可以避免转义

multiline: #&quot;这种写法可以包含 &quot;引号&quot; 而不需要转义还可以包含 \反斜杠\ 等特殊字符&quot;#

使用 + 连接多个字符串

multiline: &quot;第一行\n&quot; +           &quot;第二行\n&quot; +           &quot;第三行&quot;
ListList 可被定义为开放模式，这样便可与其它数据进行合并，
empty: []any: [...]ints: [...int]nested: [...[...string]]opened: ints &amp; [1,2,...]closed: ints &amp; [1,2,3]// list of for constrained intsip: 4 * [uint8]// sets the first elementtendot: ip &amp; [10, ...uint8]// uses constraint as second elementone72: ip &amp; [172, &gt;=16 &amp; &lt;=32, ...]mixed: any &amp; [...] &amp; [&quot;a&quot;,1, &#123; foo: &quot;bar&quot; &#125;]join: [1,2] + [3,4]Join: opened &amp; join
Struct结构体是Cue的主要内容，也是最终数据的输出。如上介绍，默认情况下它是开放模式。除了使用Json类型形式进行设置值，还可通过级联:来设置，如a: hello: &quot;world&quot;
// an open structa: &#123;	foo: &quot;bar&quot;&#125;// shorthand nested fielda: hello: &quot;world&quot;// a closed structb: close(&#123;	left: &quot;right&quot;&#125;)
模式匹配约束模式匹配允许您为与模式匹配的标签指定约束。可以将约束应用于字符串标签，并使用标识符来设置字段。
#schema: &#123;	name: string	ans: string	num: int | *42&#125;// match elem fields and alias labels to Name,// unify with schema, set name to Name by labelelems: [Name=_]: #schema &amp; &#123; name: Name &#125;elems: &#123;	one: &#123;		ans: &quot;solo&quot;		num: 1	&#125;	two: &#123;		ans: &quot;life&quot;	&#125;&#125;elems: other: &#123; ans: &quot;id&quot;, num: 23 &#125;
表达式
引用字段，使用\(**)显用其它字段

container: &#123;	repo: &quot;docker.io/cuelang&quot;	image: &quot;cue&quot;	version: &quot;v0.3.0&quot;	full: &quot;\(repo)/\(image):\(version)&quot;&#125;name: &quot;Tony&quot;msg: &quot;Hello \(name)&quot;// conver string to bytesb: &#x27;\(msg)&#x27;// convert bytes to strings: &quot;\(b)&quot;

Cue也能够为通过\(**)来设置key

apps: [&quot;nginx&quot;, &quot;express&quot;, &quot;postgres&quot;]#labels: [string]: stringstack: &#123;	for i, app in apps &#123;		&quot;\(app)&quot;: &#123;			name: app			labels: #labels &amp; &#123;				app: &quot;foo&quot;				tier: &quot;\(i)&quot;			&#125;		&#125;	&#125;&#125;

List遍历遍历List数据格式如下：[ for key, val in &lt;iterable&gt; [condition] { production } ]

nums: [1,2,3,4,5,6]sqrd: [ for _, n in nums &#123; n*n &#125; ]even: [ for _, n in nums if mod(n,2) == 0 &#123; n &#125; ]listOfStructs: [ for p, n in nums &#123;	pos: p	val: n&#125;]extractVals: [ for p, S in listOfStructs &#123; S.val &#125; ]

条件控制语句没有else，所有判断都会被执行

app: &#123;	name: string	tech: string	mem: int	if tech == &quot;react&quot; &#123;		tier: &quot;frontend&quot;	&#125;	if tech != &quot;react&quot; &#123;		tier: &quot;backend&quot;	&#125;	if mem &lt; 1Gi &#123;		footprint: &quot;small&quot;	&#125;	if mem &gt;= 1Gi &amp;&amp; mem &lt; 4Gi &#123;		footprint: &quot;medium&quot;	&#125;	if mem  &gt;= 4Gi &#123;		footprint: &quot;large&quot;	&#125;&#125;
标准库Cue的标准库中包含了很多的帮助包（helper packages）。

Encoding

package stdlibimport (	&quot;encoding/json&quot;)data: &quot;&quot;&quot;&#123;	&quot;hello&quot;: &quot;world&quot;,	&quot;list&quot;: [ 1, 2 ],	&quot;nested&quot;: &#123;		&quot;foo&quot;: &quot;bar&quot;	&#125;&#125;&quot;&quot;&quot;jval: json.Unmarshal(data)val: &#123;	hello: &quot;world&quot;	list: [1,2]	nested: foo: &quot;bar&quot;&#125;cjson: json.Marshal(val)

Strings

package stdlibimport &quot;strings&quot;s: &quot;HelloWorld&quot;u: strings.ToUpper(s)l: strings.ToLower(s)line: &quot;Cue stands for configure, unify, execute&quot;words: strings.Split(line, &quot; &quot;)lined: strings.Join(words, &quot; &quot;)haspre: strings.HasPrefix(line, &quot;Cue&quot;)index:  strings.Index(line, &quot;unify&quot;)

List

package stdlibimport &quot;list&quot;l1: [1,2,3,4,5]l2: [&quot;c&quot;,&quot;b&quot;,&quot;a&quot;]// constrain lengthl2: list.MinItems(1)l2: list.MaxItems(3)// slice a listl3: list.Slice(l1, 2,4)// get the sum and productsum: list.Sum(l1)prd: list.Product(l1)// linear search for list (no binary)lc: list.Contains(l1, 2)// sort a listls: list.Sort(l2, list.Ascending)l2s: list.IsSorted(l2, list.Ascending)lss: list.IsSorted(ls, list.Ascending)// Flatten a listll: [1,[2,3],[4,[5]]]lf: list.FlattenN(ll, 1)

Constrain

package stdlibimport (	&quot;net&quot;	&quot;time&quot;)// string with ip formatip: net.IPv4ip: &quot;10.1.2.3&quot;// string with time formatts: time.Format(time.ANSIC)ts: &quot;Mon Jan 2 15:04:05 2006&quot;
模块和包cuelang有module和package系统，可以import依赖

模块定义


通过在项目根目录创建cue.mod&#x2F;module.cue文件来定义模块。通过 cue mod init &lt;模块名&gt;来初始化。
模块名格式通常为domain.com/name或github.com/owner/repo


package组织


一个模块可以包含多个package
允许在一个目录中包含多个package


导入package


使用绝对路径导入,不允许相对路径导入
导入时可以省略domain,表示导入内置标准包
可以在导入时重命名包
同一个包内的定义和值可以直接访问,无需导入

例子
package deployimport  (	p_spec &quot;douhua.com/name/devops/pkg/spec:spec&quot;)a: p_spec.#A &amp; &#123;	name: &quot;test&quot;&#125;

使用Cue制作脚本命令工具Cue 拥有制作脚本命令工具的功能，它有一个工具层，可用来执行脚本、读写文件以及网络访问等。规范：

脚本文件以_tool.cue结尾
执行命令为cue cmd &lt;name&gt; or cue &lt;name&gt;例子：


脚本文件名为ex_tool.cue

package fooimport (	&quot;tool/cli&quot;	&quot;tool/exec&quot;	&quot;tool/file&quot;)// moved to the data.cue file to show how we can reference &quot;pure&quot; Cue files// city: &quot;Amsterdam&quot;// A command named &quot;prompter&quot;command: prompter: &#123;	// save transcript to this file	var: file: *&quot;out.txt&quot; | string @tag(file) // you can use &quot;-t flag=filename.txt&quot; to change the output file, see &quot;cue help injection&quot; for more details	// prompt the user for some input	ask: cli.Ask &amp; &#123;		prompt:   &quot;What is your name?&quot;		response: string	&#125;	// run an external command, starts after ask	echo: exec.Run &amp; &#123;		// note the reference to ask and city here		cmd:    [&quot;echo&quot;, &quot;Hello&quot;, ask.response + &quot;!&quot;, &quot;Have you been to&quot;, city + &quot;?&quot;]		stdout: string // capture stdout, don&#x27;t print to the terminal	&#125;	// append to a file, starts after echo	append: file.Append &amp; &#123;		filename: var.file		contents: echo.stdout // becuase we reference the echo task	&#125;	// also starts after echo, and concurrently with append	print: cli.Print &amp; &#123;		text: echo.stdout // write the output to the terminal since we captured it previously	&#125;&#125;

prompter为命令名
ask&#x2F;echo&#x2F;append&#x2F;print为唯一标识
cli.Ask&#x2F;exec.Run&#x2F;file.Append为函数,
&amp;{…}为函数参数


创建data.cue

package foocity: &quot;Amsterdam&quot;

运行：cue cmd prompter

$ cue cmd prompterWhat is your name? heHello he! Have you been to Amsterdam?$ cat out.txtHello he! Have you been to Amsterdam?

使用cuelang exec.Run 执行多行代码

package fooimport (        &quot;tool/exec&quot;)command: &#123;	hello: &#123;		script: #&quot;&quot;&quot;#!/bin/bashecho hello worldkey1=&quot;hello you&quot;lsecho $key1&quot;&quot;&quot;#		run: cmd: _		run: exec.Run &amp; &#123;				cmd: &quot;bash&quot;				stdin: script		&#125;    &#125;&#125;
Tips
A &amp; B &#x3D;&#x3D;&#x3D; B &amp; A
A &#x3D;&#x3D;&#x3D; A
路径短写：{a : {b: {c: 5}}} &#x3D;&#x3D; a: b: c: 5
多种类型：a | b | c
默认值：number | *1
算术： 4 + 5
变量引用：”Hello (person)”
列表遍历：[ x for x in y ]
cue 执行 当前目录下的cue文件及父目录下同一个package的cue文件
cue .&#x2F;… 以上目录 + 遍历当前目录的子目录下的cue文件
_开头的变量不会在输出结果中显示，作为局部变量
[Name&#x3D;_] 可用来定义一个模板，其中Name匹配任意字段。例如：application: [Name=_]: &#123;	name: string | *Name&#125;
_|_ 可判断是否存在。例如：if _variable !&#x3D; _|_ { &#x2F;&#x2F; … }a ?: stringif a == _|_ &#123;b: &quot;a&quot;&#125;// 结果为// cue export a.cue// b: &quot;a&quot;
a: stringif a == _|_ &#123;b: &quot;a&quot;&#125;// 结果为// cue eval a.cue// a: string// b: &quot;a&quot;
定义映射：map: [string]: string
定义切片：slice: […{name:string,value:string}]

实践  

使用 cue import 将已有的yaml转成Cue语言

$ cue import ./... -p kube -l &#x27;&quot;\(strings.ToCamel(kind))&quot; &quot;\(metadata.name)&quot;&#x27; -fR 

引入k8s资源的模块

$ go mod init main$ cue get go k8s.io/api/extensions/v1beta1 -v

导入k8s资源模块，并创建资源

package kubeimport (  &quot;k8s.io/api/core/v1&quot;  &quot;k8s.io/api/extensions/v1beta1&quot;)service &lt;Name&gt;: v1.Servicedeployment &lt;Name&gt;: v1beta1.Deployment

cue trim 可用来自动删除冗余字段，以简化配置文件

参考文档cue torialscue语法cue语言入门
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows-WSL2开发环境，一点不逊色于Mac</title>
    <url>/posts/bddce2843d8e.html</url>
    <content><![CDATA[一直使用Mac来做开发，但没想到现在Windows也这么好用。通过WSL，拥有了Windows酣畅的操作特性，又有了Linux的兼容性。作为开发环境一点不逊色于Mac，甚至更易使用。下面就操练起来吧。
WSL21. 安装控制面板 -&gt; 程序和功能 -&gt; 启用和关闭windows 功能， 勾选如下功能：    - 适用于Linux的Windows子系统    - 虚拟机平台
确定。  
2. 升级WSLhttps://docs.microsoft.com/en-us/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package下载 Linux 内核更新包 WSL2 Linux kernel update package for x64 machines，并安装。
设置wsl默认版本为 `2`， PowerShell
wsl --set-default-version 2
  升级已安装的分发，可使用wsl --set-version, PowerShell
wsl --set-version 分发版名称 版本号

3. 安装windows terminal在windows appstore中安装windows terminal
  
4. 安装Linux系统在windows appstore中安装 ubuntu 22.04
  
并在windows terminal 中设置WSL的访问

命令行：wsl.exe -d Ubuntu-20.04 
启动目录: &#x2F;&#x2F;wsl$&#x2F;Ubuntu-20.04&#x2F;home&#x2F;{username}&#x2F;

5. WSL开机启动sshd通过windows terminal进行ubuntu系统，并配置ssh开机启动
# /etc/init.wsl#!/bin/bash/etc/init.d/ssh start
5. 将端口暴露powershell:PS C:\Users\mpan&gt; netsh interface portproxy add v4tov4 listenport=22 connectaddress=127.0.0.1 connectport=22 listenaddress=* protocol=tcp
如果需要删除该策略可执行
PS C:\Users\mpan&gt; netsh interface portproxy delete v4tov4 listenport=22 protocol=tcp
如果磁盘空间紧张，可以使用pxlrbt&#x2F;move-wsl工具迁移虚机到其它盘。
zsh
安装

$ apt install zsh -y$ chsh -s /bin/zsh$ zsh

安装 zinit 和插件

$ bash -c &quot;$(curl --fail --show-error --silent --location https://raw.githubusercontent.com/zdharma-continuum/zinit/HEAD/scripts/install.sh)&quot;

配置zsh

# ~/.zshrc...# Load powerlevel10k themezinit ice depth&quot;1&quot; # git clone depthzinit light romkatv/powerlevel10k# Plugin history-search-multi-word loaded with investigating.zinit load zdharma-continuum/history-search-multi-word# Two regular plugins loaded without investigating.zinit light zsh-users/zsh-autosuggestionszinit light zdharma-continuum/fast-syntax-highlightingzinit snippet https://gist.githubusercontent.com/hightemp/5071909/raw/zinit ice lucid wait=&#x27;0&#x27;zinit snippet https://github.com/ohmyzsh/ohmyzsh/blob/master/plugins/git/git.plugin.zshautoload -Uz compinit &amp;&amp; compinit## 解决粘贴慢的问题pasteinit() &#123;  OLD_SELF_INSERT=$&#123;$&#123;(s.:.)widgets[self-insert]&#125;[2,3]&#125;  zle -N self-insert url-quote-magic # I wonder if you&#x27;d need `.url-quote-magic`?&#125;pastefinish() &#123;  zle -N self-insert $OLD_SELF_INSERT&#125;zstyle :bracketed-paste-magic paste-init pasteinitzstyle :bracketed-paste-magic paste-finish pastefinish# ~/.bashrcexport PATH=&quot;$&#123;KREW_ROOT:-$HOME/.krew&#125;/bin:$HOME/shtool:$PATH:$HOME/go/bin:/home/mpan/.local/bin&quot;export BROWSER=wslviewsource $&#123;fpath[1]&#125;/_kubectlunsetopt prompt_cr prompt_spalias pbcopy=&quot;clip.exe&quot;alias pbpaste=&quot;powershell.exe -command &#x27;Get-Clipboard&#x27;|head -n 1&quot;alias bd=&#x27;ostr=`pbpaste`; echo $&#123;ostr:0:-1&#125;  | base64 -d&#x27;alias b=&#x27;ostr=`pbpaste`; echo -n $&#123;ostr:0:-1&#125; | base64&#x27;alias gpush=&quot;git push origin HEAD:refs/for/main&quot;### ctrl+arrowsbindkey &quot;\e[1;5C&quot; forward-wordbindkey &quot;\e[1;5D&quot; backward-word
Docker
安装并启动

下载Docker Desktop安装： Install Docker Desktop on Windows | Docker Documentation

配置

开启 WSL2作为基础引擎3. 验证, WSL Ubuntu
$ docker ps


随着docker镜像越来越多，占用的空间也会越来越多，默认docker的数据目录存放在C盘(C:\Users&lt;Username&gt;\AppData\Local\Docker)，可以使用软链接的方式将数据文件移到其它分区。其中 为用户名在将Docker目录移到其它空间足够的磁盘后（通过剪切-&gt; 粘贴 或者 按住Shift键，鼠标移动）执行创建软链接命令，如：

cmd /c mklink /j  C:\Users\admin\AppData\Local\Docker E:\WSL2\Docker-Data

另外，如果只希望压缩文件大小，可以使用分区工具 diskpart
PS C:\Users\admin&gt; diskpart.exeDISKPART&gt; select vdisk file=&quot;C:\Users\admin\AppData\Local\Docker\wsl\disk\docker_data.vhdx&quot;DiskPart 已成功选择虚拟磁盘文件。DISKPART&gt; compact vdisk  100 百分比已完成DiskPart 已成功压缩虚拟磁盘文件。

Kubernetes
Docker Desktop设置中开启Kubernetes，即可获得一个单节点的K8s集群。如果镜像无法下载，可参考： https://github.com/AliyunContainerService/k8s-for-docker-desktop 预先导入镜像

也可以设置registry-mirrors，在docker desktop的设置-&gt;Docker Engine中添加配置：
&quot;registry-mirrors&quot;: [    &quot;https://docker.m.daocloud.io&quot;  ]


可使用其它方式部署K8S&#x2F;K3S，以kind为例：安装 kind

$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64$ sudo install kind-linux-amd64 /usr/local/bin/kind
创建集群
$  kind  create cluster --name wsl2-k8s
查看集群 
$ kind get clusters
删除集群 
$ kind delete cluster --name wsl2-k8s

kubectl命令扩展工具安装

# windows winget install --id ahmetb.kubectx winget install --id ahmetb.kubens# linux curl -L https://github.com/ahmetb/kubectx/releases/download/v0.9.1/kubens -o /bin/kubens chmod +x /bin/kubens# mac brew install kubectx
或者作为Kubectl的一个插件安装
kubectl krew install ns

vscode
安装 https://code.visualstudio.com/下载vscode，并安装

安装插件安装插件 Remote - WSL

通过远程资源管理器，打开连接WSL的vscode windows  

在新窗口中选择WSL系统下的目录，开始愉快编码。


wsl挂载外部硬盘
PowerShell 升级wsl，如果没有’–mount’ 命令的话

wsl --update

PowerShell 查看所有磁盘

GET-CimInstance -query &quot;SELECT * from Win32_DiskDrive&quot;

PowerShell 挂载磁盘,其中的格式为：\\.\PHYSICALDRIVE*

wsl --mount &lt;DiskPath&gt; --bare

Linux 登录wsl验证

$ lsblk

PowerShell 卸载磁盘，其中的格式为：\\.\PHYSICALDRIVE*

wsl --unmount &lt;DiskPath&gt;
小拓展：

插件：git history, gitlens, live server
添加快捷键。 命令：清除控制台   绑定键：ctrl + k  当（条件）：terminalFocus
析本机磁盘文件Windows工具分：WinDirStat

]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>SciKit-Learn-机器学习入门（一）</title>
    <url>/posts/ab8fb20764a5.html</url>
    <content><![CDATA[  
安装SciKit Learnpip install numpypip install sklearnpip install matplotlib
sklean中自带的数据

iris 鸢尾属植物数据（分类）


from sklearn import datasetsiris = datasets.load_iris()


Boston房价信息（线性回归）


from sklearn import datasetsboston = datasets.load_boston()


等等


分类器实例from sklearn import datasetsfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import train_test_splitiris = datasets.load_iris()  ## 加载数据iris_X = iris.data  ## X坐标，叶子长宽等属性iris_Y = iris.target  ## Y坐标， 植物品种[0, 1, 2]X_tain, X_test, Y_train, Y_test = train_test_split(iris_X, iris_Y, test_size=0.3)  ## 将数据中的70%做为训练数据，30%作为测试数据knn = KNeighborsClassifier()  ## 使用Neighbors分类器模型knn.fit(X_tain, Y_train)   ## 使用数据中的70%对模型进行训练print(knn.predict(X_test)) ## 对剩下的30%的属性进行预测品种print(Y_test)  ## 剩下的30%属性对应的真实品种值print(knn.score(X_test, Y_test))  ## 对该模型中的30%预测结果与真实结果比较，对模型准确度打分
以上对该分类器实例得到的准确度达到93%以上
线性回归实例from sklearn import datasetsfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_splitfrom matplotlib import pyplot as pltboston = datasets.load_boston() ## 加载波士顿房价信息boston_X = boston.data ## X坐标，房价相关属性：大小，位置等boston_Y = boston.target  ## 房价X_train, X_test, Y_train, Y_test = train_test_split(boston_X, boston_Y, test_size=0.3) ##将数据中的70%作为训练数据，30%作为测试数据model = LinearRegression() ## 使用LinearRegression线性回归模型model.fit(X_train, Y_train) ## 对模型进行训练Y_pre = model.predict(X_test) ## 对30%的测试数据进行预测# plt.scatter(X_test[:,0], Y_test) ## 在图上展示与第一个属性相关图表# plt.scatter(X_test[:,0], Y_pre)# plt.show() # y = model.coef_ * X + model.intercept_print(model.coef_) ## 得到对于每个属性的斜率print(model.intercept_) ## 得到在Y坐标的截距print(model.score(X_test, Y_test)) ## 对该模型中的30%预测结果与真实结果比较，对模型准确度打分
以上对房价预测得到的准确度只有60%左右
Normalization（Scale)如果X自变量的范围很大，会影响到数据的预测精确度，可以先对X坐标进行压缩，再对数据进行训练预测
from sklearn import preprocessingimport numpy as npdata = np.array([[10, 23, 3], [-100, 5, 2], [150, 23, 4]], dtype=np.float64)print(data)print(preprocessing.scale(data))## 执行结果[[  10.   23.    3.] [-100.    5.    2.] [ 150.   23.    4.]][[-0.09774528  0.70710678  0.        ] [-1.17294338 -1.41421356 -1.22474487] [ 1.27068866  0.70710678  1.22474487]]
更适合机器学习进行处理
Cross Validation 交叉验证对一个模型打分，之前使用score方法进行评分，但是它只能对于一部分数据作为测试数据进行比较。使用cross_val_score的方法，会将数据分成多组训练数据与测试数据，最后取平均值的方式，对模型打分更准确。  
from sklearn import datasetsfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import cross_val_scoreiris = datasets.load_iris()  ## 加载数据iris_X = iris.data  ## X坐标，叶子长宽等属性iris_Y = iris.target  ## Y坐标， 植物品种[0, 1, 2]knn= KNeighborsClassifier() ## 使用KNeighborsClassifier分类模型scores = cross_val_score(knn, iris_X, iris_Y, cv=5, scoring=&#x27;accuracy&#x27;) ## 将数据分为5份，其中4份为训练数据，1份为测试数据print(scores.mean())
这样得到的评分会更准确。对于KNeighborsClassifier模型训练时，设置n_neighbors的值多少更合适，可以对它的值进行遍历，根据cross_val_score的值得到最合适的配置参数
from sklearn import datasetsfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import cross_val_scorefrom matplotlib import pyplot as pltiris = datasets.load_iris()  ## 加载数据iris_X = iris.data  ## X坐标，叶子长宽等属性iris_Y = iris.target  ## Y坐标， 植物品种[0, 1, 2]k_range = range(1, 31)max_score = []for k in k_range:    knn= KNeighborsClassifier(n_neighbors=k) ## 使用KNeighborsClassifier分类模型    scores = cross_val_score(knn, iris_X, iris_Y, cv=10) ## 进行交叉验证打分 这个值（准确度）是越大，越准确    max_score.append(scores.mean()) ## 计算交叉验证打分的平均值## 将数据在图中打印出来，显目地看到不同n_neighbors值对结果的影响plt.plot(k_range, max_score) plt.xlabel(u&#x27;n_neighbors Value&#x27;)plt.ylabel(u&#x27;Score&#x27;)plt.show()
  
从图中可以我们选择k值，选择12到18之前的值，会有更准确的预测。
过度学习（随着训练数据越多，准确度越低）
learning_curve参数固定时，随着训练数据增多，查看准确度

import numpy as npfrom sklearn.model_selection import learning_curvefrom sklearn.datasets import load_digitsfrom sklearn.svm import SVCfrom matplotlib import pyplot as pltdigits = load_digits() ## 导入数据X = digits.data ## 数据X坐标值Y = digits.target ##数据Y坐标值train_sizes, train_loss, test_loss = learning_curve(SVC(gamma=0.001), X, Y, cv=10,                                                    train_sizes=[0.1, 0.25, 0.5, 0.75, 1]) ## 对SVC(gamma=0.001)进行训练，并标记下10%, 25%, 50%, 75%, 100%作为训练集时的精确度train_loss_mean = np.mean(train_loss, axis=1) ## 训练集准确度test_loss_mean = np.mean(test_loss, axis=1) ## 测试集准确度## 将训练准确度与测试准确度画出来，看到模型不断训练，准确度不断提升plt.plot(train_sizes, train_loss_mean, &#x27;o-&#x27;, color=&quot;r&quot;, label=&quot;Training&quot;)plt.plot(train_sizes, test_loss_mean, &#x27;o-&#x27;, color=&quot;g&quot;, label=&quot;Cross-validation&quot;)plt.show()
  

validation_curve 遍历可能的参数值，找到最合适的参数值

import numpy as npfrom sklearn.model_selection import learning_curve, validation_curvefrom sklearn.datasets import load_digitsfrom sklearn.svm import SVCfrom matplotlib import pyplot as pltdigits = load_digits()X = digits.dataY = digits.targetparam_range = np.logspace(-6, -2.3, 5)train_loss, test_loss = validation_curve(SVC(), X, Y, param_name=&#x27;gamma&#x27;, param_range=param_range, cv=10)## train_sizes为10%， 25%等记录一下train_loss_mean = np.mean(train_loss, axis=1)test_loss_mean = np.mean(test_loss, axis=1)plt.plot(param_range, train_loss_mean, &#x27;o-&#x27;, color=&quot;r&quot;, label=&quot;Training&quot;)plt.plot(param_range, test_loss_mean, &#x27;o-&#x27;, color=&quot;g&quot;, label=&quot;Cross-validation&quot;)plt.xlabel(&quot;gamma&quot;)plt.ylabel(&quot;value&quot;)plt.show()
  
从图中可以看到，gamma使用0.0005值左右的数据准确度会更适合，也不会出现过度训练的问题。
保存训练后的模型

使用pickle模块保存对象


将模型保存到save&#x2F;clf.pickle文件中
from sklearn import svmfrom sklearn import datasetsclf = svm.SVC()iris = datasets.load_iris()X, Y = iris.data, iris.targetclf.fit(X, Y)  ## 训练模型import picklewith open(&#x27;save/clf.pickle&#x27;, &#x27;wb&#x27;) as clf_file:    pickle.dump(clf, clf_file) ## 将模型保存到save/clf.pickle文件中
将模型从save&#x2F;clf.pickle文件中导入，并进行预测
from sklearn import svmfrom sklearn import datasetsiris = datasets.load_iris()X = iris.datawith open(&#x27;save/clf.pickle&#x27;, &#x27;rb&#x27;) as clf_file:    clf = pickle.load(clf_file)print(clf.predict(X[0:1]))


使用joblib


将模型保存到save&#x2F;clf.pkl
from sklearn import svmfrom sklearn import datasetsfrom sklearn.externals import joblibclf = svm.SVC()iris = datasets.load_iris()X, Y = iris.data, iris.targetclf.fit(X, Y)## 保存数据joblib.dump(clf, &#x27;save/clf.pkl&#x27;)
将模型从save&#x2F;clf.pkl文件中导入，并进行预测
from sklearn import svmfrom sklearn import datasetsfrom sklearn.externals import joblibiris = datasets.load_iris()X = iris.data## 导入对象clf = joblib.load(&#x27;save/clf.pkl&#x27;)print(clf.predict(X[0:1])) ## 对数据进行预测

参考教程：
莫烦教学 SKLearn

]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>个性化聚类推荐Kmeans实战</title>
    <url>/posts/4eeb40a1b77e.html</url>
    <content><![CDATA[数据预处理1. 去除不需要的数据目标是：一个用户，后面全是用户的特征

列数据中 Null 数据 &gt; 80% 所有数据 isnull().sum()
列数据中一样的数据 &gt; 80%所有数据 value_counts()
表示同一个意思的字段，只需要保留一个：如用户名、用户账号、支付账号、收货人姓名
根据场景分析不需要的字段：如买家应付货款、应付邮费等

2. 只获取指定的列数据df_order.ix[:, &#x27;订单编号&#x27;, &#x27;买家会员名&#x27;, &#x27;买家实际支付金额&#x27;, &#x27;收货地址&#x27;, &#x27;种类&#x27;, &#x27;数量&#x27;, &#x27;退款金额&#x27;]# ix是loc与iloc的合集，其中loc是按照列名来取数据，iloc是按照列的index来取数据

或者
df_order[[&#x27;订单编号&#x27;, &#x27;买家会员名&#x27;, &#x27;买家实际支付金额&#x27;, &#x27;收货地址&#x27;, &#x27;种类&#x27;, &#x27;数量&#x27;, &#x27;退款金额&#x27;]]

3. 对数据进行格式化df_order[&#x27;收货地址&#x27;] = df_order[&#x27;收货地址&#x27;].apply(lambda x:x.split()[0])

4. 按照某列数据给整行数据打tagdef add_tag(info):  if &#x27;月&#x27; in info:    tag = &#x27;婴儿|&#x27;  if &#x27;&#x27;:    tag = &#x27;幼儿|&#x27;  if:    tag = &#x27;学生|&#x27;  return tag[:-1]attrs[&#x27;tag&#x27;] = attrs[&#x27;适用年龄&#x27;].apply(add_tag)

5. 数据合并pd.merge(items, attrs, on=&#x27;标题&#x27;, how=&#x27;inner&#x27;)

6. 对每个用户进行分组汇总order_tag[&#x27;购买次数&#x27;] = 1test2 = order_tag.groupby([&#x27;买家会员名&#x27;, &#x27;tag2&#x27;]).count()test2.unstack(&#x27;tag2&#x27;).fillna(0) # unstack将会把索引数据中的一列转为列名

7. 对重复的数据取平均值res1 = res1.groupby([&#x27;会员&#x27;, &#x27;地址&#x27;]).mean()res1.reset_index(inplace=True)

8. 将字符串转为数值的方法有两种：
用数值表示具体值，如：好、一般、差就用相关数值表示
unstack将它转为column，或者get_dummies方法将非数值型转为column

   pd.get_dummies?result2= pd.get_dummies(res1) ##只会对非数值型有效

最后得到的数据：
       买家实际支付金额  宝贝种类  宝贝总数量  退款金额  收货地址_上海  收货地址_云南省  收货地址_内蒙古自治区  收货地址_北京  \买家会员名                                                                         409       35.31     8     11   0.0        0         0            0        0   2270      13.70     1      3   0.0        0         0            0        0   1908      55.09     7      7   0.0        0         0            0        0   1727      62.72    11     14   0.0        1         0            0        0   1976      18.80     1      2   0.0        0         1            0        0          收货地址_吉林省  收货地址_四川省    ...      收货地址_湖北省  收货地址_湖南省  收货地址_甘肃省  收货地址_福建省  \买家会员名                        ...                                               409           0         0    ...             0         0         0         0   2270          0         0    ...             0         0         0         0   1908          0         0    ...             0         0         0         0   1727          0         0    ...             0         0         0         0   1976          0         0    ...             0         0         0         0          收货地址_贵州省  收货地址_辽宁省  收货地址_重庆  收货地址_陕西省  收货地址_青海省  收货地址_黑龙江省  买家会员名                                                              409           0         0        0         0         0          0  2270          0         0        0         0         0          0  1908          0         0        0         0         0          0  1727          0         0        0         0         0          0  1976          0         0        0         0         0          0  

基于用户聚类目标：对用户进行标记分类
1. 数据标准化from sklearn.preprocessing import MinMaxScalerdata = order_df.ix[:,1:].values  #去掉第一列数据，也就是用户idmms=MinMaxScaler()data_nore = mms.fit_transform(data)print(data_nore)

2. 聚类- 手肘法from sklearn.cluster import KMeansfrom matplotlib import pyplot as pltsse = []for k in range(1,15):    km = KMeans(n_clusters=k)    km.fit(data_nore)    sse.append(km.inertia_)x = range(1, 15)y = sseplt.plot(x,y)plt.show()- 轮廓法from sklearn.metrics import silhouette_scorescore = []for k in range(2, 15):    km=KMeans(n_clusters=k)    res_km = km.fit(data_nore)    score.append(silhouette_score(data_nore, res_km.labels_))plt.plot(range(2, 15), score, marker=&#x27;o&#x27;)#根据上面的值取得最优n_clusters值，例如为8from sklearn.cluster import KMeanskm=KMeans(n_clusters=8)res_km = km.fit(data_nore)km.labels_ # 这个就是聚类结果,也是最终需要的结果

3. 将类别添加到分析数据中result2[&#x27;类别&#x27;] = km.labels_ #对原始用户进行类别标记

基于用户聚类进行推荐目标：向用户推荐产品
同一类群中，大多数人喜欢的商品，用户也喜欢
order_df[&#x27;商品购买次数&#x27;] = 1data = order_df.groupby([&#x27;类别&#x27;,&#x27;商品&#x27;].count()) #不同类别人对商品的关注程度data.reset_index(inplace=True)print(data.ix[:, [&#x27;类别&#x27;,&#x27;商品&#x27;, &#x27;商品购买次数&#x27;]]) #得到类别-&gt;商品-&gt;购买次数

推荐给用户同一个类别，而没有购买过的商品，按照购买次数排序推荐。
基础知识pandas.DataFrame基本方法df.info() #查看数据信息df.shape #获得数据行列数del(df[&#x27;a&#x27;]) #删除某列数据df[&#x27;col&#x27;].values #查看数据df.columns #查看列名df.reset_index(inplace=True) #重置索引df.index = df[&#x27;index&#x27;] #创建索引df = df.set_index(&#x27;index&#x27;) #将某列设置为索引
df合并merged_df = left_df.merge(right_df, how=&quot;left&quot;, left_on=u&#x27;主机配置&#x27;, right_on=u&#x27;主机说明&#x27;)

根据某条数据的属性添加新的数据列df[&#x27;b&#x27;] = 0df[&#x27;b&#x27;][df[&#x27;a&#x27;]==1] = 8 ##注意先选完整的列，再过滤才能赋值
读取文件时，默认是utf8格式，也可以指定编码常见的编码方式有：ascii&#x2F;utf8&#x2F;unicode&#x2F;utf-16&#x2F;gb2312&#x2F;gbk&#x2F;gb18030
pd.read_csv(&#x27;file_path.csv&#x27;, encoding=&#x27;gb2312&#x27;)

将一列数据转为多列数据df = pd.DataFrame([&#123;&quot;a&quot;: 1, &quot;name&quot;:&quot;1|2|4&quot;&#125;, &#123;&quot;a&quot;: 2, &quot;name&quot;:&quot;1|5|9&quot;&#125;])df[[&quot;b&quot;, &quot;c&quot;, &quot;d&quot;]] = df[&#x27;name&#x27;].str.split(&#x27;|&#x27;, expand=True)  # 多名字分列# 或者df = df.join(df[&#x27;name&#x27;].str.split(&#x27;|&#x27;, expand=True))result:   a   name  b  c  d0  1  1|2|4  1  2  41  2  1|5|9  1  5  9

将DataFrame中的tuple分割成数据框的多列df = pd.DataFrame(&#123;&#x27;a&#x27;:[1,2], &#x27;b&#x27;:[(1,2), (3,4)]&#125;)df[[&#x27;b1&#x27;, &#x27;b2&#x27;]] = df[&#x27;b&#x27;].apply(pd.Series)# 或者df = df.join(df[&#x27;b&#x27;].apply(pd.Series))result:     a       b  0  10  1  (1, 2)  1  21  2  (3, 4)  3  4

stack与unstack使用说明：python pandas stack和unstack函数利用当前数据的多列数据得到新的数据列df[&#x27;new_column&#x27;] = df.apply(lambda row: row[&#x27;column1&#x27;] * row[&#x27;column2&#x27;], axis=1)# 或者df[&#x27;new_column&#x27;]  = df[&#x27;column1&#x27;] * df[&#x27;column2&#x27;]# 或者df.ix[:, &#x27;new_column&#x27;] = df[&#x27;column1&#x27;] * df[&#x27;column2&#x27;]
得到类型中有关系数据库的数据df[df[u&#x27;类型&#x27;].str.contains(u&#x27;关系数据库&#x27;)]
apply函数返回多列数据def parse(item):    return pd.Series([item*2, item*4])a = pd.DataFrame(&#123;&#x27;a&#x27;: [1, 2, 3], &#x27;b&#x27;: [4, 5, 6]&#125;)a[[&#x27;2倍&#x27;, &#x27;4倍&#x27;]] = a[&#x27;a&#x27;].apply(parse)result:   a  b  2倍  4倍0  1  4   2   41  2  5   4   82  3  6   6  12
过滤逻辑类型np.logical_and
data = df[np.logical_and(df[&#x27;col1&#x27;].notnull(), df[&#x27;col1&#x27;].str.contains(&#x27;item&#x27;))]
更改列名data = df.rename(columns=&#123;&#x27;col1&#x27;: u&#x27;列1&#x27;, &#x27;col2&#x27;: u&#x27;列2&#x27;&#125;)
将数据转化为常用类型col1_data = df[&#x27;col1&#x27;].tolist()
]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>SciKit-Learn机器学习入门（二）——Pipeline</title>
    <url>/posts/6e06a73ef9d7.html</url>
    <content><![CDATA[在我们平常的机器学习得到的数据并不能直接使用，必须先对它进行预处理后才能正常使用。以股票数据为例，当前的股价并不是独立的，它与之前的股价是有关系的，一个可用的方法是对对股价处理转为增长幅度。
time1, price1time2, price2
转为
time1, Nantime2, (price2-price1)/price1
SKLear提供了Pipeline工具方便数据处理。
## 定义数据处理类ColumnExtractor，获取指定列的数据class ColumnExtractor(BaseEstimator, TransformerMixin):    def __init__(self, column_name):        self.column_name = column_name    def fit(self, X, y=None):        return self    def transform(self, X, y=None):        return X[self.column_name]## 定义数据处理类TimeSeriesDiff， 对数据作data.diff()/data.shift(1)处理class TimeSeriesDiff(BaseEstimator, TransformerMixin):    def __init__(self, k=1):        self.k = k     def fit(self, X, y=None):        return self    def transform(self, X, y=None):        if type(X) is pd.core.frame.DataFrame or type(X) is pd.core.series.Series:            return X.diff(self.k) / X.shift(self.k)        else:            raise Exception(&quot;Have to be a pandas data frame or Series object!&quot;)## 定义数据处理类TimeSeriesEmbedder，每k个数据组成新的数据，共有N-k组数据class TimeSeriesEmbedder(BaseEstimator, TransformerMixin):    def __init__(self, k):        self.k = k     def fit(self, X, y= None):        return self    def transform(self, X, y = None):        return embed_time_series(X, self.k)def embed_time_series(x, k):    n = len(x)    if k &gt;= n:         raise Exception(&quot;Can not deal with k greater than the length of x&quot;)    output_x = list(map(lambda i: list(x[i:(i+k)]),                         range(0, n-k)))    return np.array(output_x)
使用PipeLine对数据进行预处理组成新的模型
## 对&quot;Close&quot;列数据进行处理pipeline_closing_price = Pipeline([(&quot;ColumnEx&quot;, ColumnExtractor(&quot;Close&quot;)),                                   (&quot;Diff&quot;, TimeSeriesDiff()),                                   (&quot;Embed&quot;, TimeSeriesEmbedder(10)),                                   (&quot;ImputerNA&quot;, Imputer())])## 对&quot;Volume&quot;列数据进行处理pipeline_volume = Pipeline([(&quot;ColumnEx&quot;, ColumnExtractor(&quot;Volume&quot;)),                            (&quot;Diff&quot;, TimeSeriesDiff()),                            (&quot;Embed&quot;, TimeSeriesEmbedder(10)),                            (&quot;ImputerNA&quot;, Imputer())])## 联合将”Close&quot;与&quot;Volume&quot;处理后的数据merged_features = FeatureUnion([(&quot;ClosingPriceFeature&quot;, pipeline_closing_price),                                (&quot;VolumeFeature&quot;, pipeline_volume)])## 将数据中添加多项式特征pipeline_2 = Pipeline([(&quot;MergedFeatures&quot;, merged_features),                       (&quot;PolyFeature&quot;,PolynomialFeatures()),                       (&quot;LinReg&quot;, LinearRegression())])
使用新模型进行数据训练与预测
pipeline_2.fit(data_train, y_train)y_pred_2 = pipeline_2.predict(data_test)
对预测数据进行打分
from sklearn.metrics import r2_scorer2_score(y_test, y_pred_2)
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>DevOps工具汇总大全</title>
    <url>/posts/660b8e31078f.html</url>
    <content><![CDATA[DevOps神道：项目管理工具
Jenkins：CI工具
Ansible：配置管理工具
GitLab&#x2F;Gogs：代码库
Docker：容器引擎
Kubernetes：容器调度
SonarQube：代码静态扫描
Harbor：制品库
Nexus：制品库
TestLink：测试管理
Jmeter：压测工具
Selenium&#x2F;RobotFramework：UI自动化测试
Mattermost：是一款易于使用、经过简化且可扩展的团队通信和企业消息传送系统。  
  
二、  
DevOps 的工具链DevOps中包括版本控制&amp;协作开发工具、自动化构建和测试工具、持续集成&amp;交付工具、部署工具、维护工具、监控，警告&amp;分析工具等等，补充了一些国内的服务，可以让你更好的执行实施 DevOps 工作流。
版本控制&amp;协作开发：GitHub、GitLab、BitBucket、SubVersion、Coding、Bazaar
自动化构建和测试:Apache Ant、Maven 、Selenium、PyUnit、QUnit、JMeter、Gradle、PHPUnit、Nexus
持续集成&amp;交付:Jenkins、Capistrano、BuildBot、Fabric、Tinderbox、Travis CI、flow.ci Continuum、LuntBuild、CruiseControl、Integrity、Gump、Go
容器平台: Docker、Rocket、Ubuntu（LXC）、第三方厂商如（AWS&#x2F;阿里云）
容器编排：Kubernetes、helm、kustomize、cue
配置管理：Chef、Puppet、CFengine、Bash、Rudder、Powershell、RunDeck、Saltstack、Ansible
微服务平台：OpenShift、Cloud Foundry、Kubernetes、Mesosphere
服务开通：Puppet、Docker Swarm、Vagrant、Powershell、OpenStack Heat
日志管理：Logstash、CollectD、StatsD
监控，警告&amp;分析：Nagios、Ganglia、Sensu、zabbix、ICINGA、Graphite、Kibana
代码管理（SCM）：GitHub、GitLab、BitBucket、SubVersion
构建工具：Ant、Gradle、maven
自动部署：Capistrano、CodeDeploy
持续集成（CI）：Bamboo、Hudson、Jenkins
配置管理：Ansible、Chef、Puppet、SaltStack、ScriptRock GuardRail
容器：Docker、LXC、第三方厂商如AWS
编排：Kubernetes、Core、Apache Mesos、DC&#x2F;OS
服务注册与发现：Zookeeper、etcd、Consul
脚本语言：python、ruby、shell
日志管理：ELK、Logentries
系统监控：Datadog、Graphite、Icinga、Nagios、Prometheus、Grafana
性能监控：AppDynamics、New Relic、Splunk
压力测试：JMeter、Blaze Meter、loader.io
预警：PagerDuty、pingdom、厂商自带如AWS SNS
HTTP加速器：Varnish
消息总线：ActiveMQ、SQS
应用服务器：Tomcat、JBoss
Web服务器：Apache、Nginx、IIS
数据库：MySQL、Oracle、PostgreSQL等关系型数据库；cassandra、mongoDB、redis等NoSQL数据库
项目管理（PM）：Jira、Asana、Taiga、Trello、Basecamp、Pivotal Tracker
文档管理 ：HedgeDoc
  
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>GO语言相关命令</title>
    <url>/posts/004977e9e7f3.html</url>
    <content><![CDATA[下载包 go get-x ：下载依赖包，并显示执行的命令
构建 go build-n ：显示构建过程的命令，但不执行-x ：显示执行构建的命令-o ：构建输出可执行文件  
go build -o app 
运行go run . 
包依赖go mod tidy &amp;&amp; go mod vendor
如果遇到以下问题，则是由代码中引入的package地址与项目的git仓库不一致导致的。例如：代码中引入package github.com/coreos/prometheus-operator/pkg/client, 但它的git仓库是github.com/prometheus-operator/prometheus-operator/pkg/client，在执行go mod tidy则会报错：
go: example.com/operator imports        github.com/coreos/prometheus-operator/pkg/client/versioned: github.com/coreos/prometheus-operator/pkg/client@v0.57.0: parsing go.mod:        module declares its path as: github.com/prometheus-operator/prometheus-operator/pkg/client                but was required as: github.com/coreos/prometheus-operator/pkg/client
需要在当前项目中的go.mod中添加replace
module example.comgo 1.18replace github.com/coreos/prometheus-operator/pkg/client =&gt; github.com/prometheus-operator/prometheus-operator/pkg/client v0.57.0
或者执行
go mod edit -replace github.com/coreos/prometheus-operator/pkg/client=github.com/prometheus-operator/prometheus-operator/pkg/client@v0.57.0
展示包下的函数 go docgo doc net/httpgo doc fmt.Printf

Go 升级下载最新版GO:The Go Programming Language
$ go env GOROOT/usr/local/go## 将下载文件解压，替换GOROO文件夹内容
]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Go语言语法</title>
    <url>/posts/1fa648e47be7.html</url>
    <content><![CDATA[顺序编程变量var v10 intv10 = 123v11 := 234// 匿名变量示例func GetName() (firstName, lastName nickName string) &#123;  return &quot;May&quot;, &quot;Chan&quot;, &quot;Chibi&quot;&#125;_, _, nickName := GetName()
常量iota是一个特殊的常量，它会在每一个const关键字出现时重置为0，然后在下一个const出现前，每出现一次iota，它代表的值加1。如果两个const的赋值语句表达式一样，后一个赋值可省略。
const (  c0 = iota  c1 = iota  c2)
枚举const (  Sunday = iota  Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  numberOfDays)
类型
布尔：bool
整型：int8、byte、int16、int、uint、unitptr等
浮点：float32、float64
复数：complex64、complex128
字符串：string：在初始化后不能被修改
字符：rune
错误：error
指针（pointer）
数组（array）
切片（slice）：可动态增减元素
字典（map）
通道（chan）
接口（interface）
结构体（struct）

// 切片例子mySlice := make([]int, 5, 10)fmt.Println(&quot;len(mySlice):&quot;, len(mySlice))fmt.Println(&quot;cap(mySlice):&quot;, cap(mySlice))mySlice = append(mySlice, 1, 2, 3)mySlice2 := []int&#123;8, 9, 10&#125;mySlice = append(mySlice, mySlice2)slice1 := []int&#123;1, 2, 3, 4, 5&#125;slice2 := []int&#123;5, 4, 3&#125;copy(slice2, slice1) //将slice1中的前3个元素复制到slice2copy(slice1, slice2) //将slice2中的前3个元素复制到slice1
// Map例子package mainimport (	&quot;fmt&quot;)type PersonInfo struct&#123;	ID string	Name string	Address string&#125;func main()&#123;	var personDB map[string] PersonInfo	personDB = make(map[string] PersonInfo)	personDB[&quot;12345&quot;] = PersonInfo&#123;&quot;12345&quot;, &quot;Tom&quot;, &quot;Room 12345&quot;&#125;	personDB[&quot;546&quot;] = PersonInfo&#123;&quot;546&quot;, &quot;Tom&quot;, &quot;Room 546&quot;&#125;	person, ok := personDB[&quot;1234&quot;]	if ok &#123;		fmt.Println(&quot;Found person&quot;, person.Name)	&#125;else&#123;		fmt.Println(&quot;Did not find person&quot;)	&#125;	delete(personDB, &quot;546&quot;)	fmt.Println(personDB[&quot;12345&quot;].Name)&#125;
流程控制// if elsefunc if_else(x int) int &#123;	if x == 0 &#123;		return 5	&#125;else&#123;		return x	&#125;&#125;// switch casefunc switch_case(i int)&#123; //无需break	switch i&#123;	case 0:		fmt.Println(&quot;0&quot;)	case 1:		fmt.Println(&quot;1&quot;)	default:		fmt.Println(&quot;Default&quot;)	&#125;&#125;// forsum := 0for &#123;	sum ++	if sum &gt; 10 &#123;		break	&#125;&#125;for j := 0; j &lt; 5; j++ &#123;	ftm.Println(j)&#125;// gotofunc myfunc()&#123;	i := 0	HERE:	fmt.Println(i)	i++	if i &lt; 10 &#123;		goto HERE	&#125;&#125;
函数import &quot;mymath&quot;  //假设Add被放在一个叫mymath的包中c := mymath.Add(1, 2)
小写字母开头的函数只在本包内可见，大写字母开头的函数才能被其他包使用。
func myfunc(args ...int) &#123;  for _, arg := range args &#123;    fmt.Println(arg)&#125; &#125;func myfunc(args ...int) &#123;	for _, arg := range args &#123;		fmt.Println(arg)	&#125;	myfunc2(args...)&#125;
任意类型的不定参数
func Printf(format string, args ...interface&#123;&#125;)&#123;	for _, arg := range args &#123;		switch arg.(type) &#123;			case int:				fmt.Println(arg, &quot;is an int value&quot;)			case string:				fmt.Println(arg, &quot;is a string value&quot;)			case int64:				fmt.Println(arg, &quot;is an int64 value&quot;)			default:				fmt.Println(arg, &quot;is an unknown type&quot;)		&#125;	&#125;&#125;
多返回值
func (file *File) Read(b []byte) (n int, err Error)
匿名函数与闭包
package mainimport(	&quot;fmt&quot;)func main()&#123;	var j int = 5	a := func()(func())&#123;		var i int = 10		return func() &#123;			fmt.Printf(&quot;i, j: %d, %d\n&quot;, i, j)		&#125;	&#125;()	a()	j *= 2	a()&#125;// 结果// i, j: 10, 5// i, j: 10, 10
异常处理type error interface &#123;  Error() string&#125;
大多数函数，如果要返回错误，可以定义如下模式。
func Foo(param int)(n int, err error) &#123;  // ...&#125;n, err := Foo(0)if err != nil &#123;  // 错误处理&#125; else &#123;  // 使用返回值n&#125;type PathError struct &#123;  Op string  Path string  Err  error&#125;func (e *PathError) Error() string &#123;  return e.Op + &quot; &quot; + e.Path + &quot;: &quot; + e.Err.Error()&#125;
defer
func CopyFile(dst, src string) (w int64, err error)&#123;	srcFile, err := os.Open(src)	if err != nil &#123;		return	&#125;	defer srcFile.Close()	dstFile, err := os.Create(dst)	if err != nil &#123;		return	&#125;	defer dstFile.Close()	return io.Copy(dstFile, srcFile)&#125;
panic&#x2F;recover
defer func() &#123;	if r := recover(); r != nil &#123;		log.Printf(&quot;Runtime error caught: %v&quot;, r)	&#125;&#125;()
面向对象编程结构体type Integer intfunc (a Integer) Less(b Integer) bool &#123;  return a &lt; b&#125;
type Header map[string] []stringfunc (h Header) Add(key, value string)&#123;  textproto.MIMEHeader(h).Add(key, value)&#125;func (h Header) Set(key, value string)&#123;  textproto.MIMEHeader(h).Set(key, value)&#125;
引用类型：数组切片、map、channel、接口(interface)值类型：byte、int、bool、float32、float64、string、array、struct、pointer等
type Animal struct &#123;    Name   string  //名称    Color  string  //颜色    Height float32 //身高    Weight float32 //体重    Age    int     //年龄&#125;//奔跑func (a Animal)Run() &#123;    fmt.Println(a.Name + &quot;is running&quot;)&#125;type Lion struct &#123;	Animal //匿名字段&#125;func main()&#123;    var lion = Lion&#123;        Animal&#123;            Name:  &quot;小狮子&quot;,            Color: &quot;灰色&quot;,        &#125;,    &#125;    lion.Run()    fmt.Println(lion.Name)&#125;
匿名组合相当于以其类型名称（去掉包名部分）作为成员变量的名字。Go语言中，要使某个符号对其他包可见，需要将该符号定义为以大写字母开头。Go语言中符号的可访问性是包一级别，而不是类型级别。
接口Go语言中，一个类只需要实现了接口要求的所有函数，就说这个类实现了该接口。
type File struct &#123;  // ...&#125;func (f *File) Read(buf []byte) (n int, err error)func (f *File) Write(buf []byte) (n int, err error)func (f *File) Close() errortype IFile interface&#123;  IReader  IWrite  IClose&#125;type IReader interface&#123;  Read(buf []byte) (n int, err error)&#125;type IWriter interface&#123;  Write(buf []byte) (n int, err error)&#125;type IClose interface&#123;  Close() error&#125;var file1 IFile = new(File)var file2 IReader = new(File)
Go语言中任何对象实例都满足空接口interface{}。
并发编程go channel
package mainimport &quot;fmt&quot;func Count(ch chan int)&#123;	fmt.Println(&quot;Counting&quot;)	ch &lt;- 1&#125;func main()&#123;	chs := make([]chan int, 10)	for i :=0; i &lt; 10; i++ &#123;		chs[i] = make(chan int)		go Count(chs[i])	&#125;	for _, ch := range(chs) &#123;		a := &lt;- ch		fmt.Println(a)	&#125;&#125;
超时机制
timeout := make(chan bool, 1)go func()&#123;	time.Sleep(1e9)	timeout &lt;- true&#125;()select &#123;case &lt;-ch:	//从ch中读取到数据case &lt;-timeout	//一直没有从ch中读取到数据，但从timeout中读取到了数据&#125;
channel的传递
type PipeData struct &#123;	value int	handler func(int) int	next chan int&#125;func handle(queue chan *PipeData)&#123;	for data := range queue&#123;		data.next &lt;- data.handler(data.value)	&#125;&#125;
关闭channel
close(ch)
同步锁
var l sync.Mutexfunc foo()&#123;	l.Lock()	defer l.Unlock()	//...&#125;
全局唯一操作
var a stringvar once sync.oncefunc setup()&#123;	a = &quot;hello&quot;&#125;func doprint() &#123;	once.Do(setup)	print(a)&#125;func twoprint()&#123;	go doprint()	go doprint()&#125;
以上代码setup只会运行一次。
网络编程Socket编程Dial()：连接，支持如下协议：tcp,tcp4,tcp6,udp,udp4,udp6,ip,ip4,ip6
conn, err := net.Dial(&quot;tcp&quot;, &quot;192.168.0.2:2100&quot;)conn2, err := net.Dial(&quot;udp&quot;, &quot;192.168.0.2:53&quot;)conn3, err := net.Dial(&quot;ip4:icmp&quot;, &quot;www.baidu.com&quot;)
Write()与Read()方法来发送数据与接收数据。
HTTP编程net&#x2F;http包，包含HTTP客户端与服务端的具体实现。
resp, err := http.Get(&quot;http://example.com/&quot;)if err != nil&#123;	return&#125;defer resp.Body.close()io.Copy(os.Stdout, resp.Body)
resp, err := http.Post(&quot;http://example.com/upload&quot;, &quot;image/jpeg&quot;, &amp;imageDataBuf)if err != nil&#123;	return&#125;if resp.StatusCode != http.StatusOK&#123;	return&#125;
resp, err := http.PostForm(&quot;http://example.com/posts&quot;, url.Values&#123;&quot;title&quot;: &#123;&quot;article title&quot;&#125;, &quot;content&quot;: &#123;&quot;artical body&quot;&#125;&#125;)if err != nil&#123;	return&#125;
resp, err := http.Head(&quot;http://example.com/&quot;)
req, err := http.NewRequest(&quot;GET&quot;, &quot;http://example.com/posts&quot;, nil)req.Header.Add(&quot;User-Agent&quot;, &quot;Gobook User-Agent&quot;)client := &amp;http.Client&#123;...&#125;resp, err := client.Do(req)
http.HandleFunc(&quot;/bar&quot;, func(w http.ResponseWriter, r *http.Request)&#123;  fmt.Fprintf(w, &quot;hello&quot;)&#125;)
RPCnet&#x2F;rpc，包实现RPC协议的细节
func (t *T) MethodName(argType T1, replyType *T2) error
T1与T2默认会使用encoding&#x2F;gob包进行编码与解码。argType表示由RPC客户端传入的参数replyType表示要返回给RPC客户端的结果服务端示例
package mainimport (	&quot;log&quot;	&quot;net&quot;	&quot;net/http&quot;	&quot;net/rpc&quot;)type Args struct&#123;	A, B int&#125;type Arith intfunc (t *Arith) Multiply(args *Args, reply *int) error&#123;	*reply = args.A * args.B	return nil&#125;func main()&#123;	arith := new(Arith)	rpc.Register(arith)	rpc.HandleHTTP()	l, e := net.Listen(&quot;tcp&quot;, &quot;:1234&quot;)	if e != nil&#123;		log.Fatal(&quot;listen error&quot;, e)	&#125;	http.Serve(l, nil)&#125;
客户端示例
package mainimport (	&quot;net/rpc&quot;	&quot;fmt&quot;	&quot;log&quot;)type Args struct&#123;	A, B int&#125;func main()&#123;// 顺序执行RPC	client, _ := rpc.DialHTTP(&quot;tcp&quot;, &quot;127.0.0.1:1234&quot;)	args := &amp;Args&#123;7,8&#125;	var reply int	err := client.Call(&quot;Arith.Multiply&quot;, args, &amp;reply)	if err != nil&#123;		log.Fatal(err)	&#125;	fmt.Println(reply)//异步执行RPC        var reply2 int	divCall := client.Go(&quot;Arith.Multiply&quot;, &amp;Args&#123;6,8&#125;, &amp;reply2, nil)	divCall = &lt;-divCall.Done	fmt.Println(reply2)&#125;
JSONencoding&#x2F;json标准库json.Marshal()，将一组数据进行JSON格式化编码json.Unmarshal()，将JSON格式的文档解码为Go里边预期的数据结构。
data := make(map[string] string)data[&quot;name&quot;] = &quot;Michael&quot;data2, err := json.Marshal(data)if err != nil&#123;	fmt.Println(&quot;err&quot;)	return&#125;fmt.Println(string(data2[:]))
data := make(map[string] string)data_json := []byte(&quot;&#123;\&quot;name\&quot;:\&quot;Michael\&quot;&#125;&quot;)json.Unmarshal(data_json, &amp;data)fmt.Println(data)
反射type T struct&#123;	A int	B string&#125;t := T&#123;203, &quot;mh203&quot;&#125;s := reflect.ValueOf(&amp;t).Elem()typeOfT := s.Type()for i := 0; i &lt; s.NumField(); i++ &#123;	f := s.Field(i)	fmt.Printf(&quot;%d: %s %s = %v\n&quot;, i, typeOfT.Field(i).Name, f.Type(), f.Interface())&#125;
Go工具包文档查看（网站形式）
$ godoc -http=:8080$ godoc -http=:8080 -path=&quot;.&quot;
包文档查看（命令行形式）
$ go doc foo
代码格式化
$ go fmt hello.go$ gofmt -l -w .
项目构建go build：用于测试编译包，在项目目录下生成可执行文件（有main包）；不能生成包文件go install：主要用来生成库和工具。一是编译包文件（无main包），将编译后的包文件放到 pkg 目录下（$GOPATH&#x2F;pkg）。二是编译生成可执行文件（有main包），将可执行文件放到 bin 目录（$GOPATH&#x2F;bin）；生成可执行文件在当前目录下， go install 生成可执行文件在bin目录下（$GOPATH&#x2F;bin）
$ GOOS=linux GOARCH=amd64 go build calc $ go install calc
GOOS：系统，linux&#x2F;darwin&#x2F;windowsGOARCH：386&#x2F;amd64
单元测试
$ go test calc
模块管理go mod
$ go mod download  # 下载指定模块$ go mod init test.com/go  #初始化当前模块$ go mod vendor       # 将依赖复制到当前vendor目录下
使用go module后，项目代码不需要放在GOPATH目录下。import项目的package时使用module路径。
以下是beego应用使用module来管理依赖的例子
$ cat &lt;&lt;EOF &gt; go.modmodule beegoappgo 1.12replace (	golang.org/x/crypto =&gt; github.com/golang/crypto v0.0.0-20191227163750-53104e6ec876	golang.org/x/mod =&gt; github.com/golang/mod v0.1.0	golang.org/x/net =&gt; github.com/golang/net v0.0.0-20191209160850-c0dbc17a3553	golang.org/x/sync =&gt; github.com/golang/sync v0.0.0-20190911185100-cd5d95a43a6e	golang.org/x/sys =&gt; github.com/golang/sys v0.0.0-20191228213918-04cbcbbfeed8	golang.org/x/text =&gt; github.com/golang/text v0.3.2	golang.org/x/tools =&gt; github.com/golang/tools v0.0.0-20191230220329-2aa90c603ae3	golang.org/x/xerrors =&gt; github.com/golang/xerrors v0.0.0-20191204190536-9bdfabe68543)require (	github.com/astaxie/beego v1.12.0	github.com/shiena/ansicolor v0.0.0-20151119151921-a422bbe96644 // indirect	github.com/smartystreets/goconvey v1.6.4)EOF]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Go-常用设计模式</title>
    <url>/posts/4febcb7bc722.html</url>
    <content><![CDATA[  
1. 单例模式：package singletonimport (    &quot;sync&quot;)type singleton struct &#123;&#125;var ins *singletonvar once sync.Oncefunc GetInsOr() *singleton &#123;    once.Do(func() &#123;        ins = &amp;singleton&#123;&#125;    &#125;)    return ins&#125;

2. 简单工厂模式type Person struct &#123;  Name string  Age int&#125;func (p Person) Greet() &#123;  fmt.Printf(&quot;Hi! My name is %s&quot;, p.Name)&#125;func NewPerson(name string, age int) *Person &#123;  return &amp;Person&#123;    Name: name,    Age: age,  &#125;&#125;
3. 抽象工厂模式它返回的是接口而不是结构体。
type Person interface &#123;  Greet()&#125;type person struct &#123;  name string  age int&#125;func (p person) Greet() &#123;  fmt.Printf(&quot;Hi! My name is %s&quot;, p.name)&#125;// Here, NewPerson returns an interface, and not the person struct itselffunc NewPerson(name string, age int) Person &#123;  return person&#123;    name: name,    age: age,  &#125;&#125;
4. 工厂方法模式type Person struct &#123;  name string  age int&#125;func NewPersonFactory(age int) func(name string) Person &#123;  return func(name string) Person &#123;    return Person&#123;      name: name,      age: age,    &#125;  &#125;&#125;
5. 策略模式在项目开发中，我们经常要根据不同的场景，采取不同的措施，也就是不同的策略。比如，假设我们需要对 a、b 这两个整数进行计算，根据条件的不同，需要执行不同的计算方式。为了解耦，需要使用策略模式，定义一些独立的类来封装不同的算法，每一个类封装一个具体的算法（即策略）。
package strategy// 策略模式// 定义一个策略类type IStrategy interface &#123;  do(int, int) int&#125;// 策略实现：加type add struct&#123;&#125;func (*add) do(a, b int) int &#123;  return a + b&#125;// 策略实现：减type reduce struct&#123;&#125;func (*reduce) do(a, b int) int &#123;  return a - b&#125;// 具体策略的执行者type Operator struct &#123;  strategy IStrategy&#125;// 设置策略func (operator *Operator) setStrategy(strategy IStrategy) &#123;  operator.strategy = strategy&#125;// 调用策略中的方法func (operator *Operator) calculate(a, b int) int &#123;  return operator.strategy.do(a, b)&#125;
6. 模板模式模板模式就是将一个类中能够公共使用的方法放置在抽象类中实现，将不能公共使用的方法作为抽象方法，强制子类去实现，这样就做到了将一个类作为一个模板，让开发者去填充需要填充的地方。
package templateimport &quot;fmt&quot;type Cooker interface &#123;  fire()  cooke()  outfire()&#125;// 类似于一个抽象类type CookMenu struct &#123;&#125;func (CookMenu) fire() &#123;  fmt.Println(&quot;开火&quot;)&#125;// 做菜，交给具体的子类实现func (CookMenu) cooke() &#123;&#125;func (CookMenu) outfire() &#123;  fmt.Println(&quot;关火&quot;)&#125;// 封装具体步骤func doCook(cook Cooker) &#123;  cook.fire()  cook.cooke()  cook.outfire()&#125;type XiHongShi struct &#123;  CookMenu&#125;func (*XiHongShi) cooke() &#123;  fmt.Println(&quot;做西红柿&quot;)&#125;type ChaoJiDan struct &#123;  CookMenu&#125;func (ChaoJiDan) cooke() &#123;  fmt.Println(&quot;做炒鸡蛋&quot;)&#125;
##7. 代理模式可以为另一个对象提供一个替身或者占位符，以控制对这个对象的访问。
package proxyimport &quot;fmt&quot;type Seller interface &#123;  sell(name string)&#125;// 火车站type Station struct &#123;  stock int //库存&#125;func (station *Station) sell(name string) &#123;  if station.stock &gt; 0 &#123;    station.stock--    fmt.Printf(&quot;代理点中：%s买了一张票,剩余：%d \n&quot;, name, station.stock)  &#125; else &#123;    fmt.Println(&quot;票已售空&quot;)  &#125;&#125;// 火车代理点type StationProxy struct &#123;  station *Station // 持有一个火车站对象&#125;func (proxy *StationProxy) sell(name string) &#123;  if proxy.station.stock &gt; 0 &#123;    proxy.station.stock--    fmt.Printf(&quot;代理点中：%s买了一张票,剩余：%d \n&quot;, name, proxy.station.stock)  &#125; else &#123;    fmt.Println(&quot;票已售空&quot;)  &#125;&#125;
8. 选项模式package optionsimport (  &quot;time&quot;)type Connection struct &#123;  addr    string  cache   bool  timeout time.Duration&#125;const (  defaultTimeout = 10  defaultCaching = false)type options struct &#123;  timeout time.Duration  caching bool&#125;// Option overrides behavior of Connect.type Option interface &#123;  apply(*options)&#125;type optionFunc func(*options)func (f optionFunc) apply(o *options) &#123;  f(o)&#125;func WithTimeout(t time.Duration) Option &#123;  return optionFunc(func(o *options) &#123;    o.timeout = t  &#125;)&#125;func WithCaching(cache bool) Option &#123;  return optionFunc(func(o *options) &#123;    o.caching = cache  &#125;)&#125;// Connect creates a connection.func NewConnect(addr string, opts ...Option) (*Connection, error) &#123;  options := options&#123;    timeout: defaultTimeout,    caching: defaultCaching,  &#125;  for _, o := range opts &#123;    o.apply(&amp;options)  &#125;  return &amp;Connection&#123;    addr:    addr,    cache:   options.caching,    timeout: options.timeout,  &#125;, nil&#125;
选项模式通常适用于以下场景：

结构体参数很多，创建结构体时，我们期望创建一个携带默认值的结构体变量，并选择性修改其中一些参数的值。
结构体参数经常变动，变动时我们又不想修改创建实例的函数。例如：结构体新增一个 retry 参数，但是又不想在 NewConnect 入参列表中添加retry int这样的参数声明。

参考文章
Go 语言项目开发实战&#x2F;设计模式

]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Go常用资源列表</title>
    <url>/posts/6b0e27e96a4e.html</url>
    <content><![CDATA[Awesome go 最全资源列表Awesome go 中文版以下是项目中使用到的部分package。
网络mux实现了请求路由器和调度程序，用于将传入请求与其各自的处理程序匹配。例如统一HTTP与RPC服务入口。
数据库db migrate支持常见的数据库升级与降级rediselasticsearch cligormGORM 是 Go 语言的 ORM 包，功能强大，调用方便。
模板引擎pongo2非预编译，类django特性的引擎，简单易用hero预编译模板引擎，性能更好
监控Prometheus Client开发exporter
日志Uber zap对性能和内存做了极致的优化
命令工具Cobrakubectl使用Cobra开发命令行工具Kingpingo-flagscligoctl工具使用是这个命令行工具
单元测试Mock testify&#x2F;mock可用来Mock Http请求Sql mock driver可以用来模拟数据库连接。数据库是项目中比较常见的依赖，在遇到数据库依赖时都可以用它。Testify测试断言包 Automatically generate Go test boilerplate from your source code自动生成测试用例Go testing in the browserGoConvey 是一款针对 Golang 的测试框架，可以管理和运行测试用例，同时提供了丰富的断言函数，并支持很多 Web 界面特性。
go自带代码测试覆盖率分析工具
$ go test -coverprofile=coverage.out$ go tool cover -func=coverage.out$ go tool cover -html=coverage.out -o coverage.html

工具集go yamluuiduuid的创建与解析backoff重试工具，支持指数退避算法Kubernetes clientaws clienttext&#x2F;tabwriter go自带输出对齐goquery类似jquery解析html dombindata将静态文件转为GO文件cookiecutter项目初始化生成，可按照生产最佳实践快速启动各种项目
##分布式作业系统Dkron - Distributed, fault tolerant job scheduling system https://dkron.iodkron 是一个分布式、启动迅速、带容错机制的定时作业系统，支持 crontab 表达式。它具有下面这些核心特性。

易用：可以通过易操作、漂亮的 Web 界面来管理作业。
可靠：具备容错机制，一个节点不可用，其他节点可继续执行作业。
高可扩展性：能够处理大量的计划作业和数千个节点。

ouqiang&#x2F;gocron: 定时任务管理系统gocron 是国人开发的轻量级定时任务集中调度和管理系统, 用于替代 Linux-crontab。它具有下面这些核心特性。

具有 Web 界面管理定时任务。
支持 crontab 时间格式，并精确到秒。
支持 shell 命令和 HTTP 请求两种任务格式。具
有任务超时机制、任务依赖机制、任务执行失败可重试机制。
支持查看任务执行日志，并支持用邮件、Slack、Webhook 等方式通知任务执行结果。

robfig&#x2F;cron: a cron library for go (github.com)一个可以实现类似 Linux crontab 定时任务的 cron 包，支持到秒
微服务框架go-kit&#x2F;kit: 微服务标准库: 应用Go kitgo-zero: 云原生生产级别微服务框架Kratos: 一套轻量级 Go 微服务框架，包含大量微服务相关框架及工具
]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Go语言编码规范指导</title>
    <url>/posts/cf3326842a29.html</url>
    <content><![CDATA[From：茹姐  https://zhuanlan.zhihu.com/p/63250689  
本规范旨在为日常Go项目开发提供一个代码的规范指导，方便团队形成一个统一的代码风格，提高代码的可读性，规范性和统一性。本规范将从命名规范，注释规范，代码风格和 Go 语言提供的常用的工具这几个方面做一个说明。该规范参考了 go 语言官方代码的风格制定。
一、 命名规范命名是代码规范中很重要的一部分，统一的命名规则有利于提高的代码的可读性，好的命名仅仅通过命名就可以获取到足够多的信息。
Go在命名时以字母a到Z或a到Z或下划线开头，后面跟着零或更多的字母、下划线和数字(0到9)。Go不允许在命名时中使用@、$和%等标点符号。Go是一种区分大小写的编程语言。因此，Manpower和manpower是两个不同的命名。
当命名（包括常量、变量、类型、函数名、结构字段等等）以一个大写字母开头，如：Group1，那么使用这种形式的标识符的对象就可以被外部包的代码所使用（客户端程序需要先导入这个包），这被称为导出（像面向对象语言中的 public）；命名如果以小写字母开头，则对包外是不可见的，但是他们在整个包的内部是可见并且可用的（像面向对象语言中的 private ）
1、包命名：package保持package的名字和目录保持一致，尽量采取有意义的包名，简短，有意义，尽量和标准库不要冲突。包名应该为小写单词，不要使用下划线或者混合大小写。
package demo
package main
2、 文件命名尽量采取有意义的文件名，简短，有意义，应该为小写单词，使用下划线分隔各个单词。
my_test.go
3、 结构体命名采用驼峰命名法，首字母根据访问控制大写或者小写struct 申明和初始化格式采用多行，例如下面：&#x2F;&#x2F; 多行申明type User struct{    Username  string    Email     string}
&#x2F;&#x2F; 多行初始化u :&#x3D; User{    Username: “astaxie”,    Email:    “astaxie@gmail.com“,}
4、 接口命名命名规则基本和上面的结构体类型单个函数的结构名以 “er” 作为后缀，例如 Reader , Writer 。type Reader interface {        Read(p []byte) (n int, err error)}
5、变量命名和结构体类似，变量名称一般遵循驼峰法，首字母根据访问控制原则大写或者小写，但遇到特有名词时，需要遵循以下规则：如果变量为私有，且特有名词为首个单词，则使用小写，如 apiClient其它情况都应当使用该名词原有的写法，如 APIClient、repoID、UserID错误示例：UrlArray，应该写成 urlArray 或者 URLArray若变量类型为 bool 类型，则名称应以 Has, Is, Can 或 Allow 开头var isExist boolvar hasConflict boolvar canManage boolvar allowGitHook bool
6、常量命名常量均需使用全部大写字母组成，并使用下划线分词
const APP_VER &#x3D; “1.0”如果是枚举类型的常量，需要先创建相应类型：
type Scheme string
const (    HTTP  Scheme &#x3D; “http”    HTTPS Scheme &#x3D; “https”)
7、 关键字下面的列表显示了Go中的保留字。这些保留字不能用作常量或变量或任何其他标识符名称。
二、注释Go提供C风格的&#x2F;* *&#x2F;块注释和C ++风格的&#x2F;&#x2F;行注释。行注释是常态；块注释主要显示为包注释，但在表达式中很有用或禁用大量代码。
单行注释是最常见的注释形式，你可以在任何地方使用以 &#x2F;&#x2F; 开头的单行注释多行注释也叫块注释，均已以 &#x2F;* 开头，并以 *&#x2F; 结尾，且不可以嵌套使用，多行注释一般用于包的文档描述或注释成块的代码片段go 语言自带的 godoc 工具可以根据注释生成文档，生成可以自动生成对应的网站（ http://golang.org 就是使用 godoc 工具直接生成的），注释的质量决定了生成的文档的质量。每个包都应该有一个包注释，在package子句之前有一个块注释。对于多文件包，包注释只需要存在于一个文件中，任何一个都可以。包评论应该介绍包，并提供与整个包相关的信息。它将首先出现在godoc页面上，并应设置下面的详细文档。
详细的如何写注释可以 参考：http://golang.org/doc/effective_go.html#commentary
1、包注释每个包都应该有一个包注释，一个位于package子句之前的块注释或行注释。包如果有多个go文件，只需要出现在一个go文件中（一般是和包同名的文件）即可。 包注释应该包含下面基本信息(请严格按照这个顺序，简介，创建人，创建时间）：
包的基本简介（包名，简介）创建者，格式： 创建人： rtx 名创建时间，格式：创建时间： yyyyMMdd例如 util 包的注释示例如下
&#x2F;&#x2F; util 包， 该包包含了项目共用的一些常量，封装了项目中一些共用函数。&#x2F;&#x2F; 创建人： hanru&#x2F;&#x2F; 创建时间： 20190419
2、结构（接口）注释每个自定义的结构体或者接口都应该有注释说明，该注释对结构进行简要介绍，放在结构体定义的前一行，格式为： 结构体名， 结构体说明。同时结构体内的每个成员变量都要有说明，该说明放在成员变量的后面（注意对齐），实例如下：
&#x2F;&#x2F; User ， 用户对象，定义了用户的基础信息type User struct{    Username  string &#x2F;&#x2F; 用户名    Email     string &#x2F;&#x2F; 邮箱}
3、函数（方法）注释每个函数，或者方法（结构体或者接口下的函数称为方法）都应该有注释说明，函数的注释应该包括三个方面（严格按照此顺序撰写）：
简要说明，格式说明：以函数名开头，“，”分隔说明部分参数列表：每行一个参数，参数名开头，“，”分隔说明部分返回值： 每行一个返回值示例如下：
&#x2F;&#x2F; NewtAttrModel ， 属性数据层操作类的工厂方法&#x2F;&#x2F; 参数：&#x2F;&#x2F;      ctx ： 上下文信息&#x2F;&#x2F; 返回值：&#x2F;&#x2F;      属性操作类指针func NewAttrModel(ctx *common.Context) *AttrModel {}
4、代码逻辑注释对于一些关键位置的代码逻辑，或者局部较为复杂的逻辑，需要有相应的逻辑说明，方便其他开发者阅读该段代码，实例如下：
&#x2F;&#x2F; 从 Redis 中批量读取属性，对于没有读取到的 id ， 记录到一个数组里面，准备从 DB 中读取xxxxxxxxxxxxxxxxxxx
5、注释风格统一使用中文注释，对于中英文字符之间严格使用空格分隔， 这个不仅仅是中文和英文之间，英文和中文标点之间也都要使用空格分隔，例如：
&#x2F;&#x2F; 从 Redis 中批量读取属性，对于没有读取到的 id ， 记录到一个数组里面，准备从 DB 中读取上面 Redis 、 id 、 DB 和其他中文字符之间都是用了空格分隔。
建议全部使用单行注释和代码的规范一样，单行注释不要过长，禁止超过 120 字符。
三、代码风格1、缩进和折行缩进直接使用 gofmt 工具格式化即可（gofmt 是使用 tab 缩进的）；折行方面，一行最长不超过120个字符，超过的请使用换行展示，尽量保持格式优雅。我们使用Goland开发工具，可以直接使用快捷键：ctrl+alt+L，即可。
2、语句的结尾Go语言中是不需要类似于Java需要冒号结尾，默认一行就是一条数据
如果你打算将多个语句写在同一行，它们则必须使用 ;
3、括号和空格括号和空格方面，也可以直接使用 gofmt 工具格式化（go 会强制左大括号不换行，换行会报语法错误），所有的运算符和操作数之间要留空格。
&#x2F;&#x2F; 正确的方式if a &gt; 0 {
} 
&#x2F;&#x2F; 错误的方式if a&gt;0  &#x2F;&#x2F; a ，0 和 &gt; 之间应该空格{       &#x2F;&#x2F; 左大括号不可以换行，会报语法错误
}
4、import 规范import在多行的情况下，goimports会自动帮你格式化，但是我们这里还是规范一下import的一些规范，如果你在一个文件里面引入了一个package，还是建议采用如下格式：
import (    “fmt”)如果你的包引入了三种类型的包，标准库包，程序内部包，第三方包，建议采用如下方式进行组织你的包：
import (    “encoding&#x2F;json”    “strings”
&quot;myproject/models&quot;
&quot;myproject/controller&quot;
&quot;myproject/utils&quot;

&quot;github.com/astaxie/beego&quot;
&quot;github.com/go-sql-driver/mysql&quot;

)有顺序的引入包，不同的类型采用空格分离，第一种实标准库，第二是项目包，第三是第三方包。
在项目中不要使用相对路径引入包：
&#x2F;&#x2F; 这是不好的导入import “..&#x2F;net”
&#x2F;&#x2F; 这是正确的做法import “github.com&#x2F;repo&#x2F;proj&#x2F;src&#x2F;net”但是如果是引入本项目中的其他包，最好使用相对路径。
5、错误处理错误处理的原则就是不能丢弃任何有返回err的调用，不要使用 _ 丢弃，必须全部处理。接收到错误，要么返回err，或者使用log记录下来尽早return：一旦有错误发生，马上返回尽量不要使用panic，除非你知道你在做什么错误描述如果是英文必须为小写，不需要标点结尾采用独立的错误流进行处理&#x2F;&#x2F; 错误写法if err !&#x3D; nil {    &#x2F;&#x2F; error handling} else {    &#x2F;&#x2F; normal code}
&#x2F;&#x2F; 正确写法if err !&#x3D; nil {    &#x2F;&#x2F; error handling    return &#x2F;&#x2F; or continue, etc.}&#x2F;&#x2F; normal code
6、测试单元测试文件名命名规范为 example_test.go 测试用例的函数名称必须以 Test 开头，例如：TestExample 每个重要的函数都要首先编写测试用例，测试用例和正规代码一起提交方便进行回归测试
四、常用工具上面提到了很过规范， go 语言本身在代码规范性这方面也做了很多努力，很多限制都是强制语法要求，例如左大括号不换行，引用的包或者定义的变量不使用会报错，此外 go 还是提供了很多好用的工具帮助我们进行代码的规范，
gofmt 大部分的格式问题可以通过gofmt解决， gofmt 自动格式化代码，保证所有的 go 代码与官方推荐的格式保持一致，于是所有格式有关问题，都以 gofmt 的结果为准。
goimport 我们强烈建议使用 goimport ，该工具在 gofmt 的基础上增加了自动删除和引入包.
go get golang.org&#x2F;x&#x2F;tools&#x2F;cmd&#x2F;goimportsgo vet vet工具可以帮我们静态分析我们的源码存在的各种问题，例如多余的代码，提前return的逻辑，struct的tag是否符合标准等。
go get golang.org&#x2F;x&#x2F;tools&#x2F;cmd&#x2F;vet使用如下：
go vet .
]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Vagrant扩展虚拟机盘</title>
    <url>/posts/97a19be5601d.html</url>
    <content><![CDATA[扩展虚拟机盘# 停止虚拟机vagrant halt &lt;machine_name&gt;# 进入VirtualBox VMs目录，查看并记录原磁盘uuid，留作后用vboxmanage showhdinfo box-disk1.vmdk# 克隆磁盘，vmdk格式无法调整大小，需要转成vdi格式vboxmanage clonehd box-disk1.vmdk new-virtualdisk.vdi --format vdi# 调整克隆磁盘的大小，这里调整为400Gvboxmanage modifyhd new-virtualdisk.vdi --resize 409600# 在克隆磁盘的基础上再克隆vdi格式的磁盘vboxmanage clonehd new-virtualdisk.vdi resized.vmdk --format vmdk# 覆盖原磁盘（如果担心磁盘数据出现不可逆损坏，请先做好备份）mv resized.vmdk box-disk1.vmdk# 此时节已删除中间文件rm new-virtualdisk.vdi# !!!此时启动虚机或查看磁盘信息会报错，提示uuid不匹配，因为磁盘已经变了，需要改回之前记录的uuidvboxmanage internalcommands sethduuid box-disk1.vmdk &lt;old_uuid&gt;# done，可以重启虚机了，可根据需要在虚机上进行磁盘分配，这里不再展开vagrant up &lt;machine_name&gt;
进入主机，创建盘分区# 查看分区：比如/dev/sadfdisk -l # 开始分区fdisk /dev/sda# m 帮助菜单 \ p 该整磁盘详细信息 \ d 删除分区 \ n 添加分区 \w保存变更
重启并挂载目录# 重启虚拟机vagrant reload# 格式化分区mkfs.ext4 /dev/sda4# 挂载mount /dev/sda4 /mnt# 设置开机自动挂载echo &quot;/dev/sda4 /mnt ext4 defaults 0 0&quot; &gt;&gt; /etc/fstab# 查看当前挂载目录情况df -h
Vagrant国内相关镜像下载参考Vagrant使用国内镜像安装插件和box镜像
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-常见运维脚本及问题解决</title>
    <url>/posts/2318982d2c1f.html</url>
    <content><![CDATA[1.扩容计算结点在执行扩容前需检查扩容节点的以下几点信息：

内核版本
selinux已经开启enforcing
docker数据盘已经就绪
&#x2F;etc&#x2F;resolv.conf配置正确
hostname已经设置
时间同步已配置
在每个节点都能解析新增节点的域名，如果是通过&#x2F;etc&#x2F;hosts来配置域名解析，需要在配置后重启所有节点的dnsmasq服务
docker证书的问题需要添加到自动化配置中来，特别是私有镜像仓库的证书。有三个地方：
&#x2F;etc&#x2F;sysconfig&#x2F;docker配置，
&#x2F;etc&#x2F;pki&#x2F;ca-trust&#x2F;source&#x2F;anchors&#x2F;目录下的证书，
&#x2F;etc&#x2F;docker&#x2F;certs.d下docker拉取镜像认证证书



# /etc/ansible/hosts[OSEv3:children]mastersnodesetcdnew_nodes...[new_nodes]node04.internal.aws.testdrive.openshift.com openshift_node_labels=&quot;&#123;&#x27;region&#x27;: &#x27;apps&#x27;&#125;&quot; openshift_hostname=node04.internal.aws.testdrive.openshift.com openshift_public_hostname=node04.580763383722.aws.testdrive.openshift.comnode05.internal.aws.testdrive.openshift.com openshift_node_labels=&quot;&#123;&#x27;region&#x27;: &#x27;apps&#x27;&#125;&quot; openshift_hostname=node05.internal.aws.testdrive.openshift.com openshift_public_hostname=node05.580763383722.aws.testdrive.openshift.comnode06.internal.aws.testdrive.openshift.com openshift_node_labels=&quot;&#123;&#x27;region&#x27;: &#x27;apps&#x27;&#125;&quot; openshift_hostname=node06.internal.aws.testdrive.openshift.com openshift_public_hostname=node06.580763383722.aws.testdrive.openshift.com...

在dns中配置新增的节点。
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-node/scaleup.yml
注意：如果集群是通过&#x2F;etc&#x2F;hosts文件来配置的解析，则需在添加好对应配置关系后，重启所有节点的dnsmasq。否则会报“could not find csr for nodes”的错误。
2.OpenShift Metrics...[OSEv3:vars]...openshift_metrics_install_metrics=trueopenshift_metrics_cassandra_storage_type=pvopenshift_metrics_cassandra_pvc_size=10Giopenshift_metrics_hawkular_hostname=metrics.apps.580763383722.aws.testdrive.openshift.com...

ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml

3.OpenShift Logging...[OSEv3:vars]...openshift_logging_install_logging=trueopenshift_logging_namespace=loggingopenshift_logging_es_pvc_size=10Giopenshift_logging_kibana_hostname=kibana.apps.580763383722.aws.testdrive.openshift.comopenshift_logging_public_master_url=https://kibana.apps.580763383722.aws.testdrive.openshift.com...

ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml

4.OpenShift Multitenant Networkingos_sdn_network_plugin_name=redhat/openshift-ovs-multitenant

# net-proj.sh#!/bin/bash# create NetworkA, NetworkB projects/usr/bin/oc new-project netproj-a/usr/bin/oc new-project netproj-b# deploy the DC definition into the projects/usr/bin/oc create -f /opt/lab/support/ose.yaml -n netproj-a/usr/bin/oc create -f /opt/lab/support/ose.yaml -n netproj-b


#ose.yamlapiVersion: v1kind: DeploymentConfigmetadata:  name: ose  labels:    run: osespec:  strategy:    type: Rolling    rollingParams:      updatePeriodSeconds: 1      intervalSeconds: 1      timeoutSeconds: 600      maxUnavailable: 25%      maxSurge: 25%    resources:  triggers:    -      type: ConfigChange  replicas: 1  test: false  selector:    run: ose  template:    metadata:      creationTimestamp: null      labels:        run: ose    spec:      containers:        -          name: ose          image: &#x27;registry.access.redhat.com/openshift3/ose:v3.5&#x27;          command:            - bash            - &#x27;-c&#x27;            - &#x27;while true; do sleep 60; done&#x27;          resources:          terminationMessagePath: /dev/termination-log          imagePullPolicy: IfNotPresent      restartPolicy: Always      terminationGracePeriodSeconds: 30      dnsPolicy: ClusterFirst      securityContext:

#podbip.sh#!/bin/bash/usr/bin/oc get pod -n netproj-b $(oc get pod -n netproj-b | awk &#x27;/ose-/ &#123;print $1&#125;&#x27;) -o jsonpath=&#x27;&#123;.status.podIP&#125;&#123;&quot;\n&quot;&#125;&#x27;

将netproj-a网络与netproj-b网络连接oc adm pod-network join-projects netproj-a --to=netproj-boc get netnamespace

将netproj-a网络脱离oc adm pod-network isolate-projects netproj-aoc get netnamespaceoc exec -n netproj-a $POD_A_NAME -- ping -c1 -W1 $POD_B_IP

5.Node管理将Node隔离出集群oc adm manage-node node02.internal.aws.testdrive.openshift.com --schedulable=false

查看指定Node上运行的podoc adm manage-node node02.internal.aws.testdrive.openshift.com --list-pods

迁移指定Node上的pod模拟迁移oc adm manage-node node02.internal.aws.testdrive.openshift.com --evacuate --dry-run
迁移oc adm manage-node node02.internal.aws.testdrive.openshift.com --evacuate

恢复Node的可调度oc adm manage-node node02.internal.aws.testdrive.openshift.com --schedulable=true

创建volumeoc volume dc/file-uploader --add --name=my-shared-storage \-t pvc --claim-mode=ReadWriteMany --claim-size=5Gi \--claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded

Increasing Storage Capacity in CNS[...][cns]node01.580763383722.aws.testdrive.openshift.comnode02.580763383722.aws.testdrive.openshift.comnode03.580763383722.aws.testdrive.openshift.comnode04.580763383722.aws.testdrive.openshift.comnode05.580763383722.aws.testdrive.openshift.comnode06.580763383722.aws.testdrive.openshift.com[...]


ansible-playbook /opt/lab/support/configure-firewall.yaml

oc label node/node04.internal.aws.testdrive.openshift.com storagenode=glusterfsoc label node/node05.internal.aws.testdrive.openshift.com storagenode=glusterfsoc label node/node06.internal.aws.testdrive.openshift.com storagenode=glusterfs

export HEKETI_CLI_SERVER=http://heketi-container-native-storage.apps.580763383722.aws.testdrive.openshift.comexport HEKETI_CLI_USER=adminexport HEKETI_CLI_KEY=myS3cr3tpassw0rd


#/opt/lab/support/topology-extended.json&#123;    &quot;clusters&quot;: [        &#123;            &quot;nodes&quot;: [                &#123;                    &quot;node&quot;: &#123;                        &quot;hostnames&quot;: &#123;                            &quot;manage&quot;: [                                &quot;node01.internal.aws.testdrive.openshift.com&quot;                            ],                            &quot;storage&quot;: [                                &quot;10.0.1.30&quot;                            ]                        &#125;,                        &quot;zone&quot;: 1                    &#125;,                    &quot;devices&quot;: [                        &quot;/dev/xvdd&quot;                    ]                &#125;,                &#123;                    &quot;node&quot;: &#123;                        &quot;hostnames&quot;: &#123;                            &quot;manage&quot;: [                                &quot;node02.internal.aws.testdrive.openshift.com&quot;                            ],                            &quot;storage&quot;: [                                &quot;10.0.3.130&quot;                            ]                        &#125;,                        &quot;zone&quot;: 2                    &#125;,                    &quot;devices&quot;: [                        &quot;/dev/xvdd&quot;                    ]                &#125;,                &#123;                    &quot;node&quot;: &#123;                        &quot;hostnames&quot;: &#123;                            &quot;manage&quot;: [                                &quot;node03.internal.aws.testdrive.openshift.com&quot;                            ],                            &quot;storage&quot;: [                                &quot;10.0.4.150&quot;                            ]                        &#125;,                        &quot;zone&quot;: 3                    &#125;,                    &quot;devices&quot;: [                        &quot;/dev/xvdd&quot;                    ]                &#125;            ]        &#125;,        &#123;            &quot;nodes&quot;: [                &#123;                    &quot;node&quot;: &#123;                        &quot;hostnames&quot;: &#123;                            &quot;manage&quot;: [                                &quot;node04.internal.aws.testdrive.openshift.com&quot;                            ],                            &quot;storage&quot;: [                                &quot;10.0.1.23&quot;                            ]                        &#125;,                        &quot;zone&quot;: 1                    &#125;,                    &quot;devices&quot;: [                        &quot;/dev/xvdd&quot;                    ]                &#125;,                &#123;                    &quot;node&quot;: &#123;                        &quot;hostnames&quot;: &#123;                            &quot;manage&quot;: [                                &quot;node05.internal.aws.testdrive.openshift.com&quot;                            ],                            &quot;storage&quot;: [                                &quot;10.0.3.141&quot;                            ]                        &#125;,                        &quot;zone&quot;: 2                    &#125;,                    &quot;devices&quot;: [                        &quot;/dev/xvdd&quot;                    ]                &#125;,                &#123;                    &quot;node&quot;: &#123;                        &quot;hostnames&quot;: &#123;                            &quot;manage&quot;: [                                &quot;node06.internal.aws.testdrive.openshift.com&quot;                            ],                            &quot;storage&quot;: [                                &quot;10.0.4.234&quot;                            ]                        &#125;,                        &quot;zone&quot;: 3                    &#125;,                    &quot;devices&quot;: [                        &quot;/dev/xvdd&quot;                    ]                &#125;            ]        &#125;    ]&#125;

heketi-cli topology load --json=/opt/lab/support/topology-extended.json

heketi-cli topology info ##得到Cluster ID

# /opt/lab/support/second-cns-storageclass.yamlapiVersion: storage.k8s.io/v1beta1kind: StorageClassmetadata:  name: cns-silverprovisioner: kubernetes.io/glusterfsparameters:  resturl: &quot;http://heketi-container-native-storage.apps.580763383722.aws.testdrive.openshift.com&quot;  restauthenabled: &quot;true&quot;  restuser: &quot;admin&quot;  volumetype: &quot;replicate:3&quot;  clusterid: &quot;INSERT-CLUSTER-ID-HERE&quot;  secretNamespace: &quot;default&quot;  secretName: &quot;cns-secret&quot;
添加已有节点的盘# 获取NODEIDheketi-cli node list | grep ca777ae0285ef6d8cd7237c862bd591c（CLUSTERID）heketi-cli device add --name=/dev/xvde --node=33e0045354db4be29b18728cbe817605(NODEID)

移除有问题的盘heketi-cli node info 33e0045354db4be29b18728cbe817605（NODEID）
以上的结果如下：
Node Id: 33e0045354db4be29b18728cbe817605State: onlineCluster Id: ca777ae0285ef6d8cd7237c862bd591cZone: 1Management Hostname: node04.internal.aws.testdrive.openshift.comStorage Hostname: 10.0.1.23Devices:Id:01c94798bf6b1af87974573b420c4dff   Name:/dev/xvdd           State:online    Size (GiB):9       Used (GiB):1       Free (GiB):8Id:da91a2f1c9f62d9916831de18cc09952   Name:/dev/xvde           State:online    Size (GiB):9       Used (GiB):1       Free (GiB):8
移除盘
heketi-cli device disable 01c94798bf6b1af87974573b420c4dff

6.给Registry组件添加Volumeoc volume dc/docker-registry --add --name=registry-storage -t pvc \--claim-mode=ReadWriteMany --claim-size=5Gi \--claim-name=registry-storage --overwrite
7.更改dc的镜像oc patch dc nginx -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;containers&quot;:[&#123;&quot;name&quot;:&quot;nginx&quot;,&quot;image&quot;:&quot;harbor.apps.example.com/public/nginx:1.14&quot;&#125;]&#125;&#125;&#125;&#125;&#x27;
8.给A项目授予拉取B项目IS的权限oc policy add-role-to-user system:image-puller system:serviceaccount:A:default -n B
9.给Jenkins授予管理A项目资源的权限oc policy add-role-to-user edit system:serviceaccount:jenkins:jenkins -n A
10.手动维护etcdexport ETCDCTL_API=3etcdctl --cacert=/etc/origin/master/master.etcd-ca.crt --cert=/etc/origin/master/master.etcd-client.crt --key=/etc/origin/master/master.etcd-client.key --endpoints=https://master1.os10.openshift.com:2379,https://master2.os10.openshift.com:2379,https://master3.os10.openshift.com:2379 endpoint healthETCDCTL_API=3 etcdctl --cacert=/etc/origin/master/master.etcd-ca.crt --cert=/etc/origin/master/master.etcd-client.crt --key=/etc/origin/master/master.etcd-client.key --endpoints=https://master1.os10.openshift.com:2379,https://master2.os10.openshift.com:2379,https://master3.os10.openshift.com:2379 get / --prefix --keys-onlyETCDCTL_API=3 etcdctl --cacert=/etc/origin/master/master.etcd-ca.crt --cert=/etc/origin/master/master.etcd-client.crt --key=/etc/origin/master/master.etcd-client.key --endpoints=https://master1.os10.openshift.com:2379,https://master2.os10.openshift.com:2379,https://master3.os10.openshift.com:2379 del /kubernetes.io/pods/bookinfo/nginx-4-bkdb4
11.执行镜像对应的任务–restart&#x3D;Always 默认值，创建一个deploymentconfig–restart&#x3D;OnFailure 创建一个Job（但是实践证实为一个Pod）–restart&#x3D;OnFailure –schedule&#x3D;”0&#x2F;5 * * * *” 创建一个Cron Job–restart&#x3D;Never 创建一个单独的Pod
oc run nginx -it --rm  --image=nginx --restart=OnFailure  lsoc run nginx -it --rm  --image=nginx --restart=OnFailure  bash
12.清理主机容器当容器存储docker-storage的storage-driver引擎使用devicemapper时会出现如下错误：devmapper: Thin Pool has 162394 free data blocks which is less than minimum required 163840 free data blocks. Create more free space in thin pool or use dm.min_free_space option to change behavior。这个时候需要清理下容器主机的存储。具体操作如下：
# 清理exited进程：exited_containers=$(docker ps -q -f status=exited); if [ &quot;$exited_containers&quot; != &quot;&quot; ]; then docker rm $exited_containers; fi# 清理dangling volumes：dangling_volumes=$(docker volume ls -qf dangling=true); if [ &quot;$dangling_volumes&quot; != &quot;&quot; ]; then docker volume rm $dangling_volumes; fi# 清理dangling image：dangling_images=$(docker images --filter &quot;dangling=true&quot; -q --no-trunc); if [ &quot;$dangling_images&quot; != &quot;&quot; ]; then docker rmi $dangling_images; fi
参考文档 http://www.cnblogs.com/mhc-fly/p/9324425.html还可在不同在子命令下执行 prune，这样删除的就是某类资源：
$ docker container prune -f # 删除所有退出状态的容器$ docker volume prune -f # 删除未被使用的数据卷$ docker image prune -f # 删除 dangling 或所有未被使用的镜像
13.Node节点内存与CPU预留/etc/origin/node/node-config.yaml
kubeletArguments:  system-reserved:  - cpu=200m  - memory=1G  kube-reserved:  - cpu=200m  - memory=1G
14.用oc get只查看dc的镜像名[root@maser]$ oc get dc test-app --template=&#123;&#123;range.spec.template.spec.containers&#125;&#125;&#123;&#123;.image&#125;&#125;&#123;&#123;end&#125;&#125;registry.example.com/test/test-app:1.13
获取第一个dc的第一个容器的镜像
[root@maser]$ oc get dc --template=&#x27;&#123;&#123;with $dc:=(index .items 0)&#125;&#125;&#123;&#123;with $container:=(index $dc.spec.template.spec.containers 0)&#125;&#125;&#123;&#123;$container.image&#125;&#125;&#123;&#123;&quot;\n&quot;&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;&#x27;
或者使用–jsonpath
[root@maser]$ oc get dc -o jsonpath=&#x27;&#123;range.items[*]&#125;&#123;range .spec.template.spec.containers[*]&#125;&#123;.image&#125;&#123;&quot;\n&quot;&#125;&#123;end&#125;&#123;end&#125;&#x27;[root@maser]$ oc get dc -o jsonpath=&#x27;&#123;.items[0].spec.template.spec.containers[0].image&#125;&#123;&quot;\n&quot;&#125;&#x27;

15.Openshift Webconsole支持私有镜像仓库
创建私有镜像仓库的证书

[root@registry ~]# mkdir /etc/crts/ &amp;&amp; cd /etc/crts[root@registry ~]# openssl req \   -newkey rsa:2048 -nodes -keyout example.com.key \   -x509 -days 365 -out example.com.crt -subj \   &quot;/C=CN/ST=GD/L=SZ/O=Global Security/OU=IT Department/CN=*.example.com&quot;

将私有镜像仓库的CA文件拷贝到镜像仓库所在服务器的/etc/pki/ca-trust/source/anchors/目录下
在镜像仓库中配置tls，如果是docker-distribution /etc/docker-distribution/registry/config.ymlhttp:   addr: :443   tls:       certificate: /etc/crts/example.com.crt       key: /etc/crts/example.com.key
重启docker-distribution[root@registry ~]# systemctl daemon-reload &amp;&amp; systemctl restart docker-distribution &amp;&amp; systemctl enable docker-distribution
在镜像仓库所在服务器上执行update-ca-trust extract
将私有镜像仓库的CA文件拷贝到每台Openshift节点的/etc/pki/ca-trust/source/anchors/目录下
每台Openshift节点上执行update-ca-trust extract

16.Docker支持私有镜像仓库tls认证&#x2F;etc&#x2F;docker&#x2F;certs.d目录下创建对应的域名目录，如私有镜像仓库的域名为:example.harbor.com
$ mkdir -p /etc/docker/certs.d/example.harbor.com
将私有镜像仓库的CA文件拷贝到该目录下即可。
17.查看etcd数据etcdctl --cert-file=/etc/origin/master/master.etcd-client.crt --key-file /etc/origin/master/master.etcd-client.key --ca-file /etc/origin/master/master.etcd-ca.crt --endpoints=&quot;https://master1.os10.openshift.example.com:2379,https://master2.os10.openshift.example.com:2379,https://master3.os10.openshift.example.com:2379&quot;export ETCDCTL_API=3etcdctl --cacert=/etc/origin/master/master.etcd-ca.crt --cert=/etc/origin/master/master.etcd-client.crt --key=/etc/origin/master/master.etcd-client.key --endpoints=https://master1.os10.openshift.example.com:2379,https://master2.os10.openshift.example.com:2379,https://master3.os10.openshift.example.com:2379 endpoint healthETCDCTL_API=3 etcdctl --cacert=/etc/origin/master/master.etcd-ca.crt --cert=/etc/origin/master/master.etcd-client.crt --key=/etc/origin/master/master.etcd-client.key --endpoints=https://master1.os10.openshift.example.com:2379,https://master2.os10.openshift.example.com:2379,https://master3.os10.openshift.example.com:2379 get / --prefix --keys-only
计算某个项目project下所有pod的limits cpu&#x2F;memory的总和## 计算pod总的limits cpu的总和data=$(pods=`oc get pod|awk &#x27;&#123;print $1&#125;&#x27;|grep -v NAME`; for pod in $pods; do oc get pod $pod --template=&#123;&#123;range.spec.containers&#125;&#125;&#123;&#123;.resources.limits.cpu&#125;&#125;&#123;&#123;println&#125;&#125;&#123;&#123;end&#125;&#125;; done); i=0; for j in $(echo $data); do i=$(($i+$j)); done ; echo $i;## 18.计算pod总的limits memory的总和data=$(pods=`oc get pod|awk &#x27;&#123;print $1&#125;&#x27;|grep -v NAME`; for pod in $pods; do oc get pod $pod --template=&#123;&#123;range.spec.containers&#125;&#125;&#123;&#123;.resources.limits.memory&#125;&#125;&#123;&#123;println&#125;&#125;&#123;&#123;end&#125;&#125;; done);i=0; for j in $(echo $data); do mj=$(echo $j|cut -dG -f1); i=$(($i+$mj)); done; echo $i;
19.DNSMasq启动失败报错“DBus error: Connection “:1.180” is not allowed to own the service “uk.org.thekelleys.dnsmasq” ”$ cat /etc/dbus-1/system.d/dnsmasq.conf&lt;!DOCTYPE busconfig PUBLIC &quot;-//freedesktop//DTD D-BUS Bus Configuration 1.0//EN&quot; &quot;http://www.freedesktop.org/standards/dbus/1.0/busconfig.dtd&quot;&gt;&lt;busconfig&gt;	&lt;policy user=&quot;root&quot;&gt;		&lt;allow own=&quot;uk.org.thekelleys.dnsmasq&quot;/&gt;		&lt;allow send_destination=&quot;uk.org.thekelleys.dnsmasq&quot;/&gt;	&lt;/policy&gt;	&lt;policy context=&quot;default&quot;&gt;                &lt;allow own=&quot;uk.org.thekelleys.dnsmasq&quot;/&gt;                &lt;allow send_destination=&quot;uk.org.thekelleys.dnsmasq&quot;/&gt;        &lt;/policy&gt;&lt;/busconfig&gt;$ systemctl daemon-reload$ systemctl restart dbus$ systemctl restart dnsmasq

20.ssh特别慢，卡在debug1: pledge: network位置重启下systemd-logind
$ systemctl restart systemd-logind
如果是卡在Authentication上，可以把ssh client端的StrictHostKeyChecking设置为no
$ cat /etc/ssh/ssh_configHost *          GSSAPIAuthentication no          StrictHostKeyChecking no
21.清理私有镜像仓库$ cat &gt; /usr/bin/cleanregistry.sh &lt;&lt;EOF#!/bin/bashoc login -u admin -p passwordoc adm prune builds --orphans --keep-complete=25 --keep-failed=5 --keep-younger-than=60m --confirmoc adm prune deployments --orphans --keep-complete=25 --keep-failed=10 --keep-younger-than=60m --confirm#oc rollout latest docker-registry -n default#sleep 20oc adm prune images --keep-younger-than=400m --confirmEOF$ crontab -l0 0 * * * /usr/bin/cleanregistry.sh &gt;&gt; /var/log/cleanregistry.log 2&gt;&amp;1

22.docker run覆盖entrypoint$ docker run --entrypoint=&quot;/bin/bash&quot; --rm -it xhuaustc/nginx-openshift-router:1.15
23.oc image mirror同步镜像$ oc image mirror myregistry.com/myimage:latest docker.io/myrepository/myimage:stable --insecure=true

24.开通端口防火墙# vi /etc/sysconfig/iptables-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp  --dport 9100 -j ACCEPT# systemctl restart iptables

25.查看crt证书有效时间$ openssl x509 -noout -text -in ca.crt | grep Validity -A2    Validity        not Before: Sep 7 08:48.13 2018 GMT        not After: Sep 6 08:48.14 2020 GMT
26.将主机设为不可调度方法一：
$ oc adm cordon $nodename
方法二：
$ oc adm manage-node --schedulable=false $nodename
27.驱逐主机上的POD$ oc adm manage-node --evacuate $nodename
28.Service的域名正常情况下Service的域名格式为：service-name.project-name.svc.cluster.local对应的IP是Service Cluster IP设置Service的clusterIP&#x3D;None，同时该Pod需要添加subdomain字段，如果是statefulset资源需要添加serviceName字段。Service的域名格式为：service-name.project-name.svc.cluster.local对应的IP是后台对应的Pod的容器的IP同时后台对应的Pod都有DNS记录，格式为pod-name.service-name.project-name.svc.cluster.local
29.查看Docker镜像的构建历史命令docker history ${镜像名/ID} -H --no-trunc | awk -F&quot;[ ]{3,}&quot; &#39;{print $3}&#39; | sed -n -e  &quot;s#/bin/sh -c##g&quot; -e &quot;s/#(nop)  //g&quot; -e  &#39;2,$p&#39; | sed  &#39;1!G;h;$!d&#39;
例如查看镜像mysql:5.6.41的构建命令
$ docker history mysql:5.6.41 -H --no-trunc | awk -F&quot;[ ]&#123;3,&#125;&quot; &#x27;&#123;$1=&quot;&quot;;$2=&quot;&quot;;$(NF-1)=&quot;&quot;;print $0&#125;&#x27; | sed -n -e  &quot;s#/bin/sh -c##g&quot; -e &quot;s/#(nop)  //g&quot; -e  &#x27;2,$p&#x27; | sed  &#x27;1!G;h;$!d&#x27;   #(nop) ADD file:f8f26d117bc4a9289b7cd7447ca36e1a70b11701c63d949ef35ff9c16e190e50 in /   CMD [&quot;bash&quot;]   groupadd -r mysql &amp;&amp; useradd -r -g mysql mysql   apt-get update &amp;&amp; apt-get install -y --no-install-recommends gnupg dirmngr &amp;&amp; rm -rf /var/lib/apt/lists/*   ENV GOSU_VERSION=1.7   set -x  &amp;&amp; apt-get update &amp;&amp; apt-get install -y --no-install-recommends ca-certificates wget &amp;&amp; rm -rf /var/lib/apt/lists/*  &amp;&amp; wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)&quot;  &amp;&amp; wget -O /usr/local/bin/gosu.asc &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc&quot;  &amp;&amp; export GNUPGHOME=&quot;$(mktemp -d)&quot;  &amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4  &amp;&amp; gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu  &amp;&amp; gpgconf --kill all  &amp;&amp; rm -rf &quot;$GNUPGHOME&quot; /usr/local/bin/gosu.asc  &amp;&amp; chmod +x /usr/local/bin/gosu  &amp;&amp; gosu nobody true  &amp;&amp; apt-get purge -y --auto-remove ca-certificates wget   mkdir /docker-entrypoint-initdb.d   apt-get update &amp;&amp; apt-get install -y --no-install-recommends pwgen perl  &amp;&amp; rm -rf /var/lib/apt/lists/*   set -ex;  key=&#x27;A4A9406876FCBD3C456770C88C718D3B5072E1F5&#x27;;  export GNUPGHOME=&quot;$(mktemp -d)&quot;;  gpg --keyserver ha.pool.sks-keyservers.net --recv-keys &quot;$key&quot;;  gpg --export &quot;$key&quot; &gt; /etc/apt/trusted.gpg.d/mysql.gpg;  gpgconf --kill all;  rm -rf &quot;$GNUPGHOME&quot;;  apt-key list &gt; /dev/null   ENV MYSQL_MAJOR=5.6   ENV MYSQL_VERSION=5.6.41-1debian9   echo &quot;deb http://repo.mysql.com/apt/debian/ stretch mysql-$&#123;MYSQL_MAJOR&#125;&quot; &gt; /etc/apt/sources.list.d/mysql.list   &#123; echo mysql-community-server mysql-community-server/data-dir select &#x27;&#x27;; echo mysql-community-server mysql-community-server/root-pass password &#x27;&#x27;; echo mysql-community-server mysql-community-server/re-root-pass password &#x27;&#x27;; echo mysql-community-server mysql-community-server/remove-test-db select false;  &#125; | debconf-set-selections  &amp;&amp; apt-get update &amp;&amp; apt-get install -y mysql-server=&quot;$&#123;MYSQL_VERSION&#125;&quot; &amp;&amp; rm -rf /var/lib/apt/lists/*  &amp;&amp; rm -rf /var/lib/mysql &amp;&amp; mkdir -p /var/lib/mysql /var/run/mysqld  &amp;&amp; chown -R mysql:mysql /var/lib/mysql /var/run/mysqld  &amp;&amp; chmod 777 /var/run/mysqld  &amp;&amp; find /etc/mysql/ -name &#x27;*.cnf&#x27; -print0 | xargs -0 grep -lZE &#x27;^(bind-address|log)&#x27; | xargs -rt -0 sed -Ei &#x27;s/^(bind-address|log)/#&amp;/&#x27;  &amp;&amp; echo &#x27;[mysqld]\nskip-host-cache\nskip-name-resolve&#x27; &gt; /etc/mysql/conf.d/docker.cnf   VOLUME [/var/lib/mysql]   #(nop) COPY file:b79e447a4154d7150da6897e9bfdeac5eef0ebd39bb505803fdb0315c929d983 in /usr/local/bin/   ln -s usr/local/bin/docker-entrypoint.sh /entrypoint.sh # backwards compat   ENTRYPOINT [&quot;docker-entrypoint.sh&quot;]   EXPOSE 3306/tcp   CMD [&quot;mysqld&quot;]
30.应用在完成Build后推送到内部镜像仓库如下报错误 Pushing image docker-registry.default.svc:5000/apb/my-test-apb:latest ...Pushed 0/15 layers, 0% completeRegistry server Address:Registry server User Name: serviceaccountRegistry server Email: serviceaccount@example.orgRegistry server Password: &lt;&lt;non-empty&gt;&gt;error: build error: Failed to push image: unauthorized: unable to validate token
此时很大可能是因为一些变更，导致镜像仓库的Token有变化，但是镜像仓库未重启，重启镜像仓库即可恢复。
$ oc get pod -n default | grep docker-registrydocker-registry-1-8tjhk                   1/1       Running            0          4m$ oc delete pod `oc get pod -n default | grep docker-registry | awk &#x27;&#123;print $1&#125;&#x27;`
31.为容器用户指定用户名
在镜像构建中将文件&#x2F;etc&#x2F;passwd设置为容器启动用户可写

RUN chmod g=u /etc/passwd

容器启动时设置用户名ENTRYPOINT&#x2F;CMD 脚本中添加设置用户名代码

USER_NAME=$&#123;USER_NAME:-ocpuid&#125;USER_ID=$(id -u)if ! whoami &amp;&gt; /dev/null; then  if [ -w /etc/passwd ]; then    echo &quot;$&#123;USER_NAME&#125;:x:$&#123;USER_ID&#125;:0:$&#123;USER_NAME&#125; user:$&#123;HOME&#125;:/sbin/nologin&quot; &gt;&gt; /etc/passwd  fifiexec &quot;$@&quot;
32.升级Docker升级不同OpenShift组件的思路是一样，主要是如下两条。

逐个节点升级
升级前将业务应用迁走


更新yum源中的docker包

$ cp docker-rpm/* ./extras/Packages/d/$ createrepo --update extras

迁移节点上的POD并将它设置为不可调度

$ oc adm drain &lt;node_name&gt; --force --delete-local-data --ignore-daemonsets

排除不需要升级的软件

$ atomic-openshift-docker-excluder exclude$ atomic-openshift-excluder exclude

升级docker

$ yum clean all$ yum update docker

重启服务或者重启主机

Master节点
$ systemctl restart docker$ master-restart api$ master-restart controllers$ systemctl restart origin-node
Node节点
$ systemctl restart docker$ systemctl restart origin-node
或者
$ reboot

将节点设置为可调度

$ oc adm uncordon &lt;node_name&gt;
33.获取Token并请求OpenShift ASB服务的例子$ curl -k -H &quot;Authorization: Bearer `oc serviceaccounts get-token asb-client`&quot; https://$(oc get routes -n openshift-ansible-service-broker --no-headers | awk &#x27;&#123;print $2&#125;&#x27;)/osb/v2/catalog&#123;  &quot;paths&quot;: [    &quot;/ansible-service-broker/&quot;,    &quot;/apis&quot;,    &quot;/healthz&quot;,    &quot;/healthz/ping&quot;,    &quot;/healthz/poststarthook/generic-apiserver-start-informers&quot;,    &quot;/metrics&quot;  ]&#125;
34.调用OpenShift API获取Pod信息$ oc get --raw /api/v1/namespaces/&lt;namespace-name&gt;/pods/&lt;pod-name&gt; | json_reformat
35.使用HostPath挂载本地目录$ chcon -Rt svirt_sandbox_file_t /testHostPathor$ chcon -R unconfined_u:object_r:svirt_sandbox_file_t:s0 /testHostPathor$ semanage fcontext -a -t svirt_sandbox_file_t &#x27;/testHostPath(/.*)?&#x27;$ restorecon -Rv /testHostPath# 确认设置 semanage fcontext -l | grep testHostPath# 确认文件生效 ls -Z /testHostPath# 删除 配置: semanage fcontext -d &#x27;/testHostPath(/.*)?&#x27;
36.将搜索镜像导出到本地文件脚本$ docker image | grep redis | awk &#x27;&#123;image=$1; gsub(/.*\//, &quot;&quot;, $1); printf(&quot;docker save -o %s.tar %s:%s\n&quot;, $1, image, $2)&#125;&#x27; | xargs -i bash -c &quot;&#123;&#125;&quot;
37.Docker日志中有错误 container kill failed because of container not found or no such process$ # 定期检查docker日志$ journalctl -r -u docker --since &#x27;1 day ago&#x27; --no-pager | grep -i error$ # 处理办法重启docker$ systemctl restart docker

38.查看所有应用重启次数，并且排序$ oc get pod --sort-by=&#x27;.status.containerStatuses[0].restartCount&#x27; --all-namespace | sort -rn -k10

39.docker拉取镜像报错：400 unsupported docker v1 repository requestdocker的配置中添加--disable-legacy-registry配置。
$ cat /etc/sysconfig/docker...OPTIONS=&#x27;... --disable-legacy-registry ...&#x27;...
原因：当docker客户端通过v2 API请求镜像库，而镜像不存在，客户端会尝试使用v1 API请求镜像仓库，而镜像仓库不支持v1 API请求，则会返回该错误。
40.应用日志无法查看，oc exec也无法进入容器报错Error from server: Get https://master.example.com:8443/containerLogs/namespace/pod-name/console: remote error: tls: internal error处理办法，查看csr，并将它们手动授信
$ oc get csr$ oc get csr -o name | xargs oc adm certificate approve
41.netmanager工具设置了dns，无法直接通过&#x2F;etc&#x2F;resolv.conf文件更改$ nmcli con show # 查看所有的网络连接$ nmcli con show &lt;net-connect-name&gt; #查看网络连接详情，可查看dns的配置$ nmcli con mod &lt;net-connect-name&gt; -ipv4.dns &lt;dns-server-ip&gt; #删除指定的dns ip$ nmcli con mod &lt;net-connect-name&gt; +ipv4.dns &lt;dns-server-ip&gt; #添加指定的dns ip
42.查看集群当前计算节点资源的分配率$ nodes=$(oc get node --selector=node-role.kubernetes.io/compute=true --no-headers | awk &#x27;&#123;print $1&#125;&#x27;); for i in $nodes; do echo $i; oc describe node $i | grep Resource -A 3 | grep -v &#x27;\-\-\-&#x27;; donenode1  Resource    Requests          Limits  cpu         10445m (65%)      25770m (161%)  memory      22406Mi (34%)     49224Mi (76%)node2  Resource    Requests          Limits  cpu         8294m (51%)   25620m (160%)  memory      18298Mi (28%)     48600Mi (75%)
43.安装时master api服务无法访问etcdmaster主机绑定多张网卡，在&#x2F;etc&#x2F;ansible&#x2F;hosts中需要指定etcd_ip，如下所示
[etcd]master.example.com etcd_ip=10.1.2.3
另外需要确保，etcd所在主机的hostname所指定的ip确切为etcd_ip指定的ip。
44.安装时master节点有多张网卡，如何指定masterIP在master安装时master-config.yml中设置的masterIP为openshift.common.ip，为节点的默认网卡。可以通过编辑roles/openshift_facts/library/openshift_facts.py文件来设置该ip
def get_defaults(self, roles):    &quot;&quot;&quot; Get default fact values        Args:            roles (list): list of roles for this host        Returns:            dict: The generated default facts    &quot;&quot;&quot;    defaults = &#123;&#125;    ip_addr = self.system_facts[&#x27;ansible_default_ipv4&#x27;][&#x27;address&#x27;]    exit_code, output, _ = module.run_command([&#x27;hostname&#x27;, &#x27;-f&#x27;])  # noqa: F405    hostname_f = output.strip() if exit_code == 0 else &#x27;&#x27;    hostname_values = [hostname_f, self.system_facts[&#x27;ansible_nodename&#x27;],                       self.system_facts[&#x27;ansible_fqdn&#x27;]]    hostname = choose_hostname(hostname_values, ip_addr).lower()    exit_code, output, _ = module.run_command([&#x27;hostname&#x27;])  # noqa: F405    raw_hostname = output.strip() if exit_code == 0 else hostname    defaults[&#x27;common&#x27;] = dict(ip=ip_addr,                              public_ip=ip_addr,                              raw_hostname=raw_hostname,                              hostname=hostname,                              public_hostname=hostname,
另外可以通过将目标网卡设置为默认网卡来解决。还有OpenShift通过更新hosts也可以来配置，通过设置openshift_node_groups来设置kubeletArguments.node-ip的值，如下：
&#123;&#x27;name&#x27;: &#x27;node-config-node1&#x27;, &#x27;labels&#x27;: [&#x27;...,...&#x27;], &#x27;edits&#x27;: [&#123; &#x27;key&#x27;: &#x27;kubeletArguments.node-ip&#x27;,&#x27;value&#x27;: [&#x27;x.x.x.x&#x27;]&#125;]&#125;

45.部署集群时，采用自定义证书，Master1节点报x509: certificate signed by unknown authority错误检查ansible inventory hosts文件中自定义证书名是否与openshift默认的组件证书名重复了。如ca.crt等
46.部署时网络错误，需要查看是否配置了默认路由，如果没有，则需要设置$ ip route10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 102172.16.10.0/24 dev eth1 proto kernel scope link src 172.16.10.11 metric 101172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1$ ## 添加默认路由$ ip route add default via 172.16.10.1
47. 删除指定文件夹下最近一个月的文件$ find /dir -type f -mtime +30 -exec rm -rf &#123;&#125; \;
48.pod报the node was low on resource ephemeral-storage而被驱逐pod应用临时存储空间不足导致错误，需要查看本地磁盘，特别是&#x2F;var&#x2F;lib&#x2F;origin所在磁盘的空间情况。
49.自签证书
根证书创建

$ openssl genrsa -out ca.key 2048$ openssl req -new -x509 -days 36500 -key ca.key -out ca.crt -subj &quot;/C=CN/ST=shanxi/L=taiyuan/O=cn/OU=test/CN=example.com&quot;$ #或者 openssl req -new -x509 -days 36500 -key ca.key -out ca.crt 手动输入配置

创建证书并使用根证书签发

$ openssl genrsa -out app.key 2048$ openssl req -new -key app.key -out app.csr$ openssl x509 -req -in app.csr -CA ca.crt -CAkey ca.key -out app.crt -days 3650  -CAcreateserial

使用 Openssl 工具查看证书信息

$ openssl x509 -in signed.crt -noout -dates$ openssl x509 -in signed.crt -noout -subject$ openssl x509 -in signed.crt -noout -text
50. ETCD某个节点无法重启，报错rafthttp: the clock difference against peer 27de23fad174dca is too high [1m16.89887s &gt; 1s]检查ETCD服务器的时间是否同步，如果不同步，强制同步后，ETCD会自动恢复。
51. 查看最近一小时的Event 告警事件集群默认保留最近1小时的Event事件，通过field-selector过滤掉正常的事件
$ oc get event --field-selector=type=Warning --all-namespaces
52. 获取Alertmanager的告警信息$ oc exec -it alertmanager-main-0 -c alertmanager -n openshift-monitoring -- amtool alert query &#x27;severity=critical&#x27; --alertmanager.url http://localhost:9093
53. 获取statefulset中的Pod的序号[[ $(hostname) =~ -([0-9]+)$ ]]  || exitordinal=$&#123;BASH_REMATCH[1]&#125;
其中ordinal即为statefulset中的序号，一般可用在initContainers中对Pod进行初始化配置设置，具体生产实践中可灵活使用。
54. 清理镜像仓库中的镜像镜像仓库必须开启可删除功能
# curl -k -I -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; -I http://localhost:5000/v2/openshift/ocp-router/manifests/v3.11.129获取镜像层的sha256值# curl -X DELETE http://localhost:5000/v2/openshift/ocp-router/manifests/sha256:39ad17c3e10f902d8b098ee5128a87d4293b6d07cbc2d1e52ed9ddf0076e3cf9# #登录到镜像仓库# registry garbage-collect /etc/docker-distribution/registry/config.yml
55. AIX部署NFS服务，应用POD无法挂载mount.nfs: Remote I/O error.默认情况下，NFS客户端通过NFSv4协议访问NFS服务，如果AIX部署NFS时不支持NFSv4协议，则在挂载时会报mount.nfs: Remote I/O error.的错误。可通过nfsstat -s查看服务端支持的NFS版本。有两种解决方法：

重新配置NFS Server，让其支持NFSv4；
配置PV，强制使用NFSv3来访问后端NFS服务。参考配置如下：spec.mountOptions

apiVersion: v1kind: PersistentVolumemetadata:  name: pv0003spec:  capacity:    storage: 5Gi  volumeMode: Filesystem  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  mountOptions:    - hard    - nfsvers=3  nfs:    path: /tmp    server: 172.17.0.2
另外也可以通过添加annotations.volume.beta.kubernetes.io/mount-options来设置
oc patch pv pv0003 -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;volume.beta.kubernetes.io/mount-options&quot;:&quot;rw,nfsvers=3&quot;&#125;&#125;&#125;&#x27;
参考文章：https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/
56. 将POD从副本控制器中脱离在应用运行过程中，某些场景下，需要将某个POD从业务流量中脱离。例如在问题排查时，一旦应用重启，将不易于具体线上问题的排查，这时我们需要在尽快恢复应用的情况下，保留问题POD的状态。方法很简单，就是利用Label。我们知道在K8S&#x2F;OCP中，各种资源的关系都是通过Label来建立的，只需要将POD的Label去掉，让它就会成为一个孤立的POD，应用的迭代不会对POD的生命周期有影响，同时业务流量也不会分发到该POD。
# oc label pod xxx-pod --list //查看当前pod所有label# oc label pod xxx-pod &lt;LABEL-A&gt;-  &lt;LABEL-B&gt;- //删除关联的LABEL
57. Node状态变为NotReady，且检查状态为Unknown.  
可检查下CSR，是否存在Pending
$ oc get csr
批准这些CSR即可
$ oc get csr -o name | xargs oc adm certificate approve或$ kubectl get csr -o name | xargs kubectl certificate approve
58. No space left on device，但df -h查看空间空空的还需要检查一下inodes 
$ df -ihFilesystem     Inodes IUsed IFree IUse% Mounted on/dev/sdb          16M  502K   16M    4% /
如果发现IUse% 为100，就没法再存储数据了。解决办法 ： rm -rf 一些小而多的文件，如日志等。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Ansible-Tower与Jenkins集成实现CI-CD</title>
    <url>/posts/29c771c693e3.html</url>
    <content><![CDATA[Ansible Tower是什么？Ansible Tower是一个基于Web的用户界面，提供了IT自动化的企业解决方案。它有一个友好用户的仪表板来管理部署和监控资源。Ansible Tower为Ansible增加自动化，可视化管理和监控能力。
为什么要使用Ansible Tower替换Ansible与Jenkins集成？使用Ansible Tower与Jenkins集成，打开了一个充满可能性的世界。与一篇中介绍的《Ansible与Jenkins集成实现CI&#x2F;CD》进行比较，它可以从Jenkins中删除很多配置，比如部署应用的服务器的访问配置，管理playbook执行，最重要的是，避免在Jenkins端配置SSH密钥，同时Jenkins不需要知道应用程序的部署服务器地址：它可能位于公有云中，物理机中，甚至是笔记本电脑上的虚拟客户端。 把这些配置与过程都放到Ansible Tower中进行集中管理。
Ansible Tower的组件  

Credentials: Ansible Tower在运行作业时，访问Tower外部服务或者连接主机时需要进行身份验证。Credentials则是存储这些身份验证的配置信息。
Projects: Project是一组Ansible playbooks的集合，它是开始使用Tower的第一个组件。Projects中的playbooks文件可以手动添加在Tower服务器的目录中，但是一般推荐使用Git等代码仓库的方式存放playbooks文件，因为这样更容易对playbooks文件进行更改与管理。
Inventories: Inventories是ansible Tower中的Job最终执行操作的主机，它可以通过静态方式添加，也可以使用脚本动态添加，比如访问CMDB服务、调用公有云API。
Job template: Job template是Ansible Job的一个模板，它可以为Job传递不同的参数，认证证书、Inventories配置等。
Job: Job是最终执行的任务，可以在Tower上查看Job的执行过程，及日志

说明：当然以上说明只是各组件最简单的功能介绍，其实每个组件的功能都会更加丰富，我们可以自己部署Ansible Tower，并通过具体的使用来熟悉各组件的功能。
怎样集成Ansible Tower与Jenkins?前提：部署好了Ansible Tower和Jenkins
Ansible Tower上的设置
创建用户jenkins给Jenkins调用Ansible Tower Tower

  

创建Project，使用Git作为SCM Type，并给jenkins用户授权 Tower

  

创建Inventories Tower，将部署应用的主机信息添加到Hosts，并添加到”app_server” Group Tower

创建Inventory



  

添加应用服务器主机

  

给Inventory授权给Jenkins用户

  

创建访问应用主机的身份认证密钥 Tower

添加新的认证信息，可以是用户名+密码，也可以用户名+私钥



  

对认证信息授权给Jenkins用户

    

创建Job Template Tower

  
补充: 

EXTRA VARIABLES是额外的补充参数。
大部分参数都有PROMPT ON LAUNCH选项，这个是在Template Job实际运行时，通过用户交互的方式进行配置
SURVEY的配置有更加友好的方式与用户交互进行配置参数

Jenkins上的设置
安装Ansible Tower插件 Jenkins

  

设置访问Ansible Tower的登录Credentials Jenkins

  

在Configure System中设置Ansible Tower信息 Jenkins

  

新建Jenkins Job，设置Ansible Tower Job jenkins

pipeline&#123;    agent any    stages&#123;    	statge(&quot;构建应用&quot;)&#123;    		echo &#x27;构建应用&#x27;    	&#125;        statge(&quot;Jar包上传&quot;)&#123;    		echo &#x27;Jar包上传到Nexus,以供Ansible Tower中的playbook下载部署&#x27;    	&#125;    	stage(&quot;触发Ansible Tower Job&quot;)&#123;	        ansibleTower credential: &#x27;2&#x27;, 	            extraVars: &#x27;&#x27;&#x27;APPNAME: App	ARTIFACT_URL: http://test.com/app/1.0.jar&#x27;&#x27;&#x27;, 	            importTowerLogs: false, 	            inventory: &#x27;2&#x27;,	            jobTemplate: &#x27;8&#x27;, 	            templateType: &#x27;job&#x27;, 	            throwExceptionWhenFail: false, 	            towerServer: &#x27;ansible tower&#x27;, 	            verbose: false	    &#125;    &#125;&#125;

通过Jenkins Job的触发就会触发Ansible Tower中id为8的Job Template生成新的Job进行应用部署。
最后一句话作为总结：Ansible Tower与Jenkins双剑合璧才是运维自动化正确的打开方式。
参考资料Take Ansible and Jenkins Integration to the next level: CI&#x2F;CD with Ansible TowerAnsible Tower官方文档
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>自动化工具集(持续更新------)</title>
    <url>/posts/0021c1b20a24.html</url>
    <content><![CDATA[1. 部署集群kopskubespraykubeoperatork(8s) in d(ocker)
2. 部署应用kustomize

路径名中有/时，使用~1替换

kustomization.yaml---apiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomization...patchesJson6902:- target:    version: v1    kind: Namespace    name: argocd  path: patch-namespace.yamlpatch-namespace.yaml---- op: replace  path: &quot;/metadata/annotations/iam.amazonaws.com~1allowed-roles&quot;  value: &gt;    [&quot;arn:aws:iam::12313112:role/argocd&quot;]

kapphelmhelmfileargocd
3. Kubernetes管理工具k9sLens  &#x2F;  DownloadWeb Kubectl
##3. 基础设施即代码terraformtfenvansibleforemanrackn rebar
4. 构建管理工具bazel构建 go应用需要使用gazelle工具，来自动生成go包目录下的BUILD.bazel。同时构建前使用go mod init命令初始化包。
$ bazel run //:gazelle$ bazel build[run] //:sample
make
5. 模板文件与解析工具jsonnetjbboilrpython-yqgomplatecue
6. 安全加密sopsage，与Sops结合可对文件进行加密，实现对K8S中的secrets.yaml的保护。kustomize-sopskube2iamvaultlegoacme.shkubeseal新版本默认隔一个月生成一个新的公&#x2F;私钥，默认kubeseal客户端会使用最新的公钥进行加密，但是之前的公&#x2F;私钥仍然有效。registry-creds管理镜像仓库密钥
7. 应用路由ambassadorroutercert-manager :ssl证书管理apisix
8. mokemoke
9.语言gopythonshell
10.代码管理gerritgitlabgithubgitea
11. 持续构建工具jeinkinscircleciawx（用户指南）
12.监控kapacitorprometheusgrafanagrafana-libpromxy
13. 数据库kubedbvitessbitnami 应用市场mysql-operator
14. 日志lokielkgraylog
15. 存储longhorncephrookminio
16. 消息渠道slackmattermostRocket Chat
17. 开发人员门户Backstage
18. 镜像仓库HarborNexus
19. Workflow 工具Argo workflowsairflow
20. 认证Azure Active Directorykeycloakopenldapokta
21. 办公邮件iRedMail视频会议jitsi聊天工具Rocket Chat 可以与jitsi集成云盘Nextcloud
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift压测工作必不可少，使用Jmeter搞起来</title>
    <url>/posts/ff867f6f9d29.html</url>
    <content><![CDATA[毫无疑问OpenShift是一个优秀的容器平台，但是我们有没有想过这些问题呢？OpenShift集群最多能运行多少个容器？每个请求的延时是多少呢？当有大量Pod并发启动时，容器平台是否还能轻松应对呢？等等这些问题在容器平台上生产时，我们都得心里有数。否则随着容器平台的压力不断扩大，到了无法承受之痛时，而我们却毫不知情。那么对OpenShift容器平台做压力测试就成了一项必不可少的工作。但是怎样去做呢？业界使用最多的方法就是用Jmeter来模拟API请求进行压测。
新建测试计划与进程组
创建一个名为”测试OpenShift Api”的Test Plan
右击”测试OpenShift Api”，Add-&gt;Threads(Users)-&gt;ThreadGroup，创建名为”模拟客户端”的Thread Group
右击”测试OpenShift Api”，Add-&gt;Listener-&gt;Aggregate Report，创建名为“测试OpenShift API Report”的Aggregate Report

通过模拟请求获取访问OpenShift平台的ACCESS_TOKEN
右击”模拟客户端”，Add-&gt;Sampler-&gt;HTTP Request，创建名为”获取TOKEN”的HTTP Request
设置”获取TOKEN”


Protocol: https
Server Name or IP：https://master.example.com
Port Number：8443
Method：GET
Path：&#x2F;oauth&#x2F;authorize?response_type&#x3D;token&amp;client_id&#x3D;openshift-challenging-client


右击”获取TOKEN”，Add-&gt;Config Element-&gt;HTTP Header Manager，创建名为”获取TOKEN Header”的HTTP Header Manager
计算登录OpenShift平台的用户名与密码的base64值

$ echo -n &#x27;admin:password&#x27; | base64YWRtaW46cGFzc3dvcmQ=

为”获取TOKEN Header”添加Header


Authorization：Basic YWRtaW46cGFzc3dvcmQ&#x3D;


右击”获取TOKEN”，Add-&gt;Post Processors-&gt;Regular Expression Extractor，创建名为”获取TOKEN值”的Regular Expression Extractor
设置”获取TOKEN值”


Apply to：Main sample only
Field to check： Response Headers
Name of created variable：ACCESS_TOKEN
Regular Expression：#access_token&#x3D;([^&amp;]*)&amp;expires_in&#x3D;([^&amp;]*)&amp;
Template：$1$
Match NO: 1

使用ACCESS_TOKEN调用OpenShift API获取Project列表
右击”模拟客户端”，Add-&gt;Sampler-&gt;HTTP Request，创建名为”获取projects列表”的HTTP Request
设置”获取projects列表”


Protocol: https
Server Name or IP：master.example.com
Port Number：8443
Method：GET
Path：&#x2F;apis&#x2F;project.openshift.io&#x2F;v1&#x2F;projects
Parameers：pretty&#x3D;true


右击”获取projects列表”，Add-&gt;Config Element-&gt;HTTP Header Manager，创建名为”获取projects列表 Header”的HTTP Header Manager


Authorization：Basic ${ACCESS_TOKEN}


右击”获取projects列表”，Add-&gt;Listener-&gt;view Results Tree，创建名为”Projects列表Result”的view Results Tree

使用ACCESS_TOKEN调用OpenShift API创建测试Pod
右击”模拟客户端”，Add-&gt;Sampler-&gt;HTTP Request，创建名为”创建Pod”的HTTP Request
设置”创建Pod”


Protocol: https
Server Name or IP：master.example.com
Port Number：8443
Method：POST
Path：&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;test&#x2F;pods
Body Data：Pod的创建Json文件，注意”kind”必须顶在行首


右击”创建Pod”，Add-&gt;Config Element-&gt;HTTP Header Manager，创建名为”创建Pod Header”的HTTP Header Manager


Authorization：Basic ${ACCESS_TOKEN}
Accept：application&#x2F;json
Content-Type：application&#x2F;json


右击”创建Pod”，Add-&gt;Listener-&gt;view Results Tree，创建名为”Pod创建Result”的view Results Tree

最终的配置组件的部局如下图：
  
补充 使用curl来调用Master API示例
$ curl -k -X DELETE https://master.example.com:8443/oapi/v1/namespaces/test/deploymentconfigs/busyapp -d &#x27;&#123;&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Background&quot;&#125;&#x27; -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer $(oc whoami -t)&quot; -H &quot;Accept: application/json&quot;
执行测试计划
不断调整”模拟客户端”的并发数配置（以下配置模拟100个并发不断请求）


Number of Threads：100
Ramp-Up Period：1
Loop Count：Forever


在”Projects列表Result”中查看每个获取project列表请求的返回

  

在”Pod创建Result”中查看每个创建Pod请求的返回

  

在”测试OpenShift API Report”中查看所有请求的结果报告

  
生成报表
确认jmeter&#x2F;bin&#x2F;jmeter.properties中的output_format配置为csv

...jmeter.save.saveservice.output_format=csv..

执行保存的测试OpenShiftAPI.jmx生成报表文件

$ jmeter -n -t OpenShiftAPI.jmx -l result.jtl -e -o resultReport

通过浏览器访问resultReport&#x2F;index.html

  

以上通过调用获取projects列表的API的全过程，详细介绍了Jemter压测OpenShift的方法。在实际压测OpenShift过程中，我们需要调用更多的API，如创建Pod，删除Pod等。所有API的说明可查看OpenShift官方文档：OpenShift API说明
压测OpenShift集群需要做详细的测试设计名词解释

集群容量：集群中有N个Node，每个Node上30个Pod，集群容量为30 * N个Pod。
集群负载：集群中Pod总数量占集群容量百分比。通过调整Pod总数控制集群负载。
百分位指标：指标的分布性。以API调用延时指标为例，90% API调用延时为180ms，99% API调用延时为400ms。

测试目标
集群容量上限：创建Pod总数
性能瓶颈
服务请求延迟
集群支持的并发请求数
测试容器间调用的性能
平台弹性扩缩的性能

服务质量目标（SLO）
99%的Pod启动时间：5s
99%的API调用延时：1s

服务质量指标（SLI）
Pod启动时间
API调用延时

数据统计统计不同集群负载下，Pod启动时间和API调用延时这两个指标的分布性:集群负载：10%， 25%，50%，90%，100%指标分布性：90%， 95%，99%
欢迎关注本人用python实现的一个简单的openshift sdk，轻松调用openshift apiOpenshift SDK Python：https://github.com/xhuaustc/openshift-sdk-python.git
参考文章Kubernetes集群性能测试
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Airflow简单介绍及测试安装</title>
    <url>/posts/9dbd81598681.html</url>
    <content><![CDATA[Airflow 是什么
Airflow 是 Airbnb 开发的用于工作流管理的开源项目，自带 web UI 和调度。它支持编程方式创建工作流，同时在平台上管理和监控工作流程的状态。
Airflow 于 2016 年 3 月加入了 Apache Software Foundation 的孵化计划，所以它未来的持续维护性有保障。
官方地址：https://github.com/apache/airflow，现有11318个star，732个贡献者，是一个热门的开源项目。
先看下这篇介绍 ：浅谈调度工具——Airflow


为什么使用 Airflow
智能调度
图形化展示任务关系
程序化流水线定义
流水线间数据交互
扩展性强
分布式，可靠性高
执行方式多样化，除了定时执行还可手动触发，api 触发等

Airflow 常用的场景Airflow 主要用于执行预定的批处理作业。它能够很好地管理不同的批量作业的关系，并给将复杂的关系图形化展示。

系统或运用的日常维护任务，批量作业
CD 部署任务，灰度发布，蓝绿部署等
数据分析，数据流管道管理

一句话：任何批量任务或者需要手动去触发执行的任务都可以考虑一下 Airflow
Airflow 安装airlow 使用 ansible role 方式安装 https://github.com/idealista/airflow-role 0. 安装准备软件 ansible、gcc、python-devel
$ yum install ansible gcc python-devel -y


创建文件requirements.yml 

- src: idealista.airflow-role  version: 1.0.0  name: airflow


下载 airflow role

$ ansible-galaxy install -p roles -r requirements.yml -f


更改 role 中的 install.yml将 roles&#x2F;airflow&#x2F;tasks&#x2F;install.yml 中的apt模块改成package模块将 roles&#x2F;airflow&#x2F;tasks&#x2F;install.yml 中的 Copy Daemon scripts 中的 when 条件去掉

$ cat roles/airflow/tasks/install.yml...- name: Airflow | Installing dependencies  package:    name: &quot;&#123;&#123; item &#125;&#125;&quot;    state: present  with_items: &quot;&#123;&#123; airflow_required_libs &#125;&#125;&quot;...- name: Airflow | Copy Daemon scripts  template:    src: &quot;&#123;&#123; item.key &#125;&#125;.service.j2&quot;    dest: /lib/systemd/system/&#123;&#123; item.key &#125;&#125;.service    mode: 0644  notify: restart &#123;&#123; item.key &#125;&#125;  with_dict: &quot;&#123;&#123; airflow_services &#125;&#125;&quot;

更改 role 下的 config.yml 及 templates&#x2F;airflow-***.service.j2 文件将/usr/local/bin/airflow改为/usr/bin/airflow
$ cat roles/airflow/tasks/config.yml...- name: Airflow | Initializing DB  shell: AIRFLOW_HOME=&#123;&#123; airflow_home &#125;&#125; airflow initdb...$ cat roles/airflow/templates/airflow-webserver.service.j2...ExecStart=/usr/bin/airflow webserver --pid /run/airflow/webserver.pid...

目的该 role 使用 pkg 只能在 Debian  或 Ubuntu 环境下安装，更改后，可以在 centos 环境下安装 4. 更新 ansible 的 hosts 文件
$ cat /etc/ansible/hosts127.0.0.1 ansible_connection=local


创建部署 airflow 的 ansible yaml

$ cat airflow.yml---- hosts: all  roles:  - &#123; role: airflow &#125;


安装 airflow

$ ansible-playbook airflow.yml

执行完以上操作后本机就安装好了 airflow，同时也配置好了 service 等。 7. 手动初始化 db
$ HOME=/etc/airflow; airflow initdb


更新配置&#x2F;etc&#x2F;airflow&#x2F;airflow.cfg

$ cat /etc/airflow/airflow.cfg...max_threads = 1dagbag_import_timeout = 30...


启动 airflow-worker

$ systemctl restart airflow-worker

安装好后展示

问题记录
如果使用 CeleryExecutor，可以安装 Celery 管理器 flower。，默认会安装 tornado 最新版本，需要限定 tornado 版本需限制在 4.2.0。安装完成后运行AIRFLOW_HOME=/etc/airflow airflow flower
使用 Mysql 作为后台数据库时，安装 mysql 扩展请安装 mysql-python。同时 broker_url 设置为broker_url =  mysql://airflow:airflow@localhost:3306/airflow
测试 Airflow 中的 DAG 时，需要在界面上将它先把状态改为 On 后，再点击运行。

相关文章Airflow 中文文档任务调度神器 airflow 之初体验airflow 安装，部署，填坑airflow 配置 CeleryExecutor浅谈调度工具——Airflow如何部署一个健壮的 apache-airflow 调度系统Airflow 工作模式及适用场景浅谈调度工具——Airflow

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible常用模块</title>
    <url>/posts/24bf32e1f396.html</url>
    <content><![CDATA[lookup 函数在 playbooks 中可以使用一个名为 lookup()的函数，该函数用于 ansible 从外部资源访问数据，根据第一个参数的不同，该函数具有不同的功能，典型的就是读取外部文件内容。lookup()只在本地执行，而不是在远程主机上执行。例子：
debug: msg=&quot;&#123;&#123; lookup(&#x27;file&#x27;, &#x27;/etc/foo.txt&#x27;) &#125;&#125;&quot;debug: msg=&quot;password - &#123;&#123; lookup(&#x27;password&#x27;, &#x27;/tmp/random_pass.txt length=10&#x27;) &#125;&#125;&quot;debug: msg=&quot;&#123;&#123; lookup(&#x27;env&#x27;,&#x27;HOME&#x27;) &#125;&#125; is an environment variable&quot;debug: msg=&quot;&#123;&#123; lookup(&#x27;pipe&#x27;,&#x27;date&#x27;) &#125;&#125; is the raw result of running this command&quot;debug: msg=&quot;&#123;&#123; lookup(&#x27;dnstxt&#x27;, &#x27;example.com&#x27;) &#125;&#125; is a DNS TXT record for example.com&quot;debug: msg=&quot;&#123;&#123; lookup(&#x27;template&#x27;, &#x27;./some_template.j2&#x27;) &#125;&#125; is a value from evaluation of this template&quot;

k8s 模块使用 lookup 实例
- name: Read definition file from file after jinja templating  k8s:      state: present      definition: &quot;&#123;&#123; lookup(&#x27;template&#x27;, &#x27;/testing/deployment.yml&#x27;) | from_yaml &#125;&#125;&quot;

git 模块简介此模块用于 checkout 远程 git 仓库中的文件
使用要求（在执行模块的主机上）git&gt;&#x3D;1.7.1 (命令行工具)
模块参数


名称
必选
默认值
备注



accept_hostkey
no
no
如果yes，请确保“-o StrictHostKeyChecking &#x3D; no”作为 ssh 选项存在。


archive
no

使用扩展名指定存档文件路径。 如果指定，则创建包含源树树结构的指定格式的存档文件。 允许的存档格式[“zip”，“tar.gz”，“tar”，“tgz”]这将从本地目录克隆并执行 git archive


bare
no
no
如果yes，则将创建存储库作为裸存储库，否则它将是具有工作空间的标准存储库。


clone
no
yes
如果no，即使它本地不存在，也不要克隆存储库


depth
no

clone 的深度，最小值为 1， git&gt;&#x3D;1.9.1 才支持


dest
yes

应该检出存储库的路径。 除非将 clone 设置为 no，否则此参数是必需的。


executable
no

要使用的 git 可执行文件的路径


force
no
no
如果yes，则将丢弃工作存储库中的任何已修改文件。


key_file
no

私钥存放地址


recursive
no
yes
如果no，将使用–recursive 选项克隆存储库，跳过子模块。


reference
no

参考 git clone –reference


refspec
no
no
添加要获取的其他 refspec。 如果将版本设置为无法从任何分支或标记访问的 SHA-1，则可能需要此选项来指定包含 SHA-1 的 ref。 使用与’git fetch’命令相同的语法。 示例值可以是“refs &#x2F; meta &#x2F; config”。


remote
no
origin
 远程仓库名


repo
yes

git 仓库地址


separate_git_dir
no

 设置 git 仓库目录的存储


ssh_opts
no

ssh 命令参数，覆盖默认的 ssh 参数


track_submodules
no
no
如果 yes，子模块将跟踪其主分支（或.gitmodules 中指定的其他分支）上的最新提交。 如果no，则子模块将保留在主项目指定的修订版本中。 这相当于为 git 子模块更新指定了–remote 标志。


umask
no

在执行任何检出或任何其他存储库维护之前设置的 umask。


update
no
yes
如果no，请不要从源存储库中检索新修订


verify_commit
no
no
如果yes，则在克隆或签出版本时验证 GPG 签名提交的签名。git&gt;2.1.0


version
no
HEAD
clone 代码的版本号


示例# Example git checkout from Ansible Playbooks- git:    repo: &#x27;https://foosball.example.org/path/to/repo.git&#x27;    dest: /srv/checkout    version: release-0.22# Example read-write git checkout from github- git:    repo: git@github.com:mylogin/hello.git    dest: /home/mylogin/hello# Example just ensuring the repo checkout exists- git:    repo: &#x27;https://foosball.example.org/path/to/repo.git&#x27;    dest: /srv/checkout    update: no# Example just get information about the repository whether or not it has# already been cloned locally.- git:    repo: &#x27;https://foosball.example.org/path/to/repo.git&#x27;    dest: /srv/checkout    clone: no    update: no# Example checkout a github repo and use refspec to fetch all pull requests- git:    repo: https://github.com/ansible/ansible-examples.git    dest: /src/ansible-examples    refspec: &#x27;+refs/pull/*:refs/heads/*&#x27;# Example Create git archive from repo- git:    repo: https://github.com/ansible/ansible-examples.git    dest: /src/ansible-examples    archive: /tmp/ansible-examples.zip# Example clone a repo with separate git directory- git:    repo: https://github.com/ansible/ansible-examples.git    dest: /src/ansible-examples

expect 模块简介
expect模块用于在给的的节点上执行一个命令并响应提示。
它不会通过 shell 处理命令，因此不支持像$HOME这样的变量和，以及&lt;, &gt;, |, ;和&amp;等都是无效的。也就是在command模块中无法使用管道符。

使用要求（在执行模块的主机上）python &gt;&#x3D; 2.6pexpect &gt;&#x3D; 3.3
模块参数


名称
必选
默认值
备注



chdir
no

运行 command 命令前先 cd 到这个目录


command
yes

命令模块执行命令运行


echo
no
no
是否回显你的回应字符串


responses
yes

期望的字符串&#x2F;正则表达式和字符串的映射来响应。 如果响应是一个列表，则连续的匹配将返回连续的响应。 列表功能是 2.1 中的新功能。


creates
no

如果这个参数对应的文件存在，就不运行 command


removes
no

如果这个参数对应的文件不存在，就不运行 command，与 creates 参数的作用相反


timeout
no
30
以秒为单位等待预期时间


示例
在远程主机上执行脚本

- name: Case insensitve password string match  expect:    command: passwd username    responses:      (?i)password: &quot;MySekretPa$$word&quot;- name: Generic question with multiple different responses  expect:    command: /path/to/custom/command    responses:      Question:        - response1        - response2        - response3

注意事项
如果你想通过 shell 运行一个命令（比如你正在使用&lt;,&gt;,|等），你必须在命令中指定一个 shell，比如/bin/bash -c &quot;/path/to/something | grep else&quot;。
在responses下关键是一个 python 正则表达式匹配，不区分大小写的搜索用前缀?i。
默认情况下，如果多次遇到问题，则会重复其字符串响应。 如果连续问题匹配需要不同的响应，而不是字符串响应，请使用字符串列表作为响应。
expect模块设计用于简单场景，对于更复杂的需求，应该考虑在shell或script模块中使用 expect 代码

Ansible 执行命令小技巧
执行时选择在某个 host 组而不在另一个 host 组的机器

$ ansible all:&#x27;!masters&#x27; --list-hosts


如果主机的密码都是一样的，可以在 inventory 中添加变量

[k8s:children]mastersinfrasnodes[k8s:vars]ansible_user=rootansible_password=password[masters]master1.example.com ansible_host=192.168.0.2master2.example.com ansible_host=192.168.0.3[infras][nodes]

Ansible 注册结果数据ansible playbook 中使用 register 注册数据，实现不同 playbook 中数据共享。当完成 regster 后，下游 playbook 可以直接使用注册的变量，这时获取的数据为第一个 hosts 中匹配的服务器执行完成任务后注册的数据。如果需要获取全部数据，需要通过hostvars获取，它会根据主机名为 key，注册的数据为 value 保存在 hostvars 中。例如：
- name: get results  get_results:  register: results- name: out host1 result  run_once: yes  debug:    msg: &quot;&#123;&#123; hostvars[&#x27;host1&#x27;].results &#125;&#125;&quot;- name: out host2 result  run_once: yes  debug:    msg: &quot;&#123;&#123; hostvars[&#x27;host2&#x27;].results &#125;&#125;&quot;

另外通过ansible_play_hosts可获取当前运行任务的所有主机。
Ansible 学习资料
ansible 学习之一：Getting Started
ansible 学习之二：Inventory
ansible 学习之三：Host Patterns
ansible 学习之四：Playbooks
ansible 学习之五：Roles and Include Statements
https://sapser.github.io/ansible/2014/07/21/ansible-variables
ansible 学习之七：条件判断
ansible 学习之八：循环
https://sapser.github.io/ansible/2014/07/22/ansible-tags
ansible 学习之十：Error Handling In Playbooks
https://sapser.github.io/ansible/2014/07/22/ansible-prompts
ansible 学习之十二：Using Lookups

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>CI-CD流程-命令行方式与Jenkins-Blue-Ocean方式</title>
    <url>/posts/d1a90c6e8453.html</url>
    <content><![CDATA[
命令行模式
环境准备


克隆代码

git clone

启动私有docker hub

 docker run -d -p 5000:5000 -v /data/registry:/var/lib/registry --name registry --restart=always registry

制作建置环境

docker build -t localhost:5000/maven dockers/maven

dockers&#x2F;maven&#x2F;Dockerfile
FROM ## 安装openjdk## 安装MavenCMD [&quot;mvn&quot;]

集成、测试、验证

进行自动化测试


docker-compose run --rm test
docker-compose.yaml
data：  image: alpine  volumes:    - ~/.m2:/root/.m2  command: &quot;/bin/bash&quot;install:  image: localhost:5000/maven  volumes:    - ./:/app  volumes_from:    - data  working_dir: /app  command: &#x27;mvn test&#x27;server:  image: localhost:5000/maven  volumes:    - ./:/app    - ~/.m2:/root/.m2  working_dir: /app  ports:    - &quot;8000:8000&quot;  command: &quot;mvn spring-boot:run&quot;

启动 alpha server

docker-compose up -d server

人员确认启动状况
关闭alpha server

docker-compose stop server

产出报表与封存

docker-compose run --rm package

部署

构建生产镜像不需要maven，把额外的都去掉，保留最干净的环境，Dockerfile一般放在根目录下


make build-docker-prod-image
# DockerfileFROM javaCOPY target/spring-boot-simple-data-rest-0.1.0.jar /app/EXPOSE 8000WORKDIR /appCMD /bin/bash -c &#x27;java -jar spring-boot-simple-data-rest-0.1.0.jar 

发布生产镜像

docker push localhost:5000/java_simple_prod

启动生产镜像

make deploy-production-local
Makefile
start-docker-registry:  - docker run -d -p 5000:5000 -v /data/registry:/var/lib/registry --name registry --restart=always registrybuild-docker-env:  docker build -t localhost:5000/maven dockers/mavenbuild-docker-prod-image:  docker build -t localhost:5000/java_simple_prod .deploy-production-local:  - docker rm -f java_simple_prod  - docker run -d --name java_simple_prod -p 8000:8000 localhost:5000/java_simple_prod
Jenkins模式Blue Ocean# Jenkinsfile https://github.com/agileworks-tw/spring-boot-samplepipeline &#123;    agent any    stages &#123;        stage(&#x27;checkout project&#x27;) &#123;            steps &#123;                //git url: &#x27;https://github.com/agileworks-tw/spring-boot-sample.git&#x27;                checkout scm            &#125;        &#125;        stage(&#x27;check docker install and build env&#x27;) &#123;            steps &#123;                sh &quot;docker -v&quot;                sh &quot;docker-compose -v&quot;                sh &quot;docker ps&quot;            	sh &quot;make start-docker-registry&quot;                sh &quot;make build-docker-env&quot;            &#125;        &#125;        stage(&#x27;test project and serve&#x27;) &#123;            steps &#123;                sh &quot;docker-compose run --rm test&quot;                sh &quot;docker-compose up -d server&quot;            &#125;            post &#123;                always &#123;                    archiveArtifacts artifacts: &#x27;**/target/*.jar&#x27;, fingerprint: true                    junit &#x27;**/target/surefire-reports/TEST-*.xml&#x27;                &#125;            &#125;        &#125;        stage(&#x27;wait for confirm&#x27;) &#123;            input &#123;                message &quot;Does staging at http://localhost:8000 look good?&quot;                ok &quot;Deploy to production&quot;                submitter &quot;admin&quot;                parameters &#123;                    string(name: &#x27;PERSON&#x27;, defaultValue: &#x27;Mr Jenkins&#x27;, description: &#x27;Who should I say hello to?&#x27;)                &#125;            &#125;            steps &#123;                echo &quot;Hello, $&#123;PERSON&#125;, nice to meet you.&quot;            &#125;            post &#123;                 always &#123;                     sh &quot;docker-compose stop server&quot;                &#125;            &#125;        &#125;        stage(&#x27;deploy project&#x27;) &#123;            when &#123;                branch &#x27;master&#x27;            &#125;            steps &#123;                sh &quot;docker-compose run --rm package&quot;                sh &quot;make build-docker-prod-image&quot;                sh &quot;docker push localhost:5000/java_sample_prod&quot;                sh &quot;make deploy-production-local&quot;            &#125;                    &#125;            &#125;&#125;
拓展
agent docker所有应用在指定镜像中执行

pipeline &#123;    agent &#123;        docker &#123;            image &#x27;maven:3-alpine&#x27;            args &#x27;-v /root/.m2:/root/.m2&#x27;        &#125;    &#125;    stages &#123;        stage(&#x27;Build&#x27;) &#123;            steps &#123;                checkout scm            &#125;        &#125;&#125;

post当执行完有错误或者成功时运行

pipeline &#123;  agent any  stages &#123;     stage(&#x27;Test&#x27;) &#123;            steps &#123;                sh &#x27;mvn test&#x27;            &#125;            post &#123;                always &#123;                    junit &#x27;target/surefire-reports/*.xml&#x27;                &#125;            &#125;        &#125;  &#125;&#125;

input parameter用户输入

stage(&#x27;wait for input&#x27;)&#123;  input &#123;    ......  &#125;&#125;

when condition

stage(&#x27;deploy project&#x27;)&#123;  when&#123;    branch &#x27;master&#x27;  &#125;  steps&#123;    ......  &#125;&#125;
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch-+-Kibana添加到Systemctl</title>
    <url>/posts/470494acb742.html</url>
    <content><![CDATA[&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kibana.service
[Unit]Description=Elasticsearch[Service]PIDFile=/home/vagrant/kibana-5.1.1-linux-x86_64/kibana.pidExecStart=/home/vagrant/kibana-5.1.1-linux-x86_64/bin/kibanaUser=vagrantRestart=alwaysLimitMEMLOCK=infinityLimitNOFILE=1048576LimitNPROC=1048576

&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;elasticsearch.service
[Unit]Description=Elasticsearch[Service]PIDFile=/home/vagrant/elasticsearch-rtf/elasticsearch.pidExecStart=/home/vagrant/elasticsearch-rtf/bin/elasticsearchUser=vagrantRestart=alwaysLimitMEMLOCK=infinityLimitNOFILE=1048576LimitNPROC=1048576
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Git-+-Jenkins-提交Git-触发Jenkins-Job</title>
    <url>/posts/ef3675d1c305.html</url>
    <content><![CDATA[利用git的Hook机制

.git&#x2F;hooks&#x2F;pre-push

#!/bin/bashecho ====== trigger jenkins job: maglev-verify in 10s  =======(sleep 10 &amp;&amp; .git/hooks/post-push) &amp;

.git&#x2F;hooks&#x2F;post-push

#!/bin/bashbranch=$(git rev-parse --abbrev-ref @&#123;upstream&#125;)curl -k --user $&#123;USER&#125;=$&#123;TOKEN&#125; http://$&#123;JENKINS_URL&#125;/job/$&#123;JOB_NAME&#125;/buildWithParameters --data-urlencode TAG=$branch]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Gitea访问使用KeyCloak作为用户管理</title>
    <url>/posts/75da61c45e57.html</url>
    <content><![CDATA[一、安装keyCloak
二、配置KeyCloak，添加ClientsClients -&gt; createClient ID: devops-oidcEnabled: trueClient Protocol: openid-connectAccess Type: confidentialValid Redirect URIs: &#x2F;*
三、配置Gitea，添加Authentication SourcesSite Administration -&gt; Authentication Sources -&gt; Add Authentication SourceOAuth2 Provider: OpenID ConnectClient ID(key): devops-oidcClient Secret: &lt;keycload 中 devops-oidc Credentials中查询&gt;OpenID Connect Auto Discovery URL: http:&#x2F;&#x2F;&#x2F;auth&#x2F;realms&#x2F;master&#x2F;.well-known&#x2F;openid-configuration
gitea app.iniALLOW_ONLY_EXTERNAL_REGISTRATION &#x3D; true
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins-Pipeline语法实例</title>
    <url>/posts/b4bb0e80ace4.html</url>
    <content><![CDATA[Jenkins Pipeline支持的指令


指令名
说明
作用域



agent
定义执行任务的代理
stage 或pipeline


environment
设置环境变量
stage或pipeline


tools
自动下载并安装指定的工具，并将其加入到PATH变量中
stage或pipeline


input
暂停pipeline,提示输入内容
stage


options
配置Jenkins pipeline本身，如options{retry(3}},指pipeline失败时再重试2次
stage 或 pipeline


parallel
并行执行多个step
stage


parameters
执行pipeline前传入一些参数
pipeline


triggers
定义执行pipeline的触发器
pipeline


when
定义阶段执行的条件
stage


build
触发其他的job
steps


options Jenkins Pipeline配置参数


参数名
说明
例子



buildDiscarder
保留最近历史构建记录的数量
buildDiscarder(logRotator(numToKeepStr: ‘10’)


checkoutToSubdirectory
将代码从版本控制库中拉取后，保存在工作目录的子目录
checkoutToSubdirectory(‘subdir’)


disableConcurrentBuilds
禁用Jenkins同时执行多次该pipeline
disableConcurrentBuilds()


newContainerPerStage
agent为Docker或Dockerfile时，每个stage都分别运行在一个新容器中
newContainerPerStage()


retry
pipeline发生失败后重试次数
retry(4)


timeout
pipeline运行超时时间
timeout(time:10, unit: ‘HOURS’)


pipeline&#123;	agent any	options&#123;		buildDiscarder(logRotator(numToKeepStr: &#x27;10&#x27;)		disableConcurrentBuilds()		retry(4)		timeout(time:10, unit: &#x27;HOURS&#x27;)	&#125;	stages&#123;		stage(&#x27;demo&#x27;)&#123;			steps&#123;				sh &#x27;echo hello&#x27;			&#125;		&#125;	&#125;&#125;
stage间通过stash进行文件共享，即使stage不在同一个执行主机上pipeline&#123;	agent none	stages&#123;		stage(&#x27;stash&#x27;)&#123;			agent &#123; label &quot;master&quot; &#125;			steps&#123;				writeFile file: &quot;a.txt&quot;, text: &quot;$BUILD_NUMBER&quot;				stash name: &quot;abc&quot;, includes: &quot;a.txt&quot;			&#125;		&#125;		stage(&#x27;unstash&#x27;)&#123;			agent &#123; label &quot;node&quot; &#125;			steps&#123;				script&#123;					unstash(&quot;abc&quot;)					def content = readFile(&quot;a.txt&quot;)					echo &quot;$&#123;content&#125;&quot;				&#125;			&#125;		&#125;	&#125;&#125;
steps中的一些操作


命令名
说明



error
抛出异常，中断整个pipeline


timeout
timeout闭包内运行的步骤超时时间


waitUntil
一直循环运行闭包内容，直到return true，经常与timeout同时使用


retry
闭包内脚本重复执行次数


sleep
暂停pipeline一段时间，单位为秒


pipeline&#123;	agent any	stages&#123;		stage(&#x27;stash&#x27;)&#123;			steps&#123;				timeout(50)&#123;					waitUntil&#123;						script&#123;							def r = sh script: &#x27;curl http://xxx&#x27;, returnStatus: true							return (r == 0)						&#125;					&#125;				&#125;				retry(10)&#123;					script&#123;						sh script: &#x27;curl http://xxx&#x27;, returnStatus: true					&#125;				&#125;				sleep(20)			&#125;		&#125;	&#125;	post&#123;	    always&#123;	        echo &quot;结束job&quot;	    &#125;	&#125;&#125;
来一个相对复杂一点的交互式Pipelinepipeline&#123;	agent any	triggers&#123;		upstream(upstreamProjects: &#x27;job1,job2&#x27;, threshold: hudson.model.Result.SUCCESS)	&#125;	stages&#123;		stage(&#x27;pre deploy&#x27;)&#123;			steps&#123;				script&#123;					BRANCHES = sh  returnStdout: true, script: &#x27;git branch -r | grep -v HEAD &gt; out.txt; git tag &gt;&gt; out.txt; cat out.txt;&#x27;					dataMap = input message: &#x27;准备发布到哪个环境&#x27;, ok: &#x27;确定&#x27;, parameters: [choice(choices: [&#x27;dev&#x27;, &#x27;sit&#x27;, &#x27;prod&#x27;], description: &#x27;部署环境&#x27;, name: &#x27;ENV&#x27;), choice(choices: &quot;$&#123;BRANCHES&#125;&quot;, description: &#x27;分支&#x27;, name: &#x27;TAG&#x27;)], submitterParameter: &#x27;APPROVER&#x27;				&#125;			&#125;		&#125;		stage(&quot;演示一下&quot;)&#123;			steps&#123;	          	echo &quot;$&#123;dataMap[&#x27;APPROVER&#x27;]&#125;&quot;	          	echo &quot;$&#123;dataMap[&#x27;ENV&#x27;]&#125;&quot;	          	echo &quot;$&#123;dataMap[&#x27;TAG&#x27;]&#125;&quot;	        &#125;		&#125;	&#125;&#125;

共享库share library解决问题： 

pipeline不出现方法定义，把所有类库写到library中
pipeline中重复步骤可以写到library中
规范流程过程例子：https://github.com/liwei2151284/jenkins_library参考文章：https://testerhome.com/topics/10782


系统设置中设置global pipeline libraries，名字为jenkins_library，添加git地址共享库
使用：

library &quot;jenkins_library&quot;node &#123;  stage(&quot;test&quot;)&#123;    sayHello()  &#125;&#125;
参考资料《Jenkins 2.X实践指南》
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins-Pipeline触发机制</title>
    <url>/posts/63c9fc8a8ef1.html</url>
    <content><![CDATA[设置好了功能强大的流水线后，接下来我们要做的就是去执行流水线，如果都靠手动去触发流水线，显然不符合带着自动化基因新一代工程师的风格，必须自动触发。那么Jenkins的Pipeline支持哪些触发机制呢。一起来看一下。
定时触发：croncron规则与crontab的规则是一样的
pipeline&#123;	agent any	triggers&#123;		cron(&#x27;0 0 * * *&#x27;)	&#125;	stages&#123;		stage(&#x27;半夜触发&#x27;)&#123;			steps&#123;				echo &quot;凌晨执行&quot;			&#125;		&#125;	&#125;&#125;

轮询代码仓库周期性检查代码，看代码是否有更新。这种方式需要使用
pipeline&#123;	agent any	triggers&#123;		pollSCM(&#x27;* * * * *&#x27;)	&#125;	stages&#123;		stage(&#x27;每分钟检查&#x27;)&#123;			steps&#123;				echo &quot;每分钟检查代码仓库是否更新&quot;			&#125;		&#125;	&#125;&#125;
由上游任务触发pipeline&#123;	agent any	triggers&#123;		upstream(upstreamProjects: &#x27;job1,job2&#x27;, threshold: hudson.model.Result.SUCCESS)	&#125;	stages&#123;		stage(&#x27;上游触发&#x27;)&#123;			steps&#123;				echo &quot;当job1或job2执行成功时，触发该流水线&quot;			&#125;		&#125;	&#125;&#125;
hudson.model.Result包括以下状态：ABORTED：任务被手动中止FAILURE：构建失败SUCCESS：构建成功UNSTABLE：存在一些错误，但构建没失败NOT_BUILT：多阶段构建时，前面阶段问题导致后面阶段无法执行
GitLab通知触发详情请在插件Gitlab plugin的github页面上查看
pipeline&#123;	agent any	triggers&#123;		gitlab(			triggerOnPush: true, 			triggerOnMergeRequest: true,			branchFilterType: &#x27;All&#x27;,			secretToken: &quot;abcdxxxfa&quot;)	&#125;	stages&#123;		stage(&#x27;构建&#x27;)&#123;			steps&#123;				echo &quot;gitlab 触发&quot;			&#125;		&#125;	&#125;&#125;
将构建状态信息推送到GitLab
在Jenkins的系统设置中，Gitlab选项下填入Gitlab信息。例Connection name设为gitlab
按提示设置Gitlab的凭证，以对话框中输入Gitlab平台获取的API token
在pipeline的post部分，将构建结果更新到Gitlab相应的commit记录下，同时还需要在options参数中加入gitLabConnection配置

pipeline&#123;	agent any	triggers&#123;		gitlab(			triggerOnPush: true, 			triggerOnMergeRequest: true,			branchFilterType: &#x27;All&#x27;,			secretToken: &quot;abcdxxxfa&quot;)	&#125;	stages&#123;		stage(&#x27;构建&#x27;)&#123;			steps&#123;				echo &quot;gitlab 触发&quot;			&#125;		&#125;	&#125;	post&#123;		failure&#123;			updateGitlabCommitStatus name: &#x27;build&#x27;, state: &#x27;failed&#x27;		&#125;		success&#123;			updateGitlabCommitStatus name: &#x27;build&#x27;, state: &#x27;success&#x27;		&#125;	&#125;	options&#123;		gitLabConnection(&#x27;gitlab&#x27;)	&#125;&#125;
Generic Webhook Trigger插件触发Generic Webhook Trigger是一个通用的通过Webhook的方式触发pipeline的插件
pipeline&#123;	agent any	triggers&#123;		GenericTrigger(			genericVariables: [				key: &#x27;ref&#x27;, value: &#x27;$.ref&#x27;			],			token: &#x27;secret&#x27;,			causeString: &#x27;Triggered on $ref&#x27;,			printContributedVariables: true,			printPostContent: true,                        regexpFilterText: &#x27;$ref&#x27;,                        regexpFilterExpression: &#x27;refs/heads(master|dev)&#x27;		)	&#125;	stages&#123;		stage(&#x27;自定义触发&#x27;)&#123;			steps&#123;				echo &quot;Generic 触发&quot;			&#125;		&#125;	&#125;&#125;
然后通过POST请求触发
curl -X POST -H &quot;Content-Type: application/json&quot; -d &#x27;&#123;&quot;ref&quot;: &quot;refs/heads/master&quot; &#125;&#x27; -vs http://jenkins.local:8080/jenkins/generic-webhook-trigger/invoke?token=secret
触发条件依靠：token、regexpFilterText、regexpFilterExpression三个参数。首先token必须匹配，其次regexpFilterText指定的key对应的值满足regexpFilterExpression表达式。
参考资料《Jenkins 2.X实践指南》
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Jmeter简单配置使用</title>
    <url>/posts/9bf126597211.html</url>
    <content><![CDATA[
创建Test Plan 测试计划File-&gt;New或者直接点击New图标
添加Thread Group 测试任务右击测试计划，Add-&gt;Threads(Users)-&gt;Thread Group
添加HTTP Cookie 请求Cookie管理右击Thread Group，Add-&gt;Config Element-&gt;HTTP Cookie Manager
添加HTTP Header  请求头管理右击Thread Group，Add-&gt;Config Element-&gt;HTTP Header Manager
添加HTTP Request 请求右击Thread Group，Add-&gt;Sampler-&gt;HTTP Request
添加User Defined Variables 用户定义变量右击Thread Group，Add-&gt;Config Element-&gt;User Defined Variable
添加报告Result Tree 结果树右击Thread Group，Add-&gt;Listener-&gt;View Result Tree
添加报告Aggregate Report 聚合报告右击Thread Group，Add-&gt;Listener-&gt;Aggregate Report

高级用法
添加Regular Expression Extractor 请求关联 右击创建的HTTP Request，Add-&gt;Post Processors-&gt;Regular Expression Extractor在第二个请求中就可以使用这个变量了。
使用Jmeter录制测试脚本
创建一个Thread Group
创建http代理服务器



]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Mysql瓶颈分析方法</title>
    <url>/posts/78c6906d0976.html</url>
    <content><![CDATA[数据库往往会成为应用的最终瓶颈，而Mysql是被使用得最多的开源关系型数据库。如何分析执行Mysql数据库语句的性能就非常重要。但是很多开发人员并没有相关的意识与能力，但其实掌握了一些简单的常用手段，就可以让我们自己分析出数据库语句的问题。曾经看到过有对数据库查询语句中出现7个Select的语句，当时完全被震惊到了，这根本就是往系统里注入了一个大雷呀，数据量一旦增多，数据库挂，应用挂，服务挂，客户挂，公司挂。。。。还是不做破了一个鸡蛋就想着毁了一个养鸡场的推断了。我们收集下常见的数据库的分析手段。

查看当前数据库执行命令

mysql&gt; select count(*) from information_schema.processlist where COMMAND != &#x27;Sleep&#x27;;mysql&gt; select * from information_schema.processlist where COMMAND != &#x27;Sleep&#x27; limit 5;

慢查询查看慢查询时间定义

mysql&gt; show variables like &quot;long%&quot;;+-----------------+----------+| Variable_name   | Value    |+-----------------+----------+| long_query_time | 10 |+-----------------+----------+1 row in set (0.00 sec)mysql&gt; set long_query_time=1;Query OK, 0 rows affected (0.00 sec)
开启“慢查询”记录功能
mysql&gt; show variables like &quot;slow%&quot;;+---------------------+------------------------------------+| Variable_name       | Value                              |+---------------------+------------------------------------+| slow_launch_time    | 2                                  || slow_query_log      | OFF                                || slow_query_log_file | /opt/mysql/data/localhost-slow.log |+---------------------+------------------------------------+3 rows in set (0.00 sec)mysql&gt; set global slow_query_log=ON;Query OK, 0 rows affected (0.01 sec)
设置慢日志导出方式为Table或者File
mysql&gt; show variables like &quot;log_output&quot;;+---------------------+------------------------------------+| Variable_name       | Value                              |+---------------------+------------------------------------+| log_output         | FILE                                  |+---------------------+------------------------------------+3 rows in set (0.00 sec)mysql&gt; set global log_output=TABLE     Query OK, 0 rows affected (0.01 sec)

explain输出执行计划对sql进行分析

mysql&gt; explain select uid from user where mo=132342342 limit 0,1;
出现以下结果就需要优化了：

出现了Using temporary；
rows过多，或者几乎是全表的记录数；
key 是 (NULL)；
possible_keys 出现过多（待选）索引。


soar小米SQL分析工具SOAR(SQL Optimizer And Rewriter)是一个对SQL进行优化和改写的自动化工具。 由小米人工智能与云平台的数据库团队开发与维护。


查看是否有锁表

mysql&gt; show engine innodb status;
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx性能优化配置参考</title>
    <url>/posts/bc3bb21f4547.html</url>
    <content><![CDATA[系统优化
系统内核优化参考

$ cat /etc/sysctl.confnet.ipv4.tcp_max_syn_backlog = 65536net.core.netdev_max_backlog =  36768net.core.somaxconn = 36768 net.core.wmem_default = 8588608net.core.rmem_default = 8588608net.core.rmem_max = 16877216net.core.wmem_max = 16877216 net.ipv4.tcp_synack_retries = 2net.ipv4.tcp_syn_retries = 2 net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.tcp_max_orphans = 3376800net.ipv4.ip_local_port_range = 1024  65535$ sysctl -p$ cat /etc/security/limit.conf*  hard  nofile  65535*  soft  nofile  65535

Nginx配置优化
Nginx配置参考

#user  nobody;worker_processes  4;worker_cpu_affinity 0001 0010 0100 1000;worker_rlimit_nofile 65535;#pid        logs/nginx.pid;events &#123;        use epoll;        worker_connections 65535;        multi_accept on;&#125;http &#123;    include       mime.types;    default_type  application/octet-stream;    #access_log  logs/access.log  main;    log_format  main  &#x27;$http_X_Real_IP $http_CLIENTIP $remote_addr $remote_user [$time_local] &quot;$request&quot; &#x27;                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; $request_time&#x27;;    sendfile        on;    tcp_nopush     on;    keepalive_timeout  60;    keepalive_requests 10240;    tcp_nodelay on;    client_header_buffer_size 4k;    open_file_cache max=102400 inactive=20s;    open_file_cache_valid 30s;    open_file_cache_min_uses 1;    client_header_timeout 15;    client_body_timeout 15;    reset_timedout_connection on;    send_timeout 15;    server_tokens off;    client_max_body_size 10m;    gzip  off;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_http_version 1.0;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css application/xml;    gzip_vary on;    fastcgi_connect_timeout    600;    fastcgi_send_timeout 600;    fastcgi_read_timeout 600;    fastcgi_buffer_size 64k;    fastcgi_buffers 4 64k;    fastcgi_busy_buffers_size 128k;    fastcgi_temp_file_write_size 128k;    fastcgi_temp_path /usr/local/nginx/fastcgi_temp;    server &#123;        listen       80;        server_name  localhost;        access_log  /usr/local/logs/nginx/access.log  main;        root        html;        index       index.html index.htm index.php;        #图片缓存时间                location ~* \.(ico|jpe?g|gif|png|bmp|swf|flv)$ &#123;          expires 30d;          #log_not_found off;          access_log off;        &#125;        #JS和CSS缓存时间        location ~* \.(js|css)$ &#123;          expires 7d;          log_not_found off;          access_log off;        &#125;        error_page   500 502 503 504  /50x.html;        location / &#123;                try_files $uri $uri/ @rewrites;        &#125;        location @rewrites &#123;                rewrite ^ /index-development.php last;        &#125;        location = /robots.txt &#123;                access_log off;                log_not_found off;         &#125;        location ~ \.php$ &#123;            root           html;            fastcgi_pass   127.0.0.1:9000;            fastcgi_index  index.php;            fastcgi_param   SCRIPT_FILENAME  $document_root$fastcgi_script_name;            include        fastcgi_params;        &#125;    &#125;    include conf.d/*;&#125;

worker_processesnginx运行工作进程个数，一般设置cpu的核心或者核心数x2，如：worker_processes  4;
worker_cpu_affinity运行CPU亲和力，与worker_processes对应，如：worker_cpu_affinity 0001 0010 0100 1000;
worker_rlimit_nofile  Nginx最多可以打开文件数，与ulimit -n保持一致，如：worker_rlimit_nofile 65535;
events事件处理模型。如：

events &#123;  use epoll;  worker_connections 65535;  multi_accept on;&#125;
use epoll：nginx采用epoll事件模型，处理效率高work_connections：是单个worker进程允许客户端最大连接数，这个数值一般根据服务器性能和内存来制定，实际最大值就是worker进程数乘以work_connections，实际我们填入一个65535，足够了，这些都算并发值，一个网站的并发达到这么大的数量，也算一个大站了！multi_accept ：告诉nginx收到一个新连接通知后接受尽可能多的连接，默认是on，设置为on后，多个worker按串行方式来处理连接，也就是一个连接只有一个worker被唤醒，其他的处于休眠状态，设置为off后，多个worker按并行方式来处理连接，也就是一个连接会唤醒所有的worker，直到连接分配完毕，没有取得连接的继续休眠。当你的服务器连接数不多时，开启这个参数会让负载有一定的降低，但是当服务器的吞吐量很大时，为了效率，可以关闭这个参数。

http高效传输模式，如：

http &#123;include mime.types;default_type application/octet-stream;……sendfile on;tcp_nopush on;……

Include mime.types： 媒体类型,include 只是一个在当前文件中包含另一个文件内容的指令default_type：默认媒体类型，如： application&#x2F;octet-stream;sendfile ：开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I&#x2F;O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成offtcp_nopush：必须在sendfile开启模式才有效，防止网路阻塞，积极的减少网络报文段的数量（将响应头和正文的开始部分一起发送，而不一个接一个的发送。）

连接超时时间主要目的是保护服务器资源，CPU，内存，控制连接数，因为建立连接也是需要消耗资源的，如：

keepalive_timeout 60;keepalive_requests 10240;tcp_nodelay on;client_header_buffer_size 4k;open_file_cache max=102400 inactive=20s;open_file_cache_valid 30s;open_file_cache_min_uses 1;client_header_timeout 15;client_body_timeout 15;reset_timedout_connection on;send_timeout 15;server_tokens off;client_max_body_size 10m;

keepalived_timeout 60：客户端连接保持会话超时时间，超过这个时间，服务器断开这个链接keepalive_requests 10240：参数限制了一个 HTTP 长连接最多可以处理完成的最大请求数, 默认是 100。当连接处理完成的请求数达到最大请求数后，将关闭连接。tcp_nodelay：也是防止网络阻塞，不过要包涵在keepalived参数才有效client_header_buffer_size 4k：客户端请求头部的缓冲区大小，这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过 1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。open_file_cache max&#x3D;102400 inactive&#x3D;20s：这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive 是指经过多长时间文件没被请求后删除缓存。open_file_cache_valid 30s：这个是指多长时间检查一次缓存的有效信息。open_file_cache_min_uses 1：open_file_cache指令中的inactive 参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive 时间内一次没被使用，它将被移除。client_header_timeout：设置请求头的超时时间。我们也可以把这个设置低些，如果超过这个时间没有发送任何数据，nginx将返回request time out的错误client_body_timeout：设置请求体的超时时间。我们也可以把这个设置低些，超过这个时间没有发送任何数据，和上面一样的错误提示reset_timeout_connection：告诉nginx关闭不响应的客户端连接。这将会释放那个客户端所占有的内存空间。send_timeout：响应客户端超时时间，这个超时时间仅限于两个活动之间的时间，如果超过这个时间，客户端没有任何活动，nginx关闭连接server_tokens：并不会让nginx执行的速度更快，但它可以关闭在错误页面中的nginx版本数字，这样对于安全性是有好处的。client_max_body_size：上传文件大小限制

fastcgi调优fastcgi配置优化，如：

fastcgi_connect_timeout    600;fastcgi_send_timeout 600;fastcgi_read_timeout 600;fastcgi_buffer_size 64k;fastcgi_buffers 4 64k;fastcgi_busy_buffers_size 128k;fastcgi_temp_file_write_size 128k;fastcgi_temp_path/usr/local/nginx1.10/nginx_tmp;fastcgi_intercept_errors on;fastcgi_cache_path/usr/local/nginx1.10/fastcgi_cache levels=1:2 keys_zone=cache_fastcgi:128minactive=1d max_size=10g;
fastcgi_connect_timeout：指定连接到后端FastCGI的超时时间，如：600fastcgi_send_timeout：向FastCGI传送请求的超时时间，如：600fastcgi_read_timeout：指定接收FastCGI应答的超时时间，如：600fastcgi_buffer_size：指定读取FastCGI应答第一部分需要用多大的缓冲区，默认的缓冲区大小为fastcgi_buffers指令中的每块大小，可以将这个值设置更小，如： 64k。fastcgi_buffers：指定本地需要用多少和多大的缓冲区来缓冲FastCGI的应答请求，如果一个php脚本所产生的页面大小为256KB，那么会分配4个64KB的缓冲区来缓存，如果页面大小大于256KB，那么大于256KB的部分会缓存到fastcgi_temp_path指定的路径中，但是这并不是好方法，因为内存中的数据处理速度要快于磁盘。一般这个值应该为站点中php脚本所产生的页面大小的中间值，如果站点大部分脚本所产生的页面大小为256KB，那么可以把这个值设置为“8 32K”、“4 64k”等。如：4 64kfastcgi_busy_buffers_size：建议设置为fastcgi_buffers的两倍，繁忙时候的buffer，如：128kfastcgi_temp_file_write_size：在写入fastcgi_temp_path时将用多大的数据块，默认值是fastcgi_buffers的两倍，该数值设置小时若负载上来时可能报502BadGateway，如：128kfastcgi_temp_path：缓存临时目录fastcgi_intercept_errors：这个指令指定是否传递4xx和5xx错误信息到客户端，或者允许nginx使用error_page处理错误信息，如：onfastcgi_cache_path：如： &#x2F;usr&#x2F;local&#x2F;nginx1.10&#x2F;fastcgi_cachelevels&#x3D;1:2 keys_zone&#x3D;cache_fastcgi:128minactive&#x3D;1d max_size&#x3D;10g： fastcgi_cache缓存目录，可以设置目录层级，比如1:2会生成16*256个子目录，cache_fastcgi是这个缓存空间的名字，cache是用多少内存（这样热门的内容nginx直接放内存，提高访问速度），inactive表示默认失效时间，如果缓存数据在失效时间内没有被访问,将被删除，max_size表示最多用多少硬盘空间。fastcgi_cache cache_fastcgi：表示开启FastCGI缓存并为其指定一个名称。开启缓存非常有用，可以有效降低CPU的负载，并且防止502的错误放生。cache_fastcgi：为proxy_cache_path指令创建的缓存区名称
fastcgi_cache_valid 200 302 1h：用来指定应答代码的缓存时间，实例中的值表示将200和302应答缓存一小时，要和fastcgi_cache配合使用fastcgi_cache_valid 301 1d：将301应答缓存一天fastcgi_cache_valid any 1m：将其他应答缓存为1分钟fastcgi_cache_min_uses 1：该指令用于设置经过多少次请求的相同URL将被缓存。fastcgi_cache_key http:&#x2F;&#x2F;$host$request_uri：该指令用来设置web缓存的Key值,nginx根据Key值md5哈希存储.一般根据$host(域名)、$request_uri(请求的路径)等变量组合成proxy_cache_key 。fastcgi_pass：指定FastCGI服务器监听端口与地址，可以是本机或者其它

总结：nginx的缓存功能有：proxy_cache &#x2F; fastcgi_cacheproxy_cache的作用是缓存后端服务器的内容，可能是任何内容，包括静态的和动态。proxy_cache缓存减少了nginx与后端通信的次数，节省了传输时间和后端宽带。fastcgi_cache的作用是缓存fastcgi生成的内容，很多情况是php生成的动态的内容。fastcgi_cache缓存减少了nginx与php的通信的次数，更减轻了php和数据库(mysql)的压力。


gzip调优使用gzip压缩功能，可能为我们节约带宽，加快传输速度，有更好的体验，也为我们节约成本，所以说这是一个重点。Nginx启用压缩功能需要你来ngx_http_gzip_module模块，apache使用的是mod_deflate。一般我们需要压缩的内容有：文本，js，html，css，对于图片，视频，flash什么的不压缩，同时也要注意，我们使用gzip的功能是需要消耗CPU的！如：

gzip on;gzip_min_length 2k;gzip_buffers    4 32k;gzip_http_version 1.1;gzip_comp_level 6;gzip_typestext/plain text/css text/javascriptapplication/json application/javascript application/x-javascriptapplication/xml;gzip_vary on;gzip_proxied any;
gzip on：开启压缩功能gzip_min_length 1k：设置允许压缩的页面最小字节数，页面字节数从header头的Content-Length中获取，默认值是0，不管页面多大都进行压缩，建议设置成大于1K，如果小与1K可能会越压越大。gzip_buffers 4 32k：压缩缓冲区大小，表示申请4个单位为32K的内存作为压缩结果流缓存，默认值是申请与原始数据大小相同的内存空间来存储gzip压缩结果。gzip_http_version 1.1：压缩版本，用于设置识别HTTP协议版本，默认是1.1，目前大部分浏览器已经支持GZIP解压，使用默认即可gzip_comp_level 6：压缩比例，用来指定GZIP压缩比，1压缩比最小，处理速度最快，9压缩比最大，传输速度快，但是处理慢，也比较消耗CPU资源。gzip_types text&#x2F;css text&#x2F;xml application&#x2F;javascript：用来指定压缩的类型，‘text&#x2F;html’类型总是会被压缩。默认值: gzip_types text&#x2F;html (默认不对js&#x2F;css文件进行压缩)gzip_vary on：varyheader支持，改选项可以让前端的缓存服务器缓存经过GZIP压缩的页面，例如用Squid缓存经过nginx压缩的数据

expires缓存调优缓存，主要针对于图片，css，js等元素更改机会比较少的情况下使用，特别是图片，占用带宽大，我们完全可以设置图片在浏览器本地缓存365d，css，js，html可以缓存个10来天，这样用户第一次打开加载慢一点，第二次，就非常快了！缓存的时候，我们需要将需要缓存的拓展名列出来， Expires缓存配置在server字段里面，如：

location ~* \.(ico|jpe?g|gif|png|bmp|swf|flv)$ &#123;  expires 30d;  #log_not_found off;  access_log off;&#125;location ~* \.(js|css)$ &#123;  expires 7d;  log_not_found off;  access_log off;&#125; 
注：log_not_found off;是否在error_log中记录不存在的错误。默认是。

 总结：expire功能优点  expires可以降低网站购买的带宽，节约成本  同时提升用户访问体验 减轻服务的压力，节约服务器成本，是web服务非常重要的功能。 expire功能&gt; expire功能缺点被缓存的页面或数据更新了，用户看到的可能还是旧的内容，反而影响用户体验。解决办法缩短缓存时间，例如：1天，但不彻底，除非更新频率大于1天对缓存的对象改名。网站不希望被缓存的内容网站流量统计工具更新频繁的文件（google的logo）


防盗链防止别人直接从你网站引用图片等链接，消耗了你的资源和网络流量，那么我们的解决办法由几种：1：水印，品牌宣传，你的带宽，服务器足够2：防火墙，直接控制，前提是你知道IP来源3：防盗链策略下面的方法是直接给予404的错误提示如：

location ~*^.+\.(jpg|gif|png|swf|flv|wma|wmv|asf|mp3|mmf|zip|rar)$ &#123;    valid_referers noneblocked  www.benet.com benet.com;    if($invalid_referer) &#123;      #return 302  http://www.benet.com/img/nolink.jpg;      return 404;        break;    &#125;    access_log off; &#125;
参数可以使如下形式：none 意思是不存在的Referer头(表示空的，也就是直接访问，比如直接在浏览器打开一个图片)blocked 意为根据防火墙伪装Referer头，如：“Referer:XXXXXXX”。server_names 为一个或多个服务器的列表，0.5.33版本以后可以在名称中使用“*”通配符。
参考资料配置nginxNginx keepalive_requests 踩坑总结
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift上使用Nginx-Pod作灰度发布</title>
    <url>/posts/12af42c297f6.html</url>
    <content><![CDATA[参考文章：使用 Nginx 实现灰度发布
什么是灰度发布
**【百度百科】**灰度发布（又名金丝雀发布）是指在黑与白之间，能够平滑过渡的一种发布方式。 在其上可以进行A&#x2F;B testing，即让一部分用户继续用产品特性A，一部分用户开始用产品特性B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。
除了AB test灰度发布另一种思想是，只发布给一小部分用户，如：App在发布之前，针对性的给一小批用户测试下新版本，用小流量发布的方式来检验新版会不会有问题。
灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。

Openshift上实现蓝绿部署  
在openshift上的部署两个版本的Service：service01, service02。在创建Route的时候，可以非常简单地指定同一个Route流量访问两个service的百分比service01（25%流量）, service02（75%流量）。所有请求该Route的流量会被按指定的比例访问后service。  
  
对应的Route yaml配置文件如下：
apiVersion: route.openshift.io/v1kind: Routemetadata:  labels:    app: grey-test  name: grey-test  namespace: greyspec:  alternateBackends:    - kind: Service      name: service_v2      weight: 75  host: grey-test.sample.com  port:    targetPort: web  to:    kind: Service    name: service_v1    weight: 25  wildcardPolicy: None

金丝雀发布的不足


openshift中Route自带的金丝雀发布配置非常简单，也很实用。但是它没办法对请求的流量进行定义控制。
例如根据IP，指定IP为’202.38.12.10’的用户访问service_v2,那么Openshift中Route自带的分流就无法实现。
所以必须对特定用户访问特定版本的问题，我们需要寻求其它办法。使用传统的方法，Nginx是一个比较常见的选择。在openshift的灰度方案中，我们也尝试使用Nginx来实现对特定用户的灰度发布。

Nginx配置实现灰度发布
创建两个upstream
针对remote_addr进行分流，默认访问default的upstream，如果ip为’202.38.12.10’，则访问api_v2的upstream。

无需多解释，直接上配置。
upstream default&#123;    server 172.30.198.194:8080 max_fails=1 fail_timeout=60;&#125;upstream api_v2&#123;    server 172.30.198.194:8080 max_fails=1 fail_timeout=60;&#125;server &#123;  listen 80;  server_name  _;  set $group default;  if ($remote_addr ~ &quot;202.38.12.10&quot;) &#123;      set $group api_v2;  &#125;location / &#123;                           proxy_pass http://$group;    proxy_set_header   Host             $host;    proxy_set_header   X-Real-IP        $remote_addr;    proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;    index  index.html index.htm;  &#125;&#125;
对配置作进一步优化
怎么获取客户端IP？


如果直接访问Nginx服务的话，在Nginx上通过 $remote_addr 能够获得客户端IP，但是我们知道在Openshift高可用部署中，我们使用了负载均衡器。通过负载均衡器转发后，Nginx webserver获得的$remote_addr其实是负载均衡器的IP，而不是客户端真实IP。

为了能在Nginx webserver中获得客户端IP，需要在附加HTTP头字段开启客户端真实IP。同时在Nginx的配置中使用$http_x_forwarded_for来检查客户端IP.  

新的配置如下


upstream default&#123;    server 172.30.198.194:8080 max_fails=1 fail_timeout=60;&#125;upstream api_v2&#123;    server 172.30.198.194:8080 max_fails=1 fail_timeout=60;&#125;server &#123;  listen 80;  server_name  _;  set $group default;  if ($http_x_forwarded_for ~ &quot;202.38.12.10&quot;) &#123;      set $group api_v2;  &#125;location / &#123;                           proxy_pass http://$group;    proxy_set_header   Host             $host;    proxy_set_header   X-Real-IP        $remote_addr;    proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;    index  index.html index.htm;  &#125;&#125;

如何实现高可用？

这个非常简单，只需要在Openshift上将为Nginx的DeploymentConfig增加Pod数增就可以了。

我要有多个IP加入白名单，有没有更方便的配置方法？

Nginx有map函数，能将IP与访问的后台服务。
map $http_x_forwarded_for $group &#123;        default 172.30.198.194:8080;        ~202.38.12.10 172.30.198.194:8080;    &#125;server&#123;    listen  8080;    server_name _;     location /&#123;        proxy_pass http://$group;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded_For $proxy_add_x_forwarded_for;        index index.html index.htm;    &#125;&#125; 


明明有域名为啥只能是IP？


Openshift对于Service除了有固定的IP外，还有固定的域名，一般域名形式为：servername-projectname.svc。我们能否用域名代替IP,让这个配置的可读性更高，什么IP访问什么服务一目了然。
但是直接将IP更改为对应Service的域名，在启动Nginx时会报无法解析域名的错误。
Nginx需要在map配置中解析域名需要指定dns。
最终配置如下（172.30.0.1为Openshif集群默认的内部DNS，可在master的配置文件中查看）：

resolver 172.30.0.1;map $http_x_forwarded_for $group &#123;        default api_v1.server.svc:8080;        ~202.38.12.10 api_v2.server.svc:8080;    &#125;server&#123;    listen  8080;    server_name _;     location /&#123;        proxy_pass http://$group;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded_For $proxy_add_x_forwarded_for;        index index.html index.htm;    &#125;&#125; 


Openshift中对于443端口的负载均衡器使用的是TCP协议，无法转发X_forward_for，那https服务要怎么弄？

如何应用请求为Https，无法直接使用TCP负载均衡器，而Openshift部署时是通过TCP负载均衡器来作443端口的负载的。

这就需要创建一个Https负载均衡器， 后台Openshift Nginx的Route设置为正常的Http。其它配置一样。既能将clientIP传给后台webserver，同时又满足了该业务请求为Http协议。
最终的流程图如下：

拓展思考
是否一定要在Openshift上部署呢?【必然不需要】
Nginx实现灰度发布除了支持ip还支持啥？【对Header头信息等】
除了对页面HTTP请求实现这种灰度，那么对于TCP服务呢，比如说数据库？【也是OK的】

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>SOPS对配置文件数据进行加密</title>
    <url>/posts/cf903a398614.html</url>
    <content><![CDATA[SOPS 是由Mozilla 开发的一款开源的文本编辑工具，它支持对YAML, JSON, ENV, INI 和BINARY 文本格式的文件进行编辑，并利用AWS KMS, GCP KMS, Azure Key Vault 或PGP 等加密方式对编辑的文件进行加密和解密。在当前它还可以与heml secret作用，为Kubernetes的secret资源数据进行加密。本篇主要介绍sops的基本用法，而helm secret的使用不做介绍。
##GPG进行加密

生成GPG文件

$ gpg --generate-key # 根据引导生成gpg加密文件
或者自动生成
gpg --batch --generate-key &lt;&lt;EOF%echo Generating a basic OpenPGP key for HELM SecretKey-Type: RSAKey-Length: 4096Subkey-Type: RSASubkey-Length: 4096Name-Real: MichaelName-Comment: Personal PGPName-Email: mpan@sample.comExpire-Date: 0%no-ask-passphrase%no-protection%commit%echo doneEOF

查看GPG列表

$ gpg -k/home/vagrant/.gnupg/pubring.kbx--------------------------------pub   rsa3072 2021-04-07 [SC] [expires: 2023-04-07]      FA21E3EDC58BA05535435756543DF0088828FFCCuid           [ultimate] Michael &lt;micael@example.com&gt;sub   rsa3072 2021-04-07 [E] [expires: 2023-04-07]

使用SOPS对文件a.yaml进行加密

$ echo &quot;A: hello&quot; &gt; a.yaml$ sops --gpg FA21E3EDC58BA05535435756543DF0088828FFCC -e file.yaml &gt; file_encode.yaml

解密文件

$ sops -d file_encode.yaml $ #或者 使用sops file_encode.yaml对加密文件直接编辑

通过.sops.yaml来设置默认加密规则

$ cat &lt;&lt;EOF &gt; .sops.yamlcreation_rules:        - pgp: &#x27;FA21E3EDC58BA05535435756543DF0088828FFCC&#x27;EOF
**公钥用于加密，私钥用于解密。**它们的导入与导出对于文件的传输与交流很重要。5. 公钥导入与导出
 $ gpg --export --armor &quot;mpan@sample.com&quot; &gt; personal_gpg.pub$ gpg --import personal_gpg.pub

私钥的导入与导出

gpg --export-secret-key -a &quot;mpan@sample.com&quot; &gt; personal_gpggpg --import  personal_gpg

删除公钥与私钥

$ gpg --delete-keys FA21E3EDC58BA05535435756543DF0088828FFCC$ gpg --delete-secret-keys FA21E3EDC58BA05535435756543DF0088828FFCC

保存gpg密钥的网站（可选）KeyBase：https://keybase.io/KeysOpenPGPhttps://keys.openpgp.org/

KMS加密
在aws平台的KMS服务创建一个密钥，获取它的arn ID
文件加密时使用–kms设置加密密钥。前提是当前aws客户端已经设置了认证私钥，并赋予了访问该kms密钥的权限

$ sops --kms &quot;arn:aws:kms:ap-east-1:982938942:key/f2232fa3-7678-8922-8778-f2232fa3&quot; -e file.yaml &gt; file_encode.yaml

解密文件方式一致

$ sops -d file_encode.yaml
使用Age加密Age 是一个简单，现代和安全的文件加密工具。

安装

$ brew tap filippo.io/age https://filippo.io/age$ brew install age

生成密钥对

$ age-keygen -o key.txtPublic key: age1st0m5a72gtlk3pz3fnvy08huq8d9llut2vnjwxzkeudejkc0fs9s6dj9ev

使用公钥对数据进行加密

$ sops -a age1st0m5a72gtlk3pz3fnvy08huq8d9llut2vnjwxzkeudejkc0fs9s6dj9ev -e secrets.yaml &gt; secrets_enc.yaml
错误处理
在解密时报如下错误

vagrant@ubuntu-focal:~$ sops a2.yamlFailed to get the data key required to decrypt the SOPS file.Group 0: FAILED  FA21E3EDC58BA05535435756543DF0088828FFCC: FAILED    - | could not decrypt data key with PGP key:      | golang.org/x/crypto/openpgp error: Could not load secring:      | open /home/vagrant/.gnupg/secring.gpg: no such file or      | directory; GPG binary error: exit status 2
需要设置GPG_TTY环境变量
export GPG_TTY=$(tty)

参考文章Helm 插件之 helm-secrets：利用 PGP 加密你的 Values 文件Cannot decrypt with GPG 2.2.5 and SOPS 3.0.0 #304
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Scrum敏捷开发</title>
    <url>/posts/522406dc14b5.html</url>
    <content><![CDATA[团队成员
一般情况人数在5~9个左右
团队要跨职能(包含开发人员、测试人员、用户介面设计师等)
团队成员构成在Sprint内不允许变化。

用户故事建模为开发任务需求编写用户故事



重要性
用户故事
测试方法
预估时间



30
做为一个会员，可以查看自己的购物清单，以便一起购买
登录，点击购买清单Tab
16h


Sprint计划会议（两周一次，4个小时）
设定本Sprint目标及优先级
挑选本Sprint所要开发的需求（Story)
逐一将Story细分成Task
确定每日站会时间和地点
确定演示会议和回顾会议日期计划会议输出：




Story
Task
Hour



A
1. 编写自动化验收测试2. 设计用户界面3. 编写用户界面程序代码 4. 编写数据库代码5. 编写单元测试 6. 人工测试
3h5h8h8h8h8h


B
1. 编写自动化验收测试2. 设计用户界面3. 编写用户界面程序代码 4. 编写数据库代码5. 编写单元测试 6. 人工测试
3h5h8h8h8h8h


第日立会为早上9：00至9：15




Demo演示时间为：2018-07-10 上午10：00-12：00




回顾会议：2018-07-10 下午3：30-4：00




每日立会（15分钟）
昨天做了什么
今天要做什么
遇到的问题
更新燃尽图

Sprint评审会（2个小时）
团队展示Sprint中完成的功能
简单议程
全员参与
关闭Sprint

回顾会（2个小时内）
讨论做得好的，有待改善的
就1-3个关键问题提出解决方案
对改进点分配专门人员进行跟踪，确保改进方案得到实施
全员参加回顾会输出：
本次Sprint团队状况（总工时等）
记录做得好的与有待完善的点
对于有待完善的点的改进方案
对1-3个改进方案指定跟进人员

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>SonarQube应用离线安装插件</title>
    <url>/posts/7647bfc416ef.html</url>
    <content><![CDATA[SonarQube的插件地址为：https://binaries.sonarsource.com/Distribution/将下载的插件文件存放在SonarQube应用的/opt/sonarqube/extensions/plugins目录下。
参考文章Sonar 离线安装插件
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Vagrant常用操作</title>
    <url>/posts/6c2c2ce8b77c.html</url>
    <content><![CDATA[查看安装的插件列表$ vagrant plugin list
安装vagrant插件$ vagrant plugin install vagrant-disksize vagrant-ignition vagrant-vbguest
查看box列表$ vagrant box list
添加一个新的box$ vagrant box add centos-76 centos-76.box
将一个虚拟机导出为box$ vagrant package --output  centos-7.6.box
删除一个box$ vagrant box remove centos-76
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>WSL2-+-microK8s-+-K8s</title>
    <url>/posts/27114569557b.html</url>
    <content><![CDATA[
Windowns 10 (WSL2) + Ubuntu 20.04 

enable systemd snapd microk8s

启动systemd，并使用snap安装


$  sudo apt install daemonize$ sudo daemonize /usr/bin/unshare --fork --pid --mount-proc /lib/systemd/systemd --system-unit=basic.target$ sudo nsenter -t $(pidof systemd) -a su - $LOGNAME

安装microk8s

$ snap info microk8s$ sudo snap install microk8s --classic


使用microk8s

$ microk8s status$ sudo usermod -a -G microk8s mpan$ sudo chown -f -R mpan ~/.kube


添加节点

$ microk8s add-node 

注意 每次使用microk8s时，需要先进入它的namespace
sudo nsenter -t $(pidof systemd) -a su - $LOGNAME

K8Shttps://github.com/AliyunContainerService/k8s-for-docker-desktop
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>gogs创建用户</title>
    <url>/posts/99938be39f4b.html</url>
    <content><![CDATA[&#x2F;opt&#x2F;gogs&#x2F;gogs admin create-user –name&#x3D;root –password&#x3D;123456 –admin&#x3D;true --email&#x3D;abc@123.com –config&#x3D;&#x2F;etc&#x2F;gogs&#x2F;conf&#x2F;app.ini
oc project cicd &amp;&amp;gogspodname=$(oc get pod | grep gogs | grep -v postgresql| awk &#x27;&#123;print $1&#125;&#x27;)oc rsh $gogspodname /opt/gogs/gogs admin create-user --name=root --password=123456 --admin=true --email=abc@123.com --config=/etc/gogs/conf/app.ini
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>《凤凰项目—一个IT运维的传奇故事》整理</title>
    <url>/posts/fda9ff2fc746.html</url>
    <content><![CDATA[###《凤凰项目》三位作者

Gene Kim: Tripwire有限公司创始人，一直热衷于研究如何提高IT组织的效率
Kevin Behr：创建了信息技术流程研究院
George Spafford：行业分析师

向他们致敬。
故事内容雨前龙井整理的非常详细，可阅读它写的博客 凤凰项目 http://ijyun.github.io/2016/04/23/phoenix-project.html
书中的核心概念
三步工作法

本书中阐述了一个原理：所有开发运维模式都来自“三步工作法”，可以说它是我们平台开发运维的指导思想。

第一工作法是关于从开发到技术运营，再到客户的整个自左向右的工作流。为了使流量最大化，我们需要小的批量规模和工作间隔，绝不让缺陷流向下游工作中心，并且不断为了整体目标（相对于开发功能完成率、测试发现&#x2F;修复比例或运维有效性等局部目标）进行优化。流程自动化

实践：持续构建、持续集成、持续部署，按需创建环境、限制半成品，构建起能够顺利变更的安全系统和组织。

第二工作法是关于价值流各阶段自右向左的快速持续反馈流，放大其效益以确保防止问题再次发生，或者更快地发现和修复问题。这样，我们就能在所需之处获取或嵌入知识，从源头上保证质量。保证上游的质量

实践：在部署管道中的构建和测试失败时“停止生产线”、日复一日持续的改进日常工作、创建快速的自动化测试套装软件，以确保代码总是处于可部署的状态、在开发和技术运营之间建立共同的目标和共同的解决问题的机制、建立普遍的产品遥测技术，让每个人都能知道，产品和环境是否在按设定的运行，以及是否达到了客户的目标。

第三工作法是关于创造公司文化，该文化可带动两中风气的形成：不断尝试，这需要承担风险并从成功和失败中吸取经验教训、理解重复和联系是熟练掌握的前提、尝试和承担风险让我们能够不懈地改进工作系统，这经常要求我们去做一些和以往做法大不相同的事。一旦出现问题，不断重复的日常操作赋予我们的技能和经验，令我们可以撤回至安全区域并恢复正常运作。不断试错，持续改进

实践：营造一种勇于创新、敢于冒险（相对于畏惧和盲目服从命令）以及高度信任（相对于低信任度和命令控制）的文化；把至少20%的开发和技术运营周期划拨给非功能性需求，并且不断鼓励进行改进。

四种工作类型


业务项目多数开发项目所包含的业务举措，即业务部门的所有正式项目。
IT内部项目可能由业务项目衍生出的基础架构或IT运维项目，以及内部生成的改进项目（如创建新环境和部署自动化）。
变更由上述两种类型的工作引起，往往在报修系统中被跟踪。在价值流的两个部分，开发和运维中应该统一管理变更。
计划外工作或救火工作包括操作事故和操作问题，通常由以上三种类型工作导致，往往会牺牲其它计划内的工作为代码，成本往往很高。通过采用三步工作法，减少计划外工作，即使发生计划外工作，也能快速解决。

思考总结目标:拥有一条不断改进，能够自我反馈的自动化流水线。开发运维并不仅仅是简单的自动化工具的集成，虽然自动化是开发运维的很大一部分内容。更重要的是价值流导向，自始至终拥有共同的目标并共同解决问题，需要把视野放得更大一些，而不是局限在运维的主机或服务上。开发运维要提升自己的价值就需要将自己的工作与最终的业务关系挂钩，了解自己运维的系统是如何影响着业务，所以开发运维需要有同公司一致的目标。如书中比尔与约翰在了解到公司业务目标后，思维一下子打开了，不再局限在眼前的系统与应用，而是有了更大的视野。他们一起约谈各业务部门负责人，清楚了解到各系统在业务上的位置，从而能够更好地去分配工作的优先级。

引用

《凤凰项目—一个IT运维的传奇故事》凤凰项目 http://ijyun.github.io/2016/04/23/phoenix-project.htmlDevOps 漫谈:从作坊到工厂的寓言故事 https://riboseyim.github.io/2018/04/10/DevOps-Phoenix/
  
]]></content>
      <categories>
        <category>读书笔记</category>
        <category>devops</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>《启示录：打造用户喜爱的产品》读书笔记</title>
    <url>/posts/81fb69c8f46f.html</url>
    <content><![CDATA[  
好产品具备三个基本条件：价值、可用性、可行性，三者缺一不可。产品没有价值，开发团队再优秀也无济于事。
第一篇：人员现代软件产品团队成员

产品经理职责：评估产品机会；定义要开发的产品。
用户体验设计师交互设计师：负责深入理解目标用户，设计有价值的，可用的功能，以及用户导航和产品使用流程。视觉设计师：根据交互设计原型，制作美观的产品界面
项目管理人员产品经理完成产品定义后，开发团队开始开发产品。项目经理核心任务：制订计划和跟踪进度。
开发团队职责：负责产品的技术开发。
运维团队职责：互联网服务产品通常运行在服务器上，保证服务正常运行。
产品营销人员职责：负责对外发布信息、宣传产品，为扩展市场销售渠道、组织重点营销活动、促进产品销售提供支持

第二篇：流程11. 评估产品机会——确定待解决的问题只讨论待解决的问题，不应涉及具体解决方案。产品经理需要回答如下十个问题：

产品要解决什么问题？（产品价值）
为谁解决这个问题？（目标市场）
成功的机会有多大？（市场规模）
怎样判断产品成功与否？（度量指标或收益指标）
有哪些同类产品？（竞争格局）
为什么我们最适合做这个产品？（竞争优势）
时机合适吗？（市场时机）
如何把产品推向市场？（营销组合策略）
成功的必要条件是什么？（解决方案要满足的条件）
根据以上问题，给出评估结论。（继续或放弃）

12. 产品探索——定义正确的产品软件项目可以划分为两个阶段：定义正确的产品、正确地开发产品。采用流水线方式并行开发产品，在开发过程中，避免大幅更改产品需求，严重影响开发团队的开发。一旦1.0版本的产品进入项目执行阶段，就开始定义2.0版本的产品。产品探索的过程：

评估产品机会
与交互设计师一起制作产品原型
利用产品原型展开用户测试
完成产品用例，与开发团队一起评审产品原型和说明文档

定义产品的过程是产品探索，它的工作是不可预测的。产品经理应该探索是否有用户需要产品，其次探索能够解决问题的产品方案必须是有价值的、可用的、可行的。利用产品原型做产品的探索，而不要孤注一掷，用实际产品探索。在产品方案达到确认是有价值的、可用的、可行的后再全面转入执行阶段。
13. 产品原则——确定什么最重要
产品原则体现了产品团队的目标与愿景，是产品战略的重要组成部分。

形式上看，产品原则是一系列明确的、体现团队特色的产品价值准则。不仅罗列出产品原则，还需要根据重要性对其进行排序。如果你所在团队还没有制定清晰的、有关产品理念的产品原则，应该花时间确定团队最看重的价值理念。在作产品决策之前，应该先确定决策要解决的问题，让大家在以下几个要点上达成共识：

究竟要解决什么问题？
为谁解决这个问题？
产品要达到什么目标？
每项目标的优先级是什么？

14. 产品评审团——制定更及时、更可靠的产品决策
产品评审团的目标：决定产品战略方向，宏观上监督产品的研发流程，合理配置资源。产品战略需要在已定的商业战略的条件下。
产品评审团的成员：CEO&#x2F;COO&#x2F;部门总监理&#x2F;产品总监&#x2F;用户体验设计总监&#x2F;市场总监&#x2F;开发总监&#x2F;网站运营总监&#x2F;客服总监
产品评审团的职责：
启动产品评估工作–&gt;产品经理开始评估产品机会
根据产品评估机会的结果，宣称要解决的问题是否有价值，决定是否开始定义产品的解决方案–&gt;开始定义产品解决方案，制作原型、用户测试、成本估算
根据产品原型、用户测试结果、成本估算明细，决定是否开发产品–&gt;开始开发产品
评审最终产品、产品质量、发布计划、社会效益，决定是否发布产品



成本评估分两个阶段进行：评估产品机会时做粗略估算，根据最终产品说明文档做详细估算。
15. 特约用户——产品开发伙伴拥有一群忠实的、乐于推荐产品的用户会对产品的发布起到很大的促进作用。平台产品，最好提供一批起示范作用的应用程序。

产品经理要深入了解目标用户，明确产品需要解决的问题，定义出满足用户需求的产品。


特约用户数量：8-10人，互联网服务可以是10-15人
特约用户注意事项：
不要向特约用户收取参与费用。
人数不要超过10人，为满足大批心急用户，可发布预览版
如果寻找特约用户遇到困难，很可能产品机会价值不足，可以重新考虑产品计划
确保特约用户是产品的潜在目标用户
产品经理向特约用户承诺产品会持续开发
产品经理把特约用户当成开发伙伴，互相帮助
产品经理与特约用户的合作贯穿产品研发的每个环节：向他们展示产品原型，请他们参加测试，请教产品细节等
正式发布前，一定邀请特约用户试用，确保人人满意
产品经理还要和产品营销团队合作，帮忙物色特约用户，同时帮助提高特约用户的受关注度
平台产品的话，特约用户要换成特约应用，与特约应用的开发者紧密合作。



特约用户是确保产品不偏离用户需求最简单有效的办法。
16. 市场调研——理解市场调研的作用与局限性
市场调研的方法
用户调查
产品使用分析
数据挖掘
拜访用户
人物角色
可用性测试
同类产品分析


好的市场调研可以得到以下答案
谁是目标用户？
用户会怎样使用产品？
用户能想明白怎样使用产品吗？障碍在哪里？
用户为什么选用你的产品？
用户喜欢产品的那些特点？
用户希望如何改进产品，增加哪些功能？



市场调研只能作为研发产品的依据和参考，不能决定产品研发的方向。可以用于完善现有产品，但不能用来定义新产品。成功的产品基于：深入理解用户需求，以及明白怎样的解决方案在现阶段是可行的。
17. 产品人物角色——理解目标用户
人物角色：又称为用户特征记录，指通过与用户沟通交流，确定典型的目标用户类型，在理解各类目标用户特征的基础上建立的人物原型。
产品经理必须深入参与创建人物角色的工作，尤其是参与用户交流和用户调查。
人物角色的用途
用来筛选重要的产品功能。
可以帮助避免产品团队把自己的需求当成用户的需求
有助于对用户类型的优先级进行排序，识别重点需求
方便向团队描述产品的目标用户就谁，他们怎么使用产品，他们的关注点在哪里
帮助团队成员达成共识



18.重新定义产品说明文档——安息吧，纸质说明文档
主体：高保真产品原型
补充：用例描述重要的产品行为

19. 用户体验设计与实现——先定义用户体验在动手开发
需求调研和产品设计（用户体验设计）应该同步开展。
用户体验设计与软件开发不能放在一起进行。
正确的做法是用户体验设计师做好设计后再开始开发，除非开发有大量后台基础软件需要开发。

20. 基本产品——削减功能还是延长工期
推荐的产品设计方式


产品经理和设计师合作设计产品的高保真原型，它只具备商业目标的最基本功能要求，及良好的用户体验和吸引力。
邀请一位开发人员参与原型设计。帮助估算成本，指出设计上的误区，分析、评估可行性。
请真实用户验证产品原型。在产品全力开发前，产品经理和设计师必须确信产品是用户需要的，这就需要用户测试来验证。

设计产品时一定要分清最重要功能，争取设计出只满足基本要求的，不可删减的产品。
21. 产品验证——证明产品的价值、可用性、可行性
可行性测试邀请架构师和开发人员深度参与技术调研，寻找可行的方案。有些产品的技术风险较大，如果产品存在可行性风险，一定要提前解决这些问题。
可用性测试请真实用户来试用可用性原型。为了测试可用性，即使要模拟复杂的后台处理过程也是值得的，关键是要评估用户体验的实际效果。
价值测试可用性测试与价值测试同时进行。可用性测试重在观察用户如何设法完成必要的操作，而价值测试重在观察用户是否喜欢这些功能，是否满意具体的实现方式。

22. 原型测试——把产品创意呈现给真实用户让真实用户验证产品创意是必不可少的环节。

物色测试者
特约用户
同类产品展销会寻找目标用户
分类信息网站上发布广告征集测试者
邀请亲朋好友参加测试
从邮箱列表中筛选测试者
公司网站征集志愿者
大型公司定期开展测试活动（两周一次，每次10~20位测试者参加）
到产品对应用户群聚集地寻找测试者
测试前一天致电测试者


准备测试
事先拟好测试内容，测试主要功能。
在开始测试前，观察用户在不知道产品的情况下，如何解决问题
观察测试者是否从原型首页看出产品要解决的问题，哪些地方最能吸引他们
待测试完成，了解了产品功能的，通过聊天进一步收集信息。比如是否使用过同类产品，原型是否比他常用的产品好？是否会推荐给朋友使用。了解测试者对产品的评价
为每个问题的答案打分
可以先测试主要项目，向客户了解接下来希望有什么功能，获取宝贵意见


测试环境
测试者放松的环境，回答问题更坦诚和开放
用户的办公室
尽量面对面测试
产品经理应亲自参加每次原型测试，了解客户需求
产品经理要明白自己的产品设计不完美，获取用户反馈信息是完善产品设计的最佳途径
安排一个人主持测试，另一个人记录
可以邀请开发人员、交互设计师、视觉设计师等参加


测试原型
测试前不宜与测试者交谈过多
务必告诉测试者：这只是产品原型，不是最终产品，可以尽情说出问题
尽量让测试者保持平和情绪。测试重点是看测试者能否轻松完成测试任务，以及是否喜欢产品功能，多观察测试者的操作，少在意测试者的抱怨，如果测试者提出界面上的元素美观问题就跑题了
尽量保持安静，不要给测试者提示
通常有三种结果：顺利完成任务、艰难完成任务、放弃任务
尽量不要提示测试者，可以问测试者的困惑
主持人可以通过口述他们在做的事，尽量不要用感性语句，比如“好极了”，以免诱导测试者
测试的作用是理解目标用户如何看待产品要解决的问题，发现用户的期待，为后面的优化提供参考
观察测试者的表情与动作


更新原型
只要两三个测试者反馈同一个问题，就可以动手解决了。如果有连续六位测试者理解和欣赏产品的价值，而且顺利完成关键任务，就算测试原型任务完成
如果发现没法让测试者对原型产生兴趣，或者无法让原型变得足够简单易用，就应该立马收手，放弃这个产品创意。



23. 改进再有产品——不是一味地添加功能开发新产品的第一步是要明确目标。改进产品不是简单地满足个别用户的要求，能提高指标的功能才是关注的重点
24. 平滑部署——避免更新产品导致用户反感通常情况下，用户不喜欢变化。为了将版本更新带来的负面影响降到最低，可以采取以下措施：

通过公告、群发邮件、在线教育等方式提前通知用户
加倍做好测试工作，避免新版本存在影响正常使用的隐患
采用并行部署或者增量部署的方式来降低风险

平滑部署的方式很多：

部署两个版本，邀请部分用户试用新版本。待大部分用户习惯新版本后，再将新版本设为默认版本，同时保留旧版本一段时间，确定旧版本提供支持的最后期限
区域性逐步部署
将新功能分割成几个较小的部分逐步发布

25. 快速响应阶段——产品出炉后切莫虎头蛇尾
交付产品后，要留一周左右时间，快速响应处理产品发布后的用户反馈。
关键不在于是否会出现问题，在于能多快解决问题
给指标分出轻重缓急，并保持关注。页面访问量、注册用户数、访问停留时间、会员转换率、广告收益？
追踪用户的使用情况，使用追踪工具，如谷歌分析工具
上门向客户收集反馈
一旦问题反馈回来，产品团队应该至少每天召开一次简短会议，讨论问题的轻重缓急，确定最佳解决方案

26. 合理运用敏捷方法——十大秘诀
产品经理即是产品负责人，为产品负责
产品经理要明白产品的方向和目标，设定衡量产品成功与否的标准
产品经理与设计师的进度应该比开发团队领先一两个迭代周期，另外，让开发人员参与评估产品设计和产品原型及时反馈可行性、成本、解决方案的建议
产品设计目标是设计出符合基本要求的产品
产品经理的主要任务是定义有价值、可用的产品原型和用户故事，作为开发基础。
让开发人员自主划分迭代周期
产品经理、交互设计师必须出席每天的晨会
一定要等产品满足产品经理的要求，才能够进行发布
每次迭代完成后，产品经理应该向团队展示产品现状，及下次迭代的产品原型
团队展开敏捷培训

27. 合理运用瀑布式开发方法——扬长避短瀑布式开发的优点：一开始确定了产品的需求，变动很少，可以制定精确的开发计划瀑布式开发的缺点：

产品验证严重滞后
变更计划代价高
无法适应快速的市场变化

28. 创业型公司的产品管理——关键在于产品探索
推荐一种新的产品设计方式，提高产品的成功率，还能大幅节约创业成本。


创业初期只设三个职位：产品经理、交互设计师和原型开发人员。只要有人负责这三项工作即可。这个团队可以快速展开产品设计，迭代修改
创建体现用户体验的高保真原型，邀请真实的目标用户验证产品原型
产品原型不断迭代，产品会渐趋完善。这个过程需要几周到两个月时间
确定产品原型后，再招聘程序员进行开发

29. 大公司如何创新——有困难，但值得一试
20%法则
臭鼬工程，受限制的条件下，利用自己的时间，低调地进行创新研究
观察用户使用公司产品或同类产品的一举一动
改善用户体验
收购小公司

30. 在大公司施展拳脚——十大秘诀
了解公司制定决策的方式
建立人脉网络
臭鼬工程，三五个志趣相投的同事在工作之余做出产品原型
自己顶上，一切为了推出产品，不计较个人得失
有选择地据理力争
会前沟通，形成默契
合理分配时间
分享信息
向上司借力
传播你的产品理念

大公司的优点：产品会获得媒体和用户的高度关注。“大部分人游荡在黑暗里，他们只知道抱怨，却从不想办法寻找电灯开关”
第三篇：产品富有创意的产品应该具有一些鲜明特性
31. 苹果公司苹果公司很多值得觉得的经验，作者认为最重要的是以下四点：

硬件为软件服务：硬件是配合软件满足用户需求
软件为用户体验服务：把用户体验放在心里，所有工作围绕着产品的可用性、交互设计、视觉设计、工业设计展开
用户体验为情感服务：抓位用户的情感需求
产品为真正的需求服务：逐一改善用户痛点

32. 提防有特殊要求的产品客户说：产品需要增加以下七项功能，否则拒绝购买。say Yes or No?

特殊要求混淆了客户需求和产品需求，必然会使公司偏离正轨。
产品需求不能用户说了算。
在看到具体产品之前，用户很难知道自己需要什么
用户不知道什么样的产品是可行的
用户之间缺少沟通，需求很难统一


产品经理的任务是满足大众需求——这是产品公司和定制软件公司的区别

公司应该怎样回避特殊要求的产品带来的危害？

与客户沟通，一起梳理需求，找到问题的本质，提供更合理的解决方案
看能否在保持产品能用用途的前提下，设法满足客户定制要求

新瓶装老酒成功的产品往往不是什么新鲜事物，只是新瓶装老酒，“新瓶”做得更好、更方便、更便宜，改变了消费者对“老酒”的印象

对目标市场了如指掌，对现有产品的缺陷洞若观火。产品的可用性测试掌握产品情况（包括自己的和竞争对手的产品）
跟踪最新的技术趋势。用新技术解决用户的老问题。

34. 恐惧、贪婪、欲望企业级消费者出于恐惧和贪婪购买产品大众消费者购买产品原因多样化，有可能是交友的欲望、满足自豪感等只有从情感的角度重新观察市场上的产品和服务，才能体会用户的真实感受。
35. 情感接纳曲线愤怒的用户决定着产品未来的发展方向。不要一味从技术角度看待产品，从用户的角度考虑问题。

技术爱好者，他们的需求与普通大众不同，购买产品是对技术本身的痴迷
非理性消费者，情感需求与大众相同，但更强烈。产品经理应该着重关注他们
普通大众，和非理性消费者情感需求一样，但不那么强烈，随着产品完善，会加入消耗队伍
理性消费者，只购买他们认为实用、成熟的产品，只购买性价比合适的产品
超理性消费者，情感需求很弱，只要产品有半点不满意，就不会购买
观望者，他们有同样的需求，只购买公认好用的产品

非理性消费者的情感需求是推动产品跨越鸿沟的动力。

带着新生的感觉去发掘每天折磨着大众的情感——孤独、恐惧、挫折、不满，你离发现新产品的日子就不远了。

36. 可用性与美感交互设计与视觉设计缺一不可。
37. 大众网络服务产品作者总结了十条管理大众网络服务产品的要点：

可用性：产品必须具备良好的用户体验
人物角色：抽象出有代表性的用户类型，加以分析
扩展性：系统支持动态扩展，应对激增的用户访问
持续可用性：保证网络服务高可用性
客户服务：维持良好的用户体验
保护用户隐私：树立保护用户隐私的意识，设置用户资料保护机制
口碑营销：为用户提供便利，方便他们向熟人推荐产品
全球化：设计时支持本地化
平滑部署：尽量减少不必要的更新
用户社区管理：多与用户交流，了解用户的想法

38. 打造企业级产品的经验企业级产品的十大要点

可用性：也要做交互设计、视觉设计、可用性测试
产品正常工作：保证产品按设计的方式运行
特例产品：必须坚持原则，开发满足广大用户需求的产品始终是首要任务
特约用户：配合产品团队验证产品设计，体验产品。必须保证特约用户满意，才能发布产品
销售渠道的需求：按照销售与分销渠道的需求来设计产品
客户和用户的需求：参考用户的需求设计产品
产品安装：安装过程尽量简单
产品的配置、自定义、集成
产品升级：简化升级技术和流程
销售策略：传统靠的是销售人员的才智、魅力与关系，现在可以考虑网络推广策略

39. 打造平台产品的经验面向三种不同客户：应用软件供应商、开发人员、终端用户与平台产品接触最多的是开发人员，为平台买单的软件供应商，往往最终用户离平台最远。经常会被错误地把开发人员排在第一位，而把最终用户排在最后一们。

必须认识到，只有最终客户满意，平台产品才是成功的。

40. 最佳实践经验
产品经理的主要职责是定义有价值的、可用的，可行的产品


产品管理的职责：产品经理要专注在产品管理的工作上，设计出有价值、可用性、可行性的好产品
用户体验：用户体验是产品的生命
机会评估：动手设计前，先明确产品要解决什么问题，为谁解决问题，以及评估产品的标准
特约用户：要请用户反复试用产品，不断改进
产品原则：确定产品的价值标准
人物角色：把目标用户按特征分类，逐一分析，理解其情感与行为，作为决策的依据
定义产品：产品经理的主要职责是定义有价值的、可用的，可行的产品
使用原型：使用高保真原型。迫使产品经理深入定义解决方案、可以让真实用户参与测试及验证产品创意、可向直观地团队展示产品的设计思路
用户参与原型测试：获取用户反馈
根据数据改进产品：根据数据分析，不断提高产品的各项指标，完善产品

41. 产品经理反省清单作为产品经理每天问自己这十个问题

产品对目标消费者有吸引力吗？
产品的体验如何？
产品有哪些竞争力？
我了解目标用户吗？产品是否能得到他们的认可？
产品与竞品相比较有啥差别？能用2分钟解决清楚差别点吗？
产品能正常运行吗？
产品完整吗？用户对它的印象如何？销售业绩如何？
产品特色是否与目标用户的需求一致？是否鲜明？
产品价格是否合理？
我了解团队其他成员对产品的看法吗？他们怎么看待这个产品？

相关文章《启示录-打造用户喜爱的产品》
]]></content>
      <categories>
        <category>读书笔记</category>
        <category>devops</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>《持续交付发布可靠软件的系统方法》读书笔记</title>
    <url>/posts/76fde87cf7cd.html</url>
    <content><![CDATA[基础篇第一章：软件交付的问题第二章：配置管理第三章：持续集成第四章：测试策略的实现
部署流水线第五章：部署流水线解析第六章：构建与部署的脚本化第七章：提交阶段第八章：自动化验收测试第九章：非功能需求的测试第十章：应用程序的部署与发布
交付生态圈第十一章：基础设施与环境管理第十二章：数据管理第十三章：组件和依赖管理第十四章：版本控制进阶第十五章：持续交付管理

]]></content>
      <categories>
        <category>读书笔记</category>
        <category>devops</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>为啥新环境的Kafka性能这么差？</title>
    <url>/posts/39ef65c75107.html</url>
    <content><![CDATA[本故事纯属虚构，如有雷同，纯属巧合，一笑了之。公司分配给了A和B一个任务，测试容器化Kafka集群的性能。之前B在老机器上已经测试过一个版本，并写了完整的报告，算是有经验的老鸟了。现在到了一批新机器，需要在它们上面重新测试一下Kafka的性能，A主动承担这个该任务，要知道新机器不管从cpu核数还是内存大小都是老机器的两倍，而且新机器用的是SSD盘，而老机器用的是机械盘。A信心满满，认认真真地按照之前B写的文档操作。可测试结果让他大吃一惊，新机器的性能竟然不到老机器的一半。

网络问题？

A：这个Kafka集群压测数据怎么这么差？会不会是网络问题呢？B：之前我们用的是万M网卡，你这个是多少？A：网卡速度怎么看？B：用ethtool命令，后面加对应的网络接口名，看Speed值，就能知道是万M还是千M网卡了。
$ ethtool eth0Settings for em0:Supported ports: [ TP ]...Speed: 1000Mb/s...
A：这是千M网卡，怪不得性能会这么差呢。B：你确定是网卡的问题吗？你压测的时候用nload命令查看一下网络的带宽有没有跑满。A：nload?这个怎么用？B：nload命令使用非常简单，后面加对应网络接口名就能查看网卡的出入流量了。
$ nload eth0
  
A：离跑满还远着呢。那应该不是网卡带宽的问题了。

容器问题

A：我想会不会跟容器相关呀？容器的SDN什么的那么复杂，又加包头，解包头，会不会对Kafka有影响了呀？B：你直接搭一个单机版的Kafka运行在一台主机上，不做容器化，就在那台机器上测试，不走网络看下性能如何。A：Good Idea！半小时后A：性能还是没上去，看来可以排除容器和网络的因素了。

磁盘问题？

A：那磁盘呢，Kafka数据可是会落盘的，压测时磁盘的IO应该比较大吧？B：我们之前测的时候使用的是机械盘接SAS口，8k的写能达到200M&#x2F;s。你测下你的IO是多少。A：怎么测？B：磁盘的IO简单测试使用dd命令就可以，测试写时记得添加oflag&#x3D;direct，要更仔细测试就使用用fio命令。
dd
$ time dd if=/dev/zero of=test.dbf bs=8k count=300000 oflag=direct #测试写性能$ dd if=test.dbf bs=8k count=300000 of=/dev/null  #测试读性能
fio
$ #4k顺序写$ for dep in &#123;1,2,4,8,16,32,64,128&#125;;do fio -filename=/dev/vdb --ioengine=libaio -direct=1 -rw=write -bs=4k -size=50G -iodepth=$dep -group_reporting -ramp_time=10 -runtime=60 -name=model_4K_$&#123;dep&#125;_100SAS_seq_write --output=/home/model_4K_$&#123;dep&#125;_100SAS_seq_write.log -numjobs=1; done$ #4k顺序读$ for dep in &#123;1,2,4,8,16,32,64,128&#125;;do fio -filename=/dev/vdb --ioengine=libaio -direct=1 -rw=read -bs=4k -size=50G -iodepth=$dep -group_reporting -ramp_time=10 -runtime=60 -name=model_4K_$&#123;dep&#125;_100SAS_seq_read --output=/home/model_4K_$&#123;dep&#125;_100SAS_seq_read.log -numjobs=1; done$ #4k随机写$ for dep in &#123;1,2,4,8,16,32,64,128&#125;;do fio -filename=/dev/vdb --ioengine=libaio -direct=1 -rw=randwrite -bs=4k -size=50G -iodepth=$dep -group_reporting -ramp_time=10 -runtime=60 -name=model_4K_$&#123;dep&#125;_100SAS_rand_write --output=/home/model_4K_$&#123;dep&#125;_100SAS_rand_write.log -numjobs=1; done$ #4k随机读$ for dep in &#123;1,2,4,8,16,32,64,128&#125;;do fio -filename=/dev/vdb --ioengine=libaio -direct=1 -rw=randread -bs=4k -size=50G -iodepth=$dep -group_reporting -ramp_time=10 -runtime=60 -name=model_4K_$&#123;dep&#125;_100SAS_rand_read --output=/home/model_4K_$&#123;dep&#125;_100SAS_rand_read.log -numjobs=1; done$ #4k混合顺序读写$ for dep in &#123;1,2,4,8,16,32&#125;;do fio -filename=/dev/vdb --ioengine=psync -direct=1 -rw=readwrite -bs=4k -size=100G -iodepth=$dep -group_reporting -ramp_time=30 -runtime=120 -name=model_4K_$&#123;dep&#125;_100SAS_seq_read_write --output=/home/model_4K_$&#123;dep&#125;_100SAS_seq_read_write.log -numjobs=1; done$ #4k混合随机读写$ for dep in &#123;1,2,4,8,16,32&#125;;do fio -filename=/dev/vdb --ioengine=psync -direct=1 -rw=randrw -bs=4k -size=100G -iodepth=$dep -group_reporting -ramp_time=30 -runtime=120 -name=model_4K_$&#123;dep&#125;_100SAS_rand_read_write --output=/home/model_4K_$&#123;dep&#125;_100SAS_rand_read_write.log -numjobs=1; done
A：结果出来了，性能好差，写才70M&#x2F;s，算下来IOPS才8000左右，之前环境IOPS有20000多呢。看来这个磁盘性能真的有问题呀。B：你确认下它是不是SSD盘，部署机器的C跟我说挂载的是SSD盘。A：肯定不是呀，这么差。我到机房去看一看吧。B：不用去机房，你查看下系统的磁盘参数&#x2F;sys&#x2F;block&#x2F;*&#x2F;queue&#x2F;rotational，如果是0的话就是SSD。
$ grep ^ /sys/block/*/queue/rotational
A：竟然值是0，那这么差的盘竟然是SSD！别当我无知就好欺负呀。我去机房拔下来看。半小时后A：上面挂载的还真是SSD盘，但接的是SATA口。B：磁盘性能并不只跟磁盘有关，跟接口的关系也非常大，PCIE卡&gt;SAS&gt;SATA，如果有做Raid的话，性能也会有不一样。PCIE卡不能做Raid。A：怪不得，那现在是磁盘有问题？B：你可以在压测的时候看下磁盘的实时IOPS是多少。A：怎么看？B：用iostat命令，后面可以接需要观察的盘符名，看结果中的w&#x2F;s，与r&#x2F;s值就能知道实时的IOPS了。
$ iostat /dev/vdb -x 1
A：结果显示这两个值都很低呀，最高不到1000，有的时候才几十，远远没有到瓶颈呀。B：要排除磁盘问题，你还可以不用磁盘，直接把内存挂载到对应的目录下，再压测，看结果有没有变化。A：把内存挂载到对应目录？这个又是么高科技？（抓头）B：不算什么高科技，其实很简单啦。linux系统的目录&#x2F;dev&#x2F;shm是在内存上，你把kafka的数据目录指向这个目录下就好了。
linux系统的目录/dev/shm是在内存上
A：这样呀，我测试下。。。性能还是很低呀。B：嗯，又排除了磁盘的影响。

系统问题？

A：我想会不会是系统的配置问题？之前环境用的是rh 7.3，而这次我们装的是rh 7.5。B：那给现在这台机器重装下系统，使用同样的rh 7.3，再压测下试试。A：好，就这么干。半个小时后A：测试结果出来了，还是很低呀。并没有任何改进。B：看来跟系统也没有关系。

CPU问题？

B：我们看下CPU的信息。查看&#x2F;proc&#x2F;cpuinfo能查看cpu的详情。
$ cat /proc/cpuinfo......model name	: Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHZcpu MHz		: 1995.381...
A：原先主机的CPU频率我也查到了，3.00GHZ，整整大了50%B：现在两个环境系统是完全一样的，我们可以使用计算圆周率的办法测下单核cpu的能力。A：计算圆周率，我要去研究下算法了。（抓头）B：不用，使用bc命令直接计算圆周率。
$ time echo &quot;scale=5000;4*a(1)&quot; | bc -l -q
A：好吧。（一会后）新机器计算花了80s，而老机器才18s，差距这么大！B：最后我们用unixbench工具对主机性能做下全面测试，看看结果如何。A：unixbench？怎么又来了个新工具。。。这个怎么测？B：unixbench测试非常简单，它不仅能测试单核性能，还可以测试多核性能。代码在https://github.com/kdlucas/byte-unixbench，直接运行Run就可以了。
$ Run......80 CPUs in system; running 1 parallel copies of testsSystem Benchmarks Index Score                                        4678.5......80 CPUs in system; running 80 parallel copies of testsSystem Benchmarks Index Score                                        8820.4
一个小时后A：终于运行好了，新的机器测试的分数连之前机器分数的一半都不到。这不是逗我吗，测的可是新机器呀！……A：到底是哪有问题呀？！我找厂商去。
评价：把问题死磕到底，你会有非常多的收获。
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>创建自签证书步骤</title>
    <url>/posts/d5b91b0cd4d2.html</url>
    <content><![CDATA[
根证书创建

$ openssl genrsa -out ca.key 2048$ openssl req -new -x509 -days 36500 -key ca.key -out ca.crt -subj &quot;/C=CN/ST=shanxi/L=taiyuan/O=cn/OU=test/CN=example.com&quot;$ #或者 openssl req -new -x509 -days 36500 -key ca.key -out ca.crt 手动输入配置

创建证书并使用根证书签发

$ openssl genrsa -out app.key 2048$ openssl req -new -key app.key -out app.csr$ openssl x509 -req -in app.csr -CA ca.crt -CAkey ca.key -out app.crt -days 3650  -CAcreateserial

使用 Openssl 工具查看证书信息

$ openssl x509 -in app.crt -noout -dates$ openssl x509 -in app.crt -noout -subject$ openssl x509 -in app.crt -noout -text


nginx的配置中的证书使用中，ca与crt证书的保存顺序如下：

-----BEGIN CERTIFICATE-----CRT-----END CERTIFICATE----------BEGIN CERTIFICATE-----CA-----END CERTIFICATE-----
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>将字符串进行压缩后保存该如何做？</title>
    <url>/posts/0a0be0426bc4.html</url>
    <content><![CDATA[如何将zip文件挂载到容器Pod中呢？Prometheus operator中看到的一个特殊玩法。它将prometheus.yml进行压缩成.gz后再保存到secret中。可参考它来实现对数据的压缩与加密。具体的操作如下：
$ echo &quot;abc&quot; | gzip | base64H4sIAAAAAAAAA0tMSuYCAE6BiEcEAAAA
解密操作：
$ echo &quot;H4sIAAAAAAAAA0tMSuYCAE6BiEcEAAAA&quot; | base64 -d | gunzip
大家可以尝试下。另外需要注意的是，将该数据挂载到POD的文件中，文件是压缩后的gz文件。
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>性能测试方案设计与测试过程</title>
    <url>/posts/3bad2d384d31.html</url>
    <content><![CDATA[
性能测试过程
性能测试计划


按照模板生成性能测试计划
指标设计（并发数、在线数、TPS、请求超时）挑选典型交易（20%交易，覆盖80%流量）环境、数据准备（与生产环境尽量一致）场景设计（基础场景、专项场景）测试进度安排




需求分析、调研


了解业务需求


环境、数据准备：


系统部署
真实含义的业务数据
数据量为生产数据量三年以后的数据量。


场景分析设计


挑选交易，典型交易：高频交易，逻辑复杂的交易，集中时间段的场景
单交易运行——&gt;单交易负载场景
混合场景设计：混合容量设计，浪涌设计（20-&gt;100,100-&gt;20）
稳定性场景设计（48小时、72小时持续压力验证）


场景执行、应用监控


执行测试场景


问题定位、分析优化


分析问题
回归验证


性能测试报告


测试结果汇总形成报告

性能测试方案扩展引入多样化的性能监控工具（prometheus&#x2F;JVM&#x2F;pinpoint&#x2F;skywalking）丰富性能场景设计（扩展性场景、可靠性场景、网络异常等情况）可持续性能压测(Jmeter进行自动化性能测试)

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（_交付生态圈）第十五章：持续交付管理</title>
    <url>/posts/8619a1b54338.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
实现持续交付不仅仅是搭建一些工具，做一些自动化的工作，它依赖于交付过程中的每个人的协作。通过持续交付实践，可以快速且可靠地交付新版本。
配置与发布管理成熟模型
这个模型的最终目标：

缩短生产周期
减少缺陷
提高软件交付生命周期的可预测性
规范合规
有效发现和管理软件交付相关风险
交付更少缺陷的软件，降低成本

模型指导组织推进持续交付变革，使用戴明环，即计划——执行——检查——处理。

使用模型来分析所在部门的配置与发布管理模式
选择一个领域集中发力，该领域是你的薄弱环节，痛点所在
实施变革。先创建一个实施计划，选择真正感到痛苦的那部分人
一旦发生了变化，使用之前创建的验收条件来衡量这些变化是否达到了预期效果。组织所有相关人员召开回顾会议，找出改进点及潜在改进领域
重复上述步骤，积累知识，增量改进，推广到整个部门

项目生命周期团队的组建与磨合常常有以下五个阶段：创建期、风暴期、规范期、运转期、调整重组期。软件也有五个阶段：立项阶段、启动阶段、初始阶段、开发部署阶段、运维阶段。

立项阶段：业务分析、业务负责人及涉及部门有关人确立
启动阶段：需求收集和分析，规范项目范围和计划。输出有，商务分析报告、概括性的功能与非功能需求列表、发布计划、测试策略、发布策略、架构评估报告 、风险和问题列表、开发生命周期描述、执行上述内容计划描述。包括足以启用项目的细节和最多几个月需交付目标，最合理周期为3个月。
初始阶段：一到二周，确保软硬件到位；确保网络、白板、笔纸、打印机、食品等到位；建立好版本控制库；建立一个基本的持续集成环境；角色、职责、工作时间、会议时间上达成一致；为第一周做准备，目标上达成一致；创建简单的测试环境与测试数据；更详细研究预定的系统设计 ；调研识别和缓解分析、开发、测试风险；开发用户故事与需求的待办列表；创建项目结构及构建脚本和一些测试，以验证持续集成环境正常工作
开发部署阶段：迭代开发是最基本要求。软件应该一直处于可工作状态；每个迭代都能将软件部署到一个类生产环境中向用户演示；迭代长度不超过两周
运维阶段：项目开发部署阶段结束后，一般项目还会继续开发下去，此过程与开发部署阶段差不多

风险管理流程在项目的各个阶段都要做到风险的识别与预防

启动阶段结束时：验证发布策略在关于“创建发布策略”一节（10.2）节讨论过的方面都考虑到了；做好初始阶段的计划
初始阶段结束时：确保团队已经准备好开始开发软件了，持续集成环境正常工作，并且有一个类生产环境用于产品代码的部署
开发部署风险的缓解：识别、跟踪和管理风险，及时调整。查看部署计划，每次演示后做简单的回顾会议，每日立会作为风险识别的一部分。

常见交付问题和原因
构建某个版本花很长时间，而且经常失败。可能原因：部署过程非自动化；没有足够的硬件；硬件和操作系统配置没有正确管理 ；部署过程依赖于团队无法掌控的系统；没有足够多的人员理解构建和部署过程；测试、开发、分析和运营人员没有充分协作；开发人员没有遵守纪律，通过小步增量的方式的修改保证应用程序一直处于可工作状态
缺陷数量持续增加，产品质量下降可能原因：开发期间，测试与开发没有协作；用户故事在没有全面测试，被测试人员验收，并在类生产环境下给用户演示的情况下标记为“完成”；开发与测试在自动化测试套件开发方面缺少经验；团队不了解哪种类型的测试有效；没有足够的测试覆盖率；系统原来就是一个不可靠的原型。
集成周期长，迭代速度慢可能原因：自动化测试运行时间太长；提交阶段运行时间太长；自动化测试有间歇性失败，还是误报；没人得到许可就回滚别人的提交；没有足够多的人理解持续集成过程
环境不一致，导致生产故障可能原因：UAT和生产环境有差异；没有对生产环境或试运行环境的变更管理流程；在运营、数据管理团队和交付团队间协作不畅；生产环境和试运行环境中的缺陷事件的监管不够有效；应用程序中的指南和日志不充分；应用程序非功能需求测试不充分

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（交付生态圈）第十一章：基础设施和环境管理</title>
    <url>/posts/d1ede288040e.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
基础设施与环境管理的目标是让所有测试环境（包括持续集成环境）都要与生产环境相似，特别是它们的管理方式。环境是指应用程序运行所需的所有资源和它们的配置信息。有如下这些属性：组成运行环境的服务器的硬件配置信息：如CPU类型和数量、内存大小、硬盘和网卡等；应用程序运行所需要的操作系统和中间件：如消息队列、应用服务器、web服务器及数据库服务器等的配置信息。基础设施代表了所在组织中的所有环境以及支持运行的所有服务，如DNS服务器、防火墙、路由器、版本控制库、存储、监控、邮件服务、日志服务等。准备部署环境及管理它，要基于以下原理，用一个整体方法来管理所有基础设施：

使用保存于版本控制库中的配置信息来指定基础设施所处的状态
基础设施应该具有自治特性，即它应该自动地将自己设定为所需状态
通过测试设备和监控手段，应该时时都能掌握基础设施的实时状况
基础设施还应该具有非常容易重新搭建的特性

为了减少在类生产环境中的部署风险，需要精心管理以下内容:

操作系统及其配置信息，包括各个环境
中间件软件栈及其配置信息，包括应用服务器、消息系统和数据库
基础设施软件，如版本控制库、目录服务器及监控
外部集成点，比如外部系统和服务
网络基础设施，包括路由器、防火墙、交换机、DNS和DHCP等
应用程序开发团队与运维团队间的关系

理解运维团队的需要开发团队往往需要尽可能快地交付软件，而运维团队的目标是稳定性。所有人的共识是：让发布有价值的软件成为一件低风险的事情。做这件事的最好方法是频繁发布，每次发布的变更很小。运维团队使用熟悉的技术，在各个环境统一部署技术栈。开发人员是最开始部署环境的，所以一开始运维团队也需要在最开始写脚本的时候就加入其中。对应用程序配置、中间件配置、操作系统配置、数据库配置等都需要进行统一管理。将它们像源代码一样加入版本控制库中，同时它们的配置也是部署流水线的一部分。
基础设施的建模和管理如果项目处于开始阶段，这是制定基础设施配置管理策略的好时机。如果是一个遗留系统，并且没有好的控制规范的话，就要找出让它处于受控状态的方法。

在没有批准的情况下，不允许他人修改基础设施
制定一个对基础设施进行变更的自动化过程
对基础设施进行监控，一旦发生问题，迟早发现。

对基础设施的修改是一个严肃的问题，在个问题上对测试环境与对生产环境同等重要。如果无法通过一个自动化过程从头重新创建基础设施的话，首先要实现访问控制，没有审批，不得对基础设施做任何修改；接下来在不关闭访问控制的情况下，创建自动化过程来管理基础设施。当然对测试环境的变更审核可以比生产环境的变更审核更容易些。对基础设施进行修改的关键特征

无论做哪类变更，如防火墙等都要走同样的变更管理流程
流程使用一个所有人都需要登录的工单系统来管理
变更应该有详细的记录
能够看到每个环境进行的变更历史，包括部署活动
生产环境变更前，必须先在类生产环境中测试通过，自动化测试也已经运行完成
对每次修改都应该做版本控制，并通过自动化流程对基础设施进行变更
需要有一个测试来验证变更是否成功

服务器的准备及其配置的管理服务器可以使用PXE来做自动化的远程安装，或者使用虚拟化技术。同时服务器的配置需要添加到版本控制库中进行统一管理。一旦安装好操作系统后，就必须保证任何配置的修改都是以受控方式进行。除了运维团队外，任何人不能登录到服务器上，同时所有的变更都使用自动化系统来执行。
中间件的配置管理中间件如果是系统标准安装的部分，直接使用配置管理工具进行部署；而如果不是系统标准安装的部分，可以将中间件打包，并将它放在包管理服务器上，然后使用同样的方式对其进行管理；最后对于没有考虑脚本化或者后台安装的产品，需要对其进行改造，并将配置相关的文件进行版本控制。
基础设施服务的管理经常会出现已经成功完成部署流水线并在生产环境中运行的软件因为基础设施问题而不同正常工作。如DNS服务问题等。这种问题一般会比较难以诊断。有以下几个建议：

对网络基础设施的每个部分都应该进行版本控制，使用Ansible类似的工具将配置文件从版本控制库中取出放在系统中运行，实现自动化
安装一个好用的网络监控系统
应用程序中对网络连接出现问题时，日志记录
确保冒烟测试在部署时检查所有的连接
确保集成测试环境的网络拓扑尽可能与生产相似
出现问题时，可以使用tcpdump和wireshark进行排查

多宿主系统生产系统中一个重要的增强部分是不同类型的流量使用多个隔离网络，并与多宿主服务器结合使用。多宿主服务器多个网络，一个用于运行备份，一个用于服务器的监控与管理，一个用做服务器间数据传输。
基础设施和应用程序的监控创建监控策略时，需要考虑以下四点：

对应用程序和基础设施进行监测，以便可以收集必要的数据
存储数据，以便拿来分析
创建一个信息Dashboard，将数据聚合在一起图表的形式展现出来
建立通知机制，便于及时关注关心的事件

监控数据类型

硬件，通过带外监控服务器的电压、温度、系统风扇速度、peripheral health等。
构成基础设施的那些服务器上的操作系统，比如内存使用、CPU使用情况、交换分区、磁盘、IO、带宽等使用情况。
中间件，如内存、数据库连接池、线程池、连接数、响应时间等。
应用程序，设计一些数据监控的hook，比如业务交易数量、价格、转换率等。

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（交付生态圈）第十三章：组件和依赖管理</title>
    <url>/posts/4430eb688832.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
持续交付让应用程序处于随时可发布的状态。在大型重构或添加复杂功能时，要继续保持应用的可发布状态，需要对大型应用组件化。组件是指应用程序中的一个规模相当大的代码结构，它具有一套定义良好的API，而且可以被另一种实现方式代替。一个基于组件的软件系统，通常其代码库被分成多个相互分离的部分，每个部分通过有限的定义良好的接口提供一些服务与其他组件进行有限的交互。有人把组件称为模块。基于组件的设计是一种良好的架构，具有松耦合性。
保持应用程序可发布团队不断地增加新特性，可以给每次新特性创建新的分支，当新特性完成后，再将它合并到主分支。这将会导致合并周期变长，无法做到持续集成，这种方法不是最好的。提倡每个人都应该提交到主干。可是这样又该如何保证主干一直保持可发布状态呢？有如下四种策略：

将新功能隐藏起来，直到它完成为止。一种方法是把新功能直接放进主干，但对用户不可见，比如通过单独的URL来访问，通过Web服务器配置不允许访问其入口；另一种方法是通过配置项开关来管理。把功能半成品与系统其他部分一同发布是一个好实践。
将所有的变更都变成一系列的增量小修改，而每次小修改都是可发布的。首先需要用各种方式将一个需求分解成较小的任务，然后将这些任务再划分成更小的增量修改。
使用通过抽象来模拟分支的方式对代码库进行大范围的变更。在要修改的那部分代码上创建一个抽象层，然后在当前实现方法存在的同时，开发一种新的实现方式，当完成时再把原始的实现和抽象层删除。
使用组件，根据不同部分修改的频率对应用程序解耦。

依赖库是团队除了选择权以外，没有控制权的软件包，它们很少更新。组件是应用程序所依赖的代码块，它一般由团队自己开发的，更新频繁。构建时的依赖会与运行时依赖不同，管理依赖遇到问题。

依赖地狱。应用程序的依赖版本与实际部署的版本不一致。
库管理。一种方法是将库文件提交到代码版本控制库中，但时间久了后会导致版本库变大且乱，同时库文件的状态难以管理。另一种方法是使用显示声明的库管理工具，如Maven。

组件只有一个系统达到一定的复杂度时，才会考虑将它分成多个组件。组件的目的是为了提交团队的效率。

它将问题分成更小更达意的代码块
组件常常表示出系统不同部分代码的变化率不同，且有不同的生命周期
将代码划分，也便于分析系统的职责描述和维护，并且提交了对代码的理解
提供了额外的自由度来优化构建和部署过程

当我们遇到以下情况时，可以考虑将组件代码从代码库中独立出来

代码库的一部分需要独立部署
打算将系统分成一个内核和一系列组件，以便用另一种实现代替当前系统的某部分或者支持用户自扩展
组件为其他系统提供了一个接口（如API接口）
代码的编译和链接时间太长
在开发环境中打开项目时间太长
对一个团队来说，代码库太大

大多数情况下，我们建议整个应用程序使用一个构建流水线，每次提交修改时，就应该构建并测试整个应用。只有当效率太低而无法忍受时，才使用并行流水线方式。
二进制包管理使用制品库来管理二进制包，如Artifactory，Nexus。制品库不应该包含那些无法重现的产物，即便删除整个制品库，也可以方便地将二进制包恢复出来，一般通过重新构建对应的代码。最简单的制品库是磁盘上的一个目录，最重要的是它应该将一个二进制文件关联到版本控制库中生成该文件的某个源码版本对应上。流水线与制品库相结合

编译阶段会创建需要放到制品库的二进制文件
单元测试和验收测试阶段会从制品库中取出这些二进制文件，将生成的测试报告放在制品库中
用户验收测试阶段将二进制文件部署到UAT环境，用于手工测试
发布阶段从制品库中取出二进制文件，将它部署到生产环境

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（交付生态圈）第十二章：数据管理</title>
    <url>/posts/a930425c97d6.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
应用程序可以通过删除前一个版本，使用新版本替换旧版本的方式部署，但是大多数系统，数据无法使用这种方式进行变更，一旦某个系统发布到了生产环境中，关联的数据将不断增加。数据往往是系统中最有价值的部分。当我们需要对数据系统进行结构修改或者内容修改时，就需要相关的策略。对数据的修改是不可避免的，关键在于将数据迁移过程自动化。目前有一些工具对数据迁移提供了较多支持，它们还允许对数据库进行版本化管理。另一个重要部分是测试数据的管理。
数据库脚本化任何数据库的修改都应该通过自动化过程来管理。包括数据库的初始化，数据库所有的迁移都需要脚本化，并将脚本提交到版本控制库中。几乎所有的数据管理系统都支持通过自动化脚本进行数据存储的初始化工作。

清除原有的数据库
创建数据库结构、数据弯路实例以及模式等
向数据库加载数据

在大多数据项目中，数据库的使用要复杂得多。
增量式修改绝大多数据系统，对数据库更新时，要保留它们的数据。由于在部署时需要保留数据库中的已有数据，所以需要有回滚策略，以便部署失败时使用。这就需要对数据库进行版本控制。

在数据库中创建一个数据，用来保存版本号
每次数据库进行修改时，需要创建两个脚本：升级脚本、回滚脚本
有一个配置项来设置数据库与应用版本对应关系

在很多项目中，多个程序共用一套数据库存储。虽然这种方式并不推荐，最好是让程序直接交互，但是这种情况却是常见的。这种环境下对数据库变更，就需要对程序做完全的集成测试，确保数据库变更对相关影响到的程序都能测试到。同时对哪个应用使用了数据库哪个对象做登记也是可以的。
数据库回滚和无停机发布生产环境部署有两个需求会成为数据库回滚的约束。

回滚时需要保留本次升级后产生的数据
保持应用程序的可用状态


保留数据的回滚回滚脚本要满足以下条件


包括模式修改，即不迁移任何数据
只删除新版本使用的那些数据，即使这些数据丢失了也没问题

对于以下情况

涉及从临时表中将数据导回来
删除那些旧版本系统无法接受的数据

第一种方法是将那些不想丢失的数据库事务缓存，一旦应用程序被成功地重新部署，这些事件就可以重新播放一遍。第三种方法是使用蓝绿部署，发布时对生产数据库（蓝环境）做一个备份，将备份放在绿环境中应用，迁移数据，将用户切换到绿环境中。回滚时，将用户切回蓝环境，再把绿环境的数据库上发生的新事务回收，在下一次更新之前重新应用这些事务到蓝数据库上。2. 应用程序部署与数据库迁移解耦
第三种方法是将应用程序部署过程与数据迁移过程解耦，分别执行，这种可用于管理热部署。开发应用程序升级的中间版本，它既与数据库上个版本兼容，也与数据库下个版本兼容。

测试数据的管理
为单元测试进行数据库模拟单元测试不使用真正的数据库，通常它会使用测试替身对象来取代与数据库打交道的服务。如果做不到的话：


用测试替身对象来替代那些访问数据库的代码，通常使用repository模式
使用假的数据库，如H2&#x2F;SQLite&#x2F;JavaDB。单元测试运行在一个内存数据库上，让验收测试运行在平时使用的磁盘的数据库上。


管理测试与数据之间的耦合以下三种方法可以用来做测试设计，便于管理好数据的状态：


测试的独立性**[推荐]**，合理组织测试，让每个测试的数据只对该测试可见
适应性测试，运行时先对数据环境进行检查，用检查的数据作为数据基础进行测试
测试的顺序，按某种已知的序列运行，每次测试输入依赖于前一个输出

保持测试的独立性最简单的方法是确保在测试结束时，总是把数据库中的数据状态恢复到测试之前。对于支持事务的数据库来说，测试开始时创建一个事务，在事务内执行所需的数据库操作与交互，测试结束后，将该事务进行回滚。不建议创建一个连贯的“故事”进行顺序执行，这个有序的测试无法真正地代表测试的目的和内容。
数据管理和部署流水线
提交阶段的测试数据，避免复杂的数据准备。
验收测试的数据，尽可能减少测试对大型复杂数据结构的依赖。测试专属数据、测试引用数据、应用程序引用数据
容量测试的数据，为测试提供足够的输入数据，准备适当的引用数据支持测试中的用例
其他测试阶段的数据，推荐利用生产数据的一个子集或者运行一些自动化验收测试或者容量测试之后产生的数据库，为其他测试阶段提供数据。

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（交付生态圈）第十四章：版本控制进阶</title>
    <url>/posts/b682fb75753a.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
版本控制用来维护应用程序每次修改的完整历史，包括源代码、文档、数据库定义、构建脚本和测试等。团队可以在一个代码版本控制库上一起开发应用程序的不同部分。一旦团队人数超过一定数量，就需要规划版本控制库的使用，让开发更加高效。
分支与合并分支，即为选择的基线创建一个副本，该副本与原基线相互独立，开发者能在两个工作流上同时开发。团队为什么使用分支？

物理上：系统物理配置而分支，即为文件、组件和子系统而分支
功能上【最常见】：系统功能配置而分支，即为特性、逻辑修改、缺陷修复和功能增加，以及其他可交付的功能而分支
环境上：系统运行环境而分支，即由构建平台和运行时平台的不同而分支
组织上：团队的工作量而分支，即为活动&#x2F;任务、子项目、角色和群组而分支
流程上：团队的工作行为而分支，支持不同规章政策、流程和状态而分支

在开发中，经常会遇到分支合并的情况，除非那些为了发布或者技术预研而创建的分支。两次合并时间间隔越长，每个分支上工作的人越多，合并发生冲突的可能性就越大。以下两种方法来减小冲突：

创建更多的分支来减少在每个分支上的修改。这只是将痛苦延后而已。
很谨慎地创建分支，可能每个发布才创建一个分支。为了尽量减少合并的痛苦就经常做合并。一个强烈推荐的分支策略是：只为发布创建长期的分支。这种模式下，新开发的代码总是被提交到主干上，只有在发布分支上修改缺陷时才需要合并，而这具合并是从分支合并回主干。只有非常严重的缺陷修复才会从主干合并到发布分支上。

分布式版本控制系统（Git）分布式版本控制系统就是每个使用者本地都有一个完整的代码仓库。它有了很多新特性：

在几秒内就能开始使用分布式版本控制系统
可以单独从别人那里拿到他们的最新更新，不需要提交到中央代码库
可以将自己修改推送到一组人的代码库中，而不需要他们每个人来取你的修改
补丁可以通过网络用户更高效地传播
没有网络的时候，也可以对修改的代码版本控制
可以频繁地提交未完成的功能到本地代码库，而不影响其他人
将修改发送给其他人之前，可以很容易地在本地对这些提交进行修改，重排它们的顺序或者将多次提交打包成一个，这个叫‘rebasing’
很容易用本地代码库来尝试各种解决方案或想法
能在本地把多次提交打包
在本地建立和同步多个代理库很容易，有更高的可用性
全量代码有很多份副本，有更好的容错性

主干开发开发人员总是提交代码到主干，而使用很少使用分支。这有以下三个好处：

确保所有代码被持续集成
确保开发人员及时获得他人的修改
避免项目后期的“合并地狱”和“集成地狱”

开发人员在主干上工作，每天至少提交一次代码。当需要做复杂的修改时，修改会被分成一系列小的增量步骤有计划地实现，而每个步骤都会通过测试且不会破坏已有的功能。主干开发并不排斥分支，只有当不需要合并回主干时，才创建分支。如发布分支，技术预研分支。如果开发人员很多，且有多个版本发布的大型团队，软件需要良好的组件化、增量式开发和特性隐藏。要做的是持续频繁地向主干分支提交代码，每天至少一次提交。
按发布创建分支在某个版本即将发布之前，创建发布分支。该分支一旦创建，该发布版本的测试和验证全部在该分支上进行，而最新的开发工作仍然在旧的主干分支上进行。发布分支上只做严重缺陷的修复。遵循如下规则：

一直在主干上开发新功能
当待分布版本的所有功能完成，且希望继续开发新功能时才创建一个分支
在分支上只允许提交修复严重缺陷的代码，且这些修改必须立即合并到主干
执行实际的发布时，这个分支可以选择性地打个标签

不要在已有的发布分支上再创建更多的分支，所有后续分支都应该从主干上创建。一旦发布频率达到一定的频率（比如一周一次），就没必要创建分支了，在这种情况下，发布一个新版本要比在已发布的分支上打补丁更容易，成本更低。而且部署流水线机制可能为你保留了一份记录，包括发布的软件在版本控制库中对应的修订版本号。
按功能特性分支为了让开发团队更容易在“特性”层次上并行工作，并保持主干为可发布状态，这时使用特性分支。使用特性分支，需要遵循以下规则：

每天都要把主干上的所有变更合并到每个分支上
每个特性分支都应该是短生命周期的，理想情况只有几天
活跃分支的数量随时都应该少于或等于开在开发当中的用户故事的数量。只有在已经把开发的用户故事合并回主干，才允许创建新的特性分支
在合并回主干之前，该用户故事应该已经由测试人员验收通过
重构必须即时合并，从而将合并冲突最小化
技术负责人的一部分职责就是保证主干的可发布状态

按团队分支团队分支模式的工作流：多个开发人员同时工作在多个工作流上，还要保证主干随时可发布状态

创建多个小团队，每个团队自己有对应的分支
一旦某个特性或用户故事完成，让该分支稳定下来，合并到主干
每天都将主干上的变更合并到每个分支上
对于每个分支，每次提交代码都要运行单元测试和验收测试
每次分支合并回主干时，主干上也要运行所有的测试，包括集成测试

在有几个比较小而且相对独立的团队，同时各团队负责该软件系统中功能相对独立的情况下，该模式才有效。从持续集成的角度说，该策略有一些缺点，根本问题是该模式下的工作单元是一个分支，而不是一次特定的修改。这种模式与特性分支模式很类似。它的优点是：分支较少，所以集成工作会频繁一些。缺点是：各分支很快会变得差异很多，在合并操作时会更复杂。团队分支很快会和主干变得很不一样，差异很大，合并冲突可能很快就变得极其痛苦。推荐使用“功能隐藏”的方式进行增量开发，从而做到应用程序随时可发布。即使某个功能特性正在开发中，也把它隐藏起来，这种方法需要更多的纪律性，但风险小，避免了分支的不断合并带来的复杂性，而且可以快速得到某次提交对整个应用的影响的反馈，这正是真正的持续集成可以提供的。
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（基础篇）第三章：持续集成</title>
    <url>/posts/895381bf27ed.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
持续集成要求每当有人提交代码时，就对整个应用进行构建，并对其执行全面的自动化测试集合，一旦出现问题，开发团队应停下手中的工作，修复问题。持续集成的目标是：让正在研发的软件一直处于可工作的状态。
实施持续集成的先决条件
版本控制，与项目相关的所有内容都必须提交到一个版本控制库中（产品代码、测试代码、数据库脚本、构建与部署脚本、以及所有用于创建安装运行和测试该应用的程序的东西）
自动化构建：必须满足人和计算机都能通过命令行自动执行应用的构建、测试以及部署过程
团队共识：持续集成是一种实践，需要团队所有成员都遵循规则

一个基本持续集成系统
第一次在持续集成工具上执行构建时，可能会缺少一些必须的软件及配置，请将所操作的工作记录下来，并放在自己项目的知识共享库中，应花一些时间将应用程序所依赖的所有软件和配置项提交到版本控制系统中，并将重建全新环境的整个活动变成一个自动化的过程
查看一下是否有构建正在运行，如果有，等它运行完。如果它失败了，则与团队其他人一起将它修复，后再提交自己的代码
一量构建完成且测试全部通过，就从版本控制库中将该版本的代码更新到自己的开发环境上
在自己的开发机上执行构建脚本、运行测试，以确保所有代码在本地工作正常
如果本地构建成功，就将代码提交到版本控制库中，然后等待包含本次提交的构建结果
如果构建失败，就停下手中的工作，立即修复这个问题，本地测试通过后，再次提交代码到版本控制库中
如果构建成功，开始下一项任务

持续集成的前提条件
频繁提交，开发始终在主干上提交代码
创建全面的自动化测试套件【单元测试（10m），组件测试（较长），验收测试（长）】

理想情况下，提交前的预编译和测试过程与持续集成服务器上的编译和测试过程都应在几分钟内结束，10m是极限，90s内完成最理想。如果测试过程太久，则需要找出那些运行较慢的测试，优化它，缩短测试时间。测试分为两个阶段：提交阶段、提交后阶段

提交阶段：运行所有类别的单元测试，并构建部署的二进制文件。提交前运行一次，通过后再提交到持续集成环境再运行一次
提交后阶段：进行验收测试、集成测试，一旦提交测试通过，立马运行验收测试。如果超过30m，就要考虑采用高性能多进程机器缩短测试时间

使用持续集成软件
触发Job或轮询版本控制系统，如果有更新，运行构建脚本及测试
提供展示这个流程运行结果的视图，并通知报告 ，拿到生成它的安装文件等

必不可少的实践
构建失败之后不要提交新代码
提交前在本地运行所有的提交测试或让持续集成服务器完成此事
等提交测试通过后再继续工作
回家之前，构建必须处于成功状态
时刻准备着回滚到前一个版本
在回滚之前需要规定一个修复时间
不要将失败的测试注释掉
为自己导致的问题负责
测试驱动的开发

推荐的实践
极限编程开发实践
若违背架构原则，就让构建失败
若测试运行变慢，就让测试失败（2s)
若有编译警告或代码风格问题，就让构建失败

在持续集成系统之上的扩展持续集成的实施迫使你遵循两个重要的实践：

良好的配置管理
创建并维护一个自动化构建和测试流程

一个好的持续集成系统是基石，在此之上你可以构建更多的基础设置：

一个巨大的可视化指示器，用于显示构建系统所收集的信息，以提供高质量的反馈
结果报告系统，以及针对自己测试团队的安装包
为项目经理提供关于应用程序质量的数据的提供程序
使用部署流水线，可以将其延展到生产环境，为测试人员和运维团队提供一键式部署系统

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（基础篇）第一章：软件交付的问题</title>
    <url>/posts/014831746fe2.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
软件构成部分：可执行的代码、配置信息、运行环境、数据
不同环境下只进行一次编译
对环境的任何修改都应该作为配置信息管理，配置信息的更改都需要经过测试
如果运行环境需要修改，则修改后的环境也需要进行测试。环境包括：操作系统配置、应用程序依赖的软件集、网络配置及任何基础设置、外部系统
数据结构发生变化，同样需要经过测试

反馈流程：指完全以自动化的方式尽可能地测试每一次变更
创建可执行代码的流程
单元测试
质量检测：测试覆盖率以及其他与技术相关的度量项
功能测试验收
性能、有效性、安全性等非功能测试
探索性测试，给客户&#x2F;最终应用演示

自动化测试反馈【commit阶段】

运行速度快
尽可能全面，75%代码库覆盖率
环境中立，相对生产环境简单廉价
如果出现问题，绝不发布【commit之后测试】
运行速度慢一些，适合并行执行
即使有些测试问题，也可以发布应用程序
运行环境尽可能与生产相同

不同版本、不同环境的配置放在版本控制中
开发人员都拥有自己的专属开发环境
无论部署在什么目标环境都应采用同一种部署方法
开发环境是特例，可以有多变性部署方法

软件的交付原则
为软件的发布创建一个可重复且可靠的过程
将几乎所有的事情自动化（构建、部署、测试、发布）
把所有的东西都纳入版本控制（需求文档、测试脚本、自动化测试用例、网络配置脚本、部署脚本、数据库创建、升级、回滚和初始化脚本、库文件、应用程序依赖的软件集、工具链及技术文档等)
找到流程中最痛苦的事情，并提交频繁地进行：如果集成最痛苦，那应在开始阶段就不断进行集成、测试；如果发布痛苦，每次提交并通过自动化测试后就进行发布
用户故事只有到了已发布才算完成，交付成果属于每个成员，交付前每个成员都为其负责
持续改进，交付过程中，整个团队召开回顾会议，提出改进方向及方法，每个改进点应该同一个人负责跟踪，确保改进被执行，下一次回顾会议，汇报结果。

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（基础篇）第二章：配置管理</title>
    <url>/posts/3430a8b4b6e5.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
配置管理指一个过程，通过该过程，所有与项目有关的产物，以及它们之间的关系都被唯一定义、修改、存储与检索。
使用版本控制
对所有内容进行版本控制（所需的支撑软件配置信息，操作系统配置信息、DNS区域文件和防火墙配置等）

配置管理是持续集成交付过程的基础。
软件配置管理灵活性：先专注于提供具有高价值且可配置程度低的功能，冒烟测试就是一种缓解配置验证问题的方法配置分类
推荐应使构建打包生成的包，面向所有环境，并不植入配置信息

应用程序的配置管理
将特定于测试环境或生产环境的实际配置信息存放于与源代码分离的单独代码库，需要注意配置信息的版本，一定要与相应的应用软件的版本相切尔西
不要把密码放在版本控制系统中
获取配置信息：文件系统、从某个中心仓库中获取配置信息
配置信息：区分应用、版本、环境，都需要满足以下：
新增一个环境，能为这个配置应用的新环境指定一套新的配置信息
新建应用程序的一个新版本，确保在部署新版本时，使用新的配置，但是一量需要回滚时，还能够使用旧版本的配置
将新版本从一个环境移到另一个环境，确保新环境上的新配置里有效
重定向到一个数据库服务器，只需要简单更改一个配置项
通过虚拟化技术管理环境


一种方法是把预生产环境的配置信息作为默认配置，其它环境通过适当的方式覆盖这些默认值，尽量减少配置项

跨应用的配置管理每个应用程序的配置项管理都应该作为项目启动阶段的一个议题，且应维护一份应用程序配置选项索引表，记录配置项的功能，位置及生命周期，如何修改。

在应用程序的生命周期中，我们应该在什么时候注入哪类配置信息，要与系统运维和支持团队一同讨论。
将应用程序的配置项与源代码保存在一个仓库中，但要把配置项的值保存在别处，另外像用户密码这类敏感信息不应该放在版本控制库中
应该总是通过自动化的过程将配置项从保存配置信息的存储库中取出并设置好，这样就能很容易掌握不同环境中的配置信息了
配置系统应该能依据应用、版本、环境为打包、安装以及部署脚本提供不同的配置值。
对每个配置项都应用明确的命名习惯，避免使用难懂的名称。
确保配置信息是模块化且封闭的，使得对某处配置项的修改不会影响到那些与其无关的配置项。
DRY原则。定义好配置中的每个元素，使每个配置元素在整个系统中都是唯一的，其含义绝不与其他元素重叠。
最少化，即配置信息应尽可能简单且集中。
避免对配置信息的过分设计，应该尽可能简单。
确保测试已覆盖到部署或安装时的配置操作。

环境管理
环境中各种各样的操作系统，包括其版本、补丁级别及配置设置
应用程序所依赖的需要安装到每个环境中的软件包，以及这些软件包的具体版本及配置
应用程序正常工作所需的网络拓扑结构
应用程序所依赖的所有外部服务，以及这些服务的版本和配置信息
现有的数据以及其他相关信息

当评估第三方产品或服务时，应该问自己以下问题：

我们可以自行部署它吗？
我们能对它的配置做有效的版本控制吗？
如何使它适应我们的自动化部署策略？

对环境的变更过程进行管理，严格控制生产环境，未经组织内部正式的变更管理过程，任何人不得对其进行修改。应该像对待生产环境一样对待测试环境，其配置管理应该与生产环境中的配置管理一样的策略。
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（基础篇）第四章：测试策略的实现</title>
    <url>/posts/1e1968ef6b3f.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
项目在一开始阶段，测试人员就会与开发人员及客户一起写自动化测试。这些测试应该在开发前就写好。以上这些测试仅仅是系统进行功能测试，容量、安全性及其非功能性试也应尽早建立，为它们写自动化测试套件。确保不符合需求的问题尽早暴露。

业务导向且支持开发过程的测试在开发一个用户故事之前，应写好验收测试，采取完美的自动化形式。系统的验收测试应运行在类生产环境（UAT)验收测试有价值的特性：

它加快了反馈速度 
减少了测试人员的工作负荷
让测试人员集中精力做探索性测试和高价值的活动
这些验收测试也是一组回归测试套件
行为驱动开发，可以以这些测试中自动生成需求说明文档

并不是所有的东西都需要自动化。我们倾向于将自动化验收测试限于完全覆盖Happy Path的行为，并仅覆盖其它一些极其重要的部分。每一种测试都应该覆盖应用程序的80%验收测试一般都是端对端测试，但是这样很多时候验收测试的失败并不是因为真正的缺陷，而是因为界面的变更，这将导致增大了验收测试脚本的维护。有两种方法解决这个问题：

在测试与用户界面之间增加一个抽象层，以便减少因用户界面变更而导致的问题
通过公共API来运行这些验收测试，用户界面会使用这些公共API来执行真正的操作

###技术导向且支持开发过程的测试单元测试：不应该访问数据库，不应该使用文件系统，不与外部系统交互组件测试：涉及更多的准备工作并执行更多的IO，需要连接数据库，文件系统，与外部系统交互部署测试：用于检查部署过程是否正确
业务导向且评价项目的测试其中非常tgsvr一种测试是：演示探索性测试，并不只是发现缺陷，它还会致使创建新的自动化集合易用性测试，为了验证用户是否能很容易使用该应用软件完成工作Beta测试，金丝雀发布，多个版本同时运行在生产环境，收集不同版本的数据，如果分析证明新功能无法带来足够的价值，就删除它
技术导向且评价项目的测试验收测试分两类：功能性测试，非功能性测试
测试替身
哑对象：那些被传递但不被真正使用的对象
假对象：可以真正使用的实现，但通常会利用一些捷径
桩：在测试中为每个调用提供一个封装好的响应，它通常不会对测试之外的请求进行响应，只用于测试
SPY：一种可记录一些关于它们如何被调用的信息的桩
模拟对象：一种在编程时就设定了它的预期要接收的调用

现实中的情况与应对策略新项目：一开始就要写自动化验收测试
选择技术平台和测试工具
建立一个简单的自动化构建
制定遵守INVEST原则【独立的，可协商的，有价值的，可估计的，小的，可测试的】用户故事及考虑其验收条件
客户、分析师和测试人员定义验收条件
测试人员与研发人员一起基于验收条件实现验收测试的自动化
开发人员编码来满足验收条件
只要有自动化测试失败，开发人员优先修复问题

项目进行中
引入自动化测试最好的方式是选择应用程序中那些最常见，最重要且高价值的用例为起点。
让测试覆盖的范围稍稍宽于通常的用户故事级别的验收测试。
如果发现对同一个功能重复进行了多次的手工测试，就判断该功能是否还会个性。如果不会，就将这个测试自动化，否则，说明这个测试覆盖的功能一直变化，可以与客户和开发确认后，把它从测试集合中先忽略掉，并尽可能详细地写注释
如果时间紧，最好利用各种各样的测试数据来确保一定的覆盖率

遗留系统
如果没有自动构建流程，最高优先级是创建一个自动构建流程，然后创建更多的自动化功能测试来丰富它
识别系统中高价值的功能，聚焦于系统中高价值的功能
基于高价值功能，创建一套广泛的自动化测试
逐渐为新增功能添加相应的测试
只写那些有价值的自动化测试，如果只是新增功能，而不需要修改提供支撑的框架代码时这部分代码不需要写全面的测试

集成测试集成测试：那些确保系统的每个独立部分都能够正确作用于其依赖的那些服务的测试集成测试应该在两种上下文中运行：

被测试的应用程序使用其真正依赖的外部系统来运行时，或使用由外部服务供应商所提供的替代系统
应用程序运行于你自己创建的一个测试用具之上确保在正式部署生产环境之前，应用程序不要与真实的外部系统进行交互，否则就要想办法告诉外部系统，应用所发送的数据只用于测试
在测试环境中使用“防火墙”，将该应用程序与外部系统隔离开来
在应用程序中用一组配置信息，让其与外部系统的模拟版本交互把关于集成的活动放到发布计划中是非常必要的。与外部系统的集成总是比较复杂，需要花时间并制定计划。每当增加一个外部系统集成点时，项目风险就会增，集成风险：
测试服务是否准备好了？它是否能正常运行？
外部服务供应商是否有足够的资源与人力来回答我们遇到的问题，修改缺陷，添加我们提出的一些定制功能?
我们是否能直接访问真实的生产环境，以便验证外部系统是否满足我们的容量要求或可用性要求？
外部服务提供的API是否很容易与我们自己开发应用软件时所采用的技术进行集成，我们团队是否需要某些专业技能才能使用这些API?
是否需要编写并维护我们的测试服务？
当外部系统的响应与我们所期望的行为不一致时，我们自己的应用程序是否能够正确地处理？
还需要构建与维护这个集成层及相关的运行时配置，测试服务与测试策略。

流程
找出最高优先级的测试场景
代码让这些验收条件变成可执行的测试
测试人员与研发人员在开发前应尽早一起讨论验收测试

管理待修复的缺陷列表
将待修复缺陷列表可视化
一种零缺陷，关注缺陷问题，并修复
像对待功能一样对待缺陷，将功能与缺陷一起做优先级排序，让开发按优先级进行开发

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（部署流水线）第七章：提交阶段</title>
    <url>/posts/6a949e5f3291.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
提交阶段的运行应该少于5分钟，一定不要超过10分钏提交阶段的首要目标是创建可部署的产物
提交阶段的原则与实践
提供快速有用的反馈
何时令提交阶段失败
编译错误
测试失败（包括单元覆盖率低于60%）


精心对待提交阶段
提交阶段中有构建用的脚本和运行单元测试、静态分析等脚本。
随着项目的进行，不断改进提交阶段的脚本的质量、设计和性能
确保将脚本做成模块化，将那些经常使用且很少变化的常见任务与需要修改的任务分开
将部署流水线中不同阶段所用的代码分别写在不同脚本中
不要写出与具体环境相关的脚本，即要把具体环境配置与构建脚本分离


让开发人员也拥有所有权如果必要的话，即使是很普通的变更也都应该由开发人员和运维人员来执行
在超大项目团队中指定一个构建负责人
监督和指导对构建的维护
鼓励和加强构建纪律
在团队开始接触持续集成时，构建纪律还没建立起来时，提醒作用  
团队成员轮流当，比如每星期轮换一次



提交阶段结果提交阶段的输入是源代码，输出是二进制包和报告（测试结果和代码分析报告）
制品库
制品库仅保存某些版本，而不是全部。如果在部署流水线某个阶段失败，就可以删除该版本
制品库中的二进制包能够追溯到具体的代码版本控制库中的版本
良好的配置管理策略，二进制文件的构建过程应该是可重复的

提交测试套件的原则与实践
提交阶段，测试绝大部分应由单元测试组成
设计 能够快速运行的提交测试策略
运行的单元测试不应该与文件系统、数据库、库文件、框架或外部系统等交互

提交测试实践
避免用户界面
使用依赖注入
避免使用数据库单元测试不应该依赖于数据库，需要把测试的代码与其存储分离开来。这就要求代码实现良好的分层，也需要使用依赖注入。如果实在无法做到，使用内存数据库
避免异步
使用测试替身模拟技术工具集：Mockito、Rhino、EasyMock、JMock、NMock、Mocha等
最少化测试中的状态降低要构造的测试环境的复杂性
时间的伪装对于那些需要确保一定延时或者定时的行为，需要对其中的时间系统进行控制。作者团队的经验是，只要代码中需要使用时间，就会抽象到对系统时间服务的请求，而不是直接在业务逻辑中调用它们
蛮力（测试阶段运行应该少于5分钟）
将提交测试分成多个套件，在多台机器上并行执行（构建网格）
作为构建优化过程的一部分，将那些运行时间长，且不经常失败的测试放在验收测试阶段



]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（部署流水线）第九章：非功能需求的测试</title>
    <url>/posts/f510512cbabb.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
性能、吞吐量、容量概念性能：对处理单一事务所花时间的一种度量，既可以单独衡量，也可以在一定的负载下衡量。吞吐量：系统在一定时间内处理事务的数量，通常它受限于系统中的某个瓶颈容量：一定的负载下，当每个单独请求的响应时间维持在可接受范围内时，系统所能承担的最大吞吐量。非功能性：有效性、容量、安全性、可维护性等。
非功能需求管理将非功能需求与功能需求一样对待。

创建一些具体任务来管理非功能需求
有必要的话，向功能需求中加入非功能需求的验收条件

如何为容量编程
为何要做容量测试高德纳著名格言：在97%的时间里，我们都应该忘记那种小的效率提升：过早优化是所有罪恶之根。然而，我们也不能让另外非常关键的3%的机会与我们擦肩而过。一个优秀程序员不会因为这个原则而对其置之不理，他们非常聪明，只会在识别出那段关键代码后，才会非常细心地去查看。在找到解决方案之前，必须先找出问题的根源。容量测试会告诉我们是否存在问题，以便我们可以修复它。不要枉自猜测，而要先进行度量。
解决容量问题现代软件系统中，最昂贵的是网络通信或磁盘存储，在性能和应用程序的稳定性方面，跨进程或网络边界的通信是昂贵的，所以这类通信应该尽量最小化。让业务干系人决定系统的容量特性极其重要，以免方案过度设计 。为解决容量问题，可采取的策略：


为应用程序决定一种架构。通常要特别注意进程、网络边界和I&#x2F;O。
了解并使用正确的模式，避免使用那些影响系统容量和稳定性的反模式。
确保团队在已经明确的应用架构下进行开发，不要为容量做无谓的优化。在没有明确测试结果表明有容量问题时，坚决不能在代码可读性上让步。
注意在数据结构和算法方面的选择，确保它们的属性与应用程序相吻合。
处理线程时要特别注意。
创建一些自动化测试来断言所期望的容量级别。当这些测试失败时，用它们作为向导来修复这些问题。
使用调测工具主要关注测试中发现的问题，并修复它。
只要有可能，就使用真实的容量数据来做度量。

容量度量
扩展性测试：随着服务器数、服务等的增加，单个请求的响应时间和并发用户数的支持会如何变化。
持久性测试：长时间运行应用程序，是否有性能上的变化。
吞吐量测试：系统每秒能处理多少事务、消息或页面点击。
负载测试：当系统负载增加到类似生产环境大小时，系统的容量如何。

容量度量测试遵行有两种策略

把目标设定为得到稳定、可重现的结果。专为容量测试准备一个环境。
一旦某个测试通过了最低验收标准，就把验收标准提高一点，调整该测试的成功门槛
每个测试都必须体现一个具体的场景，并且只有达到某个标准门槛时，才能认为该测试通过

容量测试环境
容量测试环境与生产环境一致。
如果无法提供与生产环境相似的环境，可以把容量测试作为金丝雀发布策略的一部分来执行。更频繁的发布可以减小影响应用程序容量的修改所带来的风险
容量测试环境尽可能与生产环境相似。这样虽然无法满足容量目标，但是可以把那些严重的问题突显出来
不要依据硬件的某种特定参数对程序的扩展性作出线性推论
复制应用程序一小部分的服务器进行容量测试，是一个既可以降低环境成本又能提供适当准确度量的策略

自动化容量测试
一般我们都是把容量测试当作一项独立的工作，但是当容量非常重要时，那么就暂且忽视这些时间成本 。这时需要在部署流水线中加入容量测试阶段。
创建一个自动化容量测试套件，且每次对应用程序进行修改后，通过了提交测试和验收测试就应该执行容量测试。
容量测试要达到如下6个目标


测试具体的现实场景
预先设定成功的门槛
尽可能让测试运行时间短一些
在变更面前要更健壮一些
组合成大规模的复杂场景
可重复的，并且既能串行执行，也能并行执行

容量测试系统的附加价值容量测试系统是一个试验场所，可以根据需要有效地控制时间，设计和执行所有的试验场景来帮助诊断问题、预测问题并找到触发问题办法。

重现生产环境中发现的复杂缺陷
探测并调试内存泄漏
持久性测试
评估垃圾回收的影响
垃圾回收的调优
应用程序参数的调优
第三方应用程序配置的调优，如操作系统
模拟非正常、最糟糕的情况
评估一些复杂问题的不同解决方案
模拟集成失败的情况
度量应用程序在不同硬件配置下的可扩展性
与外部系统进行交互的负载测试
复杂部署的回滚演练
有选择地使系统部分或全部瘫痪，从而评估服务优雅降级
在短期可用的生产硬件上执行真实世界的容量基准，以便计算出长期且低配的容量测试环境中更准确的扩展因素。

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（部署流水线）第五章：部署流水线解析</title>
    <url>/posts/416068ee4e18.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
什么是部署流水线部署流水线是指软件从版本控制到用户手中这一过程的自动化表现形式。价值流图



产品可行性评估

产品探索

产品计划与评估

开发

最后测试与审核

发布



3天
1周
7天
10天
10天
10天
3天
7周
1周
2天
2小时



开发到发布的流水线：会有很多次构建通过这一流程走向最后的发布
流水线各个阶段：交付团队-&gt;版本控制库-&gt;构建和单元测试-&gt;自动化验收测试-&gt;用户验收测试-&gt;发布
一般而言，只要某个构建使这个流程任一阶段失败，都会停止，不会进入下一个阶段。
提交阶段【自动化测试（主要是单元测试），代码分析】


自动化验收测试阶段【功能与非功能测试】
手工测试阶段【对自动化测试的补充，探索性测试，集成测试等】
发布阶段【部署到生产环境或试运行环境】

最基本的部署流水线  
部署流水线的相关实践
只生成一次二进制包。对于不需要编译的语言，二制包指的是所有源文件的集合。这些二进制包应保存在文件系统的某个位置，让流水线后续阶段能够轻松访问到，但不要放在版本控制库中。二进制包应与环境无关。
对不同环境采用同一部署方式
使用属性文件保存配置信息。比如分别为每个环境保存一个属性文件，并将其放在版本控制库中，部署时，通过本地服务器的主机名来查找到正确的配置，如果环境中有多台服务器，可以将环境变量提供给部署脚本
将配置放在一个目录服务中（LDAP或ActiveDiretory）或数据库中


对部署进行冒烟测试当应用程序部署时，应用一个自动化脚本做下冒烟测试。这个测试的流程是：
启动用户程序
检查主页面
检查应用程序所依赖的服务，比如数据库，消息队列等


向生产环境的副本中部署 如果预算充足，可以建立与生产环境一样的环境
每次变更都要立即在流水线中传递对于一些特殊情况，验收测试是比较耗时的，版本在验收测试时可能会产生冲突，这时可以在单元测试结束时，将最近还没构建的所有变更全部拿来进行构建
只要有环节失败，就停止整个流水线

提交阶段
编译代码
运行一套提交测试【单元测试，容易失败的特定测试】
为后续阶段创建二进制包
执行代码分析检查代码的健康状况
为后续阶段准备工作，比如准备后续测试所用的数据库

自动化验收测试之门
每次提交后，应立即运行提交测试，提交阶段完成后，立即做验收测试，简单的验收测试为：运行代码，查看主页
尽管验收测试非常有价值，但它们的创建与维护成本也非常高，所以牢记不要把所有验收测试条件盲目的自动化

后续的测试阶段部署流水线应支持测试人员根据自己的需求将任意一个版本部署到自己的测试环境

手工测试
非功能测试

发布准备把发布环节视为部署流水线的一个自然结果

让参与项目交付过程的人共同创建维护一个发布计划
通过尽可能多的自动化过程最小化人的错误发生的可能性
在类生产环境中经常做发布流程演练
如果事情并没有按计划执行，要有撤销本次发布的能力
作为升级和撤销过程的一部分，制定配置迁移和数据迁移策略

自动部署与发布

在具有代表性环境上执行自动化验收测试套件
对生产环境的任何修改都应该通过自动化过程完成【程序的部署，配置，软件栈，网络拓扑，状态的所有修改）
管理生产环境的流程，也应用于测试环境
使用虚拟化技术，最佳配置管理降低成本

变更的撤销策略

让旧版本仍旧处于可用状态，保持一段时间
从头部署旧版本

实现一个部署流水线
对价值流建模，创建一个可工作的简单框架
将构建和部署流程自动化
将单元测试和代码分析自动化
将验收测试自动化
将发布自动化注意以下几点：
增量实现整个流水线，如果有手工操作部分，记录开始结束时间，想办法把它自动化
部署流水线是构建、部署、测试和发布整个流程中有效，也是最重要的统计数据来源
不断改进部署流水线

度量最重要的全局度量指标是流水线周期时间。用约束理论来对流水线进行优化：

识别系统中的约束
确保最大限度地提高流程中这部分的产出
根据这一约束调整其他环节的产出
为约束环节扩容，增加资源
理顺约束环节，找到下一个约束点

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（部署流水线）第八章：自动化验收测试</title>
    <url>/posts/8f583cb100a6.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
验收测试通常是在每一个通过提交测试的软件版本上执行的。

验收测试的目的：对于一个单独的验收测试，它的目的是验证一个用户故事或需求的验收条件是否被满足。如功能验收条件和非功能验收条件。
如果每次提交测试后都在该版本上运行自动化验收测试，会有如下效果：
反馈环大大缩短，能够更快地定位问题
测试、开发人员和客户需要紧密合作才能创建一个良好的自动化测试套件，这会促进他们之间的良好合作
有助于让每个人更关注业务的价值


验收测试与单元测试的区别：验收测试是针对业务的，单元测试是面向开发的。

创建验收测试

分析人员与测试人员和客户紧密合作，定义验收条件



分析人员向开发人员讲解需求，以及它的业务上下文，并检查一遍验收条件



测试人员与开发人员讨论，并就“哪些自动化验收测试来证明验收条件被满足”达成一致



开发人员认为工作完成是指所有单元测试和组件测试通过，验收测试全部实现，并证明系统满足需求。此时可以向分析人员、测试人员和客户进行演示



应用程序驱动层应用程序驱动层是一个知道如何与应用程序打交道的层次。它所用的API是以某种领域语言表达的，可以认为是一种针对它自己的领域专属语言。
实现验收测试

让测试对复杂状态的依赖最小



不要使用生产数据的备份作为验收测试的测试数据，要维护一个受控的数据最小集



满足原子性，测试的执行顺序无关紧要，可以并行执行，他创建它需要的一切，并在运行后清理干净



自动化验收测试不应该运行在包含所有外部系统集成点的环境中，应该为自动验收测试提供一个受控环境，并且被测系统应该能在这个环境下运行。使用替身对象取代外部系统，可以使系统与外部系统耦合最小。



验收测试阶段把验收测试套件作为部署流水线的一个组成部分来运行。提交测试一旦成功，就应该开始在通过提交测试的版本上运行验收测试套件。

验收测试失败的版本不能用于部署
确保验收测试一直处于通过状态

验收测试的性能验收测试没有把它放在提交测试阶段，主要是因为验收测试运行时间太长。但是我们认为自动化验收测试的全面性比测试在10分钟内运行完成更重要。然而一旦开始实现部署流水线，快速失败体系和迅速反馈环将更快地帮助发现问题，一般验收测试花费几个小时完成也是可以接受的。但是仍然有办法来提高验收测试的效率。

重构通用任务
共享昂贵资源
并行测试
使用计算网格（如：Selenium Grid、 Zalenium）

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（部署流水线）第十章：应用程序的部署与发布</title>
    <url>/posts/4ddf67db285d.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
引言发布到生产环境和部署到测试环境的差异应该被封装在一组配置文件中，遵循一样的部署过程。启动自动部署系统，将要部署的软件版本与环境名称告诉它，点击开始，后缀部署与发布使用相同的流程。我们需要有一个列表，其中包含能够部署到每个环境的所有构建，并且只要通过点击就可以选择一个软件版本向某个环境进行自动部署。同时这种方式是对环境修改的唯一途径（包括对操作系统和第三方软件配置的修改）。
创建发布策略（文档）
每个环境的部署由发布由谁负责
创建一个资产和配置管理策略
部署时所用的技术的描述。运维团队与开发团队应对其达成共识
实现部署流水线的计划
枚举所有的环境，包括用于验收测试、容量测试、集成测试、用户验收测试的环境，以及每个构建在这些环境中的移动过程
描述在测试和生产环境中部署时应该遵循的流程，比如一个变更申请，及申请授权等
对应用程序的监控需求，包括用于通知运维团队关于应用程序相关状态的API和服务
讨论部署时和运行时的配置方法如何管理，以及它们与自动化部署流程是如何关联在一起的
描述应用程序如何与所有外部系统集成
如何记录日志详情，以便运维人员能够确定应用程序的状态，识别出错原因
制定灾难恢复计划，以便在灾难发生后，恢复应用程序的状态
对软件的服务级别达成一致
生产环境的数量大小 及容量计划：应用程序会创建多少数据，需要多少个日志文件或数据库，需要多少带宽或磁盘空间，用户对响应延迟的容忍度是什么？
制定一个归档策略，以便不必为了审计或技术支持而保留生产数据
如何对生产环境进行首次部署
如何修复生产环境中出现的缺陷，并为其打补丁
如何升级生产环境中的应用程序以及迁移数据
如何做应用程序生产服务与技术支持

随着项目的进行，这个策略文档也会改变
应用程序的部署与晋级每次部署时都使用同样的方法，使用相同的流程向每个环境部署，包括生产环境。在首次向测试环境部署时就应该使用自动化部署。
首次部署在第一个迭代里，选择一至两个具有高优先级但非常简单的用户故事或需求，让部署流水线的前几个阶段可以运行，且能够部署并展示一些成果。把它看作实现部署流水线的“抽水泵”这个启动迭代结束后，应该已经完成了以下内容：

部署流水线线的提交阶段
一个用于部署的类生产环境
通过一个自动化过程获取在提交阶段中生成的二进制包，并将其部署到这个类生产环境（UAT）中
一个简单的冒烟测试，用于验证本次部署的正确性

对发布过程进行建模并让构建晋级在构建中重点注意以下内容

为了达到发布质量，一个构建版本要通过哪些测试阶段
每个阶段需要设置怎样的晋级门槛或者需要怎样的签字许可
对每个晋级门槛来说，谁有权批准让该构建通过该阶段


配置的晋级除了二进制需要晋级，环境与应用程序的配置信息也需要晋级。但是并不是所有的配置都需要晋级，这就需要对配置信息进行晋级管理。

用冒烟测试来验证配置信息的正确性。
对于中间件的配置，利用像Nagios这样工具来监控这些设置。
写一些对基础设置的测试，用于检查关键设置，并将其返回给监控软件。

联合环境SIT环境中更多的工作是部署每个应用程序的新版本，直到所有应用程序可以互相联通。
部署到试运行环境项目开始时就需要计划以下事情：

确保生产环境、容量测试环境和试运行环境已准备好
准备好一个自动化过程，对环境进行配置，包括网络配置、外部服务和基础设置
确保部署流程是经过充分冒烟测试的
度量应用程序的“预热”时长。如果应用程序使用了缓存，这一点就尤其重要，将它也纳入到部署计划中
与外部系统进行测试集成
如果可能，发布之前就把应用程序放在生产环境上部署。蓝绿部署
如果可能，把应用程序发布给所有人之前，将它发布给一小部分用户群。金丝雀发布
将每次已通过验收测试的变更版本部署在试运行环境中

部署回滚和零停机发布制定回滚计划时，需要遵循两个原则：

发布前，确保生产系统的状态（数据库和保存在文件系统中的状态）已备份
每次发布之前都练习一下回滚计划，包括从备份中恢复或把数据库备份迁移回来

通过重新部署历史版本进行回滚优点：可预知时间内恢复，且风险较低；部署操作经过运行，而回滚频率低，所以回滚脚本更不稳定缺点：部署需要时间，业务有中断；覆盖部署，难以查找问题原因；新版本运行时产生的数据丢失。
零停机部署将用户从一个版本几乎瞬间转移到另一个版本，如果出问题，可以瞬间将用户转到碑的版本上。不同版本应用独立部署。
蓝绿部署保留两个相同的生产环境版本，“蓝环境”、“绿环境”。将新版本部署在“蓝环境”下，在蓝环境下测试完成后，将路由配置到蓝环境。如果出现问题，把路由器切回到绿环境上即可，此时蓝环境用于查找问题。#####蓝绿部署要小心管理数据库。解决办法如下：在切换之前暂时将应用程序变成只读状态一小段时间，将绿数据库复制一份，并恢复到蓝数据库中，执行迁移操作，再将用户切换到蓝系统。如果一切正常，再把应用程序切换到读写方式，如果出现问题，只要把它切回绿数据库就可以了。如果问题出现时，应用程序中已经写入了一些数据到蓝系统，那么切回去之前需要将新记录迁回到绿数据库中还可以找个办法让应用程序的新版本把数据库事务同时发向新旧两个数据库
如果只有一个生产环境，也可以使用蓝录部署。让应用程序两个副本一起运行在同一个环境中。
金丝雀发布金丝雀发布：把应用程序的某个新版本部署到生产环境中的部署服务器中，从而快速得到反馈。

部署新版本到一部分服务器上，对新版本上做冒烟测试，容量测试。
选择一部分用户，把他们引到新版本上
还可以部署多个版本，将不同组用户引导到不同版本上

好处：

非常容易回滚，只要不把用户引到有问题的版本
将用记引致新旧版本，从而作A&#x2F;B测试
可以通过逐渐增加负载，慢慢地把用户引到新版本，检验应用程序是否满足容量需求

紧急修复牢记：任何情况下，都不能破坏流程。紧急修复版本也需要走构建、部署、测试和发布流程。让每个紧急修复都走完标准的部署流水线。紧急修复的另一个做法是回滚到旧的好版本上。处理生产环境的缺陷时应用考虑以下因素：

别加班到深夜来做，应该与别人一起结对做
确保有一个已经测试过的紧急修复流程
对于应用程序的变更，避免绕过标准的流程，除非在极端情况下
确保在试运行环境上对紧急修复版本做过测试
有时候回滚比部署新的修复版本更划算

持续部署使用部署流水线、让部署到生产也自动化。如果某次提交的代码通过了所有的自动化测试，就直接部署到生产环境中。持续部署可以与金丝雀发布结合，先通过自动过程发布给一小部分用户，如果没有问题，就发布给所有用户。
小贴士
真正执行部署操作的人应该参与部署过程的创建
记录部署活动
不要删除旧文件，而是移动到别的位置
部署是整个团队的责任
服务器应用程序不应该有GUI
为新部署留预热期
快速失败
不要直接对生产环境进行修改

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>持续交付发布可靠软件的系统方法（部署流水线）第六章：构建与部署的脚本化</title>
    <url>/posts/4639aac39f9a.html</url>
    <content><![CDATA[《持续交付发布可靠软件的系统方法》读书笔记
##构建工具概览

Make
Ant
NAnt与MSBuild
Maven
Rake
Buildr
Psake

构建部署脚本化的原则与实践
为部署流水线的每个阶段创建脚本
使用恰当的技术部署应用程序
使用同样的脚本向所有环境部署
使用操作系统自带的包管理工具
确保部署流程是幂等的
部署系统的增量式演进

部署脚本化
多层的部署和测试


层
配置



应用／服务／组件
应用配置


中间件
中间件配置


操作系统
操作系统配置


硬件
硬件



测试环境配置 部署前对基础设施做标准冒烟测试，如果发现问题，就让环境配置流程快速失败，并给出测试结果
确认能从数据库中拿到一条记录
确认能连上网站
断言消息代理中的已注册的消息集合是正确的
透过防火墙发送ping，证明线路通畅



推荐策略
总是使用相对路径
消除手工步骤
从二进制包到版本控制库的内建可追溯性二进制包记录版本信息，如Java应用可以在MANIFEST中包含元数据，另外可以将构建流程生成的每个二进制包的MD5值及名字和版本标识符一起放在数据库中
不要把二进制包作为构建的一部分放到版本控制库中
“test”不应该让构建失败  在构建时如果有一个test任务失败，应该设置一个标记，继续构建，当生成更多的结果报告或者更完整的测试集后再令构建失败
用集成冒烟测试来限制应用程序
.NET项目中确保bin和obj这两个目录被完全删除

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>ansible通过跳板机管理另一个网络下的主机集群</title>
    <url>/posts/efe927f19448.html</url>
    <content><![CDATA[原文地址：http://wooooe.com/2018/07/31/remote_forwarding&#x2F;
ssh端口映射例子
因为公司的网络比较深所以经常需要跳转多次。所以这次做个记录
  
需求: 需要从your host跳到client中间隔了两层跳板机。
如果单纯用代理方法只能跳一层
所以如果用端口映射+代理方式就可以跳两层了
映射命令
ssh -g -f -NL 127.0.0.1:44010:172.16.3.14:22 -p 3391 jump_host1_username@222.222.222.222

意思就是将172.16.3.14的22端口映射到127.0.0.1的44010端口,222.222.222.222是中间的代理机，3391是222.222.222.222的ssh端口。
映射完成之后。执行
ssh -p 44010 jump_host2_username@127.0.0.1

就可以直接跳转到jump_host2上
ssh走代理方法
第一种:
ssh -o ProxyCommand=&quot;ssh -W %h:%p -p 3391 -q jump_host1_username@222.222.222.222&quot; jump_host2_username@172.16.3.14

第二种:
需要在你当前用户目录下的.ssh目录下建一个config文件
Host 192.122.150.*    Port 22    User anyone    ProxyCommand ssh -p 3391 jump_host1_username@222.222.222.222 -W %h:%p

意思就是当你在当前这个用户进行ssh连接时凡是匹配到192.122.150的ip都会使用222.222.222.222的3391端口作代理.
ansible使用代理的方法
当你用playbook时可以直接写在hosts文件做全局变量
[web]172.16.3.14 ansible_ssh_user=aaaa[web:vars]ansible_ssh_common_args=&#x27;-o ProxyCommand=&quot;ssh -W %h:%p -p 3391 -q jump_host1_username@222.222.222.222&quot;&#x27;

如果你想直接附加在命令行中，那么可以这么写
--ssh-common-args=&#x27;-o ProxyCommand=&quot;ssh -W %h:%p -p 3391 -q jump_host2_username@222.222.222.222&quot;&#x27;

如果你想调用ansible api走代理，那么可以这么写
Options = namedtuple(&#x27;Options&#x27;,                 [&#x27;connection&#x27;,                  &#x27;remote_user&#x27;,                  &#x27;ask_sudo_pass&#x27;,                  &#x27;verbosity&#x27;,                  &#x27;ack_pass&#x27;,                  &#x27;module_path&#x27;,                  &#x27;forks&#x27;,                  &#x27;become&#x27;,                  &#x27;become_method&#x27;,                  &#x27;become_user&#x27;,                  &#x27;ssh_common_args&#x27;,                  &#x27;check&#x27;,                  &#x27;listhosts&#x27;,                  &#x27;listtasks&#x27;,                  &#x27;listtags&#x27;,                  &#x27;syntax&#x27;,                  &#x27;sudo_user&#x27;,                  &#x27;sudo&#x27;,                  &#x27;diff&#x27;])options = Options(connection=&#x27;smart&#x27;,                   remote_user=None,                   ack_pass=None,                   sudo_user=None,                   forks=5,                   sudo=None,                   ask_sudo_pass=False,                   verbosity=5,                   module_path=None,                   become=None,                   become_method=None,                   become_user=None,                   ssh_common_args=sshCommonArgs,                   check=False,                   diff=False,                   listhosts=None,                   listtasks=None,                   listtags=None,                   syntax=None)

你需要把参数传给sshCommonArgs,例如
ssh_common_args=&#x27;-o ProxyCommand=&quot;ssh -W %h:%p -p 3391 jump_host1_username@222.222.222.222&#x27;
ansible tower设置
使用这种跳转方式，必须将settings-&gt;JOBS-&gt;ENABLE JOB ISOLATION设置为OFF状态
在Credentials中添加访问的私钥
Inventories中添加Jumper server Host
Inventories中添加要访问的Host，并添加ssh访问参数

ansible_ssh_common_args: &#x27;-o ProxyCommand=&quot;ssh -W %h:%p -p 22 -q 100.129.71.4&#x27;

通过这些设置后，ansible就可以像访问普通主机一样访问目的Host了。

说明：

如果私钥各不一样，可以通过HOST的参数设置，其中key放在ansible tower部署机器的某个目录下。

ansible_ssh_private_key_file: keyansible_ssh_common_args: &#x27;-o ProxyCommand=&quot;ssh -W %h:%p -p 22 -q 100.129.71.4&#x27;

ansible tower还有一种方式：Isolated Nodes它的方式与zabbix proxy的方式有点像，可参考如下资料https://www.ansible.com/blog/ansible-tower-feature-spotlight-instance-groups-and-isolated-nodes

]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>系统僵尸进程管理</title>
    <url>/posts/1554de812850.html</url>
    <content><![CDATA[
查看系统僵尸进程数top  

查看僵尸进程详情ps -ef|grep defunct  

停止僵尸进程kill -9 $Parent PID


]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>测试工具汇总</title>
    <url>/posts/8d5496ec675f.html</url>
    <content><![CDATA[压力测试JmeterJmeter简单介绍与搭配Jenkins实现自动化测试实践使用JMeter进行负载测试——终极指南
LocustLocust学习总结分享性能测试工具LocustLocust 系列教程
LoadRunnerUsing LoadRunner with Jenkins
接口测试Robot FrameworkRobot Framework 在 Jenkins 上的自动化测试
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>系统性能测试与监测工具汇总</title>
    <url>/posts/f94f9d30a93e.html</url>
    <content><![CDATA[综合能力性能测试（Unixbench）[root@localhost] $ ./Run
计算查看当前CPU负载（uptime）[root@localhost] $ uptime
测试单cpu计算能力（bc）[root@localhost] $ time echo &quot;scale=5000;4*a(1)&quot; | bc -l -q
内存查看当前内存使用的情况的状态（free）[root@localhost] $ free -m
进程对内存的占用情况（pmap） [root@localhost] $ pmap -d 35713 # 查看进程35713占用内存的情况
磁盘磁盘IO测试（dd）测试磁盘的IO写速度
# time dd if=/dev/zero of=test.dbf bs=8k count=300000 oflag=direct
测试磁盘的IO读速度
# dd if=test.dbf bs=8k count=300000 of=/dev/null 
表示每次写入&#x2F;读取8k的数据，执行300000次
实时查看各磁盘的io（iostat）[root@localhost] $ yum install sysstat[root@localhost] $ iostat -x 1 100
对磁盘iops的测试（fio）安装fio
[root@localhost] $ yum install fio
ioengine: 负载引擎，我们一般使用libaio，发起异步IO请求。bs: IO大小direct: 直写，绕过操作系统Cache。因为我们测试的是硬盘，而不是操作系统的Cache，所以设置为1。rw: 读写模式，有顺序写write、顺序读read、随机写randwrite、随机读randread等。size: 寻址空间，IO会落在 [0, size)这个区间的硬盘空间上。这是一个可以影响IOPS的参数。一般设置为硬盘的大小。filename: 测试对象iodepth: 队列深度，只有使用libaio时才有意义。这是一个可以影响IOPS的参数。runtime: 测试时长
4K随机写测试
[root@localhost] $ fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -size=100G -filename=/dev/vdb -name=&quot;EBS 4KB randwrite test&quot; -iodepth=32 -runtime=60
4K随机读测试
[root@localhost] $ fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randread -size=100G -filename=/dev/vdb -name=&quot;EBS 4KB randread test&quot; -iodepth=8 -runtime=60
512KB顺序写测试
[root@localhost] $ fio -ioengine=libaio -bs=512k -direct=1 -thread -rw=write -size=100G -filename=/dev/vdb -name=&quot;EBS 512KB seqwrite test&quot; -iodepth=64 -runtime=60
进程对磁盘的读写情况（iotop）安装iotop
[root@localhost] $ yum install iotop
运行iotop查看进程对磁盘的读写
[root@localhost] $ iotop   #查看全部进程的磁盘读写情况[root@localhost] $ iotop -o   #实时查看当前进程对磁盘的读写（推荐）[root@localhost] $ iotop -p 34323   #查看进程号为34323对磁盘的读写情况
监控告警可以使用如下命令获取io的数据
[root@localhost] $ iotop -botqqq --iter=3
网络网络测试（iperf)启动服务端
[root@localhost] $ yum install iperf3 -y[root@localhost] $ iperf3 -s
客户端进行测试
[root@localhost] $ yum install iperf3 -y[root@localhost] $ iperf3 -c 10.2.2.2 -P 5
以上默认为作TCP测试，如果要UDP测试，服务端启动与客户端测试都需要加上-u
## 服务端iperf3 -s -u## 客户端iperf3 -c 10.2.2.2 -P 5 -u

网络测试（qperf)启动服务端
[root@localhost] $ yum install qperf -y[root@localhost] $ qperf
客户端进行测试网络延时
[root@localhost] $ yum install qperf -y[root@localhost] $ qperf 10.2.2.2 -t 100 -oo msg_size:8:256K:*2 tcp_lat
客户端进行测试网络带宽
[root@localhost] $ yum install qperf -y[root@localhost] $ qperf 10.2.2.2 -t 100 -oo msg_size:8:256K:*2 tcp_bw
延时与带宽可以一起测试
[root@localhost] $ qperf 10.2.2.2 -t 100 -oo msg_size:8:256K:*2 tcp_bw tcp_lat
网卡流量监测（nload）安装nload
[root@localhost] $ yum install nload
监测网卡流量输入及输出
[root@localhost] $ nload eth0 eth1
DNS服务压测工具（queryperf）queryperf [-d datafile] [-s server_addr] [-p port] [-q num_queries]-d: 后面接上一个文件，文件的内容是用户对DNS的请求，一行为一条请求，所以为了测试，我们可以在里面写上几千几万条。-s: DNS服务器地址-p: DNS服务器端口-q: 请求多少次
使用vim命令先创建一个请求文件：vim querytest.txt
www.example.com Aexample.com NStftp.example.com CNAMEblog.example.com A....# 600万行
执行测试命令
[root@localhost] $ queryperf -d querytest.txt -s 192.168.0.6
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>通过跳板机代理将本地文件传输到远端服务器</title>
    <url>/posts/f62135323f93.html</url>
    <content><![CDATA[
通过-J参数指定跳板机

$ ssh -J user@&lt;bastion:port&gt; &lt;user@remote:port&gt;// scp$ scp -J user@&lt;bastion:port&gt; sourcefile &lt;user@remote:port&gt;:

在.ssh&#x2F;config中配置

### The Bastion HostHost jump  HostName 10.2.3.2  Port 3022  User jump### The Remote HostHost remote  HostName 172.32.2.7  Port 22  User root  ProxyJump jump
使用普通参数就可以实现跳转
$ ssh remote$ scp &lt;sourcefile&gt; remote:/data/
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>项目的通用结构</title>
    <url>/posts/70668969ef36.html</url>
    <content><![CDATA[项目的通用结构，可根据真实情况进行删减
- src #代码- data #需要保存的文件，如安装包、镜像等- docs # 说明文档- scripts #相关的脚本文件- hack  - build    - Dockerfile  # 构建镜像  - Vagrantfile #Vagrant配置  - ansible/    # ansible构建部署脚本    - site.yml    - hosts    - roles/    - group_vars/    - host_vars/    - library/  - kubernetes/  # Openshift平台构建部署脚本  - docker-compose # Docker compose部署    - conf    - docker-compose.yaml- Makefile # 不同环境构统一构建入口- Jenkinsfile # Jenkins构建流水线，可调用ansible脚本或Dockerfile脚本- README.md- CHANGELOG.md- LICENSE- .travis.yml
]]></content>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible的k8s模块完全兼容OpenShift资源</title>
    <url>/posts/558c69203e45.html</url>
    <content><![CDATA[Ansible 的 k8s 模块完全兼容 openshift 资源
安装
使用 k8s 模块需要安装 openshift 的 python 扩展

$ pip install --ignore-installed openshift

安装过程中报如下错误TypeError: find_packages() got an unexpected keyword argument include需要升级 setuptools
$ pip install -U setuptools


在执行 k8s 命令的主机上登录 kubernetes&#x2F;openshift 平台

$ oc login https://master.example.com:8443 -u admin -ppassword

测试
使用 ansible k8s 模块创建 namespace

$ oc -i hosts all -m k8s -a &#x27;state=present name=testproject kind=Project&#x27;


使用 ansible k8s 模块基于 dc.yaml 文件创建 dc


创建 dc.yaml 文件

$ oc run nginx --image=nginx -n testproject --dry-run -o yaml

将内容输入到 nginx-dc.yaml 文件中,去掉些默认值，添加 namespace
$ cat nginx-dc.yamlapiVersion: v1kind: DeploymentConfigmetadata:  labels:    run: nginx  name: nginx  namespace: testprojectspec:  replicas: 1  selector:    run: nginx  template:    metadata:      labels:        run: nginx    spec:      containers:      - image: nginx        name: nginx


使用 ansible 创建

$ ansible -i hosts all -m k8s -a &#x27;state=present src=/root/sample/nginx-dc.yaml&#x27;


使用 ansible k8s 模块基于模板文件创建 dc


创建 playbook 文件

< EOF >> playbook.yaml]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Calico-BGP集群外部与集群网络打通</title>
    <url>/posts/724adf367d17.html</url>
    <content><![CDATA[把这几个路由规则加入到路由器或者客户端机器上
route add -net 10.233.55.64 netmask 255.255.255.192 gw 10.21.21.76route add -net 10.233.119.0 netmask 255.255.255.192 gw 10.21.21.74route add -net 10.233.183.64 netmask 255.255.255.192 gw 10.21.21.75
Win10客户端机器
route add -p 10.233.97.128 mask  255.255.255.192 192.168.1.225route add -p 10.233.75.0  mask  255.255.255.192 192.168.1.226route add -p 10.233.100.128 mask 255.255.255.192 192.168.1.227route add -p 10.233.90.0 mask 255.255.255.192 192.168.1.228route add -p 10.233.75.64 mask 255.255.255.192 192.168.1.42route add -p 10.233.71.0 mask 255.255.255.192 192.168.1.43route add -p 10.233.74.64 mask 255.255.255.192 192.168.1.44route add -p 10.233.102.128 mask 255.255.255.192 192.168.1.41
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS上OpenLDAP-Server使用cn=config方式配置</title>
    <url>/posts/78dd6e98294c.html</url>
    <content><![CDATA[翻译自How to Configure OpenLDAP server in Red Hat Enterprise Linux 7 using cn&#x3D;config method ? 
环境
CentOS 7
openldap-2.4

问题
使用slapd工具在CentOS上安装与配置OpenLDAP Server
使用olc方式如何配置OpenLDAP Server

操作
安装openldap包

yum install -y openldap openldap-clients openldap-servers
LDAP默认的配置目录是&#x2F;etc&#x2F;openldap&#x2F;slapd.d
[root@rhel7 ~]# cd /etc/openldap/slapd.d/[root@rhel7 slapd.d]# ls -ldrwxr-x---. 4 ldap ldap 4096 Aug  3 22:49 cn=config-rw-------. 1 ldap ldap  589 Aug  2 23:39 cn=config.ldif[root@rhel7 slapd.d]# cd cn\=config[root@rhel7 cn=config]# ls -ldrwxr-x---. 2 ldap ldap 4096 Aug  3 22:11 cn=schema-rw-------. 1 ldap ldap  378 Aug  2 22:21 cn=schema.ldif-rw-------. 1 ldap ldap  552 Aug  3 20:42 olcDatabase=&#123;0&#125;config.ldif-rw-------. 1 ldap ldap  443 Aug  2 22:21 olcDatabase=&#123;-1&#125;frontend.ldif-rw-------. 1 ldap ldap  562 Aug  2 22:21 olcDatabase=&#123;1&#125;monitor.ldifdrwxr-x---. 2 ldap ldap   65 Aug  3 22:53 olcDatabase=&#123;2&#125;hdb.ldif

设置olcSuffix,设置domain。只需要更新下文件olcDatabase&#x3D;{2}hdb.ldif中的olcSuffix

[root@rhel7 cn=config]# vi /etc/openldap/slapd.d/cn\=config/olcDatabase\=\&#123;2\&#125;hdb.ldifolcSuffix: dc=example,dc=com

推荐先创建一个专用的具有所有权限的账号，用它来更新LDAP的数据库。需要更新文件olcDatabase&#x3D;{2}hdb.ldif中的olcRootDN和olcRootPW

[root@rhel7 cn=config]# vi /etc/openldap/slapd.d/cn\=config/olcDatabase\=\&#123;2\&#125;hdb.ldifolcRootDN: cn=Manager,dc=example,dc=comolcRootPW: redhat

同时设置config数据库的管理员账号这里设置的密码很简单，请在生产环境下将密码设置复杂些

[root@rhel7 cn=config]# vi olcDatabase\=\&#123;0\&#125;config.ldifolcRootDN: cn=configolcRootPW: secret

使用slaptest命令去验证配置

[root@rhel7 cn=config]# slaptest -uconfig file testing succeeded
如果报ldif_read_file: checksum error这样的错误，是因为计算文件的CRC32码来自动校验（文件中前两行中带有校验码），需要更新校验码

删除掉报错的文件的前两行 # AUTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify.# CRC32 3e515b74
安装crc32工具（perl-Archive-Zip）[root@rhel7 cn=config]# yum install perl-Archive-Zip -y
计算新文件的CRC32码  [root@rhel7 cn=config]# crc32 &lt;(cat olcDatabase\=\&#123;2\&#125;hdb.ldif)509f92c7
将计算的CRC32码更新到文件中，添加到文件的前两行  # AUTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify.# CRC32 509f92c7


如果配置验证成功，启动openldap server

$ systemctl start slapd

创建ldap server的基础结构

$ cat base.ldifdn: dc=example,dc=comobjectClass: dcObjectobjectClass: organizationo: example.comdn: ou=users,dc=example,dc=comobjectClass: organizationalUnitobjectClass: topou: usersdn: ou=groups,dc=example,dc=comobjectClass: organizationalUnitobjectClass: topou: groups       

使用ldapadd命令向LDAP目录导入基础结构

$ ldapadd -x -D &quot;cn=Manager,dc=example,dc=com&quot; -w redhat -f base.ldifadding new entry &quot;dc=example,dc=com&quot;adding new entry &quot;ou=users,dc=example,dc=com&quot;adding new entry &quot;ou=groups,dc=example,dc=com&quot;

使用ldapsearch命令验证基础结构是否成功导入

$ ldapsearch -x -D &#x27;cn=Manager,dc=example,dc=com&#x27; -b dc=example,dc=com -w redhat

在向ldap结构中添加users和groups之前，先使用ldapadd导入schema，以免出现“object class not defined”的报错

$ ldapadd -Y EXTERNAL -H ldapi:// -f /etc/openldap/schema/cosine.ldif $ ldapadd -Y EXTERNAL -H ldapi:// -f /etc/openldap/schema/nis.ldif $ ldapadd -Y EXTERNAL -H ldapi:// -f /etc/openldap/schema/inetorgperson.ldif

向ldap结构中添加users和groups


Users
使用slappasswd命令获取需要的密码# slappasswd -s test&#123;SSHA&#125;5rMM/3f8Ki13IyarGTtwzieoTu7KMgwc
使用ldif文件和上面的hashed密码创建users.ldif$ cat users.ldifdn: uid=testuser,ou=users,dc=example,dc=comuid: testusercn: testuserobjectClass: shadowAccountobjectClass: topobjectClass: personobjectClass: inetOrgPersonobjectClass: posixAccountuserPassword: &#123;SSHA&#125;5rMM/3f8Ki13IyarGTtwzieoTu7KMgwcshadowLastChange: 17016shadowMin: 0shadowMax: 99999shadowWarning: 7loginShell: /bin/bashuidNumber: 1000gidNumber: 1000homeDirectory: /home/testusersn: testusermail: testuser@example.comdn: uid=testuser1,ou=users,dc=example,dc=comuid: testuser1cn: testuser1objectClass: shadowAccountobjectClass: topobjectClass: personobjectClass: inetOrgPersonobjectClass: posixAccountuserPassword: &#123;SSHA&#125;5rMM/3f8Ki13IyarGTtwzieoTu7KMgwcshadowLastChange: 17016shadowMin: 0shadowMax: 99999shadowWarning: 7loginShell: /bin/bashuidNumber: 1001gidNumber: 1001homeDirectory: /home/testuser1sn: testuser1mail: testuser1@example.com


Groups  创建groups.ldif$ cat groups.ldifdn: cn=testuser,ou=groups,dc=example,dc=comobjectClass: posixGroupobjectClass: topcn: testuseruserPassword: &#123;crypt&#125;xgidNumber: 1000dn: cn=testuser1,ou=groups,dc=example,dc=comobjectClass: posixGroupobjectClass: topcn: testuser1userPassword: &#123;crypt&#125;xgidNumber: 1001


使用ldapadd命令添加users和groups

$ ldapadd -x -D cn=Manager,dc=example,dc=com -f users.ldif -w redhat$ ldapadd -x -D cn=Manager,dc=example,dc=com -f groups.ldif -w redhat

使用ldapsearch命令来验证users和groups创建是否成功

$ ldapsearch -x -D cn=Manager,dc=example,dc=com -b dc=example,dc=com -w redhat
OpenLDAP的操作
添加数据

 $ cat user3.ldifdn: uid=testuser3,ou=users,dc=example,dc=comuid: testuser3cn: testuser3objectClass: shadowAccountobjectClass: topobjectClass: personobjectClass: inetOrgPersonobjectClass: posixAccountuserPassword: &#123;SSHA&#125;5rMM/3f8Ki13IyarGTtwzieoTu7KMgwcshadowLastChange: 17016shadowMin: 0shadowMax: 99999shadowWarning: 7loginShell: /bin/bashuidNumber: 1000gidNumber: 1000homeDirectory: /home/testusersn: testuser3mail: testuser@example.com$ ldapadd -x -D &#x27;cn=Manager,dc=example,dc=com&#x27; -w redhat -f user3.ldif

查询数据

$ ldapsearch -x -b &#x27;dc=example,dc=com&#x27;$ ldapsearch -x -b &#x27;dc=example,dc=com&#x27; &#x27;sn=testuser3&#x27;

修改数据

# 方法一：交互式$ ldapmodify -x -D &#x27;cn=Manager,dc=example,dc=com&#x27; -w redhatdn: uid=testuser3,ou=users,dc=example,dc=comchangetype: modifyreplace: snsn: Test User 3-add: descriptiondescription: add attribute# 方法二：配置文件式$ cat modify.ldifdn: uid=testuser3,ou=users,dc=example,dc=comchangetype: modifyreplace: snsn: Test User 3-delete: description$ ldapmodify -x -D &#x27;cn=Manager,dc=example,dc=com&#x27; -w redhat -f modify.ldif

删除数据

$ ldapdelete -x -D &#x27;cn=Manager,dc=example,dc=com&#x27; -w redhat uid=testuser3,ou=users,dc=example,dc=com

数据导出

# 方法一：用查询导出$ ldapsearch -x -b &#x27;dc=example,dc=com&#x27; &gt; export.ldif# 方法二：导出命令$ slapcat -l export.ldif
打开OpenLDAP日志
更新日志级别

$ cat loglevel.ldif dn: cn=configchangetype: modifyreplace: olcLogLevelolcLogLevel: stats$ ldapmodify -Y EXTERNAL -H ldapi:/// -f loglevel.ldif$ systemctl restart slapd

修改rsyslog配置文件

$ touch /var/log/slapd.log$ echo local4.* /var/log/slapd.log &gt;&gt; /etc/rsyslog.conf$ systemctl restart rsyslog

查看openldap日志

$ tail -f /var/log/slapd.log
容器化部署$ docker run --name ldap_core -p 389:389 -p 636:636 --env LDAP_ORGANISATION=&quot;example.com&quot; --env LDAP_DOMAIN=&quot;example.com&quot; --env LDAP_ADMIN_PASSWORD=&quot;redhat&quot; --detach osixia/openldap$ docker run --name ldap_web -p 80:80 -p 443:443 --link ldap_core:ldap_core --env PHPLDAPADMIN_LDAP_HOSTS=ldap_core --detach osixia/phpldapadmin
同时还有Openshift的openldap容器openshift&#x2F;openldap-2441-centos7
工具
浏览ldap目录列表的工具ldapbrowser
LDAPSearch命令参数详情

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS上搭建双主高可用OpenLDAP-Server</title>
    <url>/posts/60ecf55a7e34.html</url>
    <content><![CDATA[OpenLDAP单机搭建手册参考：CentOS上OpenLDAP Server使用cn&#x3D;config方式配置
配置双主高可用OpenLDAP
准备两台centos 7服务器，作为两台OpenLDAP Server的运行主机




Server ID
系统版本
IP



1
centos 7
192.168.1.2


2
centos 7
192.168.1.3



按照CentOS上OpenLDAP Server使用cn&#x3D;config方式配置的方式在两台主机上部署好OpenLDAP Server。做到第6步即可，即启动openldap server
在两台主机上启动syncprov模块

$ cat syncprov_mod.ldifdn: cn=module,cn=configobjectClass: olcModuleListcn: moduleolcModulePath: /usr/lib64/openldapolcModuleLoad: syncprov.la$ ldapadd -Y EXTERNAL -H ldapi:/// -f syncprov_mod.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry &quot;cn=module,cn=config&quot;

启动OpenLDAP主主同步在两台机器上创建configrep.ldif文件，并执行配置

$ cat configrep.ldif### Update Server ID with LDAP URL ###dn: cn=configchangetype: modifyreplace: olcServerIDolcServerID: 1 ldap://192.168.1.2olcServerID: 2 ldap://192.168.1.3### Enable replication ###dn: olcOverlay=syncprov,olcDatabase=&#123;2&#125;hdb,cn=configchangetype: addobjectClass: olcOverlayConfigobjectClass: olcSyncProvConfigolcOverlay: syncprov### Adding details for replication ###dn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyadd: olcSyncReplolcSyncRepl:  rid=001  provider=ldap://192.168.1.2  binddn=&quot;cn=Manager,dc=example,dc=com&quot;  bindmethod=simple  credentials=redhat  searchbase=&quot;dc=example,dc=com&quot;  type=refreshAndPersist  retry=&quot;5 5 300 5&quot;  timeout=1olcSyncRepl:  rid=002  provider=ldap://192.168.1.3  binddn=&quot;cn=Manager,dc=example,dc=com&quot;  bindmethod=simple  credentials=redhat  searchbase=&quot;dc=example,dc=com&quot;  type=refreshAndPersist  retry=&quot;5 5 300 5&quot;  timeout=1-add: olcMirrorModeolcMirrorMode: TRUE$ ldapmodify -Y EXTERNAL -H ldapi:/// -f configrep.ldif

配置ldap启动host，更新/etc/sysconfig/slapd

$ cat /etc/sysconfig/slapdSLAPD_URLS=&quot;ldapi:/// ldap://192.168.1.2&quot;
192.168.1.2、192.168.1.3根据主机ip确定。6. 重启slapd
$ systemctl restart slapd
至此，dc&#x3D;example,dc&#x3D;com 下的内容便可以在两个服务器上同步了。
实践测试
在ldap1服务器192.168.1.2中创建ldap server的基础结构

$ cat base.ldifdn: dc=example,dc=comobjectClass: dcObjectobjectClass: organizationo: example.comdn: ou=users,dc=example,dc=comobjectClass: organizationalUnitobjectClass: topou: usersdn: ou=groups,dc=example,dc=comobjectClass: organizationalUnitobjectClass: topou: groups       $ ldapadd -h 192.168.1.2 -x -D &quot;cn=Manager,dc=example,dc=com&quot; -w redhat -f base.ldif

检查ldap1和ldap2中的数据对比

## 检查ldap1 192.168.1.2中的数据$ ldapsearch -h 192.168.1.2 -x -D &#x27;cn=Manager,dc=example,dc=com&#x27; -b dc=example,dc=com -w redhat# extended LDIF                                                                                                                                                                                                   #                                                                                                                                                                                                                 # LDAPv3                                                                                                                                                                                                          # base &lt;dc=example,dc=com&gt; with scope subtree                                                                                                                                                                     # filter: (objectclass=*) 19L, 623C# requesting: ALL## example.comdn: dc=example,dc=comobjectClass: dcObjectobjectClass: organizationo: example.comdc: example# users, example.comdn: ou=users,dc=example,dc=comobjectClass: organizationalUnitou: users# groups, example.comdn: ou=groups,dc=example,dc=comobjectClass: organizationalUnitou: groups# search resultsearch: 2result: 0 Success# numResponses: 4# numEntries: 3
与ldap2 192.168.1.3中的数据进行对比
## 检查ldap2 192.168.1.3中的数据$ ldapsearch -h 192.168.1.3 -x -D &#x27;cn=Manager,dc=example,dc=com&#x27; -b dc=example,dc=com -w redhat# extended LDIF                                                                                                                                                                                                   #                                                                                                                                                                                                                 # LDAPv3                                                                                                                                                                                                          # base &lt;dc=example,dc=com&gt; with scope subtree                                                                                                                                                                     # filter: (objectclass=*) 19L, 623C# requesting: ALL## example.comdn: dc=example,dc=comobjectClass: dcObjectobjectClass: organizationo: example.comdc: example# users, example.comdn: ou=users,dc=example,dc=comobjectClass: organizationalUnitou: users# groups, example.comdn: ou=groups,dc=example,dc=comobjectClass: organizationalUnitou: groups# search resultsearch: 2result: 0 Success# numResponses: 4# numEntries: 3
数据是一样的，注意：ldap2 192.168.1.3服务器的数据是从192.168.1.2中同步过来的。3. 可以对ldap2 192.168.1.3中进行数据更新，再查看ldap1 192.168.1.2中的数据。确认互为主在ldap2中添加用户
$ cat users.ldifdn: uid=testuser,ou=users,dc=example,dc=comuid: testusercn: testuserobjectClass: shadowAccountobjectClass: topobjectClass: personobjectClass: inetOrgPersonobjectClass: posixAccountuserPassword: &#123;SSHA&#125;5rMM/3f8Ki13IyarGTtwzieoTu7KMgwcshadowLastChange: 17016shadowMin: 0shadowMax: 99999shadowWarning: 7loginShell: /bin/bashuidNumber: 1000gidNumber: 1000homeDirectory: /home/testusersn: testusermail: testuser@example.com$ ldapadd  -h 192.168.1.3 -x -D cn=Manager,dc=example,dc=com -f users.ldif -w redhat

参考文章OpenLDAP 极速搭建：双主同步
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph的搭建流程及openshift上使用ceph-rbd实现动态存储</title>
    <url>/posts/e6f145ab3bbb.html</url>
    <content><![CDATA[Ceph分布式块存储部署机器列表


名称
核数
内存
ip
hostname
外挂磁盘大小（G）



管理节点admin
2
4
192.168.1.2
admin.ceph.com



监控节点monitor
2
4
192.168.1.3
monitor.ceph.com



存储节点node1
2
4
192.168.1.4
node1.ceph.com
100G


存储节点node2
2
4
192.168.1.5
node2.ceph.com
100G


部署Ceph RBD
1、给每台机器设置hostname

# 设置hostname  hostnamectl --static set-hostname admin.ceph.com   #192.168.1.2 hostnamectl --static set-hostname monitor.ceph.com #192.168.1.3 hostnamectl --static set-hostname node1.ceph.com   #192.168.1.4 hostnamectl --static set-hostname node2.ceph.com   #192.168.1.5

2、给每个服务器创建用户ceph

# 添加用户ceph ansible -i hosts all -m user -a &quot;name=ceph&quot;# 登录每台机器使用passwd命令给每个用户创建密码ceph# 创建ceph用户密码 [root@admin ~]# passwd cephChanging password for user ceph.New password: cephBAD PASSWORD: The password is shorter than 8 charactersRetype new password: cephpasswd: all authentication tokens updated successfully.# 为每台服务器上的ceph用户添加root权限(用root用户登录)# 为用户添加root权限 ansible -i hosts all -m shell -a &#x27;echo &quot;ceph ALL=(root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ceph&#x27;ansible -i hosts all -m shell -a &#x27;chmod 0440 /etc/sudoers.d/ceph&#x27;


为admin节点设置访问其它服务器免密码登录


[ceph@admin ~]$ ssh-keygen Generating public/private rsa key pair.Enter file in which to save the key (/home/ceph/.ssh/id_rsa): Created directory &#x27;/home/ceph/.ssh&#x27;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/ceph/.ssh/id_rsa.Your public key has been saved in /home/ceph/.ssh/id_rsa.pub.The key fingerprint is:SHA256:rH/HNUPm4HPtxOXzbndOwGzpy6bwA1frhW9S5cywl2Q ceph@admin.ceph.comThe key&#x27;s randomart image is:+---[RSA 2048]----+|                 ||                 ||                 ||       .    .o=Eo||        S  . *O%+||       .  . +oX+%||      .   .+ =.X+||       .  .o+.+oO||        .. .o+o*=|+----[SHA256]-----+[ceph@admin ~]$ ssh-copy-id ceph@monitor.ceph.com[ceph@admin ~]$ ssh-copy-id ceph@node1.ceph.com[ceph@admin ~]$ ssh-copy-id ceph@node2.ceph.com

创建集群

# admin.ceph.com创建my-cluster文件夹 $ ssh ceph@admin$ mkdir my-cluster$ cd my-cluster# 安装ceph-deploy# 安装ceph-deploy yum install ceph-deploy# 清除之前的配置数据# 清除之前的数据 ceph-deploy uninstall admin.ceph.com monitor.ceph.com node1.ceph.com node2.ceph.com# 清除远程主机/var/lib/ceph /etc/ceph中的包和数据# 清除远程主机/var/lib/ceph /etc/ceph中的包和数据 ceph-deploy purge admin.ceph.com monitor.ceph.com node1.ceph.com node2.ceph.com# 清除/var/lib/ceph及/etc/ceph下ceph目录及以下内容全部# 清除/var/lib/ceph及/etc/ceph下ceph目录及以下内容全部： ceph-deploy purgedata admin.ceph.com monitor.ceph.com node1.ceph.com node2.ceph.com# 清除my-cluster目录中的认证密钥文件# 清除my-cluster目录中的认证密钥文件 ceph-deploy forgetkeys# 关闭所有节点的防火墙及安全防护项（青云平台机器默认是不开启的，这步可以不用做）# 关闭所有节点的防火墙及安全防护项 ansible -i ceph-hosts all -m service -a &#x27;name=iptables state=stopped&#x27;ansible -i ceph-hosts all -m shell -a &#x27;setenforce 0&#x27;# 创建集群 $ ceph-deploy new monitor.ceph.com# 在~/my-cluster下会生成三个文件# ~/my-cluster下会生成三个文件 $ lltotal 24-rw-rw-r-- 1 ceph ceph   251 Jan 12 16:34 ceph.conf-rw-rw-r-- 1 ceph ceph 15886 Jan 12 16:30 ceph.log-rw------- 1 ceph ceph    73 Jan 12 16:30 ceph.mon.keyring# 系统默认的osd pool为3，目前osd为2，需要修改默认值# 修改osd默认值为2 [ceph@admin my-cluster]$ cat ceph.conf [global]fsid = 25c13add-967e-4912-bb33-ebbc2cb9376dmon_initial_members = monitormon_host = 192.168.1.3auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxfilestore_xattr_use_omap = trueosd pool default size=2# 部署安装ceph ceph-deploy install admin.ceph.com monitor.ceph.com node1.ceph.com node2.ceph.com# 创建Ceph Monitor $ ceph-deploy mon create monitor.ceph.com $ ceph-deploy gatherkeys monitor.ceph.com# 创建ceph osd# 切到node1.ceph.com给node1数据节点挂载磁盘/dev/sdc# 挂载磁盘 $ sudo mkfs.xfs -f /dev/sdc$ mkdir /var/lib/ceph/osd/osd-0$ sudo mount /dev/sdc /var/lib/ceph/osd/osd-0$ sudo chown ceph:ceph /var/lib/ceph/osd/osd-0# 同样的方法把node2.ceph.com的磁盘挂载到/var/lib/ceph/osd/osd-1# 挂载磁盘 $ sudo mkfs.xfs -f /dev/sdc$ mkdir /var/lib/ceph/osd/osd-1$ sudo mount /dev/sdc /var/lib/ceph/osd/osd-1$ sudo chown ceph:ceph /var/lib/ceph/osd/osd-1# 准备并激活osd，返回到admin管理节点# 准备并激活ceph osd $ ceph-deploy osd prepare node1.ceph.com:/var/lib/ceph/osd/osd-0 node2.ceph.com:/var/lib/ceph/osd/osd-1$ ceph-deploy osd activate node1.ceph.com:/var/lib/ceph/osd/osd-0 node2.ceph.com:/var/lib/ceph/osd/osd-1# 拷贝配置文件及key文件$ ceph-deploy admin admin.ceph.com monitor.ceph.com node1.ceph.com node2.ceph.com# 为ceph.clinet.admin.keyring添加可读权限（admin节点与monitor节点都添加）# 添加读取权限  $ sudo chmod +r /etc/ceph/ceph.client.admin.keyring# 查看集群状态$ ceph health$ ceph osd tree #查看当前节点# 完成！！！
Openshift上创建RBD Storageclass

首先查看my-cluster文件夹下的ceph.client.admin.keyring文件


$ cat ceph.client.admin.keyring [client.admin]	key = AQBUilha86ufLhAA2BxJn7sG8qVYndokVwtvyA==	caps mds = &quot;allow *&quot;	caps mon = &quot;allow *&quot;	caps osd = &quot;allow *&quot;# 使用admin的key在openshift上创建secret


创建secret私钥


oc  create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; --from-literal=key=&#x27;AQBUilha86ufLhAA2BxJn7sG8qVYndokVwtvyA==&#x27; --namespace=kube-system


在需要使用ceph-rbd的project也需要添加secret


oc  create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; --from-literal=key=&#x27;AQBUilha86ufLhAA2BxJn7sG8qVYndokVwtvyA==&#x27; --namespace=project


创建storageclass


# storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: ceph-rbd-scprovisioner: kubernetes.io/rbdparameters:  monitors: 192.168.1.3:6789  adminId: admin  adminSecretName: ceph-secret  adminSecretNamespace: kube-system  pool: rbd  userId: admin  userSecretName: ceph-secret# oc create -f storageclass.yaml
说明:adminId默认值为admin,pool默认值为rbd, userId默认值与adminId一样.所以这三个值可以不填写。


创建PVC(可通过yaml创建也可以在openshift的webconsole中选择对应的storageclass创建)


# ceph-rbd-pvc.yaml kind: PersistentVolumeClaimapiVersion: v1metadata:  name: myclaimspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 8Gi  storageClassName: ceph-rbd-sc# oc create -f ceph-rbd-pvc.yaml -n project
结果展示

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph集群监控Prometheus+Grafana</title>
    <url>/posts/b60f02d69676.html</url>
    <content><![CDATA[
Ceph是一个分布式存储系统。同时Ceph除了能提供块存储，还可以提供文件存储、对象存储。
这里不介绍如何搭建Ceph及原理，只记录使用Ceph_exporter + Prometheus + Grafana 来对Ceph集群作监控的部署过程
Ceph集群有三种节点：Admin + Monitor + Node

Admin节点上部署Ceph_exporter
安装需要的软件golang

yum install golang git librados2-devel librbd1-devel -y

设置go的环境变量

# /etc/profile.d/go.shexport GOROOT=/usr/lib/golangexport GOBIN=$GOROOT/binexport GOPATH=/home/golangexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin
source /etc/profile.d/go.sh

安装ceph_exporter

go get -u github.com/digitalocean/ceph_exporter

运行ceph_exporter

cd /usr/lib/golang/bin/nohup ./ceph_exporter &amp;

 检验结果

curl 127.0.0.1:9128
Prometheus上添加ceph_exporter的job
添加scraper的job

# prometheus.yml...scrape_configs:  - job_name: &#x27;ceph&#x27;    static_configs:      - targets: [&#x27;ceph_host:9128&#x27;]        labels:          instance: ceph

重启prometheus

pkill -9 prometheusnohup ./prometheus &gt;/dev/null 2&gt;&amp;1 &amp;

检验结果

检查prometheus的status-&gt;targets中Ceph(1&#x2F;1up)
Grafana中添加Ceph监控展示
下载Ceph集群监控配置json：Ceph-Cluster Dashboard

Revisions -&gt; Download

展示最终效果图

  
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Confluence容器化-+-Openshift-Confluence模板创建</title>
    <url>/posts/524b504cfed1.html</url>
    <content><![CDATA[因为应用容器化部署已经是标准化的流程，无需再详篇介绍具体的部署流程。所以本文只提供相关的配置文档。如果对部署过程不了解的同学，请先自学容器基础。镜像已经创建好了，如下
docker.io/xhuaustc/confluence:6.7.1docker.io/xhuaustc/atlassian-mysql:5.7
镜像构建配置mysql镜像# my.cnf[mysqld]# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links = 0# http://www.percona.com/blog/2008/05/31/dns-achilles-heel-mysql-installation/skip_name_resolve# http://www.chriscalender.com/ignoring-the-lostfound-directory-in-your-datadir/ignore-db-dir=lost+foundcharacter_set_server=utf8init_connect=&#x27;SET NAMES utf8&#x27;collation_server=utf8_bintransaction_isolation=&#x27;read-committed&#x27;!includedir /etc/my.cnf.d
# DockerfileFROM centos/mysql-57-centos7COPY my.cnf /etc/my.cnf
confluence镜像git clone https://github.com/cptactionhank/docker-atlassian-confluence
在Dockerfile目录下添加server.xml, setenv.sh与atlassian-extras-decoder-v2-3.3.0.jar
# server.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;Server port=&quot;8000&quot; shutdown=&quot;SHUTDOWN&quot;&gt;  &lt;Service name=&quot;Tomcat-Standalone&quot;&gt;    &lt;Connector port=&quot;8090&quot; connectionTimeout=&quot;300000&quot; redirectPort=&quot;8443&quot; maxThreads=&quot;400&quot; minSpareThreads=&quot;10&quot; enableLookups=&quot;false&quot; acceptCount=&quot;100&quot; URIEncoding=&quot;UTF-8&quot; protocol=&quot;org.apache.coyote.http11.Http11NioProtocol&quot;/&gt;    &lt;Engine name=&quot;Standalone&quot; defaultHost=&quot;localhost&quot;&gt;      &lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;false&quot; startStopThreads=&quot;4&quot;&gt;        &lt;Context path=&quot;&quot; docBase=&quot;../confluence&quot; reloadable=&quot;false&quot; useHttpOnly=&quot;true&quot;&gt;          &lt;!-- Logger is deprecated in Tomcat 5.5. Logging configuration for Confluence is specified in confluence/WEB-INF/classes/log4j.properties --&gt;          &lt;Manager pathname=&quot;&quot;/&gt;          &lt;Valve className=&quot;org.apache.catalina.valves.StuckThreadDetectionValve&quot; threshold=&quot;600&quot;/&gt;        &lt;/Context&gt;        &lt;Context path=&quot;$&#123;confluence.context.path&#125;/synchrony-proxy&quot; docBase=&quot;../synchrony-proxy&quot; reloadable=&quot;false&quot; useHttpOnly=&quot;true&quot;&gt;          &lt;Valve className=&quot;org.apache.catalina.valves.StuckThreadDetectionValve&quot; threshold=&quot;600&quot;/&gt;        &lt;/Context&gt;      &lt;/Host&gt;    &lt;/Engine&gt;    &lt;!--            To run Confluence via HTTPS:             * Uncomment the Connector below             * Execute:                 %JAVA_HOME%\bin\keytool -genkey -alias tomcat -keyalg RSA (Windows)                 $JAVA_HOME/bin/keytool -genkey -alias tomcat -keyalg RSA  (Unix)               with a password value of &quot;changeit&quot; for both the certificate and the keystore itself.             * Restart and visit https://localhost:8443/             For more info, see https://confluence.atlassian.com/display/DOC/Running+Confluence+Over+SSL+or+HTTPS        --&gt;    &lt;!--        &lt;Connector port=&quot;8443&quot; maxHttpHeaderSize=&quot;8192&quot;                   maxThreads=&quot;150&quot; minSpareThreads=&quot;25&quot;                   protocol=&quot;org.apache.coyote.http11.Http11NioProtocol&quot;                   enableLookups=&quot;false&quot; disableUploadTimeout=&quot;true&quot;                   acceptCount=&quot;100&quot; scheme=&quot;https&quot; secure=&quot;true&quot;                   clientAuth=&quot;false&quot; sslProtocols=&quot;TLSv1,TLSv1.1,TLSv1.2&quot; sslEnabledProtocols=&quot;TLSv1,TLSv1.1,TLSv1.2&quot; SSLEnabled=&quot;true&quot;                   URIEncoding=&quot;UTF-8&quot; keystorePass=&quot;&lt;MY_CERTIFICATE_PASSWORD&gt;&quot;/&gt;--&gt;  &lt;/Service&gt;&lt;/Server&gt;
# setenv.sh# See the CATALINA_OPTS below for tuning the JVM arguments used to start Confluence.echo &quot;If you encounter issues starting up Confluence, please see the Installation guide at http://confluence.atlassian.com/display/DOC/Confluence+Installation+Guide&quot;# set the location of the pid fileif [ -z &quot;$CATALINA_PID&quot; ] ; then    if [ -n &quot;$CATALINA_BASE&quot; ] ; then        CATALINA_PID=&quot;$CATALINA_BASE&quot;/work/catalina.pid    elif [ -n &quot;$CATALINA_HOME&quot; ] ; then        CATALINA_PID=&quot;$CATALINA_HOME&quot;/work/catalina.pid    fifiexport CATALINA_PIDPRGDIR=`dirname &quot;$0&quot;`if [ -z &quot;$CATALINA_BASE&quot; ]; then  if [ -z &quot;$CATALINA_HOME&quot; ]; then    LOGBASE=$PRGDIR    LOGTAIL=..  else    LOGBASE=$CATALINA_HOME    LOGTAIL=.  fielse  LOGBASE=$CATALINA_BASE  LOGTAIL=.fiPUSHED_DIR=`pwd`cd $LOGBASEcd $LOGTAILLOGBASEABS=`pwd`cd $PUSHED_DIRecho &quot;&quot;echo &quot;Server startup logs are located in $LOGBASEABS/logs/catalina.out&quot;# IMPORTANT NOTE: Only set JAVA_HOME or JRE_HOME above this line# Get standard Java environment variablesif $os400; then  # -r will Only work on the os400 if the files are:  # 1. owned by the user  # 2. owned by the PRIMARY group of the user  # this will not work if the user belongs in secondary groups  . &quot;$CATALINA_HOME&quot;/bin/setjre.shelse  if [ -r &quot;$CATALINA_HOME&quot;/bin/setjre.sh ]; then    . &quot;$CATALINA_HOME&quot;/bin/setjre.sh  else    echo &quot;Cannot find $CATALINA_HOME/bin/setjre.sh&quot;    echo &quot;This file is needed to run this program&quot;    exit 1  fifiecho &quot;---------------------------------------------------------------------------&quot;echo &quot;Using Java: $JRE_HOME/bin/java&quot;CONFLUENCE_CONTEXT_PATH=`$JRE_HOME/bin/java -jar $CATALINA_HOME/bin/confluence-context-path-extractor.jar $CATALINA_HOME`export CONFLUENCE_CONTEXT_PATH$JRE_HOME/bin/java -jar $CATALINA_HOME/bin/synchrony-proxy-watchdog.jar $CATALINA_HOMEecho &quot;---------------------------------------------------------------------------&quot;JVM_MINIMUM_MEMORY=$&#123;JVM_XMS:-384m&#125;JVM_MAXIMUM_MEMORY=$&#123;JVM_XMX:-768m&#125;# Set the JVM arguments used to start Confluence. For a description of the options, see# http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.htmlCATALINA_OPTS=&quot;-Xms$&#123;JVM_MINIMUM_MEMORY&#125; -Xmx$&#123;JVM_MAXIMUM_MEMORY&#125; -XX:-PrintGCDetails -XX:+PrintGCDateStamps -XX:-PrintTenuringDistribution $&#123;CATALINA_OPTS&#125;&quot;CATALINA_OPTS=&quot;-Xloggc:$LOGBASEABS/logs/gc-`date +%F_%H-%M-%S`.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=2M $&#123;CATALINA_OPTS&#125;&quot;CATALINA_OPTS=&quot;-XX:G1ReservePercent=20 $&#123;CATALINA_OPTS&#125;&quot;CATALINA_OPTS=&quot;-Djava.awt.headless=true $&#123;CATALINA_OPTS&#125;&quot;CATALINA_OPTS=&quot;-Datlassian.plugins.enable.wait=300 $&#123;CATALINA_OPTS&#125;&quot;CATALINA_OPTS=&quot;-Dsynchrony.enable.xhr.fallback=true $&#123;CATALINA_OPTS&#125;&quot;CATALINA_OPTS=&quot;-Dorg.apache.tomcat.websocket.DEFAULT_BUFFER_SIZE=32768 $&#123;CATALINA_OPTS&#125;&quot;CATALINA_OPTS=&quot;$&#123;START_CONFLUENCE_JAVA_OPTS&#125; $&#123;CATALINA_OPTS&#125;&quot;CATALINA_OPTS=&quot;-Dconfluence.context.path=$&#123;CONFLUENCE_CONTEXT_PATH&#125; $&#123;CATALINA_OPTS&#125;&quot;export CATALINA_OPTS
修改Dockerfile
# DockerfileFROM openjdk:8-alpine# Setup useful environment variablesENV CONF_HOME     /var/atlassian/confluenceENV CONF_INSTALL  /opt/atlassian/confluenceENV CONF_VERSION  6.7.1ENV JAVA_CACERTS  $JAVA_HOME/jre/lib/security/cacertsENV CERTIFICATE   $CONF_HOME/certificate# Install Atlassian Confluence and helper tools and setup initial home# directory structure.RUN set -x \    &amp;&amp; apk --no-cache add curl xmlstarlet bash ttf-dejavu libc6-compat \    &amp;&amp; mkdir -p                &quot;$&#123;CONF_HOME&#125;&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;CONF_HOME&#125;&quot; \    &amp;&amp; mkdir -p                &quot;$&#123;CONF_INSTALL&#125;/conf&quot; \    &amp;&amp; curl -Ls                &quot;https://www.atlassian.com/software/confluence/downloads/binary/atlassian-confluence-$&#123;CONF_VERSION&#125;.tar.gz&quot; | tar -xz --directory &quot;$&#123;CONF_INSTALL&#125;&quot; --strip-components=1 --no-same-owner \    &amp;&amp; curl -Ls                &quot;https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.44.tar.gz&quot; | tar -xz --directory &quot;$&#123;CONF_INSTALL&#125;/confluence/WEB-INF/lib&quot; --strip-components=1 --no-same-owner &quot;mysql-connector-java-5.1.44/mysql-connector-java-5.1.44-bin.jar&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;CONF_INSTALL&#125;/conf&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;CONF_INSTALL&#125;/temp&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;CONF_INSTALL&#125;/logs&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;CONF_INSTALL&#125;/work&quot; \    &amp;&amp; echo -e                 &quot;\nconfluence.home=$CONF_HOME&quot; &gt;&gt; &quot;$&#123;CONF_INSTALL&#125;/confluence/WEB-INF/classes/confluence-init.properties&quot; \    &amp;&amp; xmlstarlet              ed --inplace \        --delete               &quot;Server/@debug&quot; \        --delete               &quot;Server/Service/Connector/@debug&quot; \        --delete               &quot;Server/Service/Connector/@useURIValidationHack&quot; \        --delete               &quot;Server/Service/Connector/@minProcessors&quot; \        --delete               &quot;Server/Service/Connector/@maxProcessors&quot; \        --delete               &quot;Server/Service/Engine/@debug&quot; \        --delete               &quot;Server/Service/Engine/Host/@debug&quot; \        --delete               &quot;Server/Service/Engine/Host/Context/@debug&quot; \                               &quot;$&#123;CONF_INSTALL&#125;/conf/server.xml&quot; \    &amp;&amp; touch -d &quot;@0&quot;           &quot;$&#123;CONF_INSTALL&#125;/conf/server.xml&quot; # Use the default unprivileged account. This could be considered bad practice# on systems where multiple processes end up being executed by &#x27;daemon&#x27; but# here we only ever run one process anyway.# Expose default HTTP connector port.EXPOSE 8090 8091# Set volume mount points for installation and home directory. Changes to the# home directory needs to be persisted as well as parts of the installation# directory due to eg. logs.VOLUME [&quot;/var/atlassian/confluence&quot;, &quot;/opt/atlassian/confluence/logs&quot;]# Set the default working directory as the Confluence home directory.WORKDIR /var/atlassian/confluenceCOPY docker-entrypoint.sh /COPY atlassian-extras-decoder-v2-3.3.0.jar /opt/atlassian/confluence/confluence/WEB-INF/lib/atlassian-extras-decoder-v2-3.3.0.jarCOPY server.xml /opt/atlassian/confluence/conf/server.xmlENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]# Run Atlassian Confluence as a foreground process by default.CMD [&quot;/opt/atlassian/confluence/bin/start-confluence.sh&quot;, &quot;-fg&quot;]
docker-compose配置因为mysql用mysql用户启动的，需要把data&#x2F;mysql权限改为777chmod 777 data/mysql -R
wiki:  image: xhuaustc/confluence:6.7.1  restart: always  environment:    - JVM_XMX=1024m    - JVM_XMS=512m  ports:    - &#x27;10380:8090&#x27;  links:    - db  volumes:    - ./data/confluence:/var/atlassian/confluence    - ./data/logs:/opt/atlassian/confluence/logsdb:  image: xhuaustc/atlassian-mysql:5.7  restart: always  environment:    - MYSQL_USER=confluence    - MYSQL_PASSWORD=conflence    - MYSQL_DATABASE=confluence    - MYSQL_ROOT_PASSWORD=confluence  volumes:    - ./data/mysql:/var/lib/mysql
Openshfit confluence模板apiVersion: v1kind: Templatemetadata:  creationTimestamp: null  name: confluenceobjects:- apiVersion: v1  kind: DeploymentConfig  metadata:    labels:      run: confluence    name: confluence  spec:    replicas: 1    selector:      run: confluence    strategy:      type: Recreate    template:      metadata:        labels:          run: confluence      spec:        containers:        - env:            - name: JVM_XMX              value: &#x27;2048m&#x27;            - name: JVM_XMS              value: &#x27;1024m&#x27;          image: docker.io/xhuaustc/confluence:6.7.1          imagePullPolicy: IfNotPresent          name: confluence          volumeMounts:          - mountPath: /var/atlassian/confluence            name: volume-7iy6x          - mountPath: /opt/atlassian/confluence/logs            name: volume-zsyly        volumes:        - name: volume-7iy6x          persistentVolumeClaim:            claimName: confluence        - name: volume-zsyly          persistentVolumeClaim:            claimName: log    triggers:    - type: ConfigChange- apiVersion: v1  kind: DeploymentConfig  metadata:    labels:      run: mysql    name: mysql  spec:    replicas: 1    selector:      run: mysql    strategy:      type: Recreate    template:      metadata:        labels:          run: mysql      spec:        containers:        - env:          - name: MYSQL_USER            value: confluence          - name: MYSQL_PASSWORD            value: confluence          - name: MYSQL_DATABASE            value: confluence          - name: MYSQL_ROOT_PASSWORD            value: confluence          image: docker.io/xhuaustc/atlassian-mysql:5.7          imagePullPolicy: IfNotPresent          name: mysql          volumeMounts:          - mountPath: /var/lib/mysql            name: volume-uiwfa         volumes:          - name: volume-uiwfa            persistentVolumeClaim:              claimName: mysql-data    triggers:    - type: ConfigChange- apiVersion: v1  kind: Service  metadata:    labels:      run: confluence    name: confluence  spec:    ports:    - port: 8090      protocol: TCP      targetPort: 8090    selector:      run: confluence    type: ClusterIP- apiVersion: v1  kind: Service  metadata:    labels:      run: mysql    name: mysql  spec:    ports:    - port: 3306      protocol: TCP      targetPort: 3306    selector:      run: mysql    type: ClusterIP- apiVersion: v1  kind: Route  metadata:    annotations:      haproxy.router.openshift.io/timeout: 3000s    labels:      run: confluence    name: confluence  spec:    port:      targetPort: 8090    to:      kind: Service      name: confluence      weight: 100    wildcardPolicy: None- apiVersion: v1  kind: PersistentVolumeClaim  metadata:    annotations:      volume.beta.kubernetes.io/storage-class: ceph-rbd-sc      volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/rbd    name: confluence  spec:    accessModes:    - ReadWriteOnce    resources:      requests:        storage: 20Gi- apiVersion: v1  kind: PersistentVolumeClaim  metadata:    annotations:      volume.beta.kubernetes.io/storage-class: ceph-rbd-sc      volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/rbd    name: log  spec:    accessModes:    - ReadWriteOnce    resources:      requests:        storage: 10Gi- apiVersion: v1  kind: PersistentVolumeClaim  metadata:    annotations:      volume.beta.kubernetes.io/storage-class: ceph-rbd-sc      volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/rbd    name: mysql-data  spec:    accessModes:    - ReadWriteOnce    resources:      requests:        storage: 10Gi
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker中使用命令COPY-ADD导入文件对权限的影响</title>
    <url>/posts/fbd2703af665.html</url>
    <content><![CDATA[1、文件保留权限原文件的权限所属用户与组为镜像中的用户可以使用--chown=jenkins:jenkins来指定用户所属
COPY --chown=1001:1001 sample.txt sample.txt

2、压缩包（ADD）保留权限原文件的权限保留压缩包内文件的用户与组
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Jira容器化-+-Openshift-Jira模板创建</title>
    <url>/posts/309756197f32.html</url>
    <content><![CDATA[因为应用容器化部署已经是标准化的流程，无需再详篇介绍具体的部署流程。所以本文只提供相关的配置文档。如果对部署过程不了解的同学，请先自学容器基础。镜像已经创建好了，如下
docker.io/xhuaustc/jira-software:7.11.0docker.io/xhuaustc/atlassian-mysql:5.7
镜像构建配置Jira容器构建
git clone https://github.com/cptactionhank/docker-atlassian-jira-software
将setenv.sh与atlassian-extras-3.2.jar拷贝到docker-atlassian-jira-software
# setenv.sh## One way to set the JIRA HOME path is here via this variable.  Simply uncomment it and set a valid path like /jira/home.  You can of course set it outside in the command terminal.  That will also work.##JIRA_HOME=&quot;&quot;##  Occasionally Atlassian Support may recommend that you set some specific JVM arguments.  You can use this variable below to do that.#JVM_SUPPORT_RECOMMENDED_ARGS=&quot;&quot;## The following 2 settings control the minimum and maximum given to the JIRA Java virtual machine.  In larger JIRA instances, the maximum amount will need to be increased.#JVM_MINIMUM_MEMORY=$&#123;JVM_XMS:-384m&#125;JVM_MAXIMUM_MEMORY=$&#123;JVM_XMX:-768m&#125;## The following are the required arguments for JIRA.#JVM_REQUIRED_ARGS=&#x27;-Djava.awt.headless=true -Datlassian.standalone=JIRA -Dorg.apache.jasper.runtime.BodyContentImpl.LIMIT_BUFFER=true -Dmail.mime.decodeparameters=true -Dorg.dom4j.factory=com.atlassian.core.xml.InterningDocumentFactory&#x27;# Uncomment this setting if you want to import data without notifications##DISABLE_NOTIFICATIONS=&quot; -Datlassian.mail.senddisabled=true -Datlassian.mail.fetchdisabled=true -Datlassian.mail.popdisabled=true&quot;#-----------------------------------------------------------------------------------## In general don&#x27;t make changes below here##-----------------------------------------------------------------------------------#-----------------------------------------------------------------------------------# Prevents the JVM from suppressing stack traces if a given type of exception# occurs frequently, which could make it harder for support to diagnose a problem.#-----------------------------------------------------------------------------------JVM_EXTRA_ARGS=&quot;-XX:-OmitStackTraceInFastThrow&quot;PRGDIR=`dirname &quot;$0&quot;`cat &quot;$&#123;PRGDIR&#125;&quot;/jirabanner.txtJIRA_HOME_MINUSD=&quot;&quot;if [ &quot;$JIRA_HOME&quot; != &quot;&quot; ]; then    echo $JIRA_HOME | grep -q &quot; &quot;    if [ $? -eq 0 ]; then	    echo &quot;&quot;	    echo &quot;--------------------------------------------------------------------------------------------------------------------&quot;		echo &quot;   WARNING : You cannot have a JIRA_HOME environment variable set with spaces in it.  This variable is being ignored&quot;	    echo &quot;--------------------------------------------------------------------------------------------------------------------&quot;    else		JIRA_HOME_MINUSD=-Djira.home=$JIRA_HOME    fifiJAVA_OPTS=&quot;-Xms$&#123;JVM_MINIMUM_MEMORY&#125; -Xmx$&#123;JVM_MAXIMUM_MEMORY&#125; $&#123;JAVA_OPTS&#125; $&#123;JVM_REQUIRED_ARGS&#125; $&#123;DISABLE_NOTIFICATIONS&#125; $&#123;JVM_SUPPORT_RECOMMENDED_ARGS&#125; $&#123;JVM_EXTRA_ARGS&#125; $&#123;JIRA_HOME_MINUSD&#125; $&#123;START_JIRA_JAVA_OPTS&#125;&quot;export JAVA_OPTS# DO NOT remove the following line# !INSTALLER SET JAVA_HOMEecho &quot;&quot;echo &quot;If you encounter issues starting or stopping JIRA, please see the Troubleshooting guide at http://confluence.atlassian.com/display/JIRA/Installation+Troubleshooting+Guide&quot;echo &quot;&quot;if [ &quot;$JIRA_HOME_MINUSD&quot; != &quot;&quot; ]; then    echo &quot;Using JIRA_HOME:       $JIRA_HOME&quot;fi# set the location of the pid fileif [ -z &quot;$CATALINA_PID&quot; ] ; then    if [ -n &quot;$CATALINA_BASE&quot; ] ; then        CATALINA_PID=&quot;$CATALINA_BASE&quot;/work/catalina.pid    elif [ -n &quot;$CATALINA_HOME&quot; ] ; then        CATALINA_PID=&quot;$CATALINA_HOME&quot;/work/catalina.pid    fifiexport CATALINA_PIDif [ -z &quot;$CATALINA_BASE&quot; ]; then  if [ -z &quot;$CATALINA_HOME&quot; ]; then    LOGBASE=$PRGDIR    LOGTAIL=..  else    LOGBASE=$CATALINA_HOME    LOGTAIL=.  fielse  LOGBASE=$CATALINA_BASE  LOGTAIL=.fiPUSHED_DIR=`pwd`cd $LOGBASEcd $LOGTAILLOGBASEABS=`pwd`cd $PUSHED_DIRecho &quot;&quot;echo &quot;Server startup logs are located in $LOGBASEABS/logs/catalina.out&quot;# Set the JVM arguments used to start JIRA. For a description of the options, see# http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html#-----------------------------------------------------------------------------------# This allows us to actually debug GC related issues by correlating timestamps# with other parts of the application logs.#-----------------------------------------------------------------------------------GC_JVM_PARAMETERS=&quot;&quot;GC_JVM_PARAMETERS=&quot;-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCCause $&#123;GC_JVM_PARAMETERS&#125;&quot;GC_JVM_PARAMETERS=&quot;-Xloggc:$LOGBASEABS/logs/atlassian-jira-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=20M $&#123;GC_JVM_PARAMETERS&#125;&quot;CATALINA_OPTS=&quot;$&#123;GC_JVM_PARAMETERS&#125; $&#123;CATALINA_OPTS&#125;&quot;export CATALINA_OPTS
构建Dockerfile
#DockerfileFROM openjdk:8-alpine# Configuration variables.ENV JIRA_HOME     /var/atlassian/jiraENV JIRA_INSTALL  /opt/atlassian/jiraENV JIRA_VERSION  7.11.0# Install Atlassian JIRA and helper tools and setup initial home# directory structure.RUN set -x \    &amp;&amp; apk add --no-cache curl xmlstarlet bash ttf-dejavu libc6-compat \    &amp;&amp; mkdir -p                &quot;$&#123;JIRA_HOME&#125;&quot; \    &amp;&amp; mkdir -p                &quot;$&#123;JIRA_HOME&#125;/caches/indexes&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;JIRA_HOME&#125;&quot; \    &amp;&amp; mkdir -p                &quot;$&#123;JIRA_INSTALL&#125;/conf/Catalina&quot; \    &amp;&amp; curl -Ls                &quot;https://www.atlassian.com/software/jira/downloads/binary/atlassian-jira-software-7.11.0.tar.gz&quot; | tar -xz --directory &quot;$&#123;JIRA_INSTALL&#125;&quot; --strip-components=1 --no-same-owner \    &amp;&amp; curl -Ls                &quot;https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.38.tar.gz&quot; | tar -xz --directory &quot;$&#123;JIRA_INSTALL&#125;/lib&quot; --strip-components=1 --no-same-owner &quot;mysql-connector-java-5.1.38/mysql-connector-java-5.1.38-bin.jar&quot; \    &amp;&amp; rm -f                   &quot;$&#123;JIRA_INSTALL&#125;/lib/postgresql-9.1-903.jdbc4-atlassian-hosted.jar&quot; \    &amp;&amp; curl -Ls                &quot;https://jdbc.postgresql.org/download/postgresql-42.2.1.jar&quot; -o &quot;$&#123;JIRA_INSTALL&#125;/lib/postgresql-42.2.1.jar&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;JIRA_INSTALL&#125;/conf&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;JIRA_INSTALL&#125;/logs&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;JIRA_INSTALL&#125;/temp&quot; \    &amp;&amp; chmod -R 777            &quot;$&#123;JIRA_INSTALL&#125;/work&quot; \    &amp;&amp; sed --in-place          &quot;s/java version/openjdk version/g&quot; &quot;$&#123;JIRA_INSTALL&#125;/bin/check-java.sh&quot; \    &amp;&amp; echo -e                 &quot;\njira.home=$JIRA_HOME&quot; &gt;&gt; &quot;$&#123;JIRA_INSTALL&#125;/atlassian-jira/WEB-INF/classes/jira-application.properties&quot; \    &amp;&amp; touch -d &quot;@0&quot;           &quot;$&#123;JIRA_INSTALL&#125;/conf/server.xml&quot;# Use the default unprivileged account. This could be considered bad practice# on systems where multiple processes end up being executed by &#x27;daemon&#x27; but# here we only ever run one process anyway.# Expose default HTTP connector port.EXPOSE 8080# Set volume mount points for installation and home directory. Changes to the# home directory needs to be persisted as well as parts of the installation# directory due to eg. logs.VOLUME [&quot;/var/atlassian/jira&quot;, &quot;/opt/atlassian/jira/logs&quot;]# Set the default working directory as the installation directory.WORKDIR /var/atlassian/jiraCOPY &quot;docker-entrypoint.sh&quot; &quot;/&quot;COPY atlassian-extras-3.2.jar $&#123;JIRA_INSTALL&#125;/atlassian-jira/WEB-INF/lib/atlassian-extras-3.2.jarCOPY setenv.sh $&#123;JIRA_INSTALL&#125;/bin/setenv.shENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]# Run Atlassian JIRA as a foreground process by default.CMD [&quot;/opt/atlassian/jira/bin/start-jira.sh&quot;, &quot;-fg&quot;]
构建对应的Mysql镜像
# my.cnf[mysqld]pid-file        = /var/run/mysqld/mysqld.pidsocket          = /var/run/mysqld/mysqld.sockdatadir         = /var/lib/mysqlsecure-file-priv= NULL# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0character_set_server=utf8init_connect=&#x27;SET NAMES utf8&#x27;collation_server=utf8_bintransaction_isolation=&#x27;read-committed&#x27;# Custom config should go here!includedir /etc/mysql/conf.d/
Dockerfile文件
FROM mysql:5.7COPY my.cnf /etc/mysql/my.cnf
部署镜像构建docker-compose.yml
jira:  image: xhuaustc/jira-software:7.11.0  restart: always  environment:    - JVM_XMX=2048m    - JVM_XMS=1024m  ports:    - &#x27;8080:8080&#x27;  links:    - db  volumes:    - ./data/jira:/var/atlassian/jira    - ./data/logs:/opt/atlassian/jira/logsdb:  image: xhuaustc/atlassian-mysql:5.7  restart: always  environment:    - MYSQL_USER=jira    - MYSQL_PASSWORD=jira    - MYSQL_DATABASE=jira    - MYSQL_ROOT_PASSWORD=jira  volumes:    - ./data/mysql:/var/lib/mysql
Openshift Jira模板apiVersion: v1kind: Templatemetadata:  name: jiraobjects:- apiVersion: v1  kind: DeploymentConfig  metadata:    labels:      run: jira    name: jira  spec:    replicas: 1    selector:      run: jira    strategy:      type: Recreate    template:      metadata:        labels:          run: jira      spec:        containers:        - env:          - name: JVM_XMX            value: 2048m          - name: JVM_XMS            value: 1024m          image: docker.io/xhuaustc/jira-software:7.11.0          imagePullPolicy: IfNotPresent          name: jira          volumeMounts:          - mountPath: /var/atlassian/jira            name: volume-7iy6x          - mountPath: /opt/atlassian/jira/logs            name: volume-zsyly        volumes:        - name: volume-7iy6x          persistentVolumeClaim:            claimName: jira        - name: volume-zsyly          persistentVolumeClaim:            claimName: jira-log    triggers:    - type: ConfigChange- apiVersion: v1  kind: DeploymentConfig  metadata:    labels:      run: mysql-jira    name: mysql-jira  spec:    replicas: 1    selector:      run: mysql-jira    strategy:      type: Recreate    template:      metadata:        labels:          run: mysql-jira      spec:        containers:        - env:          - name: MYSQL_USER            value: jira          - name: MYSQL_PASSWORD            value: jira          - name: MYSQL_DATABASE            value: jira          - name: MYSQL_ROOT_PASSWORD            value: jira          image: docker.io/xhuaustc/atlassian-mysql:5.7          imagePullPolicy: IfNotPresent          name: mysql-jira          volumeMounts:          - mountPath: /var/lib/mysql            name: volume-uiwfa        volumes:        - name: volume-uiwfa          persistentVolumeClaim:            claimName: mysql-jira-data    triggers:    - type: ConfigChange- apiVersion: v1  kind: Service  metadata:    labels:      run: jira    name: jira  spec:    ports:    - port: 8080      protocol: TCP      targetPort: 8080    selector:      run: jira    type: ClusterIP- apiVersion: v1  kind: Service  metadata:    labels:      run: mysql-jira    name: mysql-jira  spec:    ports:    - port: 3306      protocol: TCP      targetPort: 3306    selector:      run: mysql-jira    type: ClusterIP- apiVersion: v1  kind: Route  metadata:    annotations:      haproxy.router.openshift.io/timeout: 3000s    labels:      run: jira    name: jira  spec:    port:      targetPort: 8080    to:      kind: Service      name: jira      weight: 100    wildcardPolicy: None- apiVersion: v1  kind: PersistentVolumeClaim  metadata:    annotations:      volume.beta.kubernetes.io/storage-class: ceph-rbd-sc      volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/rbd    name: jira  spec:    accessModes:    - ReadWriteOnce    resources:      requests:        storage: 20Gi- apiVersion: v1  kind: PersistentVolumeClaim  metadata:    annotations:      volume.beta.kubernetes.io/storage-class: ceph-rbd-sc      volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/rbd    name: jira-log  spec:    accessModes:    - ReadWriteOnce    resources:      requests:        storage: 10Gi- apiVersion: v1  kind: PersistentVolumeClaim  metadata:    annotations:      volume.beta.kubernetes.io/storage-class: ceph-rbd-sc      volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/rbd    name: mysql-jira-data  spec:    accessModes:    - ReadWriteOnce    resources:      requests:        storage: 10Gi
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker安装镜像仓库的三种方法</title>
    <url>/posts/35c6bb7f80fa.html</url>
    <content><![CDATA[一、docker-distribution
安装docker-distribution软件

yum install docker-distribution

配置registry&#x2F;config.yml文件

# /etc/docker-distribution/registry/config.ymlversion: 0.1log:fields:service: registrystorage:cache:layerinfo: inmemoryfilesystem:rootdirectory: /var/lib/registryhttp:addr: :5000

启动docker-distribution服务

systemctl start docker-distributionsystemctl enable docker-distribution
运维
二、运行容器registry直接运行docker run
docker run -d -p 5000:5000 -v /data/registry:/var/lib/registry --name registry --restart=always registry
Docker安装镜像仓库的三种方法
三、搭建HarborHarbor部署在主机上部署相关资料地址：Harbor 地址下载offline部署文件：Harbor offline installer部署手册：Harbor部署手册

安装必要软件

yum install docker-1.12.6 docker-compose -y

在 &#x2F;etc&#x2F;hosts 添加harbor地址

# /etc/hosts192.168.2.3 harbor.openshift

在 harbor&#x2F;harbor.cfg 修改hostname

# harbor/harbor.cfghostname = harbor.openshift:1080
Harbor默认镜像存储在 /data/ 目录下4. 更新Harbor对外服务端口号
# harbor/docker-compose.yml 123行- 1080:80- 1443:443- 4443:4443# harbor/harbor.cfghostname = harbor.openshift:1080

启动docker

# 执行之前请确认/var/lib/docker文件夹是否单独挂盘# 更新 /etc/sysconfig/docker 配置# 添加OPTIONS=&#x27;--selinux-enabled --log-driver=journald --signature-verification=false--insecure-registry=0.0.0.0/0 --registry-mirror=https://docker.mirrors.ustc.edu.cn&#x27;systemctl start docker

安装Harbor

./install.sh

测试Harbor

docker login harbor.openshift

维护Harbora. 暂停

docker-compose stop
b. 暂停后重新启动
docker-compose start
c. 更新harbor.cfg后启动
docker-compose down -v./preparedocker-compose up -d
d. 删除Harbor
docker-compose down -vrm -r /data/databaserm -r /data/registry
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S-JFrog-DevOps实战训练营</title>
    <url>/posts/e710e9d4a5d7.html</url>
    <content><![CDATA[DevOps CAMS模型C 文化 CultureA 自动化 AutomationM 指标维度 MeasurementS 分享 Sharing
DevOps Five Stages
规范化技术栈
推进标准化 &amp; 降低不一致性因素
扩展DevOps实践，开始从一个团队，再到第二个第三个，最后到整个公司
自动化基础框架交付（PaaS平台）
提供自服务能力（自定义编排）

DevOps三步落地法

基于价值流的自动化持续交付，有六个实践，分别是：可视化、限制在制品、减少规模、减少交接数量、持续识别和拓展约束，以及在价值流中消除浪费。
每种架构一套标准化流水线   gitlab &#x2F; jenkins &#x2F; nexus &#x2F; ansible
Kanban 或 Sprint计划板实现可视化工作，价值流交付可视化


快速反馈，PaaS监控、应用监控prometheus &#x2F; grafana持续反馈，交付之前快速反馈jmeter &#x2F; redmood &#x2F; senium
DevOps度量Hygieia

Docker跨主机网络

大二层网络vxlan, flannelMac in UDP
三层路由calico必须有三层路由转发能力设备支持

流水线反模式
Docker镜像多次构建，不做升级

问题：你发布的并不是你测试的解决：一次构建，多次升级  

 缺乏质量关卡


Docker镜像不进行安全漏洞扫描

问题：

30%镜像包含已知漏洞
14% npm package 包含已知漏洞
59% maven已知漏洞未修复

解决：DevSecOps  

工具碎片化

问题： 缺少可追溯性，定位问题难解决：流水线全系统记录，静态扫描结果、测试阶段  

交付物或部署文件无版本化

问题：

部署回滚流程缺少规范及标准，管理复杂，易出错
可读性差，易出错，风险高
历史链混乱，追溯性差，定位问题难解决：Helm——Kubernetes官方包管理工具配置与应用分离

持续交付流水线设计  
基础镜像也会有单独的流水线、并且会漏洞扫描、做测试，并将它放在公共生产镜像仓库
微服务
痛点


开发语言、运行环境多

应用部署、管理难

依赖关系复杂、服务治理

配置管理

解决


容器

容器编排

Istio服务网格

ConfigMap &amp; Secret



容器云优势


打包运行环境
集群管理，扩容伸缩
集群管理，扩容伸缩
降低运维成本
降低运维成本
DevOps(20%)


使用Pod


方便管理
资源共享和通信
灵活


安全

configmap或者secret在应用启动前注入环境变量，然后在应用启动后，将环境变量去掉。

Kubernetes最佳实践


配置与应用分离
计算与存储分离
服务透明访问
善用调度特性

Kubernetes添加节点前Token忘了怎么办？两种办法，1：找回；2：生成新的加入命令

找回忘记的Token

[root@iZ2ze436suxv73x9jtiy2vZ helm-demo-master]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;cda2299d203e26b6499e8283937ffbb6489421ff761569fdf8172d03d9a889d6[root@iZ2ze436suxv73x9jtiy2vZ helm-demo-master]# kubeadm token listTOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPSqyg3si.njhyhixcqd18by2g   23h       2019-07-27T15:13:15+08:00   authentication,signing   The default bootstrap token generated by &#x27;kubeadm init&#x27;.   system:bootstrappers:kubeadm:default-node-token[root@iZ2ze436suxv73x9jtiy2vZ helm-demo-master]# #kubeadm join 172.17.3.226:6443 --token qyg3si.njhyhixcqd18by2g --discovery-token-ca-cert-hash sha256:cda2299d203e26b6499e8283937ffbb6489421ff761569fdf8172d03d9a889d6

生成新的命令

[root@iZ2ze436suxv73x9jtiy2vZ helm-demo-master]# kubeadm token create --print-join-command

Helm

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes安装手册</title>
    <url>/posts/d7d0952ffadb.html</url>
    <content><![CDATA[Kubernetes在2014年开源后，很快便占据了容器编排的主导地位，成为容器编排的事实标准。那么Kubernetes的安装是了解它的第一步。目前有非常多的工具方法来实现Kubernetes的安装，收集了一些如下。

kubeadm：https://github.com/kubernetes/kubeadmkubeadm是官方提供的最为通用的一种部署方案。
kops：https://github.com/kubernetes/kopskops通过命令行创建，销毁，升级和维护生产级，高可用性的Kubernetes集群。目前正式支持AWS（Amazon Web Services），其中GCE和OpenStack处于beta测试阶段，而VMware vSphere处于alpha阶段。
kubespray：https://github.com/kubernetes-sigs/kubespraykubespray通过 Ansible Playbook 来定义系统与 Kubernetes 集群部署的任务。
KubeOperator: https://github.com/KubeOperator/KubeOperatorKubeOperator 是一个Fit2Cloud开源项目，在离线网络环境下，通过可视化 Web UI 在 VMware、Openstack 或者物理机上规划、部署和运营生产级别的 Kubernetes 集群。
ansible-install-k8s：https://github.com/lizhenliang/ansible-install-k8sansible-install-k8s支持离线安装，是属于个人开发的ansible部署脚本
kind：https://github.com/kubernetes-sigs/kindKind（Kubernetes in Docker） 是一个 Kubernetes 孵化项目，Kind 是一套开箱即用的 Kubernetes 环境搭建方案。顾名思义，就是将 Kubernetes 所需要的所有组件，全部部署在一个 Docker 容器中，可以很方便的搭建 Kubernetes 集群。
sealos：https://github.com/fanux/sealosSealos 是一个 Go 语言开发的简单干净且轻量的 Kubernetes 集群部署工具，Sealos 能很好的支持在生产环境中部署高可用的 Kubernetes 集群。

使用kubeadm部署单机版k8sMaster节点安装
机器准备


虚机配置2核4G以上
可访问外网
操作系统CentOS 7.6


准备工作

$ systemctl stop firewalld$ systemctl disable firewalld$ setenforce 0$cat /etc/selinux/config...SELINUX=disabled...$ hostnamectl set-hostname master1

添加k8s yum源

[k8s]name=k8sbaseurl=https://mirrors.tuna.tsinghua.edu.cn/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=0

确保 iptables 工具不使用 nftables 后端

$ update-alternatives --set iptables /usr/sbin/iptables-legacy


安装软件

$ yum install -y docker kubelet kubeadm kubectl kubernetes-cni$ echo &quot;KUBELET_EXTRA_ARGS=--cgroup-driver=systemd&quot; &gt; /etc/sysconfig/kubelet

关闭系统的Swap

$ swapoff -a$ # 将/etc/fstab文件中的Swap注释掉$ cat /etc/fstab/dev/mapper/VolGroup00-LogVol00 /                       xfs     defaults        0 0UUID=1539acb0-0589-4eae-a0a4-24566186e425 /boot                   xfs     defaults        0 0#/dev/mapper/VolGroup00-LogVol01 swap                    swap    defaults        0 0

创建&#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf文件，添加如下内容

net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1vm.swappiness=0

启动docker与配置kubelet服务

systemctl start dockersystemctl enable dockersystemctl enable kubelet

对docker进行加速，在&#x2F;etc&#x2F;sysconfig&#x2F;docker配置项中添加如下内容

--registry-mirror=https://r03u9tep.mirror.aliyuncs.com

使用hub.docker.com中的镜像，例如：docker.io/aiotceo/kube-apiserver

$ # 下载aiotceo镜像$ kubeadm config images list | sed -e &#x27;s/^/docker pull /g&#x27; -e &#x27;s/k8s.gcr.io/docker.io\/aiotceo/g&#x27; -e &#x27;s/-0//g&#x27; | sh -x$ #将aiotceo重新tag到k8s.gcr.io$ docker images | grep aiotceo | awk &#x27;&#123;print &quot;docker tag &quot; $1&quot;:&quot;$2,$1&quot;:&quot;$2&#125;&#x27; | sed -e &quot;s#docker.io/aiotceo#k8s.gcr.io#2&quot; -e &quot;s#3.4.3#3.4.3-0#2&quot; | sh -x$ #删除aiotceo镜像$ docker images |grep aiotceo |awk &#x27;&#123;print &quot;docker rmi &quot;, $1&quot;:&quot;$2&#125;&#x27; |sh -x 

初始化kube集群

$ kubeadm init...kubeadm join 10.0.2.15:6443 --token 8o8yv2.tv836s1jncmrwgsp \    --discovery-token-ca-cert-hash sha256:d9d15b5905e8defc0ef6de294be5eff08d720ed9d98b8d6cb99d153bc7332e8e

创建管理员访问文件

$ mkdir -p $HOME/.kube$ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

安装网络插件，例如calico

$ kubectl apply -f https://kuboard.cn/install-script/calico/calico-3.13.1.yaml

查看k8s状态

$ kubectl get nodeNAME    STATUS   ROLES    AGE   VERSIONnode1   Ready    master   14m   v1.18.2
添加计算节点重复 0-9 步，完成环境准备，安装软件及下载镜像。10. kubeadm将计算节点加入集群
kubeadm join 10.0.2.15:6443 --token 8o8yv2.tv836s1jncmrwgsp \    --discovery-token-ca-cert-hash sha256:d9d15b5905e8defc0ef6de294be5eff08d720ed9d98b8d6cb99d153bc7332e8e

查看k8s状态

$ kubectl get nodeNAME    STATUS   ROLES    AGE   VERSIONnode1   Ready    master   14m   v1.18.2node2   Ready    &lt;none&gt;   30m   v1.18.2
补充说明：
安装时可以通过kubeadm config print init-defaults查看默认配置

$ kubeadm config print init-defaultsapiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.0123456789abcdef  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 1.2.3.4  bindPort: 6443nodeRegistration:  criSocket: /var/run/dockershim.sock  name: master1  taints:  - effect: NoSchedule    key: node-role.kubernetes.io/master---apiServer:  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns:  type: CoreDNSetcd:  local:    dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: v1.18.0networking:  dnsDomain: cluster.local  serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;

使用kubeadm init --config可以指定kubeadm配置文件

参考文章使用kubeadm安装kubernetes_v1.18.x利用 kubeadm 创建高可用集群
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes工具集（持续收集------）</title>
    <url>/posts/5d02d748c56e.html</url>
    <content><![CDATA[Kubescope cliKubescope cli 是一个可以运行在本地或 Kubernetes 中的工具，可直接从 Docker Daemon 中收集容器指标并可视化。和 cAdvisor 等其他集群指标收集服务一样， kubescope cli 收集指标的周期是 1 秒（而不是 10-15 秒）。如果周期是 10-15 秒，你可能会在测试期间错过一些引发性能瓶颈的问题。如果你使用 cAdvisor 进行测试，每次都要使用新的 Pod 作为测试对象，因为 Kubernetes 在超过资源限制时就会将 Pod 杀死，然后重新启动一个全新的 Pod。而 kubescope cli 就没有这方面的忧虑，它直接从 Docker Daemon 中收集容器指标（你可以自定义收集指标的时间间隔），并使用正则表达式来选择和过滤你想要显示的容器。

KubeMarkKubemark是K8s官方提供的一个对K8s集群进行性能测试的工具。它可以模拟出一个K8s cluster（Kubemark cluster），不受资源限制，从而能够测试的集群规模比真实集群大的多。这个cluster中master是真实的机器，所有的nodes是Hollow nodes。Hollow nodes执行的还是真实的K8s程序，只是不会调用Docker，因此测试会走一套K8s API调用的完整流程，但是不会真正创建pod。
Kubefwdkubefwd 是一个用于端口转发Kubernetes中指定namespace下的全部或者部分pod的命令行工具。 kubefwd 使用本地的环回IP地址转发需要访问的service，并且使用与service相同的端口。 kubefwd 会临时将service的域条目添加到 &#x2F;etc&#x2F;hosts 文件中。
  
kubectl-debugkubectl-debug 是一个简单的 kubectl 插件, 能够帮助你便捷地进行 Kubernetes 上的 Pod 排障诊断. 背后做的事情很简单: 在运行中的 Pod 上额外起一个新容器, 并将新容器加入到目标容器的 pid, network, user 以及 ipc namespace 中, 这时我们就可以在新容器中直接用 netstat, tcpdump 这些熟悉的工具来解决问题了, 而旧容器可以保持最小化, 不需要预装任何额外的排障工具.

nsenter命令nsenter 命令是一个可以在指定进程的命令空间下运行指定程序的命令。它位于util-linux包中。它的命令语法如下：
nsenter [options] [program [arguments]]options:-t, --target pid：指定被进入命名空间的目标进程的pid-m, --mount[=file]：进入mount命令空间。如果指定了file，则进入file的命令空间-u, --uts[=file]：进入uts命令空间。如果指定了file，则进入file的命令空间-i, --ipc[=file]：进入ipc命令空间。如果指定了file，则进入file的命令空间-n, --net[=file]：进入net命令空间。如果指定了file，则进入file的命令空间-p, --pid[=file]：进入pid命令空间。如果指定了file，则进入file的命令空间-U, --user[=file]：进入user命令空间。如果指定了file，则进入file的命令空间-G, --setgid gid：设置运行程序的gid-S, --setuid uid：设置运行程序的uid-r, --root[=directory]：设置根目录-w, --wd[=directory]：设置工作目录如果没有给出program，则默认执行$SHELL。
具体使用实例
$ docker inspect -f &#123;&#123;.State.Pid&#125;&#125; nginx ## 其中nginx为需要查看的容器名，也可以使用用Container ID$ nsenter --target 3326 --mount --uts --ipc --net --pid

kt-connectkt-connect 是一个可以让开发环境访问K8S集群下应用的工具
LensKubernetes可视化管理工具
Container-diffcontainer-diff这是一个可对两个镜像进行内容对比的工具。比较本地镜像使用daemon://。例如：
# container-diff diff daemon://modified_debian:latest remote://gcr.io/google-appengine/debian8:latest

kube-scoreKube-Score是一款针对Kubernetes的性能及安全分析工具，该工具能够对Kubernetes对象定义进行静态代码分析，并给出提升Kubernetes性能和安全性方面的建议。
kubectx、kubenskubectx快速切换管理k8s的集群kubens快速切换K8s集群的namespace将多个kubectl config配置文件进行合并
$ KUBECONFIG=a.conf:b.conf:c.conf kubectl config view --flatten &gt; .kube/config


kubecmkubecm : Manage your kubeconfig more easily.doc
webkubectlwebkubectl: 页面版k9s&#x2F;kubectl
参考文章Kubernetes集群性能测试kubemark 搭建测试集群和性能测试简化 Pod 故障诊断: kubectl-debug 介绍A Select List of Kubernetes Tools
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes更新etcd证书</title>
    <url>/posts/f5c1675e23d0.html</url>
    <content><![CDATA[在三台etcd所在的主机上执行以下操作
$ cd /etc/kubernetes/pki$ mv etcd etcd.bak$ mkdir apiserver-etcd-client$ mv apiserver-etcd-client.* apiserver-etcd-client/$ kubeadm alpha phase certs etcd-ca$ kubeadm alpha phase certs etcd-server --config=/etc/kubernetes/kubeadmcfg.yaml$ kubeadm alpha phase certs etcd-peer --config=/etc/kubernetes/kubeadmcfg.yaml$ kubeadm alpha phase certs etcd-healthcheck-client --config=/etc/kubernetes/kubeadmcfg.yaml$ kubeadm alpha phase certs apiserver-etcd-client --config=/etc/kubernetes/kubeadmcfg.yaml
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubespray安装Kubernates过程</title>
    <url>/posts/d7ab66720320.html</url>
    <content><![CDATA[
准备机器，绑定好IP（请关闭ipv6功能），关闭防火墙，并配置好hostname




在bation机器上设置免密码登录，并下载好kubespray安装脚本

先安装python3与python3-pip，并将pip进行升级


$ yum install python3 python3-pip -y$ pip3 install --upgrade pip


安装python依赖包

$ pip install -r requirements.txt  -i https://pypi.douban.com/simple


更改镜像地址，特别是国内网络安装时

$ vim roles/download/defaults/main.yml
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-3升级Docker服务</title>
    <url>/posts/6103ce3be10c.html</url>
    <content><![CDATA[背景介绍OpenShift是一个开源产品，自2015年3.0版本开始，它使用Kubernetes作为底层的编排引擎，已经有将近5年了。同时它也有企业级版本，服务了众多大中小企业。经过这些年生产上的实践，OpenShift 3不断完善，已经非常稳定了，但是它依赖组件多，不断增加新的功能，使用量不断增多，一些漏洞和问题会被发现。在生产环境中，运行着大量的生产级应用，我们该如何升级Docker等组件，把业务影响降到最小，这是每个企业都无法绕过的问题。Docker作为OpenShift 3默认的容器运行，它的可靠性也尤为重要。本篇就以Docker升级为例介绍如何对OpenShift组件进行升级。升级不同OpenShift组件的思路是一样，主要是如下两条。

逐个节点升级
升级前将业务应用迁走

升级Docker实操
更新yum源中的docker包

$ cp docker-rpm/* ./extras/Packages/d/$ createrepo --update extras

迁移节点上的POD并将它设置为不可调度

$ oc adm drain &lt;node_name&gt; --force --delete-local-data --ignore-daemonsets 

排除不需要升级的软件

$ atomic-openshift-docker-excluder exclude $ atomic-openshift-excluder exclude 

升级docker

$ yum clean all$ yum update docker

重启服务或者重启主机

Master节点
$ systemctl restart docker$ master-restart api$ master-restart controllers $ systemctl restart origin-node
Node节点
$ systemctl restart docker$ systemctl restart origin-node
或者
$ reboot

将节点设置为可调度

$ oc adm uncordon &lt;node_name&gt; 
参考文章https://access.redhat.com/solutions/3874691
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-4-3部署安装（国内需要使用科学上网支持）</title>
    <url>/posts/67a6791f98e4.html</url>
    <content><![CDATA[准备好本机访问科学上网环境
安装pip软件

$ yum install python2-pip

安装shadowsocks包

pip install shadowsocks -i https://pypi.douban.com/simple

编辑文件&#x2F;etc&#x2F;shadowsocks.json

$ cat &gt; /etc/shadowsocks.json &lt;&lt;EOF&#123;    &quot;server&quot;:&quot;shadowsocks的IP&quot;,    &quot;server_port&quot;:8388,    &quot;local_address&quot;: &quot;0.0.0.0&quot;,    &quot;local_port&quot;:1080,    &quot;password&quot;:&quot;fuuuuuuuuuuuuckgfw&quot;,    &quot;timeout&quot;:300,    &quot;method&quot;:&quot;rc4-md5&quot;,    &quot;fast_open&quot;: false&#125;EOF

启动shadowsocks客户端

$ sslocal -c /etc/shadowsocks.json -d start
如果需要暂停，执行以下命令
$ sudo sslocal -c /etc/shadowsocks.json -d stop

安装privoxy软件将sockets5转为http协议

$ yum install privoxy$ cat &gt;/etc/privoxy/config &lt;&lt;EOFforward-socks5t   /               127.0.0.1:1080 .listen-address  localhost:8118EOF$ systemctl start privoxy

设置本机的http代理服务

$ export http_proxy=http://localhost:8118$ export https_proxy=http://localhost:8118$ export no_proxy=&quot;registry.example.com,127.0.0.1,192.168.1.21&quot;
其中no_proxy为本地镜像仓库，将会用来接收OpenShift 4的官方镜像。
同步镜像镜像同步完成后的提示
[root@registry ~]# oc adm -a $&#123;LOCAL_SECRET_JSON&#125; release mirror      --from=quay.io/$&#123;PRODUCT_REPO&#125;/$&#123;RELEASE_NAME&#125;:$&#123;OCP_RELEASE&#125;      --to=$&#123;LOCAL_REGISTRY&#125;/$&#123;LOCAL_REPOSITORY&#125;      --to-release-image=$&#123;LOCAL_REGISTRY&#125;/$&#123;LOCAL_REPOSITORY&#125;:$&#123;OCP_RELEASE&#125;info: Mirroring 109 images to registry.fcloudy.com:5000/ocp4/openshift4 ...registry.fcloudy.com:5000/......info: Mirroring completed in 4m46.5s (3.265MB/s)SuccessUpdate image:  registry.fcloudy.com:5000/ocp4/openshift4:4.4.3-x86_64Mirror prefix: registry.fcloudy.com:5000/ocp4/openshift4To use the new mirrored repository to install, add the following section to the install-config.yaml:imageContentSources:- mirrors:  - registry.fcloudy.com:5000/ocp4/openshift4  source: quay.io/openshift-release-dev/ocp-release- mirrors:  - registry.fcloudy.com:5000/ocp4/openshift4  source: quay.io/openshift-release-dev/ocp-v4.0-art-devTo use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy:apiVersion: operator.openshift.io/v1alpha1kind: ImageContentSourcePolicymetadata:  name: examplespec:  repositoryDigestMirrors:  - mirrors:    - registry.fcloudy.com:5000/ocp4/openshift4    source: quay.io/openshift-release-dev/ocp-release  - mirrors:    - registry.fcloudy.com:5000/ocp4/openshift4    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
检查本地镜像仓库状态$ curl -u admin:admin -k https://registry.fcloudy.com:5000/v2/_catalog


下载相关的coreos文件文件列表可参考 https://github.com/RedHatOfficial/ocp4-helpernode/blob/master/vars/main.yml列表如下：
https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.4/latest/rhcos-4.4.3-x86_64-metal.x86_64.raw.gzhttps://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.4/latest/rhcos-4.4.3-x86_64-installer-kernel-x86_64https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.4/latest/rhcos-4.4.3-x86_64-installer-initramfs.x86_64.imghttps://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.4.3/openshift-client-linux-4.4.3.tar.gzhttps://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.4.3/openshift-install-linux-4.4.3.tar.gz




问题1、安装master与node节点时，需要关闭helper机器的selinux
$ setenforce 0
2、Vmware 节点引导使用BIOS3、
创建机器下载rhcos镜像地址 https://mirror.openshift.com/pub/openshift-v4/x86_64&#x2F;dependencies&#x2F;rhcos&#x2F;4.4&#x2F;latest&#x2F;通过vmware导入创建rhcos服务器，分别作为bootstrap、master、node机器。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-ImageStream指向的镜像是否为内部镜像仓库</title>
    <url>/posts/fcc1189de870.html</url>
    <content><![CDATA[
ImageStream是OpenShift中的一个特有的资源。在DeploymentConfig&#x2F;BuildConfig中使用ImageStream和ImageStreamTag时经常会有这样的疑惑：究竟它指向的镜像有没有pull到OpenShift内部的镜像仓库呢？我们使用DeploymentConfig发布应用时，集群会从内部镜像仓库中pull镜像，还是从指向的外部镜像仓库中拉取？大家来看一下下面这个例子：
$ oc import-image openshift/jenkins:v3.10 --from=docker.io/openshift/jenkins-2-centos7:v3.10 --confirm
猜一猜，以上这条命令是否会将镜像拉取到内部镜像仓库？答案是否定的，内部镜像仓库并不会拉取镜像本身，只是获取镜像的元数据，将创建的ImageStreamTag openshift/jenkins:v3.10指向镜像docker.io/openshift/jenkins-2-centos7:v3.10。这里的关键就是ImageStream[Tag]的一个配置项：referencePolicy.type。导入镜像时，referencePolicy.type默认为source。所以如果需要将导入镜像到ImageStream[Tag]，并将它自动拉取到内部镜像仓库，需要设置referencePolicy.type为local，在DeploymentConfig第一次部署拉取镜像时，镜像也会导入到私有镜像仓库中。
apiVersion: v1kind: ImageStreammetadata:  ...  tags:    - ...      referencePolicy:        type: Local
在使用oc import-image时需要添加参数--reference-policy=&#39;local&#39;，如下
$ oc import-image openshift/jenkins:v3.10 --reference-policy=&#x27;local&#x27; --from=docker.io/openshift/jenkins-2-centos7:v3.10 --confirm

或者是通过docker push将镜像推送到镜像仓库
$ docker pull docker.io/openshift/jenkins-2-centos7:v3.10 $ docker tag docker.io/openshift/jenkins-2-centos7:v3.10 docker-registry-default.example.com/openshift/jenkins-2-centos7:v3.10 $ docker login -p `oc whoami -t`  -e unused -u unused docker-registry-default.example.com$ docker push docker-registry-default.example.com/openshift/jenkins-2-centos7:v3.10




参考文章：https://austindewey.com/2018/12/09/a-word-about-openshift-imagestreams/
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Kafka-Operator测试报告</title>
    <url>/posts/ca6a355f0867.html</url>
    <content><![CDATA[1. 测试环境物理机设备：集群（3 master 2 infra 3 node）  管理（1manager 部署LB+外部镜像仓库+DNS）物理机配置：40c 378GOpenshift：v3.11.0+1a90c5c-83Kubernetes：v1.11.0+d4cacc0Kafka Operator：amq-streams-1.1.0Kafka：2.1.1
2. 测试要求和内容根据测试要求，本次具体测试的功能点如下：



序号
测试项目
测试项
优先级



1
Kafka Operator部署
Kafka Operator部署
高




Kafka集群创建
高




Kafka监控部署
高


2
Kafka功能测试
Kafka创建Topic
高




Openshift集群内部Kafka producer与consumer测试
高




Openshift集群外部Kafka producer与consumer测试（NodePort方案）
高




Openshift集群外部Kafka producer与consumer测试（Router方案）
高


3
Kafka性能测试
Kafka写入消息压力测试[集群内部]
高




Kafka 消费消息压力测试[集群内部]
高




Kafka写入消息压力测试[集群外部NodePort方案]
高




Kafka 消费消息压力测试[集群外部NodePort方案]
高




Kafka写入消息压力测试[集群外部Router方案]
中




Kafka 消费消息压力测试[集群外部Router方案]
中


4
高可用性测试
Broker异常，恢复时长
中




磁盘故障，数据影响
中


3. 测试方法和结果3.1 Kafka Operator部署3.1.1 Kafka Operator部署3.1.1.1 测试方法和步骤测试准备
步骤一：拉取需要镜像，并将它们导入内部镜像仓库
步骤二：下载部署脚本，并解压服务器上
$ wget http://sha.stor.mbcloud.com/openshift-deploy-material/amq-streams-1.1.0-ocp-install-examples.zip$ unzip amq-streams-1.1.0-ocp-install-examples.zip
步骤三：将脚本中访问外网镜像仓库地址，改成内网镜像地址将install&#x2F;cluster-operator&#x2F;050-Deployment-strimzi-cluster-operator.yaml中的registry.access.redhat.com更改为harbor.apps.it.mbcloud.com
测试过程
步骤一：创建项目myproject
$ oc new-project myproject

步骤二：安装部署Operator
$ oc apply -f install/cluster-operator
3.1.1.2 测试结果完成Kafka Operator部署
3.1.2  Kafka集群创建3.1.2.1 测试方法和步骤步骤一：进入myproject项目
$ oc project myproject

步骤二：创建Kafka集群my-cluster,三副本，存储无持久化
$ cat &lt;&lt; EOF | oc create -f -apiVersion: kafka.strimzi.io/v1alpha1kind: Kafkametadata:  name: my-clusterspec:  kafka:    version: 2.1.1    replicas: 3    listeners:      plain: &#123;&#125;      tls: &#123;&#125;      external:        type: route    config:      offsets.topic.replication.factor: 3      transaction.state.log.replication.factor: 3      transaction.state.log.min.isr: 2      log.message.format.version: &quot;2.1&quot;    storage:      type: ephemeral  zookeeper:    replicas: 3    storage:      type: ephemeral  entityOperator:    topicOperator: &#123;&#125;    userOperator: &#123;&#125;EOF
步骤三：查看部署结果
$ oc get podNAME                                              READY     STATUS    RESTARTS   AGEmy-cluster-entity-operator-78dc68cc86-9ghmw       3/3       Running   0          6dmy-cluster-kafka-0                                2/2       Running   0          6dmy-cluster-kafka-1                                2/2       Running   0          6dmy-cluster-kafka-2                                2/2       Running   0          6dmy-cluster-zookeeper-0                            2/2       Running   0          6dmy-cluster-zookeeper-1                            2/2       Running   0          6dmy-cluster-zookeeper-2                            2/2       Running   0          6dstrimzi-cluster-operator-7f46bcb9c6-s8x5r         1/1       Running   0          6d
3.1.2.2 测试结果部署Kafka集群my-cluster。3 zookeeper + 3 kafka

3.1.3 Kafka监控部署3.1.3.1 测试方法和步骤步骤一：在kafka my-cluster的配置中添加metrics设置，
参考example&#x2F;metrics&#x2F;kafka-metrics.yaml文件中metrics的属性
步骤二：给prometheus添加查看myproject权限
$ oc policy add-role-to-user view system:serviceaccount:openshift-monitoring:prometheus-k8s -n myproject
步骤三：在openshift-monitoring下新建servicemonitor
$ cat &lt;&lt; EOF | oc create -f -apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  labels:    k8s-app: prometheus  name: my-cluster-kafka  namespace: openshift-monitoringspec:  endpoints:  - port: metrics  namespaceSelector:    matchNames:    - myproject  selector:    matchLabels:      strimzi.io/name: my-cluster-kafka-bootstrap
步骤四：在grafana中添加Kafka的监控Dashboard
Dashboard配置参考examples&#x2F;metrics&#x2F;grafana-dashboards&#x2F;strimzi-kafka.json
3.1.3.2 测试结果在grafana上查看Kafka的监控图表

3.2.Kafka功能测试3.2.1 Kafka创建Topic3.2.1.1 测试方法和步骤步骤一：进入myproject项目
$ oc project myproject
步骤二：创建Topic
$ cat &lt;&lt; EOF | oc create -f -apiVersion: kafka.strimzi.io/v1alpha1kind: KafkaTopicmetadata:  name: my-topic  labels:    strimzi.io/cluster: my-clusterspec:  partitions: 3  replicas: 3  config:    retention.ms: 7200000    segment.bytes: 1073741824EOF
步骤三：验证Topic状态
$ oc exec -c zookeeper  my-cluster-zookeeper-0 --  bin/kafka-topics.sh --zookeeper 127.0.0.1:21810 --topic my-topic --describeOpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=NTopic:my-topic  PartitionCount:3  ReplicationFactor:3  Configs:message.format.version=2.1-IV2Topic: my-topic  Partition: 0  Leader: 0  Replicas: 0,1,2  Isr: 0,1,2Topic: my-topic  Partition: 1  Leader: 1  Replicas: 1,2,0  Isr: 0,1,2Topic: my-topic  Partition: 2  Leader: 2  Replicas: 2,0,1  Isr: 0,1,2
3.2.1.2 测试结果my-topic Topic创建成功。为3分区3复本。
3.2.2 Openshift集群内部Kafka producer与consumer测试3.2.2.1 测试方法和步骤步骤一：运行consumer端
$ oc run kafka-consumer -ti --image=amq7/amq-streams-kafka:1.1.0-kafka-2.1.1 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-topic
步骤二：运行producer端
$ oc run kafka-producer -ti --image=amq7/amq-streams-kafka:1.1.0-kafka-2.1.1 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic my-topic
步骤三：在producer端发送数据，在consumer端显示发送的数据
3.2.2.2 测试结果通过my-topic Topic，producer端产生数据，consumer端消费数据

3.2.3 Openshift集群外部Kafka producer与consumer测试（NodePort方案）3.2.3.1 测试方法和步骤TLS设置为False
步骤一：将Kafka的配置项中添加external为nodeport，并固定nodeport为32678
listeners:      external:        tls: false        type: nodeport        overrides:          bootstrap:            nodePort: 32678
步骤二：集群外网环境创建Kafka Producer，连接Kafka
from kafka import KafkaProducerproducer = KafkaProducer(bootstrap_servers=[&#x27;99.248.81.70:32678&#x27;])producer.send(&#x27;my-topic&#x27;, &#x27;hello world&#x27;)producer.close()
步骤三：集群外网环境新建Kafka Consumer，消费Kafka中my-topic主题的数据
from kafka import KafkaConsumerconsumer = KafkaConsumer(&#x27;my-topic&#x27;, bootstrap_servers=[&#x27;99.248.81.70:32678&#x27;])for msg in consumer:    print(msg)
TLS设置为True
步骤一：将Kafka的配置项中添加external为nodeport，并固定nodeport为32678
listeners:      external:        type: nodeport        overrides:          bootstrap:            nodePort: 32678
步骤二：获得secret&#x2F;my-cluster-clients-ca&#x2F;ca.key与secret&#x2F;my-cluster-clients-ca-cert&#x2F;ca.crt
$ oc extract secret/my-cluster-cluster-ca-cert --keys=ca.crt --to=- &gt; ca.crt$ oc extract secret/my-cluster-clients-ca --keys=ca.key --to=- &gt; ca.key
步骤三：集群外网环境创建Kafka Producer，连接Kafka
from kafka import KafkaProducerproducer = KafkaProducer(bootstrap_servers=[&#x27;99.248.81.70:32678&#x27;], ssl_check_hostname=False, ssl_cafile=&#x27;ca.crt&#x27;, ssl_certfile=&#x27;ca.key&#x27;, security_protocol=&#x27;SSL&#x27;)producer.send(&#x27;my-topic&#x27;, &#x27;hello world&#x27;)producer.close()
步骤三：集群外网环境新建Kafka Consumer，消费Kafka中my-topic主题的数据
from kafka import KafkaConsumerconsumer = KafkaConsumer(&#x27;my-topic&#x27;, bootstrap_servers=[&#x27;99.248.81.70:32678&#x27;], ssl_check_hostname=False, ssl_cafile=&#x27;ca.crt&#x27;, ssl_certfile=&#x27;ca.key&#x27;, security_protocol=&#x27;SSL&#x27;)for msg in consumer:    print(msg)
3.2.3.2 测试结果与TLS设置为False一样
通过nodePort连接到Kafka集群，Producer将会往Topic my-topic发送一条“hello world”的消息，Consumer将会将消息输出到console端
C:\Python27\python.exe D:/workspace/mb_autopf/utils/kafka-test/consumer.pyConsumerRecord(topic=u&#x27;my-topic&#x27;, partition=0, offset=333338, timestamp=1558511698388L, timestamp_type=0, key=None, value=&#x27;hello world&#x27;, checksum=-1265845277, serialized_key_size=-1, serialized_value_size=11)
3.2.4 Openshift集群外部Kafka producer与consumer测试（Router方案）3.2.4.1 测试方法和步骤步骤一：将Kafka的配置项中添加external为nodeport，并固定nodeport为32678
listeners:      external:        type: route
步骤二：获得secret&#x2F;my-cluster-clients-ca&#x2F;ca.key与secret&#x2F;my-cluster-clients-ca-cert&#x2F;ca.crt
$ oc extract secret/my-cluster-cluster-ca-cert --keys=ca.crt --to=- &gt; ca.crt$ oc extract secret/my-cluster-clients-ca --keys=ca.key --to=- &gt; ca.key
步骤三：获取对外的router域名
$ oc get route my-cluster-kafka-bootstrap --template=&#123;&#123;range.status.ingress&#125;&#125;&#123;&#123;.host&#125;&#125;&#123;&#123;println&#125;&#125;&#123;&#123;end&#125;&#125;my-cluster-kafka-bootstrap-myproject.apps.paas1.mbcloud.com
步骤四：集群外网环境创建Kafka Producer，连接Kafka
from kafka import KafkaProducerproducer = KafkaProducer(bootstrap_servers=[&#x27;my-cluster-kafka-bootstrap-myproject.apps.paas1.mbcloud.com:443&#x27;], ssl_check_hostname=False, ssl_cafile=&#x27;ca.crt&#x27;, ssl_certfile=&#x27;ca.key&#x27;, security_protocol=&#x27;SSL&#x27;)producer.send(&#x27;my-topic&#x27;, &#x27;hello world&#x27;)producer.close()
步骤三：集群外网环境新建Kafka Consumer，消费Kafka中my-topic主题的数据
from kafka import KafkaConsumerconsumer = KafkaConsumer(&#x27;my-topic&#x27;, bootstrap_servers=[&#x27;my-cluster-kafka-bootstrap-myproject.apps.paas1.mbcloud.com:443&#x27;], ssl_check_hostname=False, ssl_cafile=&#x27;ca.crt&#x27;, ssl_certfile=&#x27;ca.key&#x27;, security_protocol=&#x27;SSL&#x27;)for msg in consumer:    print(msg)
3.2.4.2 测试结果通过Route域名连接到Kafka集群，Producer将会往Topic my-topic发送一条“hello world”的消息，Consumer将会将消息输出到console端
C:\Python27\python.exe D:/workspace/mb_autopf/utils/kafka-test/consumer.pyConsumerRecord(topic=u&#x27;my-topic&#x27;, partition=0, offset=333340, timestamp=1558514729435L, timestamp_type=0, key=None, value=&#x27;hello world&#x27;, checksum=-459554926, serialized_key_size=-1, serialized_value_size=11)
3.3 Kafka性能测试3.3.1 测试工具：Kafka自带的压测脚本bin/kafka-producer-perf-test.shbin/kafka-consumer-perf-test.sh
3.3.1.1 参数说明:bin&#x2F;kafka-producer-perf-test.sh–num-records：发送的总的消息数  –record-size：消息大小–throughput：最大消息吞吐限制–producer-props：kafka集群参数acks：1默认值副本收到数据得到确认后发送下一条数据，0可靠性最低无需确认副本是否收到数据，-1可靠性最高等待副本所有的follower都确认收到数据后再发送下一条数据retries：2linger.ms:基于时间的batching策略1batch.size:基于大小的batching策略100000compression.type:压缩类型,速度排序lz4 &#x3D; snappy &lt; gzip
以下条件之一达到了,认为一个batch是完毕的
1. batch.size达到2. linger.ms达到3. 同一个broker的其他batch已经完毕4. flush()和close()被调用
bin&#x2F;kafka-consumer-perf-test.sh–zookeeper：zookeeper连接信息–broker-list：kafka连接信息–topic：topic名称–fetch-size：每次fetch的数据大小，1048576为1M–messages：总共要消费的消息数
3.3.1.2 测试命令bin/kafka-producer-perf-test.sh --topic my-topic --num-records 100000000 --record-size 100 --throughput 5000000 --producer-props bootstrap.servers=my-cluster-kafka-bootstrap.myproject.svc:9092 retries=2 batch.size=100000bin/kafka-consumer-perf-test.sh --broker-list my-cluster-kafka-bootstrap.myproject.svc:9092 --topic my-topic --messages 100000000
3.3.2 Kafka生产消息压力测试[集群内部]3.3.2.1 测试方法和步骤步骤一：创建4种类型Topic：1副本1分区、1副本3分区、3副本1分区、3副本3分区
步骤二：运行kafka写入压测命令
bin/kafka-producer-perf-test.sh --topic my-topic --num-records 10000000 --record-size 100 --throughput 5000000 --producer-props bootstrap.servers=my-cluster-kafka-bootstrap.myproject.svc:9092 linger.ms=1
3.3.2.2 测试结果使用kafka-producer-perf-test.sh测试Kafka消息写入性能：



副本数
分区数
线程数
消息&#x2F;s
流量MB&#x2F;s
延时99th ms



1
1
1
1514692
144.45
2


1
3
1
1462009
139.43
7


3
1
1
987069
94.13
9


3
3
1
918020
87.55
23


3.3.3 Kafka 消费消息压力测试[集群内部]3.3.3.1 测试方法和步骤步骤一：使用3.3.2中创建的消息进行消费
步骤二：运行kafka消费压测命令
bin/kafka-consumer-perf-test.sh --broker-list my-cluster-kafka-bootstrap.myproject.svc:9092 --topic my-topic --fetch-size 1048576 --messages 100000000 --threads 1
3.3.3.2 测试结果使用kafka-consumer-perf-test.sh测试Kafka消息消费性能：



副本数
分区数
线程数
消息&#x2F;s
流量MB&#x2F;s



1
1
1
800262.4861
76.3190


1
3
1
1000100.0100
95.3770


3
1
1
3037667.0717
289.6945


3
3
1
796854.8513
75.9940


1
1
3
800409.8098
76.3330


1
3
3
974896.4173
92.9734


3
1
3
3033060.3579
289.2552


3
3
3
795820.1480
75.8953


3.3.4 Kafka生产消息压力测试[集群外部NodePort方案]3.3.4.1 测试方法和步骤步骤一：创建4种类型Topic：1副本1分区、1副本3分区、3副本1分区、3副本3分区
步骤二：运行kafka写入压测命令
bin/kafka-producer-perf-test.sh --topic my-topic --num-records 10000000 --record-size 100 --throughput 5000000 --producer-props bootstrap.servers=99.248.82.11:32678 linger.ms=1
3.3.4.2 测试结果使用kafka-producer-perf-test.sh测试Kafka消息写入性能：



副本数
分区数
线程数
消息&#x2F;s
流量MB&#x2F;s
延时99th ms



1
1
1
884486
84.35
34


1
3
1
880669
83.99
73


3
1
1
822842
78.47
258


3
3
1
607053
57.89
193


3.3.5 Kafka 消费消息压力测试[集群外部NodePort方案]3.3.5.1 测试方法和步骤步骤一：使用3.3.2中创建的消息进行消费
步骤二：运行kafka消费压测命令
bin/kafka-consumer-perf-test.sh --broker-list 99.248.82.11:32678 --topic my-topic --fetch-size 1048576 --messages 100000000 --threads 1
3.3.5.2 测试结果


副本数
分区数
线程数
消息&#x2F;s
流量MB&#x2F;s



1
1
1
387672.0295
36.9713


1
3
1
389226.2183
37.1195


3
1
1
564939.8339
53.8769


3
3
1
432991.3929
41.2932


1
1
3
386488.3667
36.8584


1
3
3
389468.7646
37.1426


3
1
3
563920.3744
53.7796


3
3
3
433067.1543
41.3004


3.4 附录3.4.1 镜像列表harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-cluster-operator:1.1.0harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-zookeeper:1.1.0-kafka-2.1.1harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-kafka:1.1.0-kafka-2.1.1harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-kafka-connect:1.1.0-kafka-2.1.1harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-kafka-connect-s2i:1.1.0-kafka-2.1.1harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-kafka-mirror-maker:1.1.0-kafka-2.1.1harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-topic-operator:1.1.0harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-user-operator:1.1.0harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-kafka-init:1.1.0harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-zookeeper-stunnel:1.1.0harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-kafka-stunnel:1.1.0harbor.apps.it.mbcloud.com&#x2F;amq7&#x2F;amq-streams-entity-operator-stunnel:1.1.0
3.4.2 部署脚本strimzi-kafka-operator
百度云盘：&#x2F;招银云创&#x2F;openshift&#x2F;redhat-kafka&#x2F;
while true; do a&#x3D;$(date); echo $a; echo $a &gt; ~&#x2F;a.txt; .&#x2F;kafka-console-producer.sh –broker-list 99.248.82.11:31761,99.248.82.12:31304,99.248.82.21:31271 –topic my_topic &lt; ~&#x2F;a.txt; done
.&#x2F;kafka-console-consumer.sh –bootstrap-server 99.248.82.11:31761,99.248.82.12:31304,99.248.82.21:31271 –topic my_topic
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Kubernetes集群-Calico-BGP管理工具calicoctl配置</title>
    <url>/posts/4b14c7672798.html</url>
    <content><![CDATA[
calico 是容器网络的一种解决方案，也是当前最流行的方案之一。它完全利用路由规则实现动态组网，通过BGP协议通告路由。Calico BGP没有像ovs那样需要封包解包，所以它的网络性能更好。管理calico网络免不了使用calicoctl工具，本篇介绍如何在OpenShift&#x2F;Kubernetes环境下，配置calicoctl来管理集群网络。
calico元数据支持两种存储类：etcd与kubernetes

安装calicoctl

$ curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.10.0/calicoctl$ chmod a+x calicoctl

确认Calico部署时使用的元数据存储类型，查看calico-config

$ oc describe cm calico-config -n kube-system | grep datastore_type
可以为kubernetes、etcdv3。默认为etcdv3。设置为kubernetes时表示直接使用k8s api存取数据库服务；
使用kubernetes类型创建calicoctl访问的配置文件calicoctl.conf
$ mkdir /etc/calico$ cat &lt;&lt; EOF &gt; /etc/calico/calicoctl.cfgapiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec:  datastoreType: &quot;kubernetes&quot;  kubeconfig: &quot;/root/.kube/config&quot;EOF
使用etcdv3类型
创建calicoctl访问的配置文件calicoctl.conf

for openshift
$ mkdir /etc/calico$ cat &lt;&lt; EOF &gt; /etc/calico/calicoctl.cfgapiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec:  datastoreType: &quot;etcdv3&quot;  etcdEndpoints: https://master1.example.com:2379  etcdKeyFile: /etc/cni/net.d/calico-tls/etcd-key  etcdCertFile: /etc/cni/net.d/calico-tls/etcd-cert  etcdCACertFile: /etc/cni/net.d/calico-tls/etcd-caEOF
for kubernetes
$ mkdir /etc/calico$ cat &lt;&lt; EOF &gt; /etc/calico/calicoctl.cfgapiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec:  datastoreType: &quot;etcdv3&quot;  etcdEndpoints: https://master1.example.com:2379  etcdKeyFile: /etc/kubernetes/pki/etcd/server.key  etcdCertFile: /etc/kubernetes/pki/etcd/server.crt  etcdCACertFile: /etc/kubernetes/pki/etcd/ca.crtEOF

执行calicoctl获取workloadendpoints

$ ./calicoctl get workloadendpointsWORKLOAD                   NODE                        NETWORKS           INTERFACE         docker-registry-3-fr8zn    infra1.example.com    10.129.200.29/32   cali046d7771a9f   registry-console-3-bxbck   master1.example.com   10.131.9.210/32    cali6d8bb449db0$ ./calicoctl get workloadendpoints -a # 查看所有namespace下的workloadendpoints
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Master1彻底挂了，如何恢复？</title>
    <url>/posts/ddc119be7456.html</url>
    <content><![CDATA[
小强维护着一套生成的OpenShift集群，突然有一天集群的master1节点出现异常，自动关机了。他尝试了多次，都无法开机，怎么办？他需要赶快恢复master1节点，来满足集群的高可用性。
原来的masters在ansible&#x2F;hosts中的顺序如下
[masters]master1  ## 重要主节点，安装完后单独保存/etc/etcd/ca中的证书master2master3[etcd]master1master2master3[nodes]master1master2master3

恢复过程如下：

新建一台 master1节点，hostname 与 IP 都和原 master1 节点一致
在master2上恢复master主节点的证书、ca.serial.txt及openshift软件。
通过新增 master 的方式将这个节点重新加回集群
通过新增 etcd 的方法，恢复了这台 master 节点 etcd 的状况

以下是恢复的具体步骤。
一、初始化Master节点
与部署机互信
开启selinux
关闭firewalld
DNS配置或者&#x2F;etc&#x2F;hosts
HostName设置
NTP
ETCD盘与docker盘，如果需要提前准备的话
Yum源设置

二、删除问题Master节点oc delete node master1

三、在master2节点上准备好相关证书及软件
恢复master主节点的证书在&#x2F;etc&#x2F;ansible&#x2F;hosts的[masters]&#x2F;[etcd]&#x2F;[nodes]组中注释掉master1节点，后执行

$ ansible-playbook playbooks/openshift-etcd/redeploy-ca.yml

恢复ca.serial.txt文件在备份的配置中将ca.serial.txt文件拷贝到&#x2F;etc&#x2F;origin&#x2F;master&#x2F;ca.serial.txt

$ scp ca.serial.txt master2:/etc/origin/master/ca.serial.txt

安装openshift命令

$ yum install openshift
如果显示找不到该包，请查看&#x2F;etc&#x2F;yum.conf的配置，将openshift包从exclude中删除
四、恢复Master节点
&#x2F;etc&#x2F;ansible&#x2F;hosts中将需要恢复的节点从[masters]&#x2F;[etcd]&#x2F;[nodes]组移到[new_masters]、[new_nodes]组，并将三个组添加到[OSEv3:children]

[OSEv3:children]mastersetcdnodesnew_mastersnew_etcdnew_nodes[masters]master2master3[etcd]master2master3[nodes]master2master3[new_masters]master1[new_etcd]master1[new_nodes]master1

执行master节点的scale_up.yaml

ansible-playbook playbooks/openshift-master/scaleup.yaml

检查master1的恢复状态

$ oc get node $ oc get pod -n kube-system

将master1节点从[new_masters]&#x2F;[new_nodes]中移到[masters]&#x2F;[nodes]中，并将它放到各个组的最后面。

[OSEv3:children]mastersetcdnodesnew_etcd[masters]master2master3master1[etcd]master2master3[nodes]master2master3master1[new_etcd]master1

五、恢复ETCD服务
在正常的master节点，如master2上移除有问题的etcd节点

$ etcdctl2 member list23868989e8af989: name=master1 isLeader=false45f23429e8af943: name=master2 isLeader=false5323823fadaf989: name=master3 isLeader=true$ etcdctl2 member remove 23868989e8af989

执行etcd的扩容操作etcd scaleup.yaml

ansible-playbook playbooks/openshift-etcd/scaleup.yaml

待执行完成后，检查etcd pod状态

$ oc get pod -n kube-system$ etcdctl2 cluster-health

将master1节点从[new_etcd]组移到[etcd]组的最后面。

[OSEv3:children]mastersetcdnodes[masters]master2master3[etcd]master2master3[nodes]master2master3

恢复master1节点的etcdctl命令将&#x2F;etc&#x2F;profile.d&#x2F;etcdctl.sh文件拷贝到master1节点的&#x2F;etc&#x2F;profile.d&#x2F;etcdctl.sh，并执行source &#x2F;etc&#x2F;profile.d&#x2F;etcdctl.sh

总结
多Master节点的集群具有高可用性，任一节点出现问题都不会影响到集群的服务，但是我们必须尽快恢复它，以免再出现一台主节点出现异常，这将会导致集群故障。
多台Master节点中，第一台Master节点是特殊的节点，有些证书、配置文件只放在这台节点上，但是扩容时会需要用到。（详情可看openshift-ansible中的代码，很多证书都是从openshift_ca_host主机复制，而openshift_ca_host&#x3D;&lt;!–swig￼26–&gt;）。
在完成恢复后，需要及时恢复ansible&#x2F;hosts中的内容，将恢复的节点加到对应组的后面，方便下次主机的扩容与维护。

参考文章OpenShift集群故障恢复–只剩一个master节点可用，没有备份
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Prometheus使用集群外部Prometheus级联的方法实现多集群统一监控告警</title>
    <url>/posts/a33a393500a3.html</url>
    <content><![CDATA[
背景大家知道OpenShfit官方通过Prometheus Operator可以快速构建高可用的监控告警平台，它不仅能够收集集群本身的监控指标，还可以通过ServiceMonitor扩展以监控应用自身的指标，实现单集群内部的统一监控。但是生产实践中，我们并不会只采用一个OpenShift部署应用，还会部署非容器应用，在非容器环境下也可能会部署Prometheus来监控相关服务（特别是现在Prometheus已经成为了监控的一个标配），甚至有可能会在不同的网络区部署多个集群，由于不同网络的隔离特性显然一组Prometheus是无法满足所有平台及应用的监控的。这就使得生产中会有很多套Prometheus集群需要管理与维护，查看监控指标也很分散，也就让我们想到必须采用一种办法能够将各处的指标统一收集，实现多集群多Prometheus服务的统一监控。目前实现这种要求的方法主要有两个，一个是通过Prometheus联邦 Federate，另一个是通过Thanos。今天主要介绍集群外部如何使用Prometheus联邦机制收集OpenShift内部Prometheus服务的监控指标。
操作实现其实很简单，主要就是配置外部Prometheus的prometheus.yml文件来实现Prometheus联邦。具体配置如下，后面有相关重要配置的详细说明
global:  evaluation_interval: 15s  scrape_interval: 15s  scrape_timeout: 10s- job_name: prometheus_test  honor_labels: true  metrics_path: &#x27;/federate&#x27;  params:    &#x27;match[]&#x27;:      - &#x27;&#123;__name__=~&quot;.+&quot;&#125;&#x27;  scheme: https  tls_config:    insecure_skip_verify: true#basic_auth:#  username: &#x27;internal&#x27;#  password: &lt;PASSWORD&gt;  bearer_token: &lt;TOKEN&gt;  static_configs:  - targets: [&#x27;prometheus-k8s-openshift-monitoring.app.group.domain:443&#x27;]    labels:      env: test

&#39;match[]&#39;= &#39;{__name__=~&quot;.+&quot;}&#39;表示所有的指标都收集，如果只需要收集部分的话，可以通过类似- &#39;{job=&quot;node-exporter&quot;}&#39;格式筛选Label指定。
除了使用bearer_token的方式认证，也可以使用basic_auth的方式认证，openshift默认安装的Prometheus支持这两种方式认证，basic_auth的用户名：internal与密码在secret grafana-datasources中，具体获取方式如下：

$ #用户名为 internal$ #获取密码脚本$ oc get secret grafana-datasources -o yaml -n openshift-monitoring | grep prometheus | awk &#x27;&#123;print $2&#125;&#x27; | base64 -d | grep basicAuthPassword | awk -F\&quot; &#x27;&#123;print $4&#125;&#x27;

通过创建一个serviceaccount来获取它的Token

apiVersion: v1kind: ServiceAccountmetadata:  name: prom-fed  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: prom-fedroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: prom-fed  namespace: kube-system
$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep prom-fed | awk &#x27;&#123;print $1&#125;&#x27;)


]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Prometheus添加Alert-Rules</title>
    <url>/posts/34b58d420e4b.html</url>
    <content><![CDATA[

prometheus.yml配置中绑定alertmanager服务

......alerting:  alertmanagers:  - scheme: http    static_configs:    - targets:      - &quot;localhost:9093&quot;


prometheus.rules设置prometheus告警规则

...rules:- alert: TooManyPods  expr: kuberlet_running_pod_count &gt; 10  for: 2m  labels:    team: node  annotations:    summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: has &#123;&#123;$value&#125;&#125; pods&quot;    description: &quot;&#123;&#123;$labels.instance&#125;&#125; be cateful&quot;
规则：kuberlet_running_pod_count指标持续2分钟超过10，则发出告警。3. AlertManager中设置告警接收器alertmanager.yml
global:  smtp_smarthost: &#x27;mail.xx.com:25&#x27;  smtp_from: &#x27;service@xx.com&#x27;  smtp_auth_username: &#x27;service@xx.com&#x27;  smtp_auth_password: &#x27;password&#x27;  smtp_require_tls: falseroute:  group_by: [&#x27;alertname&#x27;]  receiver: alert-emailreceivers:- name: alert-email  email_configs:  - to: &#x27;pan@xx.com&#x27;    send_resolved: true
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Router通过分片实现不同环境网络南北流量隔离</title>
    <url>/posts/496c2ceb4f14.html</url>
    <content><![CDATA[在企业实践中，通常会部署多个OpenShift集群：开发测试、生产等。每个集群都是独立的，通过物理资源进行隔离。这种方式管理简单，易于理解，但是消耗的资源更多，每个集群都需要额外的控制节点及运维节点。有没有办法，使不同环境运行在同一个集群上，并且它们之间实现隔离呢？答案是可以的。对于不同的环境，做好资源隔离，我们需要对计算资源——宿主机做好规划，同时还需要对网络做好规划。宿主机的隔离，可以通过给主机添加label的方法，规划pod的调度。本篇中，我们只针对网络Route部分做好开发测试环境与生产环境的隔离。
OpenShift集群Route分片机制大家都知道OpenShift管理南北流量是通过Route来实现的，所谓的Route本质就是一个Haproxy&#x2F;Nginx服务，与K8S中的Ingress类似。默认情况下，OpenShift集群的Router是全局共用的，也就是说，在创建新的Route资源、Pod更新或者证书更新时，所有的OpenShift Router Pod都会更新Haproxy&#x2F;Nginx的配置，并重新加载。所有的Route后台应用可以通过任一Router服务访问。通过创建多个Router服务，并使用Route分片机制，将不同的应用配置到不同的Router上，实现应用Router服务的隔离。下图为多Router节点分片的架构图。

该架构图中，并没有考虑将节点隔离，只是通过适当的路由来做流量划分。

流量入口为集群外部的负载均衡器。我们只考虑*.apps-prod.example.com与*.apps-dev.example.com域名访问情况。*.apps-prod.example.com域名的后端服务为router-prod*.apps-dev.example.com域名的后端服务为router-dev
每个router都强制设置Route的域名subdomain格式【可选】router-prod路由设置的subdomain为：${name}-${namespace}.apps-prod.example.comrouter-dev路由设置的subdomain为：${name}-${namespace}.apps-dev.example.com

$ oc adm router router-prod --replicas=2 --force-subdomain=&#x27;$&#123;name&#125;-$&#123;namespace&#125;.apps-prod.example.com&#x27;$ oc adm router router-dev --replicas=1 --force-subdomain=&#x27;$&#123;name&#125;-$&#123;namespace&#125;.apps-dev.example.com&#x27;
对于已完成部署的Router服务可以使用如下命令设置
$ oc adm router router-prod  --replicas=2 --force-subdomain=&#x27;$&#123;name&#125;-$&#123;namespace&#125;.apps-prod.example.com&#x27; --dry-run -o yaml | oc apply -f -
此时新建的所有Route的host将无法自定义设置，而会被将被强制设置为两个，其格式为：${name}-${namespace}.apps-prod.example.com 与${name}-${namespace}.apps-dev.example.com。3. 接下来是最重要的一步，为每个Router应用设置Project过滤器，只有带有指定Label的Project下的Route资源才能在该Router下创建配置。router-pod路由设置过滤器为：router&#x3D;prodrouter-dev路由设置过滤器为：router&#x3D;dev
$ oc set env dc/router-prod NAMESPACE_LABELS=&quot;router=prod&quot;$ oc set env dc/router-dev NAMESPACE_LABELS=&quot;router=dev&quot;

将对应的router服务与计算节点绑定确保带有Labelrouter=prod的Router应用部署在带有Labelrouter=prod的Infra节点上，同样带有Labelrouter=dev的Router应用部署在带有Labelrouter=dev的Infra节点上。该创建步骤与步骤3合在一起的脚本如下，即在创建的时候指定Node，及环境变量

$ # prod router节点$ oc label node infra1 &quot;router=prod&quot;$ oc label node infra2 &quot;router=prod&quot;$ oc adm router router-prod --replicas=2 --force-subdomain=&#x27;$&#123;name&#125;-$&#123;namespace&#125;.apps-prod.example.com&#x27; --selector=router=prod$ oc set env dc/router-prod NAMESPACE_LABELS=&quot;router=prod&quot;$ # dev router节点$ oc label node infra3 &quot;router=dev&quot;$ oc adm router router-dev --replicas=1 --force-subdomain=&#x27;$&#123;name&#125;-$&#123;namespace&#125;.apps-dev.example.com&#x27; --selector=router=dev$ oc set env dc/router-dev NAMESPACE_LABELS=&quot;router=dev&quot;

设置对应Label的Project，将会自动匹配该Project下的Route资源与Router服务创建新的project，添加Labelrouter=prod，将会把该Project下的Route资源配置在prod Router服务中，同理Labelrouter=dev下的Route资源配置将会在dev Router服务中配置。

$ # 创建project project-prod-1设置Label router=prod$ oc new-project project-prod-1$ oc label namespace project-prod-1 router=prod$ # 创建project project-dev-1设置Label router=dev$ oc new-project project-dev-1$ oc label namespace project-dev-1 router=dev

此时创建的应用，将会自动进行Router选择配置。Projectrouter=prod下创建的Route将会自动在Routerrouter=prod下配置，同时它的域名格式为：${name}-${namespace}.apps-prod.example.com。同样的Projectrouter=dev下创建的Route将会自动在Routerrouter=dev下配置，同时它的域名格式为：${name}-${namespace}.apps-dev.example.com。

补充以上是通过Router启动添加NAMESPACE_LABEL来设置项目级别的分片，也可以通过为dc&#x2F;route中的ROUTE_LABEL环境变量来设置Route级别的分配。

为dc&#x2F;route设置ROUTE_LABEL

$ oc set env dc/router-prod ROUTE_LABELS=&quot;router=prod&quot;

为Route资源对象指定Label

$ oc label route &lt;route=name&gt; router=prod
该route只会部署在设置有router=prod的路由节点上配置。3. 如果要在一台节点上部署多个Router实例来承载业务，需要注意其HTTP、HTTPS、STATS端口不能重复。该端口可以通过ROUTER_SERVICE_HTTP_PORT、ROUTER_SERVICE_HTTPS_PORT、ROUTER_LISTEN_ADDR环境变量进行设置。
总结
企业级容器云平台建设之资源管理一文中，总结了资源管理分为四部分：计算、网络、存储、镜像仓库。真正实现不同环境的隔离，这四个方面都需要考虑。本文的主要内容说明了网络部分南北流量的隔离。
通过Route流量分片机制，将不同环境下的应用部署在同一个OpenShift集群中，在满足网络南北流量隔离的情况下，减少了集群的数量，节约管理及硬件成本。
要实现集群网络中东西流量的隔离，可以在不同环境下的宿主机之间建立防火墙来实现，同时也可以使用OpenShift的ovs-multitenant或者ovs-networkpolicy网络策略来实现。可阅读之前写的文章：Openshift的网络策略networkpolicy
计算隔离必须保证不同环境下的应用不会运行在同一台宿主机下，以避免它们之间相互影响，抢占资源。这就需要使用OpenShift的调度策略来实现。可阅读之前写的文章：玩转Openshift中Pod调度
存储隔离。可以通过创建不同的storageclass为不同的环境提供服务。
镜像仓库隔离。可以创建多个镜像仓库，同时也可以使用一套镜像仓库，而使用不同的project来作镜像间的逻辑隔离。

参考文章OpenShift Router Sharding for Production and Development TrafficOpenShift Route配置加载的机制可以参考文章：OpenShift Router配置重新加载机制官方文档：Using the Default HAProxy RouterOpenShift Router ShardingOpenShift Using Router Shards
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Router配置重新加载机制</title>
    <url>/posts/0bd399c36cb8.html</url>
    <content><![CDATA[
OpenShift的Router是几乎所有南北流量的入口，对它的运行机制的了解非常重要，尤其是Router的配置更新加载机制。在服务请求出现异常情况下，我们能够快速分析出问题的原因，及时修复，保证应用的连续性。本章主要介绍OpenShift Router的配置加载机制。
OpenShift路由默认是基于Haproxy实现的。当Pod有更新或者证书更新等情况时会重新加载Haproxy的配置，来保证集群的路由信息是最新的。重载配置是否会对当前在线业务产生影响，这是系统管理员担心的问题。
一、 Haproxy配置重载的过程Haproxy在重新加载配置过程分两步。

生成最新的配置
重启Haproxy进程

Haproxy生成最新的配置OpenShift上以下三种资源的改变会触发Haproxy配置的更新

Routes改变
Pod IP&#x2F;Endpoint 改变
证书改变

OpenShift Route有一个配置模板文件，最终的配置会根据这个模板文件来创建。该模板文件，默认路径为/var/lib/haproxy/conf/haproxy-config.template，也可以通过环境变量TEMPLATE_FILE来指定。
$ oc exec router-3-r2scd cat haproxy-config.template | head -n5&#123;&#123;/*    haproxy-config.cfg: contains the main config with helper backends that are used to terminate                        encryption before finally sending to a host_be which is the backend that is the final                        backend for a route and contains all the endpoints for the service*/&#125;&#125;...
Haproxy配置的加载过程OpenShift Router Pod中运行着两个进程
$ oc exec router-3-r2scd -n default -- ps -efUID         PID   PPID  C STIME TTY          TIME CMD1000000+      1      0  0 Nov07 ?        00:00:08 /usr/bin/openshift-router1000000+     5076      1  0 Nov07 ?        00:00:02 /usr/sbin/haproxy -f /var/lib/haproxy/conf/haproxy.config -p /var/lib/haproxy/run/haproxy.pid -x /var/lib/haproxy/run/haproxy.sock -sf 5061...
其中Haproxy进程的父进程是openshift-router，它管理着haproxy进程。openshift-router将通过Master API查询Route、EndPoint及证书的状态，生成最新的Haproxy配置，并执行重新加载操作。在每次重新加载后，haproxy 进程将会被终止并重新启动，haproxy的进程ID将会变化。
$ oc exec router-3-r2scd -n default -- ps -efUID         PID   PPID  C STIME TTY          TIME CMD1000000+      1      0  0 Nov07 ?        00:00:08 /usr/bin/openshift-router1000000+     5208      1  0 Nov07 ?        00:00:02 /usr/sbin/haproxy -f /var/lib/haproxy/conf/haproxy.config -p /var/lib/haproxy/run/haproxy.pid -x /var/lib/haproxy/run/haproxy.sock -sf 5148 5193...
二、 openshift-router环境变量openshift-router接受很多环境变量，来控制haproxy的配置，以及haproxy的重载过程。
RELOAD_SCRIPT重载haproxy的脚本，默认地址为/var/lib/haproxy/reload-haproxy。一般不作更改。openshift-router进程将会隔一定的时间周期去运行该脚本，其中时间周期通过环境变量RELOAD_INTERVAL来设置，默认是5s。
RELOAD_INTERVAL设置openshift-router进程执行RELOAD_SCRIPT的时间周期，默认为5s。将它调大可以减少openshift-router重载haproxy的次数。
##三、 加快Haproxy的重载速度对Haproxy来说，快速完成配置的加载非常重要。这样客户端就不会达到其SYN重试限制，从而导致连接失败。
OpenShift路由器优化方法
更新到3.9以上版本，支持无缝重载
使用路由分片方式，减少每个路由的策略，从而达到加快加载的目的
增大环境变量RELOAD_INTERVAL值，减少周期性Reload调用

检查重载速度通过检查HAProxy的重载速度，可以帮助我们确认连接问题是否是由于重新加载引起的。prometheus中会保存haproxy的加载时间信息到监控项template_router_reload_seconds中。
sh-4.2$ curl -s  http://admin:aAIKAyrX1s@localhost:1936/metrics | grep template_router_reload_seconds# HELP template_router_reload_seconds Measures the time spent reloading the router in seconds.# TYPE template_router_reload_seconds summarytemplate_router_reload_seconds&#123;quantile=&quot;0.5&quot;&#125; NaNtemplate_router_reload_seconds&#123;quantile=&quot;0.9&quot;&#125; NaNtemplate_router_reload_seconds&#123;quantile=&quot;0.99&quot;&#125; NaNtemplate_router_reload_seconds_sum 7.299802469000001template_router_reload_seconds_count 140
看到重新加载总数为140，重新加载总和约为7.3秒。平均装载时间为7.3 &#x2F; 140 &#x3D; 0.05秒，相当快！
自动关闭Router重载后保留的旧Haproxy进程OpenShift 3.9以上版本的Router在重载Haproxy后，会保留旧的Haproxy进程一段时间。因为它要等待正在连接中的请求关闭，所以旧的进程将会挂起一段时间。这些请求只通过以下两种方式关闭：

客户端关闭
连接超时

OpenShift默认的连接超时时间为1h，可以通过降低环境变量ROUTER_DEFAULT_TUNNEL_TIMEOUT的值来减少Haproxy进程挂起时间，同时通过降低ROUTER_BACKEND_CHECK_INTERVAL环境变量，能够调整后序健康检查的时间。
参考文章OpenShift Router Reload Technical Overview

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Route会话保持与负载均衡策略</title>
    <url>/posts/863aac1da72a.html</url>
    <content><![CDATA[Route会话保持OpenShift Router是基于Haproxy反向代理实现的，客户端请求与后端应用POD通过cookie来实现会话保持。

默认是开启会话保持的，基于cookie的会话保持。
如果设置haproxy.router.openshift.io/disable_cookies为True，将会禁用基于cookie的会话保持，而使用balance的负载策略。

此过程分为两个阶段：第一次请求阶段、再次发起请求阶段
第一次请求阶段客户端发起第一次请求时，Router会给返回的数据中添加一条指定的cookie。

客户端请求到达Router节点。
Router节点通过默认的负载均衡策略选择后端应用POD。
后端应用POD返回数据到Router节点。
Router根据后端应用POD,给返回的数据包set-cookie: cookiename[HASH值]&#x3D;cookievalue(HASH值），有效期为SESSION(浏览器客户端关闭前有效)。

再次发起请求阶段客户端再次发起请求时，会带上第一次Router设置的cookie,Router根据该cookie值选择对应的POD。

客户端再次请求，带上第一次请求获得的cookie(HASH值)，访问Router节点。
Router节点根据请求中带的cookie(HASH值），将请求负载到对应的应用POD上。
后端应用POD返回数据到Router，进而返回客户端。


Router会话保持可以保证浏览器不关闭的情况下，同一个客户端请求被负载到同一个POD。但是如果浏览器重启后，第一个请求会重新负载获取新的COOKIE，这时就不能保证请求到同一个POD了。

如果某些应用一定需要满足同一个客户端必须负载到同一个POD的话，可以设置Route的负载策略为source的方式来满足。
$ oc annotate route &lt;route_name&gt; \    --overwrite haproxy.router.openshift.io/balance=source 

Route负载均衡策略而haproxy.router.openshift.io/balance是设置的负载策略。这个值对于passthrough类型的Route默认使用的是基于源地址策略，而对于非passthrough类型的Route默认使用的是最小连接策略。

全局默认值可以通过环境变量ROUTER_TCP_BALANCE_SCHEME与ROUTER_LOAD_BALANCE_ALGORITHM设置。
ROUTER_TCP_BALANCE_SCHEME：对于pass-through类型的Route的负载策略，默认为基于源地址。
ROUTER_LOAD_BALANCE_ALGORITHM：对于非pass-through类型的Route的负载策略，默认为最小连接。

负载均衡类型有以下三种：

roundrobin：轮询。根据其权重依次使用每个端点。 当服务器的处理时间保持均匀分布时，这是最流畅，最公平的算法。
leastconn：最小连接数。当多个端点具有相同的最低连接数时，执行循环。 当需要非常长的会话（例如LDAP，SQL，TSE或其他会话）时，请使用此算法。 不适用于通常使用短会话的协议，例如HTTP。
source：源地址。对源IP地址进行哈希处理，然后除以运行中服务器的总权重，以指定将接收请求的服务器。 这样可以确保相同的客户端IP地址始终会到达同一服务器，只要没有服务器出现故障即可。 如果哈希结果由于正在运行的服务器数量变化而变化，那么许多客户端将被定向到不同的服务器。 此算法通常与直通路由一起使用。

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Route支持TCP负载均衡改造与使用</title>
    <url>/posts/ac610158c667.html</url>
    <content><![CDATA[
Route作为TCP负载均衡器的部署
获取当前Route的haproxy-template配置

# oc project default# oc get podNAME              READY     STATUS    RESTARTS   AGErouter-16-5rv4q    2/2         Running   2            18h# oc rsh router-16-5rv4q cat haproxy-config.template &gt; haproxy-config.template

编辑导出的haproxy-config.template文件在内容&#123;&#123;- end &#125;&#125;&#123;&#123;/*end tls==passthrough*/&#125;&#125;下一行，添加以下内容：

&#123;&#123;/*TCP support*/&#125;&#125;&#123;&#123;- if eq &quot;tcp&quot; (index $cfg.Annotations &quot;haproxy.router.openshift.io/proxy-type&quot;) &#125;&#125;  &#123;&#123;- if (isInteger (index $cfg.Annotations &quot;haproxy.router.openshift.io/external-tcp-port&quot;)) &#125;&#125; frontend tcp-&#123;&#123; (index $cfg.Annotations &quot;haproxy.router.openshift.io/external-tcp-port&quot;) &#125;&#125;  bind :&#123;&#123; (index $cfg.Annotations &quot;haproxy.router.openshift.io/external-tcp-port&quot;) &#125;&#125;  mode tcp  option tcplog  default_backend &#123;&#123;genBackendNamePrefix $cfg.TLSTermination&#125;&#125;:&#123;&#123;$cfgIdx&#125;&#125;  &#123;&#123;- end&#125;&#125;&#123;&#123;/* end haproxy.router.openshift.io */&#125;&#125;&#123;&#123;- end&#125;&#125;&#123;&#123;/* end */&#125;&#125;&#123;&#123;/*end TCP support*/&#125;&#125;

创建configmap模板保存新的haproxy-template文件

# oc project default# oc create configmap customrouter --from-file=haproxy-config.template

部署新的专门为TCP负载服务的Router节点，将HTTP、HTTPS默认端口换成别的端口不要产生冲突。

# oc adm router router-tcp --replicas=0 --selector=router=true --image=registry.example.com/openshfit3/ose-haproxy-router:v3.11 --stats=port=1937 -o yaml# oc set env dc/router-tcp ROUTER_LABELS=router=tcp ROUTER_SERVICE_HTTP_PORT=81 ROUTER_SERVICE_HTTPS_PORT=444# oc set volume dc/router --add --overwrite --name=config-volume --mount-path=/var/lib/haproxy/conf/custom --source=&#x27;&#123;&quot;configMap&quot;: &#123; &quot;name&quot;: &quot;customrouter&quot;&#125;&#125;&#x27;# oc set env dc/router-tcp TEMPLATE_FILE=/var/lib/haproxy/conf/custom/haproxy-config.template# oc scale dc/router-tcp --replicas=2

为Route节点添加防火墙

# vi /etc/sysconfig/iptables-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 11000:29999 -j ACCEPT# systemctl restart iptables
使用Route作为TCP负载均衡器创建Route资源
#  cat &lt;&lt; EOF | oc create -f -apiVersion: route.openshift.io/v1kind: Routemetadata:  annotations:    haproxy.router.openshift.io/external-tcp-port: &#x27;16379&#x27;    haproxy.router.openshift.io/proxy-type: tcp  labels:    router: tcp  name: myredisspec:  port:    targetPort: redis  tls:    insecureEdgeTerminationPolicy: None    termination: passthrough  to:    kind: Service    name: myredisEOF
参考文章灵魂拷问x10：OpenShift 4层Ingress实现方式大全
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Service的域名</title>
    <url>/posts/fef1f9cfd255.html</url>
    <content><![CDATA[
正常情况下Service的域名格式为：service-name.project-name.svc.cluster.local对应的IP是Service Cluster IP
设置Service的clusterIP&#x3D;NoneService的域名格式为：service-name.project-name.svc.cluster.local对应的IP是后台对应的Pod的容器的IP同时后台对应的Pod都有DNS记录，格式为Pod-name.service-name.project-name.svc.cluster.local

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-云原生容器应用设计原则</title>
    <url>/posts/83a1dcc308fe.html</url>
    <content><![CDATA[
引自：容器化应用的设计原则来源自RedHat云原生容器应用设计原则白皮书
本文来自于Red Hat咨询顾问Bilgin Ibryam所编写的一篇白皮书，名为《PRINCIPLES OF CONTAINER-BASED APPLICATION DESIGN》。这篇文章在作者的Blog上发表后，作者的twitter被Kubernetes官方twitter转发。白皮书在Red Hat官网的下载地址：https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper 文本是对这篇文章的学习和整理。

先回顾经典的软件设计原则：


保持简单，愚蠢（KISS）
不要重复自己（DRY）
你不会需要它 （YAGNI）
关注点分离（SoC）
Single responsibility, Open&#x2F;closed, Liskov substitution, Interface segregation, Dependency inversion （SOLID）


然后是Red Hat的云原生容器设计原则：


唯一关注性原则（SCP）
高度可观测性原则（HOP）
生命周期一致性原则（LCP）
镜像不可变性原则（IIP）
进程可处置性原则（PDP）
自包含性原则（S-CP）
运行时约束性原则（RCP）

很多组织都理解云原生的重要性和必要性，但是并不知道从哪里开始。那么请确保：云原生平台和容器化应用能无缝的运行在一起，并且具备抵御故障的能力，甚至在底层的基础架构出现宕机的时候，也能通过过弹性扩展的方式表现出可靠性。本文描述了容器化应用时需要遵循的基本准则，实施这些原则有助于使之与云原生平台Kubernetes更加适配。
唯一关注性原则 SINGLE CONCERN PRINCIPLE（SCP）在许多方面，唯一关注性原则与来自SOLID的SRP是类似的，它建议一个类应该只有一个责任。SRP背后的动机是每个责任是变更的一个轴心，一个类应该有，且也只有一个需要改变的理由。SCP原则中的“关注”一词强调关注是一种更高层次的抽象的责任，而且它更好地将范围描述为一个容器而不是一个类。虽然SRP的主要动机是变化原因的唯一性，而SCP的主要动机是容器镜像重用和可替换性。如果你创建一个解决单个问题的容器，并且以功能完整的方式来实现，不同应用程序中的容器镜像重用的可能性就会更高。
因此，SCP原则规定每个集容器都应该解决一个问题，并做得很好。 实现这一点，通常比在面向对象的世界中实现SRP更容易，容器通常管理的一个单一的进程，大多数情况下一个进程解决一个问题。

如果你的容器化微服务需要解决多个问题，它可以使用这样的模式，将多个容器用sidecar和init-containers的模式合并成一个部署单元（pod），这样每个容器仍然是处理单个问题。同样，您可以替换处理同样问题的容器。 例如，将Web服务器容器或队列实现容器，更新为更具可扩展性的容器。
高度可观测性原则 HIGH OBSERVABILITY PRINCIPLE（HOP）容器提供了一种统一的方式来打包和运行应用程序，将它们视为一个黑盒子对象。 但任何旨在成为云原生公民的容器都必须提供API支持，要为运行时环境编写接口（API），以观察容器的健康状况和行为。 这是自动化容器更新和生命周期回收的基本先决条件和统一的方式，从而提高系统的弹性和用户体验。

实际上，您的容器化应用程序必须至少为其提供不同类型的健康检查的API–活动和就绪等状态。更好的应用程序的行为则必须提供其他手段来观察容器化应用程序的状态。应用程序应该将重要事件记录到标准错误（STDERR）和标准输出（STDOUT）中，从而通过统一的日志聚合工具（诸如Fluentd和Logstash之类的工具）进行分析，并与跟踪和指标收集库相结合，例如OpenTracing，Prometheus等。
将您的应用程序视为黑盒子，但实施所有必要的API以帮助平台对其进行观测，并以最佳方式管理您的应用程序。
生命周期一致性原则 LIFE-CYCLE CONFORMANCE PRINCIPLE（LCP）HOP规定了你的容器提供供平台观测的API。 LCP则规定：您的应用程序有办法读取来自平台的事件。 此外，除了获得事件以外，容器还应该对这些事件相应地作出反应。这就是此原则名字由来。这几乎就像在应用程序通过一个“写入API”与平台进行交互。

来自管理平台的各种事件都是为了帮助您管理您的容器的生命周期的。决定处理哪些事件取决于您的应用程序 以及是否对这些事件做出反应。
但有些事件比其他事件更重要。例如，任何需要一个干净的关闭进程，这就需要捕获信号：终止（SIGTERM）消息，并尽可能迅速关闭。 这是为了避免通过强制关闭信号：kill（SIGKILL），之后跟随一个SIGTERM。
还有其他事件，例如PostStart和PreStop，可能对您的应用程序生命周期管理也非常重要。 例如，某些应用程序需要在服务之前进行预热请求和一些需要在关闭干净之前释放资源。
镜像不可变性原则 IMAGE IMMUTABILITY PRINCIPLE（IIP）IMAGE IMMUTABILITY PRINCIPLE（IIP）容器化的应用程序是不可变更的，镜像一旦完成了构建，预计在不同的环境中运行都不会改变。这意味着在因外部环境的不同，在需要的时候需要使用外部手法处理所依赖的外部配置数据，而不是每个环境修改或者构建不同的容器。而容器应用程序中的任何变更，都应该因此触发构建新的容器映像，并在所有环境中重用它。相同于这个原理的，不可变服务器和不可变基础架构的概念也很受欢迎，并且对于服务器&#x2F;主机管理也是如此。

在遵循IIP原则的情况下，应该防止为不同的环境创建相似的容器镜像，要始终坚持为所有环境只配置一个容器映像。 这个原则允许在应用程序更新期间，采用自动回滚和前滚等做法，这是云原生自动化的重要方面。
进程可处置性原则 PROCESS DISPOSABILITY PRINCIPLE（PDP）迁移到容器应用程序的主要动机之一是：容器需要尽可能做到临时性，并做好在任何时候被另一个容器实例替换的准备。需要更换容器的原因有很多，比如：健康检查失败、缩容、应用程序将容器迁移到不同的主机，平台资源匮乏或其它的问题。

这意味着容器化的应用程序必须保持其状态为向外扩展的或分布式和冗余的。这也意味着应用程序应该快速启动和关闭，甚至为彻底的硬件故障做好准备。 实施这一原则的另一个有用的做法是创建小容器。 容器在云原生环境可以自动调度并在不同的主机上启动。较小的容器可以实现更快启动时间，因为在重新启动之前容器镜像需要被物理地复制到主机系统。
自包含性原则 SELF-CONTAINMENT PRINCIPLE（S-CP）这个原则规定一个容器应该在构建时包含所有需要的东西。容器的存在应该仅仅依赖于Linux®内核，在并添加相关额外的库，在容器构建时加入它们。除了库之外，它还应该包含语言运行时，应用程序平台（如果需要），以及运行所需的其他依赖关系，等运行容器化应用所需要的诸如此类的东西。

唯一的例外是：由于不同环境之间差异，并且只能在运行时提供的配置; 例如，通过Kubernetes提供的ConfigMap。
某些应用程序由多个容器组件组成。 例如，容器化的Web应用程序也可能需要数据库容器。 根据这个原则，并不建议合并两个容器。相反，它建议的是数据库容器只包含运行数据库所需的所有内容，Web应用程序容器只包含运行Web应用程序所需的所有内容，如Web服务器。 在运行时，Web应用程序容器将根据需要依赖于并访问数据库容器。
运行时约束性原则 RUNTIME CONFINEMENT PRINCIPLE（RCP）S-CP从构建时的角度查看容器，并关注于生成的二进制文件及其内容。但是容器不仅仅是磁盘上一个只有尺寸大小的单一维度的黑盒子。 容器运行时有多个维度，例如内存使用维度，CPU使用维度等资源消耗维度。

这个RCP原则建议每个容器申报资源需求，并发送信息到平台。它应该分享容器的资源配置文件，从CPU，内存，网络，磁盘的角度声明。这影响到平台如何执行调度，自动扩展，容量 管理以及容器常规的服务级别协议（SLA）等。
除了向平台声明容器的资源需求之外，还有一点也很重要， 应用被约束在使用所声明的资源需求内。如果应用程序对资源的使用保持在约束的范围内，则当资源匮乏发生时，平台不太可能将其终止和迁移。
结论云原生不仅仅是一种最终状态 - 它也是一种工作方式。 本份白皮书描述了一系列容器应用的基本原则，必须遵守才能成为优秀的云原生公民。
除了这些原则之外，创建良好的容器应用程序还需要熟悉其他容器相关的最佳实践和技术。 尽管上述原则非常根本，适用于大多数用例，下面列出的最佳实践在应用和不应用的时候，则需要判断力。以下是一些与容器相关的更常见的最佳实践：

镜像要尽可能的小。 通过清理临时文件，并避免安装不必要的软件包来构建小尺寸镜像。 这减少了容器的尺寸，构建时间和复制容器镜像的网络传输时间。
支持任意用户ID。 避免使用sudo命令或要求特定用户名运行你的容器。
标记重要的端口。 虽然可以在运行时指定端口号，然而使用EXPOSE命令在运行的时候指定，则可以让镜像的使用者更轻松。
为持久数据使用卷。 在容器摧毁之后还需要保存的容器数据的，必须将数据写入一个数据卷。
设置镜像元数据。 以标签和注释形式存在的镜像元数据可以使您的容器镜像更加实用，从而为使用您的容器的开发人员提供了更好的体验。使主机和镜像同步。 一些容器应用需要容器在某些属性（如时间和机器ID）上与主机同步。

这里是指向各种模式和最佳实践的资源的链接，以帮助您能有效地实现上述目标：
• https://www.slideshare.net/luebken/container-patterns• https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices• http://docs.projectatomic.io/container-best-practices• https://docs.openshift.com/enterprise/3.0/creating_images/guidelines.html• https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdf• https://leanpub.com/k8spatterns/• https://12factor.net
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-如何设置使用Prometheus来监控Router</title>
    <url>/posts/67c1601a81ee.html</url>
    <content><![CDATA[OpenShift集群中的Router服务作为几乎所有南北流量的入口非常重要，对它的监控有很大的意义，既能够查看流量的变化，及时发现业务的变化，也可以发现异常请求及时发现问题。 红帽对于Router服务其实已经开启了监控指标的服务，但是默认并没有与Prometheus服务对接，需要手动对接，具体对接的操作如下。

获取Router应用的访问用户名与密码

$ oc set env dc/router -n default --list  | grep STATSSTATS_PASSWORD=Oby3Y2FXs5STATS_PORT=1936STATS_USERNAME=admin

在prometheus项目下创建访问Router指标的密钥

$ oc create secret generic router-auth --from-literal=user=admin --from-literal=password=Oby3Y2FXs5 -n openshift-monitoring

在openshift-monitoring项目下创建ServiceMonitor

$ cat &lt;&lt;EOF | oc create -f -apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: router-metrics  metadata:  labels:    k8s-app: haproxy-router  namespace: openshift-monitoringspec:  endpoints:  - basicAuth:      password:        name: router-auth        key: password      username:        name: router-auth        key: user    interval: 5s    path: /metrics    port: 1936-tcp    scheme: http  jobLabel: k8s-app  namespaceSelector:    matchNames:    - default  selector:    matchLabels:      router: routerEOF
但是此时只是完成了Prometheus监控Router服务的指标，而没有通过Grafana查看。所以还需要单独部署Grafana4. 部署独立的Grafana

导入Haproxy的监控面板

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-通过Operator-SDK制作Operator</title>
    <url>/posts/852e3377ec0a.html</url>
    <content><![CDATA[制作Operator过程
下载安装operator-sdk
创建github仓库
operator-sdk创建应用

$ operator-sdk new memcached-go-operator --repo=github.com/example-inc/memcached-operator$ # or$ operator-sdk new memcached-ansible-operator --api-version=fcloudy.com/v1alpha1 --kind=Memcached --type=ansible$ # or$ operator-sdk new memcached-helm-operator --api-version=fcloudy.com/v1alpha1 --kind=Memcached --type=helm
operator-sdk new默认使用的type为go，必须在$GOPATH目录下创建项目。type为go时项目的目录结构如下：项目的目录结构
type的可选值还可以为：ansible与helm，如果是这两个，则还可以带参数–kind来定义CRD示例：GOAnsibleHelm更多实例：etcd,memcached,vault,bitcoin-sv4. 创建ansible role

创建应用时使用变量指的是CR中metadata的变量，而指的是CR中spec的变量。
如果只有一个role的话，在watches.yaml中指定role即可，例如role: /opt/ansible/roles/memcached
如果有多个role的话，可以添加playbook.yaml，并指定它为启动脚本，例如playbook: /opt/ansible/playbook.yaml


构建operator镜像

$ operator-sdk build docker.io/xhuaustc/memcached-operator:v0.0.1$ docker push docker.io/xhuaustc/memcached-operator:v0.0.1

使用operator创建应用

$ # 1. 导入crd文件$ create -f deploy/crds/cache_v1alpha1_memcached_crd.yaml$ # 2. 替换deploy/operator.yaml文件中的变量REPLACE_IMAGE，pull_policy$ sed -i &quot;&quot; &#x27;s|&#123;&#123; REPLACE_IMAGE &#125;&#125;|docker.io/xhuaustc/memcached-operator:v0.0.1|g&#x27; deploy/operator.yaml$ sed -i &quot;&quot; &#x27;s|&#123;&#123; pull_policy\|default(&#x27;\&#x27;&#x27;Always&#x27;\&#x27;&#x27;) &#125;&#125;|Always|g&#x27; deploy/operator.yaml$ # 3. 创建相关资源$ oc create -f deploy

创建CR资源

$ oc create -f deploy/crds/cache_v1alpha1_memcached_cr.yaml
监控全局Namespace默认情况下，Operator只能监听当前project下创建的应用。如果要监听所有的namespace,需将WATCH_NAMESPACE值设置为””，同时将role更换为clusterrole，rolebinding更新为clusterrolebinding。以下为具体实例。

将deploy&#x2F;operator.yaml中的WATCH_NAMESPACE值设置为””

apiVersion: apps/v1kind: Deployment...spec:  ...  template:    ...    spec:      ...      serviceAccountName: memcached-operator      containers:      - name: memcached-operator        ...        env:          - name: WATCH_NAMESPACE          value: &quot;&quot;

将deploy&#x2F;role.yaml更改为ClusterRole

apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: memcached-operator...

将deploy&#x2F;role_binding.yaml更改为ClusterRoleBinding

kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: memcached-operatorsubjects:- kind: ServiceAccount  name: memcached-operator  namespace: &lt;operator-namespace&gt;roleRef:  kind: ClusterRole  name: memcached-operator  apiGroup: rbac.authorization.k8s.io
总结operator会不断监听创建的CRD资源配置的变化，更新资源的状态，保证资源处于需要的状态下。目前Operator只支持openshift 3.11及以上版本。通过使用operator-sdk可以非常方便地创建自己的operator。它不仅可以轻易将通用参数提取出来，同时支持编码的方式，让资源的定义更加灵活，而且可以通过operator的设置资源的调用顺序及依赖。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-集群为项目提供专属节点</title>
    <url>/posts/2cba55920597.html</url>
    <content><![CDATA[背景介绍在生产实践中，某些系统因一些特殊的要求，需要独占节点，即专属节点：具有特殊要求的应用POD必须运行在这些专属节点上；同时这些节点不为其它应用服务。本文将提供一种具体的配置策略，以满足这种特殊的调度需求。该策略主要使用到两种工具：

节点标签实现应用与节点的绑定
污点配置实现其它应用不会被调度到该节点。

设置步骤
为专属节点设置特定标签LABEL：dedicated&#x3D;sys0，及设置污点

$ oc label node node-10 dedicated=sys0$ oc adm taint nodes node-10 dedicated=sys0:NoSchedule

在使用专属节点的应用编排中绑定的Node

spec:  replicas: 1  template:    metadata:      name: nginx      labels:        app: nginx    spec:      nodeSelector:        dedicated: &quot;sys0&quot;      containers:...
或
spec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:          - matchExpressions:              - key: dedicated                operator: In                values:                  - sys0  containers:...

使用专属节点的应用编排中指定的标签及容忍污点

spec:  tolerations:  - key: &quot;dedicated&quot;    operator: &quot;Equal&quot;    value: &quot;sys0&quot;    effect: &quot;NoExecute&quot;
注意，本例中使用的operator是”Equal”，会比较污点的键与值。如果operator使用”Exists”，则应用可以容忍所有带有dedicated污点。
去除专属节点操作要是需要去除专属节点的限制，让它恢复成一个正常应用的节点。只需要将它的污点移除即可。
$ oc adm taint nodes dedicated-
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift上使用NFS作为共享存储的线上问题</title>
    <url>/posts/e9021798429a.html</url>
    <content><![CDATA[
NFS exports文件配置格式&lt;输出目录&gt; [客户端1 选项（访问权限,用户映射,其他）] [客户端2 选项（访问权限,用户映射,其他）]
a. 输出目录：输出目录是指NFS系统中需要共享给客户机使用的目录；
b. 客户端：客户端是指网络中可以访问这个NFS输出目录的计算机客户端常用的指定方式
指定ip地址的主机：192.168.0.200指定子网中的所有主机：192.168.0.0&#x2F;24 192.168.0.0&#x2F;255.255.255.0指定域名的主机：david.bsmart.cn指定域中的所有主机：.bsmart.cn所有主机：
c. 选项：选项用来设置输出目录的访问权限、用户映射等。
NFS主要有3类选项：

访问权限选项

设置输出目录只读：ro设置输出目录读写：rw

用户映射选项all_squash：将远程访问的所有普通用户及所属组都映射为匿名用户或用户组（nfsnobody）；no_all_squash：与all_squash取反（默认设置）；root_squash：将root用户及所属组都映射为匿名用户或用户组（默认设置）；no_root_squash：与rootsquash取反；anonuid&#x3D;xxx：将远程访问的所有用户都映射为匿名用户，并指定该用户为本地用户（UID&#x3D;xxx）；anongid&#x3D;xxx：将远程访问的所有用户组都映射为匿名用户组账户，并指定该匿名用户组账户为本地用户组账户（GID&#x3D;xxx）；

其它选项secure：限制客户端只能从小于1024的tcp&#x2F;ip端口连接nfs服务器（默认设置）；insecure：允许客户端从大于1024的tcp&#x2F;ip端口连接服务器；sync：将数据同步写入内存缓冲区与磁盘中，效率低，但可以保证数据的一致性；async：将数据先保存在内存缓冲区中，必要时才写入磁盘；wdelay：检查是否有相关的写操作，如果有则将这些写操作一起执行，这样可以提高效率（默认设置）；****no_wdelay：若有写操作则立即执行，应与sync配合使用；subtree：若输出目录是一个子目录，则nfs服务器将检查其父目录的权限(默认设置)；no_subtree：即使输出目录是一个子目录，nfs服务器也不检查其父目录的权限，这样可以提高效率；
例如：&#x2F;data&#x2F;nfsTest&#x2F; 127.0.0.1(rw,sync,no_root_squash)
OpenShift相关的账号权限问题
SUPGROUP为容器运行的用户添加所属组，可动态添加

securityContext:  supplementalGroups:    - 1001    - 1002
如果NFS服务共享目录中设置了anongid，则可以通过设置supgroup来赋予容器用户对NFS目录的访问权限。2. 指定NFS请求版本配置PV，强制使用NFSv3来访问后端NFS服务。参考配置如下：spec.mountOptions
apiVersion: v1kind: PersistentVolumemetadata:  name: pv0003spec:  capacity:    storage: 5Gi  volumeMode: Filesystem  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  mountOptions:    - hard    - nfsvers=3  nfs:    path: /tmp    server: 172.17.0.2
另外也可以通过添加annotations.volume.beta.kubernetes.io&#x2F;mount-options来设置
oc patch pv pv0003 -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;volume.beta.kubernetes.io/mount-options&quot;:&quot;rw,nfsvers=3&quot;&#125;&#125;&#125;&#x27;
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift中etcd集群的某个etcd服务文件损坏，导致节点故障，恢复过程</title>
    <url>/posts/f7e207fa4001.html</url>
    <content><![CDATA[问题etcd集群中某个etcd出现故障，使用docker ps -a | grep etcd | grep -v POD查看etcd节点，发现它异常退出。
$ docker logs -f &lt;etcd-container-id&gt;....etcdserver : open wal error: wal: file not found 
恢复步骤大方向步骤：一、将问题etcd节点从etcd集群剥离；二、将恢复的新的etcd节点添加到etcd集群。具体步骤如下：

查看etcd状态

$ etcdctl2 cluster-health$ ## 获取问题节点的member ID$ etcdctl2 member remove  &lt;member ID&gt;$ ## 将问题etcd服务从etcd集群中删除

停止问题节点上的etcd服务

$ mkdir -p /etc/origin/node/pods-stopped$ mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/

删除问题节点上的etcd数据

$ rm -rf /var/lib/etcd/*
4.更新ansible中的inventory hosts内容，设置new_etcd配置
[OSEv3:children]mastersetcdnodesnew_etcd[OSEv3:vars]openshift_is_atomic=false[masters]master-1master-2master-3[nodes]master-1master-2master-3...[etcd]master-1#master-2master-3[new_etcd]master-2
需要注意：将问题节点从masters&#x2F;nodes中注释掉，否则执行etcd扩容会报如下错误。TASK [Run variable sanity checks] fatal: [master-1]: FAILED! =&gt; {&quot;msg&quot;: &quot;last_checked_host: master-2, last_checked_var: ansible_python;&#39;NoneType&#39; object has no attribute &#39;__getitem__&#39;&quot;}

更新节点的node group配置

$ ansible-playbook playbooks/openshift-master/openshift_node_group.yml

执行etcd扩容脚本

$ ansible-playbook playbooks/openshift-etcd/scaleup.yml

执行完成扩容后，进行验证。注意：如果在hosts中将masters对应的节点注释掉，在etcd安装时会使用本地服务的方式安装运行，这个适合独立的节点部署etcd。

etcd命令etcd压测脚本
$ etcdctl --write-out=table check perf
参考文章Adding etcd Hosts to existing cluster
&#96;
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift中如何将PV与PVC绑定</title>
    <url>/posts/9730fbcd8bb6.html</url>
    <content><![CDATA[PV&#x2F;PVC是什么？PV（Persistent Volume）：描述的是持久化的Volume实体概念，生命周期与Pod创建和销毁事件无关。要么运行事先准备好，要么通过动态创建。PVC（PersistentVolumeClaim）：PVC是对PV的请求，申明Pod所希望使用的持久化存储的属性，例如容量，读写权限。
Kubernete Volumes能够帮忙应用持久化数据，PV&#x2F;PVC是Kubernetes Volumes存储类型的一种，其它类型还有：本地存储：emptyDir &#x2F; hostPath网络存储：in-tree: aws ElasticBlockStore &#x2F; gcePersistentDisk &#x2F; nfsout-of-tree：csi等网络存储插件Project Volume：secret &#x2F; configmap &#x2F; downwardAPI &#x2F; serviceAccountToken
PV&#x2F;PVC的意义
使用不同的控制器来管理计算与存储资源，解耦POD与Volume的生命周期，实现计算与存储分离。
PVC只需要关注应用需要知道的配置，如存储大小、访问模式，读写模式等，而不需要知道存储的细节，实现开发与运维职责分离。开发只需要提需求，知道自己需要的存储容量，模式就够了，他并不关心存储是由什么设备提供的，资源池的够不够，而运维人员则相反，他更关心的底层的存储状态。

PV创建的两种方式：静态与动态静态创建方式，下图为静态创建的示意图。


研发用户需要创建存储资源，于是创建了PVC（持久化存储卷请求），申明需要的存储资源大小以及访问模式
集群管理人员（运维人员）根据需求配置手动创建对应的PV（持久化存储卷）
OpenShift&#x2F;K8S会根据配置将PV&#x2F;PVC进行绑定，让PVC与真实的存储资源关联。

这种方式有几个问题：

需要手动创建PV，增加了运维管理的复杂度。
如果有大量配置一样的PVC需求时，PVC与设定的PV需要单独的设置进行绑定。

动态创建方式，下图为动态创建的示意图。0. 集群管理人员（运维人员）提前部署好存储的provisioner，并创建好对应storageclass

研发用户需要创建存储资源，于是创建了PVC（持久化存储卷请求），申明需要的存储的provisioner以及存储大小
provisioner监听到PVC资源的创建，自动创建PV，并与PVC进行绑定，让PVC与真实的存储资源关联。

一旦准备好动态创建存储环境，存储资源便以服务的方式提供给研发人员，实现存储资源自服务。
如何将PV与PVC绑定在上一部分介绍了静态创建存储资源与动态创建存储资源的过程与特点，很明显动态创建存储资源使用更方便，但是在生产中，受到环境的限制，静态创建存储资源的方式仍然很常见。这时，如何准确地绑定PVC与对应的PV就是需要注意的问题了。下面列出了解决这个问题的三种方法。
第一种，在PV中添加label，在PVC中添加matchLabels进行关联创建PV
apiVersion: v1kind: PersistentVolumemetadata:  name: nfs-pv1  labels:    pv: nfs-pv1spec:  capacity:    storage: 1Gi  accessModes:    - ReadWriteMany  nfs:    server: 10.2.1.2    path: &quot;/exports/pv1&quot;

apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: pvc1spec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 1Gi  selector:    matchLabels:      pv: &quot;nfs-pv1&quot;    matchExpressions:      - &#123;key: environment, operator: In, values: [dev]&#125;

# 本地盘PVkind: PersistentVolumeapiVersion: v1metadata:  name: test2-pv  namespace: kubeflow  labels:    pv: test2spec:  capacity:    storage: 100Mi  accessModes:    - ReadWriteOnce  hostPath:    path: &quot;/data/test2&quot;
第二种，PV配置中指定关联的PVCapiVersion: v1kind: PersistentVolumemetadata:  name: nfs-pv1  labels:    pv: nfs-pv1spec:  capacity:    storage: 1Gi  claimRef:    apiVersion: v1    kind: PersistentVolumeClaim    name: openldap-volume-1    namespace: openldap  accessModes:    - ReadWriteMany  nfs:    server: 10.2.1.2    path: &quot;/exports/pv1&quot;
第三种，PVC中设置volumeNameapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: pvc1spec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 1Gi  volumeName: nfs-pv1
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift中如何通过脚本获取Alertmanager的告警数据</title>
    <url>/posts/87d0ba016aa6.html</url>
    <content><![CDATA[一、为什么需要通过脚本获取告警信息
二、Alertmanager的告警信息
三、amtool工具获取告警信息
# oc exec -it alertmanager-main-0 -c alertmanager -n openshift-monitoring -- amtool alert query &#x27;severity=critical&#x27; --alertmanager.url http://localhost:9093# oc exec -it alertmanager-main-0 -c alertmanager -n openshift-monitoring -- amtool alert query &#x27;severity=critical&#x27; --alertmanager.url http://localhost:9093 -o extended
-o：默认值为simple，输出格式，还可以是extended，json
四、amtool更多功能
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift中的Grafana监控界面无法完成登录</title>
    <url>/posts/c6fbc7c6afa5.html</url>
    <content><![CDATA[
问题描述：访问OpenShift默认的grafana无法跳转到OpenShift console界面。报如下错误：
&#123;&quot;error&quot;:&quot;invalid_request&quot;,&quot;error_description&quot;:&quot;The request is missing a required parameter, includes an invalid parameter value, includes a parameter more than once, or is otherwise malformed.&quot;&#125;
解决办法：
$ oc patch sa grafana -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;serviceaccounts.openshift.io/oauth-redirecturi.first&quot;:&quot;https://grafana.apps.example.com:450&quot;&#125;&#125;&#125;&#x27;

解释说明：因为某些原因，外部负载均衡器并没有使用与Router对应的80&#x2F;443端口，而是使用了83&#x2F;450端口。默认情况下OpenShift oauth-proxy登录验证会传入clientId为应用的serviceaccount，它会将跳转链接与serviceaccount中的redirecturi校验。如果匹配则允许跳转，否则会报参数不合法的错误。
grafana serviceaccount的默认跳转相关的配置如下：
$ oc get serviceaccount grafana -n openshift-monitoring -o yaml...  serviceaccounts.openshift.io/oauth-redirectreference.grafana: &#x27;&#123;&quot;kind&quot;:&quot;OAuthRedirectReference&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;reference&quot;:&#123;&quot;kind&quot;:&quot;Route&quot;,&quot;name&quot;:&quot;grafana&quot;&#125;&#125;&#x27;...
它会从grafana Route中获取跳转链接，它与真实的链接（83&#x2F;450）不匹配，所以会报出不合法的错误。故而需要添加指定的认证链接https://grafana.apps.example.com:450。
除了grafana外，其它OpenShift默认的服务也会出类似的问题，也可以通过同样的方法解决。参考文档：https://docs.openshift.com/container-platform/3.11/architecture/additional_concepts/authentication.html
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift中节点的垃圾收集——容器与镜像</title>
    <url>/posts/ab2bfd6bf012.html</url>
    <content><![CDATA[参考文档https://docs.openshift.com/container-platform/3.11/admin_guide&#x2F;garbage_collection.html
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift之应用环境变量篇</title>
    <url>/posts/16a234e5e16a.html</url>
    <content><![CDATA[通过环境变量向容器暴露 Pod 信息
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift企业版离线安装：下载RedHat-Repo源及镜像文件</title>
    <url>/posts/1242150b382a.html</url>
    <content><![CDATA[
为了安全起见，在很多企业，网络环境是不能连接外网的，而需要使用OpenShift的话，就需要离线部署。离线部署与在线部署最大区别，就是我们得自己准备介质源：YUM源与镜像仓库。只要你拥有红帽订阅，下载最新的介质，其实很简单。
一、同步YUM源RHEL主机注册到红帽订阅，并关联到相应的YUM仓库
$ subscription-manager register$ subscription-manager refresh$ subscription-manager list --available --matches &#x27;*OpenShift*&#x27;$ subscription-manager attach --pool=&lt;pool_id&gt;$ subscription-manager repos --disable=&quot;*&quot;$ subscription-manager repos \    --enable=&quot;rhel-7-server-rpms&quot; \    --enable=&quot;rhel-7-server-extras-rpms&quot; \    --enable=&quot;rhel-7-fast-datapath-rpms&quot; \    --enable=&quot;rhel-7-server-ose-3.7-rpms&quot;
同步yum源，reposync命令添加-n标记，表示只下载最新的
$ yum -y install yum-utils createrepo docker git$ mkdir -p /opt/repos/$ for repo in \rhel-7-server-rpms \rhel-7-server-extras-rpms \rhel-7-fast-datapath-rpms \rhel-7-server-ose-3.7-rpmsdo  reposync --gpgcheck -lm --repoid=$&#123;repo&#125; --download_path=/opt/repos/ -n  createrepo -v /opt/repos/$&#123;repo&#125; -o /opt/repos/$&#123;repo&#125;done
将YUM源文件打包
$ cd /opt/repos/$ tar zcvf rhel-7-server-rpms.tar.gz rhel-7-server-rpms $ tar zcvf rhel-7-server-extras-rpms.tar.gz rhel-7-server-extras-rpms$ tar zcvf rhel-7-fast-datapath-rpms.tar.gz rhel-7-fast-datapath-rpms$ tar zcvf rhel-7-server-ose-3.7-rpms.tar.gz rhel-7-server-ose-3.7-rpms
二、同步镜像登录红帽镜像仓库
$ docker login registry.redhat.io
下载镜像，例如&#x3D;v3.11.219
$ docker pull registry.redhat.io/openshift3/apb-base:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/apb-tools:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/automation-broker-apb:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/csi-attacher:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/csi-driver-registrar:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/csi-livenessprobe:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/csi-provisioner:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/grafana:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/local-storage-provisioner:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/manila-provisioner:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/mariadb-apb:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/mediawiki:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/mediawiki-apb:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/mysql-apb:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-ansible-service-broker:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-cli:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-cluster-autoscaler:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-cluster-capacity:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-cluster-monitoring-operator:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-console:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-configmap-reloader:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-control-plane:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-deployer:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-descheduler:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-docker-builder:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-docker-registry:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-efs-provisioner:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-egress-dns-proxy:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-egress-http-proxy:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-egress-router:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-haproxy-router:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-hyperkube:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-hypershift:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-keepalived-ipfailover:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-kube-rbac-proxy:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-kube-state-metrics:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-metrics-server:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-node:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-node-problem-detector:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-operator-lifecycle-manager:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-ovn-kubernetes:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-pod:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-prometheus-config-reloader:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-prometheus-operator:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-recycler:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-service-catalog:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-template-service-broker:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-tests:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-web-console:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/postgresql-apb:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/registry-console:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/snapshot-controller:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/snapshot-provisioner:&lt;tag&gt;$ docker pull registry.redhat.io/rhel7/etcd:3.2.26$ docker pull registry.redhat.io/rhel7/etcd:3.2.22$ docker pull registry.redhat.io/openshift3/ose-efs-provisioner:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/metrics-cassandra:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/metrics-hawkular-metrics:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/metrics-hawkular-openshift-agent:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/metrics-heapster:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/metrics-schema-installer:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/oauth-proxy:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-logging-curator5:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-logging-elasticsearch5:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-logging-eventrouter:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-logging-fluentd:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/ose-logging-kibana5:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/prometheus:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/prometheus-alertmanager:&lt;tag&gt;$ docker pull registry.redhat.io/openshift3/prometheus-node-exporter:&lt;tag&gt;$ docker pull registry.redhat.io/cloudforms46/cfme-openshift-postgresql$ docker pull registry.redhat.io/cloudforms46/cfme-openshift-memcached$ docker pull registry.redhat.io/cloudforms46/cfme-openshift-app-ui$ docker pull registry.redhat.io/cloudforms46/cfme-openshift-app$ docker pull registry.redhat.io/cloudforms46/cfme-openshift-embedded-ansible$ docker pull registry.redhat.io/cloudforms46/cfme-openshift-httpd$ docker pull registry.redhat.io/cloudforms46/cfme-httpd-configmap-generator$ docker pull registry.redhat.io/rhgs3/rhgs-server-rhel7$ docker pull registry.redhat.io/rhgs3/rhgs-volmanager-rhel7$ docker pull registry.redhat.io/rhgs3/rhgs-gluster-block-prov-rhel7$ docker pull registry.redhat.io/rhgs3/rhgs-s3-server-rhel7
将镜像打包
$ docker save -o ose3-images.tar \    registry.redhat.io/openshift3/apb-base \    registry.redhat.io/openshift3/apb-tools \    registry.redhat.io/openshift3/automation-broker-apb \    registry.redhat.io/openshift3/csi-attacher \    registry.redhat.io/openshift3/csi-driver-registrar \    registry.redhat.io/openshift3/csi-livenessprobe \    registry.redhat.io/openshift3/csi-provisioner \    registry.redhat.io/openshift3/grafana \    registry.redhat.io/openshift3/local-storage-provisioner \    registry.redhat.io/openshift3/manila-provisioner \    registry.redhat.io/openshift3/mariadb-apb \    registry.redhat.io/openshift3/mediawiki \    registry.redhat.io/openshift3/mediawiki-apb \    registry.redhat.io/openshift3/mysql-apb \    registry.redhat.io/openshift3/ose-ansible-service-broker \    registry.redhat.io/openshift3/ose-cli \    registry.redhat.io/openshift3/ose-cluster-autoscaler \    registry.redhat.io/openshift3/ose-cluster-capacity \    registry.redhat.io/openshift3/ose-cluster-monitoring-operator \    registry.redhat.io/openshift3/ose-console \    registry.redhat.io/openshift3/ose-configmap-reloader \    registry.redhat.io/openshift3/ose-control-plane \    registry.redhat.io/openshift3/ose-deployer \    registry.redhat.io/openshift3/ose-descheduler \    registry.redhat.io/openshift3/ose-docker-builder \    registry.redhat.io/openshift3/ose-docker-registry \    registry.redhat.io/openshift3/ose-efs-provisioner \    registry.redhat.io/openshift3/ose-egress-dns-proxy \    registry.redhat.io/openshift3/ose-egress-http-proxy \    registry.redhat.io/openshift3/ose-egress-router \    registry.redhat.io/openshift3/ose-haproxy-router \    registry.redhat.io/openshift3/ose-hyperkube \    registry.redhat.io/openshift3/ose-hypershift \    registry.redhat.io/openshift3/ose-keepalived-ipfailover \    registry.redhat.io/openshift3/ose-kube-rbac-proxy \    registry.redhat.io/openshift3/ose-kube-state-metrics \    registry.redhat.io/openshift3/ose-metrics-server \    registry.redhat.io/openshift3/ose-node \    registry.redhat.io/openshift3/ose-node-problem-detector \    registry.redhat.io/openshift3/ose-operator-lifecycle-manager \    registry.redhat.io/openshift3/ose-ovn-kubernetes \    registry.redhat.io/openshift3/ose-pod \    registry.redhat.io/openshift3/ose-prometheus-config-reloader \    registry.redhat.io/openshift3/ose-prometheus-operator \    registry.redhat.io/openshift3/ose-recycler \    registry.redhat.io/openshift3/ose-service-catalog \    registry.redhat.io/openshift3/ose-template-service-broker \    registry.redhat.io/openshift3/ose-tests \    registry.redhat.io/openshift3/ose-web-console \    registry.redhat.io/openshift3/postgresql-apb \    registry.redhat.io/openshift3/registry-console \    registry.redhat.io/openshift3/snapshot-controller \    registry.redhat.io/openshift3/snapshot-provisioner \    registry.redhat.io/rhel7/etcd:3.2.22 \    registry.redhat.io/rhel7/etcd:3.2.26$ docker save -o ose3-optional-imags.tar \    registry.redhat.io/openshift3/metrics-cassandra \    registry.redhat.io/openshift3/metrics-hawkular-metrics \    registry.redhat.io/openshift3/metrics-hawkular-openshift-agent \    registry.redhat.io/openshift3/metrics-heapster \    registry.redhat.io/openshift3/metrics-schema-installer \    registry.redhat.io/openshift3/oauth-proxy \    registry.redhat.io/openshift3/ose-logging-curator5 \    registry.redhat.io/openshift3/ose-logging-elasticsearch5 \    registry.redhat.io/openshift3/ose-logging-eventrouter \    registry.redhat.io/openshift3/ose-logging-fluentd \    registry.redhat.io/openshift3/ose-logging-kibana5 \    registry.redhat.io/openshift3/prometheus \    registry.redhat.io/openshift3/prometheus-alertmanager \    registry.redhat.io/openshift3/prometheus-node-exporter \    registry.redhat.io/cloudforms46/cfme-openshift-postgresql \    registry.redhat.io/cloudforms46/cfme-openshift-memcached \    registry.redhat.io/cloudforms46/cfme-openshift-app-ui \    registry.redhat.io/cloudforms46/cfme-openshift-app \    registry.redhat.io/cloudforms46/cfme-openshift-embedded-ansible \    registry.redhat.io/cloudforms46/cfme-openshift-httpd \    registry.redhat.io/cloudforms46/cfme-httpd-configmap-generator \    registry.redhat.io/rhgs3/rhgs-server-rhel7 \    registry.redhat.io/rhgs3/rhgs-volmanager-rhel7 \    registry.redhat.io/rhgs3/rhgs-gluster-block-prov-rhel7 \    registry.redhat.io/rhgs3/rhgs-s3-server-rhel7 

小坑留意
OpenShift官方文档中在同步YUM源时采用全量同步，这会导致光rhel-7-server-rpms这个源下载文件就有40G，下载时间长。其实只需要在同步YUM源时添加参数-n，只会下载最新的包，同样是rhel-7-server-rpms源，文件总共 是5G左右。
大家都知道的原因，国内网络下载很慢，所以可以购买一台（香港&#x2F;日本）云主机进行下载，再把介质压缩后，拷贝到本地。所有介绍压缩后，大概为11G。

参考文章OpenShift官方文档：Disconnected installation
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift使用HostPath挂载本地目录无权限</title>
    <url>/posts/67d0da6a2d75.html</url>
    <content><![CDATA[apiVersion: v1kind: DeploymentConfigmetadata:  name: centos  namespace: monitorspec:  replicas: 1  template:    metadata:      labels:        busybox: &#x27;true&#x27;    spec:      containers:        - args:          image: &#x27;centos:v2&#x27;          imagePullPolicy: IfNotPresent          name: busybox          securityContext:            runAsUser: 1000            runAsGroup: 2000  #该特性在k8s 1.10之后才支持，本环境未支持，参见Support for RunAsGroup as a pod security context          volumeMounts:            - mountPath: /centos              name: centos-volume      securityContext: &#123;&#125;      nodeSelector:        kubernetes.io/hostname: test      volumes:        - hostPath:            path: /home/testHostPath          name: centos-volume      serviceAccountName: new-sa  triggers:    - type: ConfigChange
host上&#x2F;home&#x2F;testHostPath的权限如下：
# ls -Zdrwxr-xr-x. root root unconfined_u:object_r:home_root_t:s0 testHostPath
进入容器,可以看到该文件夹已经挂载进去,但没有任何操作该文件夹的权限
sh-4.2$ cd /centos                                                                                                                                                                                                                 sh-4.2$ ls                                                                                                                                                                                                                         ls: cannot open directory .: Permission denied
登陆该容器所在node节点，查看该容器的SELinux设置如下，显然创建的文件夹的SELinux与容器不匹配，将host上文件夹的SELinux设置为与容器相匹配。
$ docker inspect c21736278d1a|grep &quot;MountLabel&quot;        &quot;MountLabel&quot;: &quot;system_u:object_r:svirt_sandbox_file_t:s0:c15,c10&quot;,
$ chcon -Rt svirt_sandbox_file_t /testHostPathor $ chcon -R unconfined_u:object_r:svirt_sandbox_file_t:s0 /testHostPathor $ semanage fcontext -a -t svirt_sandbox_file_t &#x27;/testHostPath(/.*)?&#x27;$ restorecon -Rv /testHostPath# 确认设置 semanage fcontext -l | grep testHostPath# 确认文件生效 ls -Z /testHostPath
解决完SELinux之后，查看该容器对应进程(docker inspect $CONTAINERID |grep Pid)的信息&#x2F;proc&#x2F;$PID&#x2F;status(具体含义参见&#x2F;proc&#x2F;[pid]&#x2F;status)。可以看到该容器使用的user id为1000，group id为0，supplemental groups为100023000。user id和supplemental groups(Groups)使用了所在project的默认值，group id(含fsgroup)则使用了0。
# cat /proc/23032/status......Uid:    1000    1000    1000    1000Gid:    0       0       0       0FDSize: 2048Groups: 1000230000......
参考文章openshift scc解析
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift使用KeepAlived+LVS实现外部负载均衡器</title>
    <url>/posts/8fb7edf5d1e4.html</url>
    <content><![CDATA[OpenShift集群资源列表



主机名
类型
IP



相关节点IP




master0
master
192.168.0.2


master1
master
192.168.0.3


master2
master
192.168.0.4


infra0
infra
192.168.0.5


infra1
infra
192.168.0.6


vip地址分配




master-vip
vip
192.168.0.250


infra-vip
vip
192.168.0.251


部署Master API负载均衡器作为集群内外访问的入口
在三个Master节点安装KeepAlived与LVS

$ ansible masters -m package -a &#x27;name=keepalived state=present&#x27;$ ansible masters -m package -a &#x27;name=ipvsadm state=present&#x27;

选择master0作为VIP的Master节点

$ cat /etc/keepalived/keepalived.confglobal_defs &#123;   #指定keepalived在发生切换时需要发送email到的对象   notification_email &#123;       receiver@example.com    #邮件接收者   &#125;   notification_email_from sender@example.com  #邮件发送者   smtp_server 127.0.0.1     #邮件服务器地址，如未安装，使用本地   smtp_connect_timeout 30   router_id LVS_DEVEL&#125;#IP漂移协议vrrp_instance VI_1 &#123;    state MASTER   #指定A节点为主节点 备分机上设置为BACKUP即可    interface eth0  #绑定IP漂移的网络接口，在该接口上绑定VIP    virtual_router_id 51   #VRRP组名，三个节点的设置必须一样    priority 100   #优先级，同一个 vrrp_instance 的 MASTER 优先级必须比 BACKUP 高    advert_int 1   #MASTER 与 BACKUP 负载均衡器之间同步检查的时间间隔，单位为秒    authentication &#123;    # 设置认证        auth_type PASS    #认证方式，支持 PASS 和 HA        auth_pass 1111    #证密码为明文，同一 vrrp 实例 MASTER 与 BACKUP 使用相同的密码才能正常通信    &#125;    virtual_ipaddress &#123;    #虚拟IP地址(VIP)，可以有多个地址，每个地址占一行        192.168.0.250  #绑定VIP    &#125;&#125;#只接收8443端口过来的请求virtual_server 192.168.232.206 8443 &#123;    delay_loop 6  # 服务健康检查周期，6秒    lb_algo wlc   # 负载均衡调度算法，一般用wrr、rr、wlc    lb_kind DR   # 负载均衡转发规则。一般包括DR,NAT,TUN 3种    nat_mask 255.255.255.0    persistence_timeout 50   #会话保持时间。把用户请求请求间隔在未超过保持时间时，一直分发到某个服务节点    protocol TCP    #RS服务器1    real_server 192.168.0.2 8443 &#123;        weight 1        #健康检查        TCP_CHECK &#123;     # 通过TcpCheck判断RealServer的健康状态            connect_timeout 10    # 连接超时时间            nb_get_retry 3            # 重连次数            delay_before_retry 3  # 重连时间间隔            connect_port 8443     # 检测端口        &#125;    &#125;    #RS服务器2    real_server 192.168.0.3 8443 &#123;        weight 1        #健康检查        TCP_CHECK &#123;     # 通过TcpCheck判断RealServer的健康状态            connect_timeout 10    # 连接超时时间            nb_get_retry 3            # 重连次数            delay_before_retry 3  # 重连时间间隔            connect_port 8443     # 检测端口        &#125;    &#125;    #RS服务器2    real_server 192.168.0.4 8443 &#123;        weight 1        #健康检查        TCP_CHECK &#123;     # 通过TcpCheck判断RealServer的健康状态            connect_timeout 10    # 连接超时时间            nb_get_retry 3            # 重连次数            delay_before_retry 3  # 重连时间间隔            connect_port 8443     # 检测端口        &#125;    &#125;&#125;
KeepAlive健康检查类型有五种：TCP_CHECK&#x2F;HTTP_GET&#x2F;SSL_GET&#x2F;MISC_CHECK&#x2F;SMTP_CHECK

TCP_CHECK：TCP端口是否能通

TCP_CHECK &#123;     # 通过TcpCheck判断RealServer的健康状态    connect_timeout 10    # 连接超时时间    nb_get_retry 3        # 重连次数    delay_before_retry 3  # 重连时间间隔    connect_port 6500     # 检测端口&#125;

HTTP_GET：请求HTTP URL的Code是否为200

HTTP_GET &#123;    url &#123;      path /      status_code 200    &#125;    connect_timeout 3    nb_get_retry 3    delay_before_retry 3&#125;

SSL_GET：请求HTTPS URL的Code是否为200

SSL_GET &#123;    url &#123;      path /      status_code 200    &#125;    connect_timeout 3    nb_get_retry 3    delay_before_retry 3&#125;

MISC_CHECK：脚本运行的返回码是否为0

MISC_CHECK &#123;    misc_path &quot;/etc/keepalived/misc_check.sh https://192.168.0.2:6500/check_status&quot;    # 外部程序或者脚本的路径和参数    misc_timeout 10   # 脚本执行的超时时间    misc_dynamic    #动态权重标志。脚本返回0则检测成功权重不变，返回1表示失败权重设置为0&#125;

SMTP_CHECK：SMTP健康检查器

SMTP_CHECK      &#123;	# 用于smtp HELO请求的可选字符串	helo_name hello	host &#123;		# 默认值为真实服务器的IP		connect_ip 192.168.0.3		# 默认值为25		connect_port 25	&#125;	# 连接超时时间	connect_timeout 5	# 重试失败检查的次数	retry 3	# 重试前延迟秒	delay_before_retry &lt;INTEGER&gt;&#125; 
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift使用MetalLB，打开了Service通向外界的大门</title>
    <url>/posts/370dfa8a122e.html</url>
    <content><![CDATA[
#背景在K8S&#x2F;OpenShift中，如果要向集群外部暴露应用的服务，目前使用的方法有：NodePort、HostPort、Route、LoadBalancer、HostNetwork。

Route：为最常用的方法，但是一般只支持7层负载（默认），使用一个外部LB来负载多个Route实例，对外的访问的IP为该LB的IP。
NodePort：在私有集群中也常用来暴露服务，但是它必须使用30000以上的端口，数量有限，不便于管理，而且对于一些特殊的服务，如DNS，必须使用小端口号，那么使用NodePort则无法满足。
HostNetwork：对于集群中的一些特殊服务使用该方式，它将容器与宿主机的网络绑定，如OpenShift中的Router服务就是使用HostNetwork与Router节点绑定。它的缺点是pod必须与主机绑定，同时每个Node上只能运行一个Pod实例，因为端口无法被复用。
HostPort：与HostNetwork类似，将Pod指定的端口与Node对应的端口绑定。
LoadBalancer：一般只在公有云平台使用，可以使Service获得与主机同一平面的IP，方便对服务进行控制与对外输出。缺点是依赖于公有云平台。

从上可看出对于HTTP服务使用Route可以很方便地对外提供服务，而对于TCP&#x2F;UDP服务比较好的方式是使用LoadBalancer（当然HTTP服务也方便使用该方式），但是它依赖于云平台，有没有一种方式能够帮助集群在非IaaS平台上使用LoadBalancer方式呢？答案是有的，那就是Google的项目MetalLB。
什么是MetalLBMetalLB的官方地址：https://metallb.universe.tf/MetalLB是使用标准路由协议的裸机Kubernetes&#x2F;OpenShift集群的软负载均衡器，可以在物理机环境下实现对Service服务分配IP。
MetalLB支持两种申明模式：

Layer 2模式：ARP&#x2F;NDP


Layer 2模式下，每个service会有集群中的一个node来负责。当服务客户端发起ARP解析的时候，对应的node会响应该ARP请求，之后，该service的流量都会指向该node（看上去该node上有多个地址）。
Layer 2模式并不是真正的负载均衡，因为流量都会先经过1个node后，再通过kube-proxy转给多个end points。如果该node故障，MetalLB会迁移 IP到另一个node，并重新发送免费ARP告知客户端迁移。现代操作系统基本都能正确处理免费ARP，因此failover不会产生太大问题。
Layer 2模式更为通用，不需要用户有额外的设备；但由于Layer 2模式使用ARP&#x2F;ND，地址池分配需要跟客户端在同一子网，地址分配略为繁琐。同时Layer 2模式所有流量会进入到一个Node，该Node的带宽可能会成为一个网络瓶颈。


BGP模式。


BGP模式下，集群中所有node都会跟上联路由器建立BGP连接，并且会告知路由器应该如何转发service的流量。
BGP模式是真正的LoadBalancer。
MetalLB由两个组件组成：
controller，负责IP地址的分配，以及service和endpoint的监听。
speaker，负责保证service地址可达，例如Layer 2模式下，speaker会负责ARP请求应答。

部署MetalLBKubernetes
安装metalLB相关的应用

# kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.1/manifests/metallb.yaml

创建configmap配置

# cat &lt;&lt; EOF | kubectl apply -f -apiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    address-pools:    - name: default      protocol: layer2      addresses:      - 192.168.1.240-192.168.1.250EOF
OpenShift
关闭OpenShift自带的LoadBalancer功能，这一步很重要，否则会跟删除&#x2F;etc&#x2F;origin&#x2F;master&#x2F;master-config.yaml中的externalIPNetworkCIDRs设置

networkConfig:  externalIPNetworkCIDRs:  - 0.0.0.0/0

下载MetalLB安装文件

# wget https://raw.githubusercontent.com/google/metallb/v0.8.1/manifests/metallb.yaml

删除将文件中DaemonSet资源中的配置spec.template.spec.securityContext.runAsUser
添加权限

# oc adm policy add-scc-to-user privileged -n metallb-system -z speaker

运行安装

# oc apply -f metallb.yaml

创建configmap配置

# cat &lt;&lt; EOF | kubectl apply -f -apiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    address-pools:    - name: default      protocol: layer2      addresses:      - 192.168.1.240-192.168.1.250EOF
测试
对已有的一个应用创建新LoadBalancer类型的svc

[root@master1 ~]# oc expose dc/flask-app --name=metallb-app --type=LoadBalancer --port=5000

查看当前svc的状态

[root@master1 ~]# oc get svcNAME        TYPE           CLUSTER-IP      EXTERNAL-IP                         PORT(S)          AGEflask-app   ClusterIP      172.30.193.89   &lt;none&gt;                              5000/TCP         35dmetallb-app    LoadBalancer   172.30.7.121    192.168.1.240,172.29.114.1          5000:32276/TCP   6m
其中service flask-app为之前创建的ClusterIP类型；service metallb-app为新建的LoadBalancer类型的服务，它带有一个由MetalLB控制器分配的一个IP 192.168.1.240（另外还有一个OpenShift分配的一个额外IP 172.29.114.1，该IP可忽略）。3. 使用openshift集群外部的机器访问该应用
[root@i-avler8qs ~]# curl http://192.168.1.240:5000Hello world v2
总结在没有使用了LoadBalancer（如MetalLB）前，OpenShift上的Service像是待在这里的宠物，外部访问它非常受限；使用了LoadBalancer后，Service主动向外面世界敞开了怀抱，每个Service都绑定有一个独立的IP，方便外部应用访问，同时也方便使用传统的防火墙方式进行控制访问。LoadBalancer扩大了对OpenShift&#x2F;Kubernetes集群使用的想象空间，而MetalLB无疑是性价比最高的方式。#参考文章MetalLB - 贫苦 K8S 用户的负载均衡支持为裸金属K8S集群提供外部负载均衡器OpenShift上安装MetalLB
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift制作NginxLB-Operator实战</title>
    <url>/posts/88502991e28c.html</url>
    <content><![CDATA[背景近期需要在OpenShift集群中部署Nginx服务做为负载均衡器，负载集群外部服务，如NTP、DNS、项目App等。因为不同的服务的配置都是不一样的，不仅后台服务的IP不一样，而且使用的协议也不一样，HTTP&#x2F;TCP&#x2F;UDP都有可能，如果按照传统的方式来实施的话，每一个应用单独定义Nginx配置，分别部署，每增加一个新的应用被负载都需要做一次复杂的过程，那么有没有办法能够让这过程变得简单呢，甚至能够自动化处理，我们只需要提供最简单的信息？下面我们来分析下常用的几种方法。
打包方案选择1. Template对于OpenShift熟悉的朋友，会马上想到使用Template模板来实现。Template模板是OpenShift特有的应用打包方式，它描述了一组对象，同时对这些对象的配置可以进行参数化处理，生成OpenShift 容器平台创建的对象列表。在模板中可以设置所有在项目中有权限创建的任何资源。
不足之处：

OpenShift特有，如果是使用OpenShift容器平台的话，这个不足可忽略。
无法保证线上资源状态始终与参数设定的结果一致，如手动增加rc的副本数时，不会自动恢复到与参数设定的副本数。
在创建的时候设置参数，如果在应用运行时对参数动态更新的话，则需要使用脚本命令使用所有的参数，重新生成资源列表。参数需要额外管理，不可靠。
如果应用有创建的顺序有依赖，则无法满足。
无法根据参数的不同对资源进行条件控制。

2. Helm对于Kubernetes熟悉的朋友，会马上想到使用Helm来实现。Helm是Kubernetes生态系统中的一个软件包管理工具，与Template类似。
不足之处：

需要额外部署Helm客户端及Tiller。
需要额外管理helm中的charts资源。
无法保证线上资源状态始终与参数设定的结果一致。
如果应用有创建的顺序有依赖，则无法满足。
参数更新时，需要手动执行helm脚本

3. 创建Ansible playbook对于熟悉各种自动化工具的运维开发，会想到使用自动化配置管理工具来做，如ansible。利用ansible的k8s模块，创建各种资源，而且可以充分发挥ansible强大的控制功能。
不足之处：

需要额外部署Ansible，及对ansible访问集群的访问认证。
需要额外管理ansible的playbook文件。
无法保证线上资源状态始终与参数设定的结果一致。
参数更新时，需要手动执行ansible playbook脚本

4. operatorOperator即为今天的主角，我将给予更加详细的介绍。Operator是由coreOS公司（已被RedHat收购）开发的一种打包，部署和管理Kubernetes&#x2F;OpenShift应用的方法。Kubernetes&#x2F;OpenShift应用是一个部署在集群上并使用Kubernetes&#x2F;OpenShift API和kubectl&#x2F;oc工具进行管理的应用程序。Operator类似于Helm和Template，但是比它们都更加灵活，更加强大，更加方便。Operator本质上是一个自定义的控制器。它会在集群中运行一个Pod与Kubernetes&#x2F;OpenShift API Server交互，并通过CRD引入新的资源类型，这些新创建的资源类型与集群上的资源类型如Pod等交互方式是一样的。同时Operator会监听自定义的资源类型对象的创建与变化，并开始循环执行，保证应用处于被定义的状态。为什么说Operator能够更好地解决这类问题呢？因为它不仅能够很好地满足自定义打包的需求，同时也弥补了以上三种方式的不足。使用Operator-sdk能够非常方便地创建自定义的Operator，它支持三种类型：go、ansible、helm。

go类型，它的实现更加灵活，可以随心所欲，扩展性也最强，构建出的operator镜像也不大，但是它对于编程能力要求高，同时没有ansible和helm类型拿来即用，可读性也不及ansible与helm类型。
ansible类型，它使用ansible的playbook方式来定义应用的构建与保证应用的状态，它的实现也很灵活，依赖于ansible的模块，但是这使得构建出的operator镜像较大，一般为600多M，因为它包含了ansible应用及默认的各个模块。
helm类型，它使用helm的charts方式来定义应用的构建与保证应用的状态，它的镜像一般为200多M，但是它的灵活度不及另外两种类型。

一般情况下，以上三种方式都能够满足要求，建议大家使用自己最熟悉的方式。构建方式并不是我们的约束点，我们最关心的是能够部署按要求的应用，并保证应用一直处于稳定的状态。
构建分析1. 资源类型
deployment，运行Nginx应用
service，运行Nginx service
configmap，设置Nginx负载均衡上游及协议类型等配置
route，对于HTTP协议可以设置指定的域名
NginxLB，添加的CRD资源对象名

2. 参数设置
nginx_image, 指定Nginx应用镜像
size，Nginx应用运行的副本数
loadbalancers，设定的负载均衡参数配置列表
loadbalancers[].protocol，负载均衡网络协议，支持HTTP&#x2F;TCP&#x2F;UDP
loadbalancers[].port，负载均衡Nginx监听的端口
loadbalancers[].nodeport，如果负载均衡使用nodeport方式对外提供服务，则可以用该参数指定nodeport端口号
loadbalancers[].upstreams，负载均衡上游服务列表
loadbalancers[].hostname，对于HTTP协议，可以指定hostname来创建OpenShift Route资源

最终需要实现的NginxLB资源的参数例子为：
apiVersion: fcloudy.com/v1alpha1kind: NginxLBmetadata:  name: example-nginxlbspec:  nginx_image: &quot;docker.io/xhuaustc/nginx:alpine&quot;  size: 2  loadbalancers:    - protocol: TCP      port: 53      nodeport: 32287      upstreams:        - 192.168.4.5:53        - 192.168.5.3:53    - protocol: HTTP      port: 80      upstreams:        - 192.168.4.5:80      hostname: xx.nginx.fcloudy.com

以下为NginxLB Operator相关资源的关系
  
3. Operator类型
选择ansible类型，使用它的主要是与集群运维及自动化运维等技术栈统一。

制作Operator通用步骤与说明可以参考OpenShift 通过Operator SDK制作Operator，本案例的具体操作如下

新建一个operator项目（type&#x3D;ansible 资源类型为NginxLB)

$ operator-sdk new nginxlb-operator --api-version=fcloudy.com/v1alpha1 --kind=NginxLB --type=ansible

在roles&#x2F;nginxlb&#x2F;templates中添加模板文件nginx-deployment.yaml.j2、nginx-svc.yaml.j2、nginx-cm.yaml.j2及nginx-route.yaml.j2nginx-deployment.yaml.j2

apiVersion: v1kind: Deploymentmetadata:  labels:    nginxlb: &#123;&#123; meta.name &#125;&#125;    app: &#123;&#123; meta.name &#125;&#125;  name: &#123;&#123; meta.name &#125;&#125;  namespace: &#123;&#123; meta.namespace &#125;&#125;spec:  replicas: &#123;&#123; size &#125;&#125;  selector:    matchLabels:      nginxlb: &#123;&#123; meta.name &#125;&#125;  template:    metadata:      labels:        nginxlb: &#123;&#123; meta.name &#125;&#125;    spec:      containers:      - image: &quot;&#123;&#123; nginx_image | default(&#x27;docker.io/xhuaustc/nginx:alpine&#x27;) &#125;&#125;&quot;        name: nginx        volumeMounts:        - mountPath: /etc/nginx/nginx.conf          name: nginx-config-hgj4i          subPath: nginx.conf          readOnly: true      volumes:        - configMap:            defaultMode: 420            name: nginx            items:              - key: nginx.conf                path: nginx.conf          name: nginx-config-hgj4i
nginx-svc.yaml.j2
apiVersion: v1kind: Servicemetadata:  name: &#123;&#123; meta.name &#125;&#125;-&#123;&#123; item.port &#125;&#125;-nginx-service  namespace: &#123;&#123; meta.namespace &#125;&#125;spec:  ports:    - name: &#123;&#123; item.protocol | lower &#125;&#125;-&#123;&#123; item.port | lower &#125;&#125;      port: &#123;&#123; item.port &#125;&#125;&#123;% if item.protocol == &#x27;HTTP&#x27; %&#125;      protocol: TCP&#123;% else %&#125;      protocol: &#123;&#123; item.protocol &#125;&#125;&#123;% endif %&#125;&#123;% if item.nodeport is defined %&#125;      nodePort: &#123;&#123; item.nodeport&#125;&#125;&#123;% endif %&#125;  selector:    nginxlb: &#123;&#123; meta.name &#125;&#125;&#123;% if item.nodeport is defined %&#125;  type: NodePort&#123;% else %&#125;  type: ClusterIP&#123;% endif %&#125;
nginx-cm.yaml.j2
apiVersion: v1kind: ConfigMapmetadata:  name: nginx  namespace: &#123;&#123; meta.namespace &#125;&#125;data:  nginx.conf: |    worker_processes  1;    error_log  /var/log/nginx/error.log warn;    pid        /var/run/nginx.pid;    events &#123;        worker_connections  1024;    &#125;        stream&#123;&#123;% for lb in loadbalancers %&#125;&#123;% if lb.protocol in [&quot;TCP&quot;, &quot;UDP&quot;] %&#125;      upstream &#123;&#123;meta.name&#125;&#125;-&#123;&#123;lb.protocol&#125;&#125;-&#123;&#123;lb.port&#125;&#125;&#123;&#123;% for upstream in lb.upstreams %&#125;          server &#123;&#123;upstream&#125;&#125;;&#123;% endfor %&#125;      &#125;      server &#123;&#123;% if lb.protocol in [&quot;UDP&quot;] %&#125;      listen &#123;&#123;lb.port&#125;&#125; udp;&#123;% else %&#125;        listen &#123;&#123;lb.port&#125;&#125;;&#123;% endif %&#125;        proxy_pass &#123;&#123;meta.name&#125;&#125;-&#123;&#123;lb.protocol&#125;&#125;-&#123;&#123;lb.port&#125;&#125;;      &#125;&#123;% endif %&#125;&#123;% endfor %&#125;    &#125;    http &#123;        include       /etc/nginx/mime.types;        default_type  application/octet-stream;        log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;                          &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                          &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;        access_log  /var/log/nginx/access.log  main;        sendfile        on;        #tcp_nopush     on;        keepalive_timeout  65;        gzip  on;                &#123;% for lb in loadbalancers %&#125;&#123;% if lb.protocol in [&quot;HTTP&quot;] %&#125;      upstream &#123;&#123;meta.name&#125;&#125;-&#123;&#123;lb.protocol&#125;&#125;-&#123;&#123;lb.port&#125;&#125;&#123;&#123;% for upstream in lb.upstreams %&#125;          server &#123;&#123;upstream&#125;&#125;;&#123;% endfor %&#125;      &#125;      server &#123;        listen &#123;&#123;lb.port&#125;&#125;;        location / &#123;          proxy_pass http://&#123;&#123;meta.name&#125;&#125;-&#123;&#123;lb.protocol&#125;&#125;-&#123;&#123;lb.port&#125;&#125;;        &#125;      &#125;&#123;% endif %&#125;&#123;% endfor %&#125;    &#125;
nginx-route.yaml.j2
apiVersion: route.openshift.io/v1kind: Routemetadata:  name: &#123;&#123; meta.name &#125;&#125;-&#123;&#123; item.port &#125;&#125;-nginx-route  namespace: &#123;&#123; meta.namespace &#125;&#125;spec:  host: &quot;&#123;&#123; item.hostname &#125;&#125;&quot;  port:      targetPort: &#123;&#123; item.protocol | lower &#125;&#125;-&#123;&#123; item.port | lower &#125;&#125;  to:    kind: Service    name: &#123;&#123; meta.name &#125;&#125;-&#123;&#123; item.port &#125;&#125;-nginx-service

在roles&#x2F;nginxlb&#x2F;tasks&#x2F;main.yaml中添加执行任务

---- name: create nginx configmap  k8s:    state: present    definition: &quot;&#123;&#123; lookup(&#x27;template&#x27;, &#x27;nginx-cm.yaml.j2&#x27;) | from_yaml &#125;&#125;&quot;- name: create nginx DeploymentConfig  k8s:    state: present    definition: &quot;&#123;&#123; lookup(&#x27;template&#x27;, &#x27;nginx-dc.yaml.j2&#x27;) | from_yaml &#125;&#125;&quot;- name: create nginx service  k8s:    state: present    definition: &quot;&#123;&#123; lookup(&#x27;template&#x27;, &#x27;nginx-svc.yaml.j2&#x27;) | from_yaml &#125;&#125;&quot;  with_items: &quot;&#123;&#123; loadbalancers &#125;&#125;&quot;      - name: create nginx route  k8s:    state: present    definition: &quot;&#123;&#123; lookup(&#x27;template&#x27;, &#x27;nginx-route.yaml.j2&#x27;) | from_yaml &#125;&#125;&quot;  when: item.hostname is defined  with_items: &quot;&#123;&#123; loadbalancers &#125;&#125;&quot;   

构建nginx-lb operator镜像，并推送到镜像仓库

$ operator-sdk build docker.io/xhuaustc/nginxlb-operator:v0.0.1$ docker push docker.io/xhuaustc/nginxlb-operator:v0.0.1

operator-sdk默认是只能在operator应用所在的namespace下创建资源，如果需要在集群下全局的namespace都能使用NginxLB资源，需要对deploy&#x2F;operator.yaml作修改。
将WATCH_NAMESPACE值设置为””
更新为步骤4中构建的镜像



apiVersion: apps/v1kind: Deploymentmetadata:  name: nginxlb-operatorspec:  replicas: 1  selector:    matchLabels:      name: nginxlb-operator  template:    metadata:      labels:        name: nginxlb-operator    spec:      serviceAccountName: nginxlb-operator      containers:        - name: ansible          command:          - /usr/local/bin/ao-logs          - /tmp/ansible-operator/runner          - stdout          # Replace this with the built image name          image: &quot;docker.io/xhuaustc/nginxlb-operator:v0.0.1&quot;          imagePullPolicy: &quot;Always&quot;          volumeMounts:          - mountPath: /tmp/ansible-operator/runner            name: runner            readOnly: true        - name: operator          # Replace this with the built image name          image: &quot;docker.io/xhuaustc/nginxlb-operator:v0.0.1&quot;          imagePullPolicy: &quot;Always&quot;          volumeMounts:          - mountPath: /tmp/ansible-operator/runner            name: runner          env:            - name: WATCH_NAMESPACE              value: &quot;&quot;            - name: POD_NAME              valueFrom:                fieldRef:                  fieldPath: metadata.name            - name: OPERATOR_NAME              value: &quot;nginxlb-operator&quot;      volumes:        - name: runner          emptyDir: &#123;&#125;

更新deploy&#x2F;role.yaml与deploy&#x2F;role_binding.yaml
role.yaml与role_binding.yaml中的kind: Role更新为kind: ClusterRole
role_binding.yaml中的kind: RoleBinding更新为kind: ClusterRoleBinding
添加额外的权限，如route资源类型的权限等role.yaml



apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  creationTimestamp: null  name: nginxlb-operatorrules:- apiGroups:  - &quot;&quot;  resources:  - pods  - services  - endpoints  - persistentvolumeclaims  - events  - configmaps  - secrets  verbs:  - &#x27;*&#x27;- apiGroups:  - apps  resources:  - deployments  - daemonsets  - replicasets  - statefulsets  verbs:  - &#x27;*&#x27;- apiGroups:  - extensions  resources:  - deployments  - daemonsets  - replicasets  - statefulsets  - deployments/finalizers  verbs:  - &#x27;*&#x27;- apiGroups:  - route.openshift.io  attributeRestrictions: null  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - monitoring.coreos.com  resources:  - servicemonitors  verbs:  - get  - create- apiGroups:  - apps  resourceNames:  - nginxlb-operator  resources:  - deployments/finalizers  verbs:  - update- apiGroups:  - fcloudy.com  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;
role_binding.yaml
kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: nginxlb-operatorsubjects:- kind: ServiceAccount  name: nginxlb-operator  namespace: nginxlb-operatorroleRef:  kind: ClusterRole  name: nginxlb-operator  apiGroup: rbac.authorization.k8s.io
至此完成了NginxLB Operator的制作，制作的结果输出为：

Operator镜像：docker.io&#x2F;xhuaustc&#x2F;nginxlb-operator:v0.0.1
deploy中的yaml配置文件：operator.yamlrole.yamlrole_binding.yamlservice_account.yamlcrds&#x2F;fcloudy_v1alpha1_nginxlb_crd.yaml

测试验证
创建nginxlb-operator项目

[root@master ~]# oc new-project nginxlb-operator --display=NginxLBOperator

部署nginxlb-operator

[root@master ~]# oc create -f deploy/crds/fcloudy_v1alpha1_nginxlb_crd.yaml[root@master ~]# oc create -f deploy/

查看nginxlb-operator运行状态

[root@master ~]# oc get podNAME                                READY     STATUS    RESTARTS   AGEnginxlb-operator-85c77c8cdc-c2gpp   2/2       Running   10         1m

新建NginxLB项目

[root@master ~]# oc new-project nginxlb --display-name=NginxLB

使用NginxLB创建负载均衡器Nginx应用

[root@master ~]# cat &lt;&lt; EOF | oc create -f -apiVersion: fcloudy.com/v1alpha1kind: NginxLBmetadata:  name: example-nginxlbspec:  size: 2  loadbalancers:  - nodeport: 32289    port: 8123    protocol: TCP    upstreams:    - 192.168.4.5:123    - 192.168.5.3:123  - hostname: xx.nginx.fcloudy.com    port: 8080    protocol: HTTP    upstreams:    - 192.168.4.5:80EOF

查看NginxLB资源状态

[root@master ~]# oc get all NAME                                  READY     STATUS    RESTARTS   AGEpod/example-nginxlb-6788db776-42rsz   1/1       Running   0          5spod/example-nginxlb-6788db776-8cxm9   1/1       Running   0          5sNAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGEservice/example-nginxlb-8080-nginx-service   ClusterIP   172.30.167.107   &lt;none&gt;        8080/TCP         2sservice/example-nginxlb-8123-nginx-service   NodePort    172.30.108.138   &lt;none&gt;        8123:32289/TCP   3sNAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/example-nginxlb   2         2         2            2           5sNAME                                        DESIRED   CURRENT   READY     AGEreplicaset.apps/example-nginxlb-6788db776   2         2         2         5sNAME                                                        HOST/PORT              PATH      SERVICES                             PORT        TERMINATION   WILDCARDroute.route.openshift.io/example-nginxlb-8080-nginx-route   xx.nginx.fcloudy.com             example-nginxlb-8080-nginx-service   http-8080                 None

更新NginxLB  example-nginxlb，将size更新为1，只使用一个Nginx应用副本

[root@master ~]# oc patch NginxLB example-nginxlb -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;size&quot;:1&#125;&#125;&#x27;  --type=mergenginxlb.mbcloud.com/example-nginxlb patched[root@master ~]# oc get pod NAME                              READY     STATUS    RESTARTS   AGEexample-nginxlb-6788db776-8cxm9   1/1       Running   0          2m
总结
以上实例只是对一种CRD进行控制与管理，其实一个Operator可以同时管理与控制多个CRD。
Operator能够非常灵活地实现对资源的重新管理及控制，方便对应用生命周期管理。
使用Operator-sdk，我们可以轻松创建自己的Operator。

参考文章https://www.openshift.com/learn/topics/operatorshttps://coreos.com/operators/
]]></content>
      <tags>
        <tag>openshift</tag>
        <tag>operator</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift存储管理方案——Rook</title>
    <url>/posts/ad266545adf3.html</url>
    <content><![CDATA[
存储的重要意义存储资源是容器云平台中的一个核心基础设施，为不同的应用服务提供可靠的持久化服务。大家都知道，容器运行过程中产生的数据是临时数据，并不可靠，一旦容器挂了，这些数据都会丢失。所以对数据可靠性有要求的应用就必须使用存储资源。存储的方案有很多种，常用的有本地盘存储、NFS、Ceph、Gluster FS等等。其中Ceph是一个开源的分布式文件系统，同时支持对象存储、块存储、文件存储,为云计算平台提供了最全面的存储方案。它以可靠、高性能等特性得到了很多企业的认可，并使用它来作为生产环境的存储。但是运维Ceph存储集群是一件较复杂工作，通过Rook项目，我们可以非常方便简单地实施Ceph存储方案，并且已有企业使用Rook来运维生产级别的存储方案。
Rook：CNCF云原生存储项目Rook于2018年1月加入了CNCF，成为了CNCF第15个项目，同时它也是CNCF首个云原生存储项目。Rook并不是自己开发一套存储方案，而是将现有的分布式存储系统云原生化，让它们能够实现自我管理，自我扩展，自我修复。 它使存储管理员的任务自动化：部署，引导，配置，配置，扩展，升级，迁移，灾难恢复，监视和资源管理。大大降低了存储系统的运维门槛，大大减少了维护成本。
Rook支持多种存储系统服务

Ceph（v1）
EdgeFS（v1）
Minio（Alpha）
CockroachDB（Alpha）
Cassandra（Alpha）
NFS（Alpha）
Yugabyte DB（Alpha）

Rook的特性

简单可靠的自动化资源管理
超大规模或超融合存储集群
高效地分发和复制数据以最大程度地减少损失
通过多个存储提供程序配置，文件，阻止和对象
管理开源存储技术
轻松地在数据中心中启用弹性存储
根据Apache 2.0许可发布的开源软件
优化商品硬件上的工作负载

以下是Rook在Kubernetes上部署的架构
OpenShift 容器平台部署RookOpenShift是红帽开发的K8S的企业级方案，它为原生K8S增加了许多安全及其他特性，特别是约束了运行中的Pod的权限。在部署与使用Rook时，需要允许应用拥有这些权限。


下载部署需要的代码


代码所在地址：https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph

common.yaml：创建Namespace、CRD、Role、RoleBinding、ClusterRole、ClusterRoleBinding、PodSecurityPolicy、ServiceAccount
operator-openshift.yaml: 创建SCC以及完成Rook Operator部署
cluster.yaml：创建CephCluster
object-openshift.yaml: 使用rgw创建对象存储，并设置对象存储服务对外的端口



修改operator-openshift.yaml文件



将ROOK_ENABLE_FLEX_DRIVER设置为true
将FLEXVOLUME_DIR_PATH值设置为&#x2F;usr&#x2F;libexec&#x2F;kubernetes&#x2F;kubelet-plugins&#x2F;volume&#x2F;exec



修改cluster.yaml指定OSD主机及目录&#x2F;盘符，以下列出了三种配置方式，可任意搭配。


storage:     useAllNodes: false    useAllDevices: false    topologyAware: true    deviceFilter:    location:    config:      nodes:      - name: &quot;172.17.4.101&quot;        directories:         - path: &quot;/rook/storage-dir&quot;        resources:          limits:            cpu: &quot;500m&quot;            memory: &quot;1024Mi&quot;          requests:            cpu: &quot;500m&quot;            memory: &quot;1024Mi&quot;      - name: &quot;172.17.4.201&quot;        devices:         - name: &quot;sdb&quot;        - name: &quot;nvme01&quot;           config:            osdsPerDevice: &quot;5&quot;        config:           storeType: filestore      - name: &quot;172.17.4.301&quot;        deviceFilter: &quot;^sd.&quot;


部署安装rook


$ oc create -f common.yaml$ oc create -f operator-openshift.yaml$ oc create -f cluster.yaml
完成部署好后，在rook-ceph Project下将会创建对应的资源
$ oc get  all -n rook-cephNAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGEservice/rook-ceph-mgr             ClusterIP   172.30.21.61     &lt;none&gt;        9283/TCP            17hservice/rook-ceph-mgr-dashboard   ClusterIP   172.30.243.194   &lt;none&gt;        8443/TCP            17hservice/rook-ceph-mon-a           ClusterIP   172.30.61.7      &lt;none&gt;        6789/TCP,3300/TCP   17hservice/rook-ceph-mon-b           ClusterIP   172.30.22.104    &lt;none&gt;        6789/TCP,3300/TCP   17hservice/rook-ceph-mon-c           ClusterIP   172.30.29.27     &lt;none&gt;        6789/TCP,3300/TCP   17hNAME                             DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGEdaemonset.apps/rook-ceph-agent   9         6         6         6            6           &lt;none&gt;          14hdaemonset.apps/rook-discover     9         6         6         6            6           &lt;none&gt;          14hNAME                                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/rook-ceph-mgr-a      1         1         1            1           17hdeployment.apps/rook-ceph-mon-a      1         1         1            1           17hdeployment.apps/rook-ceph-mon-b      1         1         1            1           17hdeployment.apps/rook-ceph-mon-c      1         1         1            1           17hdeployment.apps/rook-ceph-operator   1         1         1            1           1ddeployment.apps/rook-ceph-osd-0      1         1         1            1           17hdeployment.apps/rook-ceph-osd-1      1         1         1            1           15h


为Ceph Dashboard创建Route


$ cat &lt;&lt;EOF | oc create -f -apiVersion: route.openshift.io/v1kind: Routemetadata:  labels:    app: rook-ceph-mgr    rook_cluster: rook-ceph  name: rook-ceph-mgr-dashboard  namespace: rook-cephspec:  host: dashboard-rook.apps.example.com  port:    targetPort: https-dashboard  tls:    termination: passthrough  to:    kind: Service    name: rook-ceph-mgr-dashboard    weight: 100  wildcardPolicy: NoneEOF$ oc get routeNAME                      HOST/PORT                               SERVICES                  PORT              TERMINATION rook-ceph-mgr-dashboard   dashboard-rook.apps.ceshi.fenhang.com   rook-ceph-mgr-dashboard   https-dashboard   passthrough
此时便可通过浏览器访问Ceph Dashboard页面，查看Ceph集群状态



创建Ceph Block Pool及storageclass


$ cat &lt;&lt;EOF | oc create -f -apiVersion: ceph.rook.io/v1kind: CephBlockPoolmetadata:  name: replicapool  namespace: rook-cephspec:  failureDomain: host  replicated:    size: 3---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:   name: rook-ceph-blockprovisioner: ceph.rook.io/blockparameters:  blockPool: replicapool  # 设置为rook cluster operator所在的namespace  clusterNamespace: rook-ceph  # 文件系统，默认是ext4  fstype: xfs# reclaimPolicy，默认是&quot;Delete&quot;，可以设置为&quot;Retain&quot;,&quot;Recycle&quot; reclaimPolicy: Retain#  Kubernetes 1.14+支持Resize功能，只支持ext3, ext4, xfs类型allowVolumeExpansion: trueEOF
至此完成了通过Rook部署Ceph集群，并创建了相应的StorageClass，该StorageClass便可以为应用提供使用。如果需要对Ceph集群扩容，只需要更新CephCluster的配置，在storage项中添加新加入的目录或磁盘（生产上建议不要使用目录），Rook Operator将会自动将它加入到集群，实现集群的扩容。

参考文章Rook官网Rook 文档OpenShiftRook文档——Ceph存储Rook文档-Ceph Block
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift容器中读取Project信息</title>
    <url>/posts/6622dbfd97de.html</url>
    <content><![CDATA[
背景在日常运维管理中，经常需要获取OpenShift集群资源的信息，甚至创建、编辑或删除资源。我们都很清楚，使用oc命令就能够非常方便地完成这些操作。但是有时，我们希望通过调用接口来实现，以便于与其它组件或者应用进行集成。那么我们该如何做呢？本篇就以读取Project信息为例，展示如何通过HTTP请求操作OpenShift的资源。
操作
首先需要创建具有读取Project信息权限的clusterrole project_view

$ cat &lt;&lt;EOF | oc create -f -apiVersion: authorization.openshift.io/v1kind: ClusterRolemetadata:  name: project_viewrules:- apiGroups:  - project.openshift.io  resources:  - projects  verbs:  - getEOF

为需要调用的应用POD添加获取Project信息的权限

$ oc adm policy add-cluster-role-to-user project_view -z default
为所有应用授予获取Project信息的权限
$ oc adm policy add-cluster-role-to-group project_view system:serviceaccounts

通过发送serviceaccount认证信息，应用Pod实例中便可通过HTTP请求获取Project信息

$ TOKEN=$(cat /run/secrets/kubernetes.io/serviceaccount/token)$ CACERT=/run/secrets/kubernetes.io/serviceaccount/ca.crt$ PROJECT_OBJ=$(curl -s -H &quot;Authorization: bearer $TOKEN&quot;) --caert $CACERT https://openshift.default.svc/api/project.openshift.io/v1/projects/$NAMESPACE)$ PROJEXT_UUID=$(echo $PROJECT_OJB | jq --raw-output .metadata.uid)

完成

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift容器云平台建设之部署前准备</title>
    <url>/posts/ce0c8fadaa8b.html</url>
    <content><![CDATA[
在企业级部署OpenShift前，需要先考虑以下几个问题：

使用的主机架构是什么?IBM Power还是x86。
集群多大的容量，运行多少个Pod?




Limit Type
3.7 Limit
3.9 Limit
3.10 Limit
3.11 Limit



节点数 [1]
2,000
2,000
2,000
2,000


pod数 [2]
120,000
120,000
150,000
150,000


每台节点支持的pod数
250
250
250
250


每核支持的pod数
默认为10. 最大值为主机支持的pod数
默认为10. 最大值为主机支持的pod数
无默认值. 最大值为主机支持的pod数
无默认值. 最大值为主机支持的pod数


namespaces数量
10,000
10,000
10,000
10,000


Pipeline构建策略数量
N&#x2F;A
10,000 (默认pod内存为512Mi)
10,000 (默认pod内存为512Mi)
10,000 (默认pod内存为512Mi)


每个namespace下创建的pod数 [3]
3,000
3,000
3,000
25,000


services数 [4]
10,000
10,000
10,000
10,000


每个namespace下创建的services数
N&#x2F;A
N&#x2F;A
5,000
5,000


每个service的back-ends数
5,000
5,000
5,000
5,000


每个namespace下创建的deployments数[3]
2,000
2,000
2,000
2,000


另外根据部署的目标确定集群的节点数与节点配置。






3. 集群部署多少台主机，使用什么架构部署？多Master多Node，还是一个Master多Node，或者All in One?






4. 是否需要高可用集群？






5. 是否安装集群监控？这会消耗系统资源，默认是安装的。






6. 使用哪种账号认证体系？htpasswd，openldap，openid等。






7. 与其它应用集成时，兼容性查看地址






8. service常用的有两种技术：iptables与ipvs。如果集群中serive总数量为1000以下，endpoints为20000以下，则推荐使用iptables，否则推荐使用ipvs。






来自以下文章：OpenShift官方文档——准备安装
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift应用补丁检查与升级</title>
    <url>/posts/b9fba3e32e8f.html</url>
    <content><![CDATA[OpenShift版本补丁查看：https://docs.openshift.com/container-platform/3.11/release_notes/ocp_3_11_release_notes.html
自动查看新增的补丁版本脚本
# -*- coding: utf-8 -*-
&quot;&quot;&quot;
-------------------------------------------------
   File Name：     get_update_v3.11
-------------------------------------------------
&quot;&quot;&quot;

last_index=&quot;RHSA-2020:2992&quot;

last_flag = False

import requests
from pyquery import PyQuery

url=&quot;https://docs.openshift.com/container-platform/3.11/release_notes/ocp_3_11_release_notes.html&quot;

content=requests.get(url)
query_doc = PyQuery(content.content)(&quot;.sectlevel2:eq(2) li&quot;).items()


for update_item in query_doc:
    if not last_flag:
        item_text = update_item.text()
        if item_text.find(last_index) &gt; -1:
            last_flag = True
        continue

    print(update_item.text())

-----------------

RHBA-2020:3245 - OpenShift Container Platform 3.11.272 bug fix update
RHSA-2020:3541 - Important: OpenShift Container Platform security update
RHBA-2020:3695 - OpenShift Container Platform 3.11.286 bug fix update
RHSA-2020:3727 - Moderate: OpenShift Container Platform security update
RHBA-2020:4170 - OpenShift Container Platform 3.11.306 bug fix update
RHSA-2020:4223 - Important: OpenShift Container Platform security update
RHBA-2020:4430 - OpenShift Container Platform 3.11.317 bug fix update
RHBA-2020:5107 - OpenShift Container Platform 3.11.318 bug fix update
RHSA-2020:5102 - Moderate: OpenShift Container Platform security update
RHSA-2020:5363 - OpenShift Container Platform 3.11.346 security and bug fix update

软件包：https://access.redhat.com/downloads/content/package-browser
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift强大的oc-set命令详解</title>
    <url>/posts/a40d5162aa40.html</url>
    <content><![CDATA[
OpenShift的client工具oc功能非常强大，几乎能实现所有的操作，它的每一个子命令都能实现非常多的功能。其中有个容易被忽视却很强大的子命令oc set可以实现对资源很多额外的配置，包括有环境变量、资源限制、持久化卷挂载、健康检查等等。下面是oc set的帮助详情。
[root@master ~]# oc setConfigure application resourcesUsage:  oc set COMMAND [flags]Replication controllers, deployments, and daemon sets:  env             Update environment variables on a pod template  resources       Update resource requests/limits on objects with pod templates  volumes         Update volumes on a pod template  probe           Update a probe on a pod template  deployment-hook Update a deployment hook on a deployment config  image           Update image of a pod templateManage secrets:  build-secret    Update a build secret on a build configManage application flows:  image-lookup    Change how images are resolved when deploying applications  triggers        Update the triggers on one or more objects  build-hook      Update a build hook on a build configControl load balancing:  route-backends  Update the backends for a routeUse &quot;oc set &lt;command&gt; --help&quot; for more information about a given command.Use &quot;oc options&quot; for a list of global command-line options (applies to all commands).
环境变量 oc set env
列出sample dc设置的环境变量

$ oc set env dc/sample --list  # 列出sample dc设置的环境变量

为sample dc设置环境变量NAME&#x3D;value

$ oc set env dc/sample NAME=value # 为sample dc设置环境变量NAME=value

从secret mysecret中向sample dc导入环境变量

$ oc set env --from=secret/mysecret dc/sample #从secret mysecret中向sample dc导入环境变量

从configmap myconfigmap中筛选出前缀为MYSQL_的变量向sample dc中导入为环境变量

$ oc set env --from=configmap/myconfigmap --prefix=MYSQL_ dc/sample #从configmap myconfigmap中筛选出前缀为MYSQL_的变量向sample dc中导入为环境变量

删除sample dc中的名为NAME的环境变量

$ oc set env dc/sample NAME- #删除sample dc中的名为NAME的环境变量
资源限制 oc set resources
为dc&#x2F;sample中的容器设置内存限制

$ oc set resources dc/sample --limits=memory=2Gi --requests=memory=1Gi

给sample dc中的所有容器设置资源cpu和memory限制

$ oc set resources dc/sample --limits=cpu=200m,memory=2Gi --requests=cpu=100m,memory=1Gi #给sample dc中的所有容器设置资源cpu和memory限制

给sample dc中的nginx容器设置资源cpu和memory限制

$ oc set resources dc/sample -c=nginx --limits=cpu=200m,memory=2Gi --requests=cpu=100m,memory=1Gi #给sample dc中的nginx容器设置资源cpu和memory限制

删除sample dc资源限制配置

$ oc set resources dc/sample --limits=cpu=0,memory=0 --requests=cpu=0,memory=0 #删除资源限制配置

不执行更新，只显示修改的配置

$ oc set resources dc/sample --limits=cpu=200m,memory=512Mi -o yaml #不执行更新，只显示修改的配置
挂载点 oc set volumes
查看sample dc的挂载点

$ oc set volumes dc/sample

查看当前项目下的所有dc的挂载点

$ oc set volumes dc --all

给sample dc创建一个挂载点&#x2F;data，挂载emptyDir

$ oc set volumes dc/sample --add --mount-path=/data

使用一个现有的名为pvc1的pvc挂载到名为v1的挂载点

$ oc set volumes dc/sample --add --name=v1 -t pvc --claim-name=pvc1 --overwrite

删除sample dc中名为v1的挂载点

$ oc set volumes dc/sample --remove --name=v1

新建一个pvc挂载到sample dc中名为v1的挂载点

$ oc set volumes dc/sample --add --name=v1 -t pvc --claim-size=1G --overwrite

更改名为v1的挂载点对应在sample dc中的目录为&#x2F;data2

$ oc set volumes dc/sample --add --name=v1 -m /data2 --overwrite

删除sample dc中的nginx容器中名为v1的挂载点

$ oc set volumes dc/sample --remove --name=v1 --containers=nginx

将configmap中的nginx.conf配置挂载到&#x2F;data&#x2F;nginx.config文件中

$ oc set volumes dc/sample --add --name=v1 --mount-path=/data/nginx.conf --type=configmap --configmap-name=nginx --sub-path=nginx.conf
另外还有一些常用的参数–claim-name：指定pvc的名字–claim-class：指定pvc使用的StorageClass名–claim-mode：指定pvc访问类型，默认为ReadWriteOnce(rwo)，另外还有ReadWriteMany(rwm)和ReadOnlyMany(rom)–default-mode：挂载目录的权限，默认为0644，必须在0000和0777之间取值–configmap-name：挂载到挂载点的configmap名，必须设置挂载类型为configmap–secret-name：挂载到挂载点的secret名，必须设置挂载类型为secret–sub-path：只挂载指定的子目录–path：本地主机路径，必须设置挂载类型为hostPath–type：挂载类型，emptyDir，hostPath，secret，configmap
健康检查 oc set probe
给sample dc添加liveness检查shell

$ oc set probe dc/sample --liveness --failure-threshold 3 --initial-delay-seconds 40 -- echo ok

给sample dc添加readiness检查HTTP

$ oc set probe dc/sample --readiness --failure-threshold 3 --initial-delay-seconds 20 --get-url=http://:9000/about

给sample dc添加readiness检查TCP

$ oc set probe dc/sample --readiness --open-tcp=3306

设置readiness的初始检查延时时间30s，超时时间为3s

$ oc set probe dc/sample --readiness --initial-delay-seconds=30 --timeout=3

删除rediness和liveness检查

$ oc set probe dc/sample --remove --readiness --liveness
另外还有一些常用的参数–period-seconds：默认为10，两次检查间隔时长，单位为s–success-threshold：默认为1，检查成功次数达到1次，才会触发成功处理操作–failure-threshold：默认为3，检查失败次数达到3次，才会触发失败处理操作
设置Deploymentconfig钩子 oc set deployment-hook
为sample dc中的strategy设置pre钩子

$ oc set deployment-hook dc/sample --container=sample --pre -v data -- /var/lib/migrate-db.sh

为sample dc中的strategy设置mid钩子

$ oc set deployment-hook dc/sample --mid -v data -e VAR1=value1 -e VAR2=value2 -- /var/lib/prepare-deploy.sh

删除sample dc的pre、mid及post的钩子

$ oc set deployment-hook dc/sample --remove --pre --mid --post
更新镜像 oc set image
为sample dc中的容器sample1设置镜像sample1:v2，为容器sampel2设置镜像sample2:v2

$   oc set image dc/sample sample1=sample1:v2 sample2=sample2:v2

为sample dc中的容器app设置镜像为imagestream tag sample&#x2F;ruby:2.3

$   oc set image dc/sample app=sample/ruby:2.3 --source=imagestreamtag

为所有dc中的容器sample设置镜像为sample:v2

$   oc set image dc sample=sample:v2 --all
设置BuildConfig的拉取&#x2F;推送镜像、拉取代码的密钥 oc set build-secret
设置samplebuild bc的拉取镜像的密钥dockerhubsecret

$  oc set build-secret --pull bc/samplebuild dockerhubsecret

设置samplebuild bc的推送镜像的密钥dockerhubsecret

$  oc set build-secret --push --pull bc/samplebuild dockerhubsecret

设置samplebuild bc的拉取代码的密钥githubsecret

$  oc set build-secret --source  bc/samplebuild  githubsecret

删除sample bc设置的推送镜像密钥

$   oc set build-secret --push --remove bc/sample
设置触发器 oc set triggers
列出sample dc当前的触发器

$ oc set triggers dc/sample

关闭sample dc所有的自动触发器

$ oc set triggers dc/sample --manual

开启sample dc所有的自动触发器

$ oc set triggers dc/sample --auto

更新sample bc的github触发器的密钥值

$ oc set triggers bc/sample --from-github

删除sample bc的所有触发器

$   oc set triggers bc/sample --remove-all

删除sample dc的ConfigChange触发器

$ oc set triggers dc/sample --from-config --remove

更新sample bc的ImageStreamTag触发器的镜像Tag为test&#x2F;sample:latest

$  oc set triggers bc/sample --from-image=test/sample:latest

更新sample dc容器sample的ImageStreamTag触发器的镜像Tag为test&#x2F;sample:latest

$  oc set triggers dc/sample --from-image=test/sample:latest -c sample
设置BuildConfig的postcommit钩子 oc set build-hook
设置samplebuild bc的command类型的post-commit

$  oc set build-hook bc/samplebuild --post-commit --command -- /bin/bash -c /var/lib/test-image.sh

设置samplebuild bc的script类型的post-commit

$ oc set build-hook bc/samplebuild --post-commit --script=&quot;/var/lib/test-image.sh param1 param2 &amp;&amp; /var/lib/done.sh&quot;

设置samplebuild bc的args类型的post-commit

$  oc set build-hook bc/samplebuild --post-commit  -- arg1 arg2
command类型、script类型与args类型的区别：

script：执行时使用&#x2F;bin&#x2F;sh -ic替换掉默认的entrypoint来执行代码
command：不指定解析器，也不使用entrypoint，而是直接执行代码
args：使用默认的entrypoint执行代码


删除samplebuild bc的post-commit

$   oc set build-hook bc/samplebuild --post-commit --remove
设置Route的后端服务oc set route-backends
列出sample route的后端服务

$ oc set route-backends sample

设置sample route的后端服务为两个并且流量比为1：9，samplev1（10%），samplev2（90%）

$ oc set route-backends sample samplev1=1 samplev2=9

为sample route后端服务samplev1添加10%的流量

$ oc set route-backends sample --adjust samplev1=+10%

将sample route所有后端服务流量设为0，该route将无法对外提供服务

$ oc set route-backends sample --zero
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift支持Calico-BGP-路由反射（RR）模式</title>
    <url>/posts/c1de1fa83c9a.html</url>
    <content><![CDATA[
一、Calico 是什么calico 是容器网络的一种解决方案，也是当前最流行的方案之一。和其他虚拟网络最大的不同是，它没有采用 overlay 网络做报文的转发，提供了纯 3 层的网络模型。三层通信模型表示每个容器都通过 IP 直接通信，中间通过路由转发找到对方。在这个过程中，容器所在的节点类似于传统的路由器，提供了路由查找的功能。
要想路由工作能够正常，每个虚拟路由器（容器所在的主机节点）必须有某种方法知道整个集群的路由信息，calico 采用的是BGP 路由协议，全称是 Border Gateway Protocol。
除了能用于 docker 这样的容器外，它还能集成到容器集群平台 OpenShift&#x2F;kubernetes、公有云平台 AWS、GCE 等， 而且也能很容易地集成到 openstack 等 Iaas 平台。
二、Calico网络方式2.1 IPIP模式从字面来理解，就是把一个IP数据包又套在一个IP包里，即把 IP 层封装到 IP 层的一个 tunnel，看起来似乎是浪费，实则不然。它的作用其实基本上就相当于一个基于IP层的网桥！一般来说，普通的网桥是基于mac层的，根本不需 IP，而这个 ipip 则是通过两端的路由做一个 tunnel，把两个本来不通的网络通过点对点连接起来。ipip 的源代码在内核 net&#x2F;ipv4&#x2F;ipip.c 中可以找到。
2.2 BGP模式边界网关协议（Border Gateway Protocol, BGP）是互联网上一个核心的去中心化自治路由协议。它通过维护IP路由表或‘前缀’表来实现自治系统（AS）之间的可达性，属于矢量路由协议。BGP不使用传统的内部网关协议（IGP）的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。BGP，通俗的讲就是讲接入到机房的多条线路（如电信、联通、移动等）融合为一体，实现多线单IP，BGP 机房的优点：服务器只需要设置一个IP地址，最佳访问路由是由网络上的骨干路由器根据路由跳数与其它技术指标来确定的，不会占用服务器的任何系统。
根据路由规则的同步方式，BGP模式也有两种方式，mesh模式与RR模式。
2.2.1 mesh 模式mesh 模式又称为全互联模式，就是一个 BGP Speaker 需要与其它所有的 BGP Speaker 建立 bgp 连接（形成一个bgp mesh）。BGP Speaker越多，将会消耗越多的连接。所以它只能支持小规模集群，一般50-100为上限。
2.2.2 RR模式RR模式，就是在网络中指定一个或多个 BGP Speaker 作为反射路由（Router Reflector），RR与所有的 BGP Speaker 建立 bgp 连接。每个BGP Speaker只与RR建立连接，交换路由信息就能获得全网的路由信息。Calico 中通过Global Peer来实现RR模式。
默认情况下，每个 calico 节点会和集群中其他所有节点建立 BGP peer 连接，也就是说这是一个 O(n^2) 的增长趋势。在集群规模比较小的情况下，这种模式是可以接受的，但是当集群规模扩展到百个节点、甚至更多的时候，这样的连接数无疑会带来很大的负担。为了解决集群规模较大情况下 BGP client 连接数膨胀的问题，calico 引入了 RR（Router Reflector） 的功能。
RR 的基本思想是选择一部分节点（一个或者多个）作为 Global BGP Peer，它们和所有的其他节点互联来交换路由信息，其他的节点只需要和 Global BGP Peer 相连就行，不需要之间再两两连接。更多的组网模式也是支持的，不管怎么组网，最核心的思想就是所有的节点能获取到整个集群的路由信息。
三、Calico组件和架构calico 的工作有：

分配和管理 IP
配置上容器的 veth pair 和容器内默认路由
根据集群网络情况实时更新节点上路由表

完成以上所有的功能，除了 etcd 保存了数据外，还在每个节点运行 了calico-node 容器。calico&#x2F;node 这个容器运行了多个组件：

libnetwork plugin
BIRD
confd
felix

下图为Calico各组件之间的相互关系图。
3.1 libnetwork pluginlibnetwork-plugin 是 calico 提供的 docker 网络插件，主要提供的是 IP 管理和网络管理的功能。
默认情况下，当网络中出现第一个容器时，calico 会为容器所在的节点分配一段子网（子网掩码为 &#x2F;26，比如192.168.196.128&#x2F;26），后续出现在该节点上的容器都从这个子网中分配 IP 地址。这样做的好处是能够缩减节点上的路由表的规模，按照这种方式节点上 2^6 &#x3D; 64 个 IP 地址只需要一个路由表项就行，而不是为每个 IP 单独创建一个路由表项。节点上创建的子网段可以在etcd 中 &#x2F;calico&#x2F;ipam&#x2F;v2&#x2F;host&#x2F;&#x2F;ipv4&#x2F;block&#x2F; 看到。
calico 还允许创建容器的时候指定 IP 地址，如果用户指定的 IP 地址不在节点分配的子网段中，calico 会专门为该地址添加一个 &#x2F;32 的网段。
3.2 BIRDBIRD（BIRD Internet Routing Daemon） 是一个常用的网络路由软件，支持很多路由协议（BGP、RIP、OSPF等）。它会在每台宿主机上运行，calico 用它在实现主机间传播路由信息。
BIRD 对应的配置文件在 &#x2F;etc&#x2F;calico&#x2F;confd&#x2F;config&#x2F; 目录。
3.3 confdconfd是一个简单的配置管理工具。bird 的配置文件会根据用户设置的变化而变化，因此需要一种动态的机制来实时维护配置文件并通知 bird 使用最新的配置，这就是 confd 的工作。它会监听 etcd 的数据，用来更新 bird 的配置文件，并重新启动 bird 进程让它加载最新的配置文件。confd 的工作目录是 &#x2F;etc&#x2F;calico&#x2F;confd，里面有三个目录：

conf.d：confd 需要读取的配置文件，每个配置文件告诉 confd 模板文件在什么，最终生成的文件应该放在什么地方，更新时要执行哪些操作等
config：生成的配置文件最终放的目录
templates：模板文件，里面包括了很多变量占位符，最终会替换成 etcd 中具体的数据具体的配置文件很多，以下是一个例子：

/ # cat /etc/calico/confd/conf.d/bird.toml[template]src = &quot;bird.cfg.mesh.template&quot;dest = &quot;/etc/calico/confd/config/bird.cfg&quot;prefix = &quot;/calico/bgp/v1&quot;keys = [&quot;/host&quot;,&quot;/global&quot;]check_cmd = &quot;bird -p -c &#123;&#123;.src&#125;&#125;&quot;reload_cmd = &quot;pkill -HUP bird || true&quot;
它会监听 etcd 的 &#x2F;calico&#x2F;bgp&#x2F;v1 路径，一旦发现更新，就用其中的内容更新模板文件 bird.cfg.mesh.template，把新生成的文件放在 &#x2F;etc&#x2F;calico&#x2F;confd&#x2F;config&#x2F;bird.cfg，文件改变之后还会运行 reload_cmd 指定的命令重启 bird 程序。
3.4 felixfelix 负责最终网络相关的配置，也就是容器网络在 linux 上的配置工作，比如：

更新节点上的路由表项
更新节点上的 iptables 表项

它的主要工作是从 etcd 中读取网络的配置，然后根据配置更新节点的路由和 iptables，felix 的代码在 http:&#x2F;&#x2F;github.com&#x2F;calico&#x2F;felix。
四、部署Calico BGP RR模式4.1 安装Calico BGP网络插件在之前的文章中已经分享过Calico BGP网络插件的部署过程。Openshift开启Calico BGP 与 OVS性能PK安装完成后，Calico BGP默认使用的是Mesh模式。
4.2 开启RR模式4.2.1 开启单RR部署（单个主机节点作为RR）
关闭当前Calico Mesh模式

 [root@master calico]# cat &lt;&lt; EOF | calicoctl create -f -apiVersion: projectcalico.org/v3kind: BGPConfigurationmetadata:  name: defaultspec:  logSeverityScreen: Info  nodeToNodeMeshEnabled: false  asNumber: 63400

设置指定Node为RR，比如选择infra01_node为RR，“添加router-reflector标签，设置routeReflectorClusterID”。

[root@master calico]# calicoctl get node infra01_node  --export -o yaml  &gt; infra01_node.yaml[root@master calico]# cat infra01_node.yamlapiVersion: projectcalico.org/v3kind: Nodemetadata:  labels:    i-am-a-route-reflector: &quot;true&quot;  name: infra01_nodespec:  bgp:    ipv4Address: 192.168.0.3/16    routeReflectorClusterID: 224.0.0.1[root@master calico]# calico apply -f infra01_node.yaml

配置BGPPeer资源，告诉Node节点路由反射器。

[root@master calico]# cat &lt;&lt; EOF | calicoctl create -f -apiVersion: projectcalico.org/v3kind: BGPPeermetadata:  name: peer-to-rrsspec:  nodeSelector: !has(i-am-a-route-reflector)  peerSelector: has(i-am-a-route-reflector)

查看bgppeer

[root@master1 calico]# calicoctl get bgppeers NAME          PEERIP   NODE                          ASN    peer-to-rrs            (global)                      0   

通过netstat命令查看节点间calico-node的连接

[root@master1 calico]# netstat -natp | grep bird
可以看到非RR节点只与RR节点建立连接，而RR节点与所有节点建立连接。
4.2.2 多RR节点部署（多个主机节点作为RR）当只有一个RR节点时，会有一定的风险。当该节点出现异常时，所有节点将无法获取路由，这将会影响到整个OpenShift集群的网络。所以需要考虑RR节点的高可用，一种最简单的方式就是设置多个RR节点。下面演示如何添加一个新RR节点。

参考4.2.1，选择infra02_node作为新的RR节点。与infra01_node设置同样的routeReflectorClusterID值。
额外添加一条新的BGPPeer资源

[root@master calico]# cat &lt;&lt; EOF | calicoctl create -f -apiVersion: projectcalico.org/v3kind: BGPPeermetadata:  name: rr-meshspec:  nodeSelector: has(i-am-a-route-reflector)  peerSelector: has(i-am-a-route-reflector)

查看bgppeer

[root@master1 calico]# calicoctl get bgppeers NAME          PEERIP   NODE                          ASN    peer-to-rrs            (global)                      0    rr-mesh                has(i-am-a-route-reflector)   0 
以上配置后，每个非RR节点会与所有RR节点建立连接，同时RR节点之间建立连接，RR节点间是一个双活的配置。
4.2.3 硬件设备RR节点部署（硬件交换机作为RR节点）前提：硬件交换机支持BGP协议。

查看硬件交换机对应vlan IP。

/ Display current-configuration interface vlan 16interface Vlanif16 ip address 192.168.16.251 255.255.255.0 vrrp vrid 84 virtual-ip 192.168.16.253 vrrp vrid 84 priority 150 vrrp vrid 84 preempt-mode timer delay 20 vrrp vrid 84 track interface XGigabitEthernet0/0/1 reduced 30 vrrp vrid 84 track interface XGigabitEthernet0/0/2 reduced 30

在OpenShift集群关闭Calico mesh模式，并设置ASNumber。

[root@master calico]# cat &lt;&lt; EOF | calicoctl create -f -apiVersion: projectcalico.org/v3kind: BGPConfigurationmetadata:  name: defaultspec:  logSeverityScreen: Info  nodeToNodeMeshEnabled: false  asNumber: 63400

在OpenShift集群创建与硬件交换机的BGPPeer配置。

[root@master calico]# cat &lt;&lt; EOF | calicoctl create -f -apiVersion: projectcalico.org/v3kind: BGPPeermetadata:  name: bgppeer-globalspec:  peerIP: 192.168.16.251  asNumber: 63400

硬件交换机和NODE 在AS号63400 中,通过IBGP建立了对等体

/ bgp 63400 router-id 192.168.16.11 peer 192.168.16.1 as-number 63400 peer 192.168.16.2 as-number 63400 peer 192.168.16.3 as-number 63400

硬件交换机的192.168.16.251 作为RR和NODE建立了BGP

/  ipv4-family unicast  undo synchronization  peer 192.168.16.1 enable  peer 192.168.16.1 reflect-client  peer 192.168.16.2 enable  peer 192.168.16.2 reflect-client  peer 192.168.16.3 enable                   peer 192.168.16.3 reflect-client

在硬件交换机上查看bgp peer信息

/  display  bgp peer BGP local router ID : 192.168.16.11 Local AS number : 63400 Total number of peers : 3                Peers in established state : 3  Peer           V     AS  MsgRcvd  MsgSent  OutQ  Up/Down   State PrefRcv  192.168.16.1   4    63400   26       29   0 00:19:34     Established    1  192.168.16.2   4    63400   26       29     0 00:19:34   Established    1  192.168.16.3   4    63400   22       22     0 00:16:27   Established    1
BGP状态为established 表示已建立状态。7. 在硬件交换机上查看路由信息
/  display  bgp routing-table  BGP Local router ID is 192.168.16.11  Total Number of Routes: 3      Network            NextHop        MED        LocPrf    PrefVal Path/Ogn *&gt;i  10.200.0.0/24      192.168.16.3                100        0      i *&gt;i  10.200.1.0/24      192.168.16.2                100        0      i *&gt;i  10.200.2.0/24      192.168.16.1                100        0      i
并能通过NODE学到POD的地址段,从192.168.16.1学到10.200.2.0&#x2F;24的路由,192.168.16.2学到10.200.1.0&#x2F;24的路由,192.168.16.3学到10.200.0.0&#x2F;24的路由。
五、Calico BGP管理工具calicoctl管理calico网络需要使用calicoctl工具，该工具的配置可参考OpenShift&#x2F;Kubernetes集群 Calico BGP管理工具calicoctl配置


查看calico节点状态

$ calicoctl node status

查看ip池

$ calicoctl get ipPool

查看node信息

$ calicoctl get node -o wide$ calicoctl get node infra01_node -o yaml

创建新的资源

$ calicoctl create -f resource.yaml
补充OpenShift Calico BGP网络模式下默认使用的是etcd存储，同时访问etcd的证书保存secret calico-etcd-secrets中。
$ oc get secret | grep calico-etcd-secretscalico-etcd-secrets   Opaque    3     205d
如果calico的存储etcd使用的正是OpenShift集群的etcd集群，那么当etcd的证书有更新时，请务必更新calico证书的内容，也就是更新该secret的内容，否则集群网络将无法为新的POD分配网络。
$ oc project kube-system$ oc create secret generic calico-etcd-secrets \ --from-file=etcd-ca=/etc/etcd/ca.crt \--from-file=etcd-cert=/etc/etcd/server.crt \--from-file=etcd-key=/etc/etcd/server.key -o yaml \ --dry-run | oc apply -f -

参考文章calico官方部署文档calico官方配置案例Calico官方架构文档docker 容器网络方案：calico 网络模型在私有云上运行Calico：IP互连网络结构k8s使用calico网络calico网络原理、组网方式和使用Calico配置双RR架构
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift数据库挂了问题解决</title>
    <url>/posts/f9994802a76a.html</url>
    <content><![CDATA[每个计算节点都无法启动，报错信息为：
Jan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.895622    4307 feature_gate.go:226] feature gates: &amp;&#123;&#123;&#125; map[RotateKubeletServerCertificate:true RotateKubeletClientCertificate:true]&#125;Jan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.902964    4307 mount_linux.go:211] Detected OS with systemdJan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.908967    4307 server.go:383] Version: v1.10.0+b81c8f8Jan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.909036    4307 feature_gate.go:226] feature gates: &amp;&#123;&#123;&#125; map[RotateKubeletClientCertificate:true RotateKubeletServerCertificate:true]&#125;Jan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.909150    4307 plugins.go:89] No cloud provider specified.Jan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.909162    4307 server.go:499] No cloud provider specified: &quot;&quot; from the config file: &quot;&quot;Jan 05 00:05:10 node1.example.com origin-node[4307]: E0105 00:05:10.931121    4307 bootstrap.go:198] Part of the existing bootstrap client certificate is expired: 2020-01-04 07:20:00 +0000 UTCJan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.931145    4307 bootstrap.go:56] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig fileJan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.932606    4307 certificate_store.go:131] Loading cert/key pair from &quot;/etc/origin/node/certificates/kubelet-client-current.pem&quot;.Jan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.959131    4307 csr.go:105] csr for this node already exists, reusingJan 05 00:05:10 node1.example.com origin-node[4307]: I0105 00:05:10.967338    4307 csr.go:113] csr for this node is still valid
一、更新证书后，&#x2F;etc&#x2F;origin&#x2F;node&#x2F;cerxx**&#x2F;client-current.(server).
如果有csr的话，就需要将csr（CertificateSigningRequest）批准通过
oc get csr -o name | xargs oc adm certificate approve
需要去查的是：

为什么1月4日会自动去更新kubelet证书因为生产上kubelet证书的默认有效期为1年，到期会自动更新证书。计算节点上相关的配置项为kubeletArguments.rotate-certificates: [&#39;true&#39;]
为什么csr为Pending，而没有被批准这是openshift 3.11的Master节点恰好刚过期，但是此时的bootstrap的token没有过期，Node节点会向Master申请证书csr。而在OpenShift中csr的审批需要手动通过。所以这块需要做好监控与告警，确保生产上的证书不要过期。

相关文章：https://access.redhat.com/solutions/3716861https://access.redhat.com/solutions/4565991
二、数据库问题数据库使用的镜像是：centos/mysql-57-centos7由于是操作数据库mysql改变root的密码，而common.sh中会校验数据库的状态，但是该镜像中的common.sh默认root密码是空的，需要更改该脚本的代码，(添加ROOT密码)：
// 第54行mysql_flags=&quot;-u root -p$MYSQL_ROOT_PASSWORD --socket=/tmp/mysql.sock
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift日志审计功能与策略配置</title>
    <url>/posts/470695cf24c2.html</url>
    <content><![CDATA[
OpenShift支持审计功能，它可以记录下所有API服务的请求。如果将所有请求记录下来，它的量非常庞大，同时也是没有太大意义的。所以OpenShift审计记录当然出会支持请求的过滤，通过方便地策略配置，可以有选择地记录下请求的内容。通常我们会记录对集群资源作更改的请求。
打开审计功能在部署OpenShift时可以打开审计功能
openshift_master_audit_config=&#123;&quot;enabled&quot;: true, &quot;auditFilePath&quot;: &quot;/var/lib/origin/audit-ocp.log&quot;, &quot;maximumFileRetentionDays&quot;: 14, &quot;maximumFileSizeMegabytes&quot;: 500, &quot;maximumRetainedFiles&quot;: 5, &quot;policyFile&quot;: &quot;/etc/origin/master/adv-audit.yaml&quot;, &quot;logFormat&quot;:&quot;json&quot;&#125;openshift_master_audit_policyfile=&quot;/&lt;path&gt;/adv-audit.yaml&quot;
在/etc/origin/master/master-config.yaml中的配置如下：
auditConfig:  auditFilePath: &quot;/var/lib/origin/audit-ocp.log&quot;  enabled: true  maximumFileRetentionDays: 10  maximumFileSizeMegabytes: 10  maximumRetainedFiles: 5  policyFile: &quot;/etc/origin/master/adv-audit.yaml&quot;  logFormat: json
其中：auditFilePath：审计日志保存地址maximumFileRetentionDays：最长保留时间maximumFileSizeMegabytes：每个文件最大大小maximumRetainedFiles：保留最大文件数policyFile：审计规则配置路径openshift_master_audit_policyfile：安装时指定部署机上的审计规则配置文件路径logFormat：日志格式
审计规则设置审计规则策略实例
apiVersion: audit.k8s.io/v1beta1kind: Policyrules:  # Do not log watch requests by the &quot;system:kube-proxy&quot; on endpoints or services  - level: None     users: [&quot;system:kube-proxy&quot;]     verbs: [&quot;watch&quot;]     resources:     - group: &quot;&quot;      resources: [&quot;endpoints&quot;, &quot;services&quot;]  # Do not log authenticated requests to certain non-resource URL paths.  - level: None    userGroups: [&quot;system:authenticated&quot;]     nonResourceURLs:     - &quot;/api*&quot; # Wildcard matching.    - &quot;/version&quot;  # Log the request body of configmap changes in kube-system.  - level: Request    resources:    - group: &quot;&quot; # core API group      resources: [&quot;configmaps&quot;]    # This rule only applies to resources in the &quot;kube-system&quot; namespace.    # The empty string &quot;&quot; can be used to select non-namespaced resources.    namespaces: [&quot;kube-system&quot;]   # Log configmap and secret changes in all other namespaces at the metadata level.  - level: Metadata    resources:    - group: &quot;&quot; # core API group      resources: [&quot;secrets&quot;, &quot;configmaps&quot;]  # Log all other resources in core and extensions at the request level.  - level: Request    resources:    - group: &quot;&quot; # core API group    - group: &quot;extensions&quot; # Version of group should NOT be included.  # A catch-all rule to log all other requests at the Metadata level.  - level: Metadata   # Log login failures from the web console or CLI. Review the logs and refine your policies.  - level: Metadata    nonResourceURLs:    - /login*     - /oauth* 
每个事件可记录的有四个级别level：

None：不记录与此规则匹配的事件
Metadata：记录请求元数据（请求用户，时间戳，资源，动词等），但不记录请求或响应正文。 此级别与基本审核中使用的级别相同。
Request：记录事件元数据和请求正文，但不记录响应正文。
RequestResponse：记录事件元数据，请求和响应主体。

用户users：规则适用的用户列表。 空列表表示每个用户请求类型verbs：调用 API的请求类型，(get, list, watch, create, update, patch, delete, deletecollection以及 proxy)。空列表表示每个动词。资源类型resources：规则适用的资源列表。 空列表表示所有资源。组列表userGroups：规则适用的组列表。 空列表表示每个组。nonResourceURLs ：规则适用的非资源URL列表。namespaces：规则适用的名称空间列表。 空列表表示每个名称空间。

一个实用的例子apiVersion: audit.k8s.io/v1beta1kind: Policyrules:  - level: None    userGroups: [&quot;system:masters&quot;, &quot;system:nodes&quot;, &quot;system:serviceaccounts:kube-system&quot;]  - level: None    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - level: Metadata
该配置将会过滤掉大部分组件之间交互的请求，同时也会过滤掉一些查询请求的审计。
参考文章：OpenShift官方文档：master-node-config-advanced-audit
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift日志级别相关</title>
    <url>/posts/95364a8cb9ef.html</url>
    <content><![CDATA[总共需要考虑的服务日志有

OpenShift 服务日志
Docker 守护进程日志
Etcd 日志
命令行日志
Pod&#x2F;容器日志
OpenShift Builder 日志
OpenShift 路由器日志

OpenShift服务日志
docker的日志级别docker日志级别总共有以下：”debug”, “info”, “warn”, “error”, “fatal”五个。docker –log-level默认为info

参考文章https://access.redhat.com/zh_CN&#x2F;solutions&#x2F;3345021
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift本地镜像仓库打不开</title>
    <url>/posts/ca88845f35ee.html</url>
    <content><![CDATA[访问OpenShift本地镜像仓库https://registry.paas.net，报错
&#123;&quot;error&quot;: &quot;invalid_request&quot;,&quot;error_description&quot;: &quot;The request is missing a required parameter, includes an invalid parameter value, includes a parameter more than once, or is otherwise malformed.&quot;&#125;
一般情况下，访问镜像仓库
$ oc get oauthclientNAME                           SECRET                                                                 WWW-CHALLENGE   TOKEN-MAX-AGE   REDIRECT URIScockpit-oauth-client           userqUADAdq1SKikUFwSSBwnq5fqgRjomXivWGbuoJV04mNPSYiOlaL1BSLBFLjjh4Ao   FALSE           default         https://registry-console-default.paas.net,https://registry.paas.netopenshift-browser-client       imwY5ps0HftJH0VZ_V25i4S4a0E75iTtUKjNOuW9_5Q                            FALSE           default         https://paas.net:8443/oauth/token/displayopenshift-challenging-client                                                                          TRUE            default         https://paas.baison.net:8443/oauth/token/implicitopenshift-console              c6vG5H3raNYBGqndetmn6BYaFAG6onUF                                       FALSE           default         https://console.paas.net/openshift-web-console                                                                                 FALSE           default         https://paas.net:8443/console/
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift根据etcd备份数据恢复etcd集群</title>
    <url>/posts/0c6466211503.html</url>
    <content><![CDATA[# ssh master1.etcd.com# ETCD_ALL_ENDPOINTS=` etcdctl3 --write-out=fields   member list | awk &#x27;/ClientURL/&#123;printf &quot;%s%s&quot;,sep,$3; sep=&quot;,&quot;&#125;&#x27;`# etcdctl3 --endpoints=$ETCD_ALL_ENDPOINTS  endpoint status  --write-out=table +-----------------------------------+------------------+---------+---------+-----------+-----------+------------+|           ENDPOINT                |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |+-----------------------------------+------------------+---------+---------+-----------+-----------+------------+|     https://master1.etcd.com:2379 | d91b1c20df818655 |  3.2.22 |   17 MB |      true |         6 |       42   ||           https://10.0.88.33:2379 |  d35cfd2fedc078f |  3.2.22 |   17 MB |     false |         6 |       42   ||           https://10.0.88.22:2379 | c9624828ed10ae36 |  3.2.22 |   17 MB |     false |         6 |       42   ||           https://10.0.88.11:2379 | d91b1c20df818655 |  3.2.22 |   17 MB |      true |         6 |       42   |+-----------------------------------+------------------+---------+---------+-----------+-----------+------------+# etcdctl3 snapshot save /var/lib/etcd/snapshot.db# cp /var/lib/etcd/snapshot.db /tmp/snapshot.db# cp /var/lib/etcd/member/snap/db /tmp/db# scp /tmp/snapshot.db master2.etcd.com:/tmp/snapshot.db# scp /tmp/snapshot.db master3.etcd.com:/tmp/snapshot.db# mkdir -p /etc/origin/node/pods-stopped/# mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/#  mv /var/lib/etcd/member /tmp/etcd-backup-$(date +%d-%m-%y)# rm -rf /var/lib/etcd# ssh master2.etcd.com# mkdir -p /etc/origin/node/pods-stopped/# mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/#  mv /var/lib/etcd/member /tmp/etcd-backup-$(date +%d-%m-%y)# rm -rf /var/lib/etcd# ssh master3.etcd.com# mkdir -p /etc/origin/node/pods-stopped/# mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/#  mv /var/lib/etcd/member /tmp/etcd-backup-$(date +%d-%m-%y)# rm -rf /var/lib/etcd# ssh master1.etcd.com # source /etc/etcd/etcd.conf# export ETCDCTL_API=3# echo -e  &quot;$ETCD_INITIAL_CLUSTER \n$ETCD_INITIAL_CLUSTER_TOKEN&quot;  master1.etcd.com=https://10.0.88.11:2380,master2.etcd.com=https://10.0.88.22:2380,master3.etcd.com=https://10.0.88.33:2380    etcd-cluster-1# ETCDCTL_API=3 etcdctl snapshot restore /tmp/snapshot.db \  --name master1.etcd.com \  --initial-cluster master1.etcd.com=https://10.0.88.11:2380,master2.etcd.com=https://10.0.88.22:2380,master3.etcd.com=https://10.0.88.33:2380 \  --initial-cluster-token etcd-cluster-1 \  --initial-advertise-peer-urls https://10.0.88.11:2380 \  --data-dir /var/lib/etcd 2019-02-05 12:49:04.103233 I | mvcc: restore compact to 23617442019-02-05 12:49:04.135995 I | etcdserver/membership: added member d35cfd2fedc078f [https://10.0.88.33:2380] to cluster 1a196dd3442fbe592019-02-05 12:49:04.136161 I | etcdserver/membership: added member c9624828ed10ae36 [https://10.0.88.22:2380] to cluster 1a196dd3442fbe592019-02-05 12:49:04.136267 I | etcdserver/membership: added member d91b1c20df818655 [https://10.0.88.11:2380] to cluster 1a196dd3442fbe59# restorecon -Rv /var/lib/etcd# ssh master2.etcd.com# ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \  --name master2.etcd.com \  --initial-cluster master1.etcd.com=https://10.0.88.11:2380,master2.etcd.com=https://10.0.88.22:2380,master3.etcd.com=https://10.0.88.33:2380 \  --initial-cluster-token etcd-cluster-1 \  --initial-advertise-peer-urls https://10.0.88.22:2380 \  --data-dir /var/lib/etcd 2019-02-05 12:51:25.179801 I | mvcc: restore compact to 23569502019-02-05 12:51:25.193709 I | etcdserver/membership: added member d35cfd2fedc078f [https://10.0.88.33:2380] to cluster 1a196dd3442fbe592019-02-05 12:51:25.193745 I | etcdserver/membership: added member c9624828ed10ae36 [https://10.0.88.22:2380] to cluster 1a196dd3442fbe592019-02-05 12:51:25.193759 I | etcdserver/membership: added member d91b1c20df818655 [https://10.0.88.11:2380] to cluster 1a196dd3442fbe59# restorecon -Rv /var/lib/etcd# ssh master3.etcd.com# ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \  --name master3.etcd.com \  --initial-cluster master1.etcd.com=https://10.0.88.11:2380,master2.etcd.com=https://10.0.88.22:2380,master3.etcd.com=https://10.0.88.33:2380 \  --initial-cluster-token etcd-cluster-1 \  --initial-advertise-peer-urls https://10.0.88.33:2380 \  --data-dir /var/lib/etcd 2019-02-05 12:53:06.612149 I | mvcc: restore compact to 23569502019-02-05 12:53:06.634761 I | etcdserver/membership: added member d35cfd2fedc078f [https://10.0.88.33:2380] to cluster 1a196dd3442fbe592019-02-05 12:53:06.634905 I | etcdserver/membership: added member c9624828ed10ae36 [https://10.0.88.22:2380] to cluster 1a196dd3442fbe592019-02-05 12:53:06.635001 I | etcdserver/membership: added member d91b1c20df818655 [https://10.0.88.11:2380] to cluster 1a196dd3442fbe59# restorecon -Rv /var/lib/etcd# ssh master1.etcd.com# mv /etc/origin/node/pods-stopped/etcd.yaml /etc/origin/node/pods/# ssh master2.etcd.com# mv /etc/origin/node/pods-stopped/etcd.yaml /etc/origin/node/pods/# ssh master3.etcd.com# mv /etc/origin/node/pods-stopped/etcd.yaml /etc/origin/node/pods/# ssh master1.etcd.com# ETCD_ALL_ENDPOINTS=` etcdctl3 --write-out=fields   member list | awk &#x27;/ClientURL/&#123;printf &quot;%s%s&quot;,sep,$3; sep=&quot;,&quot;&#125;&#x27;`# etcdctl3 --endpoints=$ETCD_ALL_ENDPOINTS  endpoint status  --write-out=table +-----------------------------------+------------------+---------+---------+-----------+-----------+------------+|           ENDPOINT                |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |+-----------------------------------+------------------+---------+---------+-----------+-----------+------------+|     https://master1.etcd.com:2379 | d91b1c20df818655 |  3.2.22 |   17 MB |      true |         6 |       42   ||           https://10.0.88.33:2379 |  d35cfd2fedc078f |  3.2.22 |   17 MB |     false |         6 |       42   ||           https://10.0.88.22:2379 | c9624828ed10ae36 |  3.2.22 |   17 MB |     false |         6 |       42   ||           https://10.0.88.11:2379 | d91b1c20df818655 |  3.2.22 |   17 MB |      true |         6 |       42   |+-----------------------------------+------------------+---------+---------+-----------+-----------+------------+# ssh master1.etcd.com# mv /etc/origin/node/pods-stopped/* /etc/origin/node/pods/# ssh master2.etcd.com# mv /etc/origin/node/pods-stopped/* /etc/origin/node/pods/# ssh master3.etcd.com# mv /etc/origin/node/pods-stopped/* /etc/origin/node/pods/# oc get nodes,pods -n  kube-system
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift灾备的方案及步骤</title>
    <url>/posts/c1cb5e8c61ba.html</url>
    <content><![CDATA[https://docs.openshift.com/container-platform/3.11/admin_guide&#x2F;assembly_restoring-cluster.htmlhttps://docs.openshift.com/container-platform/3.11/admin_guide&#x2F;assembly_replace-master-host.htmlhttps://docs.openshift.com/container-platform/3.11/admin_guide&#x2F;assembly_restore-etcd-quorum.htmlhttps://docs.openshift.com/container-platform/3.11/admin_guide&#x2F;assembly_replace-etcd-member.html
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift生产集群物理机部署与虚拟机部署各自优缺点</title>
    <url>/posts/f6b6028b1b52.html</url>
    <content><![CDATA[
OpenShift是物理机部署，还是虚拟机部署？这是企业在做容器平台架构选型时必须考虑的问题。尤其是大企业，有机器，有业务。今天我们就来列一列物理机与虚拟机部署各自的优缺点。
物理机部署
优点


性能好，无虚拟化层性能开销
小业务集群，节点少
配置高能够支持更大规模的容器数量
License成本更低
架构复杂度低（PaaS）
需要额外准备LB与存储
缺点


运维复杂度高
集群扩容周期长
如果出现故障，恢复周期长，需要及时去机房诊断修复
一台主机出现故障，影响业务范围更广
合规限制，需要更高要求的安全与隐私控制

虚拟机部署
优点


灵活部署、配置、维护简单
快速扩容
能与IaaS资源混合使用（LB、存储等）
一台虚拟机出现故障，影响业务范围小
虚拟机技术成熟，有热迁移技术，恢复快
IaaS高可用+PaaS高可用，两层保障
虚拟机支持虚拟快照，方便做主机回滚，集群可靠怀提升
IaaS目前已经有成熟的合规方案，在其上构建PaaS可以更方便地满足合规要求
缺点


性能较差（网络、计算等）
架构复杂度提升（IaaS + Paas）
配置较低，承载的容器容量更小（节点更多）
需要更多License成本

补充关于License问题，从红帽官方得到的收费规则如下：OpenShift订阅按照主机CPU核数来收费，如果虚拟机的话，2vcpu按一个核来收费。
所以建议OpenShift on IaaS方案，要把超分调小，这样可以在保证算力的情况下，节约License成本。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift用户与组的管理功能如何在Kubernetes上实现</title>
    <url>/posts/dbbeaaab9bb5.html</url>
    <content><![CDATA[
OpenShift中有用户及组的概念，并且提供了User与Group资源类型，可以非常方便地为OpenShift集群创建用户，创建组，以组给用户进行组管理，并授权。这也大大方便了集群账号的管理与权限的控制。
[openshift@master01 ~] oc create user dev-user # 创建用户dev-user[openshift@master01 ~] oc adm groups new my-group # 创建组my-group[openshift@master01 ~] oc adm groups new my-group dev-user # 创建组my-group，并添加dev-user到该组[openshift@master01 ~] oc adm groups add-users my-group dev-user # 为组my-group添加用户dev-user[openshift@master01 ~] oc adm groups remove-users my-group dev-user # 将用户dev-user从my-group组中移除
与OpenShift一样，K8S也是通过RBAC实现权限控制。RBAC(Role-Based Access Control)即为基于角色的访问控制。K8S中与RBAC相关的资源类型有：Role、ClusterRole、RoleBinding、ClusterRoleBinding。那么针对K8S集群，有没有办法实现类似于OpenShift的用户、组的管理呢？答案是肯定的，但是需要执行一系列的脚本操作。
本篇将介绍如何通过命令为Kubernetes创建用户及用户组，并对用户进行授权。同时设计了一个脚本工具来模拟实现类似OpenShift用户与组的功能。
K8S获取用户信息及对用户（组）授权获取当前用户名
[k8s@master01 ~] kubectl config view -o=jsonpath=&quot;&#123;.contexts[0].context.user&#125;&quot;dev-user
为用户赋予namespace的管理员权限
[k8s@master01 ~] kubectl create rolebinding dev-user-admin-binding --clusterrole=admin --user=dev-user --namespace=dev[k8s@master01 ~] kubectl create rolebinding dev-user-admin-binding --clusterrole=admin --user=dev-user -n sit
为组授权，赋予namespace管理员权限
[k8s@master01 ~] kubectl create rolebinding dev-group-admin-binding --clusterrole=admin --group=dev-group --namespace=dev[k8s@master01 ~] kubectl create rolebinding dev-group-admin-binding --clusterrole=admin --group=dev-user -n sit
生成用户及组证书
配置信息脚本 config.sh

#!/bin/bashUSERNAME=username  #用户名GROUP=/O=group          #用户所属组，多个组可以使用/O=group1/O=group2DEFAULT_NS=kube-system   # 默认nsAPI_SERVER=https://master.k8s.com:6443  #K8S API Server地址K8S_PKI_PATH=/etc/kubernetes/pki/    # K8S证书存放目录

为新用户创建证书脚本 add_user.sh

#!/bin/bashsource config.sh   # 导入配置信息K8S_PKI_PATH=$&#123;K8S_PKI_PATH%/&#125;mkdir -p .tmpopenssl genrsa -out .tmp/$USERNAME.key 2048 # 生成密钥openssl req -new -key .tmp/$USERNAME.key -out .tmp/$USERNAME.csr -subj &quot;/CN=$&#123;USERNAME&#125;$&#123;GROUP&#125;&quot; #生成csr证书，包含用户名及组信息openssl x509 -req -in .tmp/$USERNAME.csr -CA $&#123;K8S_PKI_PATH&#125;/ca.crt -CAkey $&#123;K8S_PKI_PATH&#125;/ca.key -CAcreateserial -out .tmp/$USERNAME.crt -days 3650 #生成crt证书# 使用k8s证书与生成的用户证书生成访问配置文件$USERNAME.confkubectl config --kubeconfig=$USERNAME.conf set-cluster kubernetes --server=$API_SERVER --certificate-authority=$&#123;K8S_PKI_PATH&#125;/ca.crt --embed-certskubectl config --kubeconfig=$USERNAME.conf set-credentials $USERNAME --client-certificate=.tmp/$USERNAME.crt --client-key=.tmp/$USERNAME.key --embed-certs=truekubectl config --kubeconfig=$USERNAME.conf set-context $USERNAME@kubernetes --cluster=kubernetes --user=$USERNAME --namespace=$DEFAULT_NSkubectl config --kubeconfig=$USERNAME.conf use-context $USERNAME@kubernetesecho -e &quot;请将下面的文件内容复制到需要的用户目录.kube/config文件中\n\n&quot;cat $USERNAME.confecho -e &quot;\n\n&quot;

在k8s的主节点运行add_user.sh，即可完成用户证书的生成。

用户、组权限与访问控制工具k8sum（K8S User Manager)设计该工具包含以下功能0. 功能说明

创建用户
为用户分配组信息
为用户或者组绑定role，设置权限
为用户或者组绑定clusterrole，设置权限

工具使用手册说明0. 功能说明
$ k8sum [-h/--help]k8sum是一个Kubernetes用户权限管理工具，它能够实现用户创建、分组、权限分配等功能。格式：k8sum [command] [--args]command:  create 创建访问配置文件  bind 为用户/组绑定权限

创建用户

$ k8sum create --user=user-dev

为用户分配组信息

$ k8sum create --user=user-dev --group=group1,goup2

为用户或者组绑定role

$ k8sum bind --user=user-dev --role=role1$ k8sum bind --group=group1 --role=role1

为用户或者组绑定clusterrole

$ k8sum bind --user=user-dev --clusterrole=clusterrole1$ k8sum bind --group=group1 --clusterrole=clusterrole1
总结RBAC设计来源于OpenShift，最后融入到了K8S。虽然都是基于RBAC，但是OpenShift在对用户权限的管理这一块考虑得更加全面，也更为实用。K8S真正要做好权限管理这块，还需要做更多的集成或二次开发的工作。
补充说明：OpenShift也好，Kubernetes也好，都可以对接第三方认证，如Keystone，LDAP，OpenID Connect等。OpenShift与OpenLDAP结接，可参考之前分享的文章：Openshift上部署OpenLDAP实战：为账号一统
参考文章为Kubernetes集群添加用户RBAC——基于角色的访问控制
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift管理节点Master与计算节点Node的配置说明</title>
    <url>/posts/471d3cbe64a8.html</url>
    <content><![CDATA[配置文件Master节点的配置文件地址：&#x2F;etc&#x2F;origin&#x2F;master&#x2F;master-config.yamlNode节点的配置文件在：openshift-node项目下的configmap中。OpenShift集群的配置信息都在这两个文件中，包括有覆盖默认插件，连接到etcd，自动创建服务帐户，构建映像名称，自定义项目请求，配置卷插件等等。
参考文章Master and Node Configuration
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift自带的日志搜索引擎ES服务的扩容</title>
    <url>/posts/8b9a311fc207.html</url>
    <content><![CDATA[
有以下两种扩容方式：

存储扩容
节点扩容

存储扩容
查看集群的状态

$ oc exec $es-pod-name -c elasticsearch -- health

停止ES节点分片平衡

$ oc exec $es-pod-name -c elasticsearch -- curl --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XPUT https://localhost:9200/_cluster/settings -d &#x27;&#123;&quot;transient&quot;: &#123;&quot;cluster.routing.allocation.enable&quot;:&quot;none&quot;&#125;&#125;&#x27;

为每个ES节点的DC执行以下操作3.1 获取需要暂停的ES pod所在的Node节点

$ oc get pods -o wide 
3.2 暂停第一个ES pod
$ oc scale dc &lt;es-dc-01&gt; --replicas=0
3.3 将节点的数据复制到更大的存储盘目录中3.4 更新该ES节点的DC配置，将数据盘挂载到新的目录3.5 恢复ES节点
$ oc scale dc &lt;es-dc-01&gt; --replicas=1

按照3的步骤为剩下的ES节点进行扩容
恢复ES节点分片平衡

$ oc exec $es-pod-name -c elasticsearch -- curl --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XPUT https://localhost:9200/_cluster/settings -d &#x27;&#123;&quot;transient&quot;: &#123;&quot;cluster.routing.allocation.enable&quot;:&quot;all&quot;&#125;&#125;&#x27;
节点扩容
添加infra-logging节点，配置保持与之前的logging节点一致，并准备好数据盘目录
停止ES节点分片平衡

$ oc exec $es-pod-name -c elasticsearch -- curl --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XPUT https://localhost:9200/_cluster/settings -d &#x27;&#123;&quot;transient&quot;: &#123;&quot;cluster.routing.allocation.enable&quot;:&quot;none&quot;&#125;&#125;&#x27;

修改部署配置文件ansible hosts文件，增大ES节点大小

openshift_logging_es_cluster_size=5openshift_logging_es_number_of_shards=3openshift_logging_es_number_of_replicas=1openshift_logging_elasticsearch_storage_type=hostmountopenshift_logging_elasticsearch_hostmount_path=/es-data

运行logging的部署安装playbook

$ ansible-playbook openshift-ansible/playbooks/openshift-logging/config.yml
5.恢复ES节点分片平衡
$ oc exec $es-pod-name -c elasticsearch -- curl --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XPUT https://localhost:9200/_cluster/settings -d &#x27;&#123;&quot;transient&quot;: &#123;&quot;cluster.routing.allocation.enable&quot;:&quot;all&quot;&#125;&#125;&#x27;
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift节点kubelet证书过期异常的解决步骤</title>
    <url>/posts/7a15051d8249.html</url>
    <content><![CDATA[
环境：OpenShift 3.10 or 3.11问题:

重新部署了新的CA，节点不再处于就绪状态。
如何手动强制创建新证书。
节点无法更新其证书，并出现以下错误：

atomic-openshift-node[3715]: I0313 11:40:48.864375    3715 bootstrap.go:56] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig fileatomic-openshift-node[3715]: I0313 11:40:48.865525    3715 bootstrap.go:86] No valid private key and/or certificate found, reusing existing private key or creating a new oneatomic-openshift-node[3715]: F0313 11:40:48.893737    3715 server.go:262] failed to run Kubelet: cannot create certificate signing request: Unauthorized
步骤：

为节点创建一个新的bootstrap.kubeconfig（主节点仅需要复制admin.kubeconfig）。 从任何Master节点运行：

# oc serviceaccounts create-kubeconfig node-bootstrapper -n openshift-infra --config /etc/origin/master/admin.kubeconfig &gt; ~/bootstrap.kubeconfig

在Master节点上将admin.kubeconfig文件复制到&#x2F;etc&#x2F;origin&#x2F;node&#x2F;bootstrap.kubeconfig。 在每个Master上运行：

# cp /etc/origin/master/admin.kubeconfig /etc/origin/node/bootstrap.kubeconfig

将第1步中的〜&#x2F; bootstrap.kubeconfig分发给计算节点(worker,infra)，替换计算节点的&#x2F;etc&#x2F;origin&#x2F;node&#x2F;bootstrap.kubeconfig文件。 还要将其分发到所有主节点（master）上的&#x2F;etc&#x2F;origin&#x2F;master&#x2F;bootstrap.kubeconfig中（注意，它是master文件夹，而不是node文件夹）。
在所有节点上移动node.kubeconfig 和 client-ca.crt文件

# mv /etc/origin/node/client-ca.crt&#123;,.old&#125;# mv /etc/origin/node/node.kubeconfig&#123;,.old&#125;

删除每个节点的&#x2F;etc&#x2F;origin&#x2F;node&#x2F;certificates&#x2F;目录（包括master，worker, infra）

# rm -rf  /etc/origin/node/certificates

重启node服务

# systemctl restart atomic-openshift-node.service 

批准CSR，每个节点（master，worker, infra）应批准2个：

# oc get csr -o name | xargs oc adm certificate approve
或者
# for i in $(oc get csr | grep -i Pending | awk &#x27;&#123; print $1 &#125;&#x27;); do oc adm certificate approve $i ; done

检查Node状态

# oc get node# for i in `oc get nodes -o jsonpath=$&#x27;&#123;range .items[*]&#125;&#123;.metadata.name&#125;\n&#123;end&#125;&#x27;`; do oc get --raw /api/v1/nodes/$i/proxy/healthz; echo -e &quot;\t$i&quot;; done

Manually recreate OpenShift Node TLS bootstrapped certificates and kubeconfig files.
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift解决Route-Https--Http无法跳转的问题</title>
    <url>/posts/8cbac354b58c.html</url>
    <content><![CDATA[
问题现象通过Route创建的edge类型的HTTPS，后端应用使用的是HTTP服务。但在某些情况下，后端应用使用Location时，Location地址为HTTP地址。这时浏览器将因为无法访问HTTP服务而无法实现跳转。
原理Haproxy在返回客户端时，检查返回Head，将Location地址中的HTTP替换为HTTPS，从而让浏览器获取正确的跳转链接。
解决办法更新haproxy-config.template，为edge请求的应用添加标注判断haproxy.router.openshift.io/location-scheme，如果它为https，则将Location请求中的http替换为https。具体配置如下：搜索 ssl_fc_alpn -i h2在其后添加如下配置
&#123;&#123;- if eq &quot;https&quot; (index $cfg.Annotaions &quot;haproxy.router.openshift.io/location-scheme&quot;)&#125;&#125;  acl check_location res.hdr(Location) -m found  rspirep ^Location:\ http://(.*) Location:\ https://\1 if check_location&#123;&#123;- end&#125;&#125;

示例
apiVersion: route.openshift.io/v1kind: Routemetadata:  annotations:    haproxy.router.openshift.io/location-scheme: &#x27;https&#x27;  labels:    router: web-app  name: web-appspec:  host: test.com  port:    targetPort: http  tls:    termination: edge  to:    kind: Service    name: web-appEOF
对于已经创建的Route可通过命令设置annotations
$ oc annotate route web-app haproxy.router.openshift.io/location-scheme=&quot;https&quot;
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift访问外部需要认证的镜像仓库</title>
    <url>/posts/e607688e15ca.html</url>
    <content><![CDATA[docker login登录成功后，会在~&#x2F;.docker&#x2F;中创建config.json文件，内容格式如下：
&#123;          &quot;auths&quot;: &#123;                  &quot;https://registry.example.com&quot;: &#123;                          &quot;auth&quot;: &quot;c2xmams6c2RmbGtq&quot;                  &#125;&#125;      
两种方式：

认证信息必须存放在OpenShift的NameSpace中的Secret下。

oc -n &lt;test-namespace&gt; create secret docker-registry &lt;pull-secret&gt; --docker-server=registry.example.com --docker-username=&lt;username&gt; --docker-email=test@example.com --docker-password=&lt;password&gt;oc -n &lt;test-namespace&gt; secrets link &lt;serviceaccount&gt; &lt;pull-secret&gt; --for=pull

在Master节点与Node节点的 /var/lib/origin/.docker/config.json文件中添加认证TOKEN

$ cp ~/.docker/config.json /var/lib/origin/.docker/config.json; systemctl restart atomic-openshift-node
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift运维点汇总</title>
    <url>/posts/3bc605c0336e.html</url>
    <content><![CDATA[应用
部署

镜像同步（UAT-&gt;PRO） 应用部署配置

 New Project&#x2F;New App环境变量cpu与内存资源限制健康检查复本数量创建router特殊Node节点绑定pod的亲和与互斥pod网速限速（特别是对外提供服务的pod）

应用配置中心(gitlab)


更新

镜像同步应用新版本部署


监控

Pod EFKRegistry与Router等重要服务的监控



物理层
创建资源

负载均衡器NAS存储应用私有网络


监控

vpc负载均衡器主机状态监控（CPU与内存）



集群层
备份

etcd全量备份应用配置备份(Ark)应用备份（oc export)


集群版本管理

集群升级（每个大版本升级）


日志归档与清理

EFK日志清理hawkular-cassandra日志清理journal日志归档
 journalctl --vacuum-time=3days或者journalctl --vacuum-size=200M或者在/etc/systemd/journald.conf中设置日志大小
 message日志归档


Node节点管理

ansible脚本扩容Node的隔离与恢复（关闭调度维护）


资源管理

资源配置范围管理（LimitRange）资源的配额管理PV（Persistent Volumes）的创建项目间的网络隔离


监控告警

Heapster + Influxdb + Grafana 集群监控统一的日志监控 EFKPod监控Restart数Node节点资源使用情况（Limit + Request + Real）


权限控制

管理员账号运维人员账号


Harbor私有镜像仓库的运维

镜像备份高可用可用性监控与告警



网络区域划分
核心区DMZ区互联网区

中间件层
Redis
RDB
Rabbitmq
FastDFS

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift通过EgressIP为Project设置对外的出口IP</title>
    <url>/posts/fa53403b8d6b.html</url>
    <content><![CDATA[集群管理员可以为项目分配特定的静态IP，方便外部系统能够识别项目下的应用的出口流量。

将出口IP分配到指定的Node

支持指定特定的IP
$ oc patch hostsubnet &lt;node-name&gt; -p &#x27;&#123;&quot;egressIPs&quot;: [&quot;192.168.1.2&quot;, &quot;192.168.1.3&quot;]&#125;&#x27;
支持指定特定的IP段
$ oc patch hostsubnet &lt;node-name&gt; -p &#x27;&#123;&quot;egressCIDRs&quot;: [&quot;192.168.1.0/24&quot;]&#125;&#x27;

为项目指定静态出口IP

$ oc patch netnamespace &lt;project-name&gt; -p &#x27;&#123;&quot;egressIPs&quot;: [&quot;192.168.1.100&quot;]&#125;&#x27;

OpenShift容器平台会自动以平衡的方式将特定的出口IP地址分配给可用节点。 在这种情况下，它将出口IP地址192.168.1.100分配给。

一个Project只能指定一个EgressIP
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift部署时如何延长组件证书的有效期</title>
    <url>/posts/1d3a207e5984.html</url>
    <content><![CDATA[
延长集群核心证书的有效期OpenShift集群正常运行中涉及到非常多的证书，有各节点通信的证书，有数据库的证书，有私有镜像仓库的证书，还有各种组件的证书（EFK、ClusterMonitor、Metrics等）。对于集群的相关证书可以通过在inventory文件中添加如下配置就可以非常方便地更改相关证书的默认有效期。
openshift_hosted_registry_cert_expire_days=3650openshift_ca_cert_expire_days=3650openshift_master_cert_expire_days=3650etcd_ca_default_days=3650
但是以上的几个配置对于一些组件并不起作用，如EFK等。
延长一些特殊组件证书的有效期openshift自签应用证书的有效期为2 or 5年。其中crt证书默认为两年，ca证书为5年。例如es中的证书中，admin-ca有效期为5年，admin-crt有效期为2年。
也就是说按照当前OpenShift安装的默认步骤安装的EFK组件，证书最短有效期为两年，两年后需要执行证书更新操作。而ES证书更新时，需要对ES应用进行重启，这将会影响到ES的可用性。那么如果我们希望在在安装部署EFK组件的时候，将证书设置为更长的时间的话，就不需要进行证书的更新，也就不会影响到ES的服务了。那么怎样做到这一点呢？
带着这个问题，仔细研究了OpenShift部署的脚本，发现要实现这点并不困难，只需要将生成证书的地方添加过期时间就ok了。
与相关证书有效期有关的地方有如下五类：
1. 使用create-signer-cert命令创建的证书以ES服务为例，admin-ca证书的有效期时间默认为5年，将roles/openshift_logging/tasks/generate_certs.yaml中的oc adm ca create-signer-cert命令添加过期时间--expire-days=36500，例如
&#123;&#123; openshift_client_binary &#125;&#125; adm --config=&#123;&#123; mktemp.stdout &#125;&#125;/admin.kubeconfig ca create-signer-cert    --key=&#123;&#123;generated_certs_dir&#125;&#125;/ca.key --cert=&#123;&#123;generated_certs_dir&#125;&#125;/ca.crt    --serial=&#123;&#123;generated_certs_dir&#125;&#125;/ca.serial.txt --name=logging-signer-test --expire-days=36500
2. 使用keytool创建的证书以ES服务为例，admin-crt有效期时间默认为2年。将roles/openshift_logging/files/generate-jks.sh脚本中的默认过期时间设置-validity设置为36500（一百年）即可，例如
keytool -genkey \        -alias     $NODE_NAME \        -keystore  $dir/$NODE_NAME.jks \        -keyalg    RSA \        -keysize   2048 \        -validity  36500\        -startdate &quot;$startdate&quot; \        -keypass $ks_pass \        -storepass $ks_pass \        -dname &quot;CN=$NODE_NAME, OU=OpenShift, O=Logging&quot;
3. 使用create-server-cert命令创建的证书oc adm ca create-server-cert命令创建的证书的过期时间为2年，所以对于这些命令生成的证书脚本也需要设置过期时间--expire-days=36500,例如metrics_server/tasks/generate_certs_and_apiservice.yaml文件中：
- name: generate new serving cert secrets if needed  when: existing_metrics_server_secret.rc != 0  block:  - name: generate ca certificate chain    command: &gt;      &#123;&#123; openshift_client_binary &#125;&#125; adm ca create-signer-cert      --config=&#123;&#123; mktemp.stdout &#125;&#125;/admin.kubeconfig      --key=&#x27;&#123;&#123; mktemp.stdout &#125;&#125;/ca.key&#x27;      --cert=&#x27;&#123;&#123; mktemp.stdout &#125;&#125;/ca.crt&#x27;      --serial=&#x27;&#123;&#123; mktemp.stdout &#125;&#125;/ca.serial.txt&#x27;      --name=&quot;metrics-signer@&#123;&#123;lookup(&#x27;pipe&#x27;,&#x27;date +%s&#x27;)&#125;&#125;&quot;      --expire-days=36500  - name: generate metrics-server keys    command: &gt;      &#123;&#123; openshift_client_binary &#125;&#125; adm ca create-server-cert      --config=&#123;&#123; mktemp.stdout &#125;&#125;/admin.kubeconfig      --key=&#x27;&#123;&#123; mktemp.stdout &#125;&#125;/metrics-server.key&#x27;      --cert=&#x27;&#123;&#123; mktemp.stdout &#125;&#125;/metrics-server.crt&#x27;      --hostnames=&#x27;metrics-server,metrics-server.&#123;&#123; openshift_metrics_server_project &#125;&#125;.svc,metrics-server.&#123;&#123; openshift_metrics_server_project &#125;&#125;.svc.cluster.local&#x27;      --signer-cert=&#x27;&#123;&#123; mktemp.stdout &#125;&#125;/ca.crt&#x27;      --signer-key=&#x27;&#123;&#123; mktemp.stdout &#125;&#125;/ca.key&#x27;      --signer-serial=&#x27;&#123;&#123; mktemp.stdout &#125;&#125;/ca.serial.txt&#x27;      --expire-days=36500
4. 使用oc adm ca create-master-certs创建的证书oc adm ca证书创建相关的命令还有create-master-certs，通过它能够创建master的证书，OpenShift已经提供了相关的配置ansible&#x2F;hosts中添加变量：openshift_master_cert_expire_days与openshift_ca_cert_expire_days来设置证书的过期时间，不需要更改playbook脚本与shell脚本。
5. 使用openssl创建的证书openssl req -out xx.csr -days 712这是脚本中创建证书默认指定为2年。可将-days 712改为新的有效期时间。例如roles/openshift_logging/tasks/generate_pems.yaml。另外openssl ca -in ...也需要添加-days 36500，也在该文件下
openssl ca -in &#123;&#123;generated_certs_dir&#125;&#125;/&#123;&#123;component&#125;&#125;.csr -notext -out &#123;&#123;generated_certs_dir&#125;&#125;/&#123;&#123;component&#125;&#125;.crt    -config &#123;&#123;generated_certs_dir&#125;&#125;/signing.conf -extensions v3_req -batch -extensions server_ext -days 36500
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift集群健康检查</title>
    <url>/posts/6f6257a9edfd.html</url>
    <content><![CDATA[参考文章https://docs.openshift.com/container-platform/3.11/day_two_guide&#x2F;environment_health_checks.html#day-two-guide-router-and-registry-health
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshfit-4-1部署手册</title>
    <url>/posts/13ee717138f0.html</url>
    <content><![CDATA[设备与网络准备
网络要求
能够访问OpenShift Infrastructure Providers以下载安装程序
能访问quay.io以获取安装群集所需的软件包
能访问Red Hat’s software as a service page以获取相关订阅


准备节点  1 bootstrap节点 RHCOS，引导计算机部署集群，在完成部署后该机器可被删除  3 master节点 RHCOS  2 compte节点 RHCOS or RHEL 7.6

部署前准备
外部负载均衡器
配置主机端口
配置DNS
确保网络连接


以下端口必须能被集群中的所有机器访问




端口
说明



2379-2380
etcd server, peer, and metrics ports


6443
Kubernetes API


9000-9999
Host level services, including the node exporter on ports 9100-9101 and the Cluster Version Operator on port 9099.


10249-10259
The default ports that Kubernetes reserves


10256
openshift-sdn


30000-32767
Kubernetes NodePort



提供四层负载均衡器




Port
Machines
Internal
External
Description



6443
Bootstrap and control plane. You remove the bootstrap machine from the load balancer after the bootstrap machine initializes the cluster control plane.
x
x
Kubernetes API server


22623
Bootstrap and control plane. You remove the bootstrap machine from the load balancer after the bootstrap machine initializes the cluster control plane.
x
Machine Config server



443
The machines that run the Ingress router pods, compute, or worker, by default.
x
x
HTTPS traffic


80
The machines that run the Ingress router pods, compute, or worker by default.
x
x
HTTP traffic


3. DNS必须满足以下条件









Component
Record
Description



Kubernetes API
api..
This DNS record must point to the load balancer for the control plane machines. This record must be resolvable by both clients external to the cluster and from all the nodes within the cluster.


Kubernetes API
api-int..
This DNS record must point to the load balancer for the control plane machines. This record must be resolvable from all the nodes within the cluster.The API server must be able to resolve the worker nodes by the host names that are recorded in Kubernetes. If it cannot resolve the node names, proxied API calls can fail, and you cannot retrieve logs from Pods.


Routes
*.apps..
A wildcard DNS record that points to the load balancer that targets the machines that run the Ingress router pods, which are the worker nodes by default. This record must be resolvable by both clients external to the cluster and from all the nodes within the cluster.


etcd
etcd-..
OpenShift Container Platform requires DNS records for each etcd instance to point to the control plane machines that host the instances. The etcd instances are differentiated by  values, which start with 0 and end with n-1, where n is the number of control plane machines in the cluster. The DNS record must resolve to an unicast IPV4 address for the control plane machine, and the records must be resolvable from all the nodes in the cluster.


etcd
_etcd-server-ssl._tcp..
For each control plane machine, OpenShift Container Platform also requires a SRV DNS record for etcd server on that machine with priority 0, weight 10 and port 2380. A cluster that uses three control plane machines requires the following records:


# _service._proto.name.                            TTL    class SRV priority weight port target._etcd-server-ssl._tcp.&lt;cluster_name&gt;.&lt;base_domain&gt;  86400 IN    SRV 0        10     2380 etcd-0.&lt;cluster_name&gt;.&lt;base_domain&gt;._etcd-server-ssl._tcp.&lt;cluster_name&gt;.&lt;base_domain&gt;  86400 IN    SRV 0        10     2380 etcd-1.&lt;cluster_name&gt;.&lt;base_domain&gt;._etcd-server-ssl._tcp.&lt;cluster_name&gt;.&lt;base_domain&gt;  86400 IN    SRV 0        10     2380 etcd-2.&lt;cluster_name&gt;.&lt;base_domain&gt;.

生成SSH私钥，并启动ssh-agent
生成SSH私钥

 $ ssh-keygen -t rsa -b 4096 -N &#x27;&#x27; \-f &lt;path&gt;/&lt;file_name&gt; 

后台运行ssh-agent

 $ eval &quot;$(ssh-agent -s)&quot; Agent pid 31874

将SSH私钥添加到ssh-agent

 $ ssh-add &lt;path&gt;/&lt;file_name&gt; Identity added: /home/&lt;you&gt;/&lt;path&gt;/&lt;file_name&gt; (&lt;computer_name&gt;)
下载安装程序
访问OpenShift Infrastructure Providers网页
下载对应操作系统的安装包
将安装包解压

 $ tar xvf &lt;installation_program&gt;.tar.gz  

在OpenShift Infrastructure Providers网页下载镜像的拉取密钥


安装Openshfit 客户端工具
手动创建安装配置文件

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshfit上用ConfigMap来自定义Nginx配置</title>
    <url>/posts/2412027f46e0.html</url>
    <content><![CDATA[Nginx是最常用的代理软件，也是最常用的WebServer，怎样很方便地在openshift上部署呢？同时又能很方便地对它自定义进行配置？直接使用docker的nginx会有以下问题：


启动时权限问题。默认openshift的应用会使用类似10000100的user_id启用docker，但是官方nginx镜像会使用root启用；还有nginx默认会启用80端口这个也是需要root权限的。
无法动态更新nginx配置。nginx配置在镜像中设置，如果需要更新配置的话，需要重新编译镜像。
应用代码更新。如果用户的代码，如静态代码，版本更新，是否可以不更新镜像，而完成版本的升级？


在Openshift上部署一个nginx应用如何解决以上3个问题。


权限问题


 解决办法: 制作专用nginx镜像，1. 更改特殊文件夹的权限为777；2.将默认端口设置为8080default.conf
server&#123;    listen  8080;    server_name _;    location /&#123;      root  /usr/share/nginx/html;      index index.html index.htm;    &#125;    error_page 500 502 503 504 /50x.html;    location = /50x.html &#123;      root  /usr/share/nginx/html;    &#125;&#125;

Dockerfile文件
#DockerfileFROM docker.io/nginx:1.14LABEL io.openshift.expose-services=&quot;8080:http&quot;COPY default.conf /etc/nginx/conf.d/default.confRUN chmod -R 777 /var/log/nginx /var/cache/nginx /var/run \    &amp;&amp; chgrp -R 0 /etc/nginx \    &amp;&amp; chmod -R g=u /etc/nginxEXPOSE 8080
制作镜像
docker build -t harbor.apps.example.com/public/nginx:1.14 .docker push harbor.apps.example.com/public/nginx:1.14
使用新的镜像部署应用(在nginx-project中创建DeploymentConfig nginx-demo)
oc new-app harbor.apps.example.com/public/nginx:1.14 --allow-missing-images --name=nginx-demo -n nginx-project
创建Service对应pod的8080端口
oc expose dc nginx-demo --port=8080
创建Route对应Service，使得服务能够对外提供服务
oc expose svc nginx-demo --hostname=www.web.example.com
本地绑定hostnamewww.web.example.com与Openshift集群的Router所在主机的ip，即可通过浏览器访问到服务。

2.自定义nginx配置

解决办法：使用ConfigMap创建nginx的配置，并挂载到/etc/nginx/conf.d目录
ConfigMap文件
apiVersion: v1kind: ConfigMapmetadata:  name: nginx-config  namespace: nginx-projectdata:  demo.conf: &quot;proxy_set_header Host $host;\r\nproxy_set_header X-Real-IP $remote_addr;\r\nproxy_set_header     X-Forwarded-Server $host;\r\nserver &#123;\r\n        listen       8080;\r\n        server_name  _;\r\n    \tlocation / &#123;\r\n        \tproxy_pass http://test.back.svc:28080/; \r\n    \t&#125;error_page   500 502 503 504  /50x.html;\r\n        location = /50x.html &#123;\r\n            root   html;\r\n        &#125;\r\n    &#125;&quot;
ConfigMap的内容即为nginx的conf.d目录下的所有conf配置文件。将它们挂载到&#x2F;etc&#x2F;nginx&#x2F;conf.d目录下。通过openshift的web console可以非常方便地操作如下图  
当然我们也可以更改DeployConfig的配置来实现同样的效果在nginx-demo的DeployConfig中添加挂载点及ConfigMap挂载我们可以把所有配置一次性挂载到目录下
...spec:      containers:        - image: &#x27;harbor.apps.example.com/public/nginx:1.14&#x27;          imagePullPolicy: IfNotPresent          name: nginx-demo          volumeMounts:            - mountPath: /etc/nginx/conf.d              name: nginx-config-hgj4i              readOnly: true      volumes:        - configMap:            defaultMode: 420            name: nginx-config          name: nginx-config-hgj4i...
同时我们也可以只挂载指定的文件，同时不覆盖同目录下的其他文件
...spec:      containers:        - image: &#x27;harbor.apps.example.com/public/nginx:1.14&#x27;          imagePullPolicy: IfNotPresent          name: nginx-demo          volumeMounts:            - mountPath: /etc/nginx/conf.d/test.conf              name: nginx-config-hgj4i              subPath: test.conf              readOnly: true      volumes:        - configMap:            defaultMode: 420            name: nginx-config            items:              - key: default.conf                path: test.conf          name: nginx-config-hgj4i...
对于不同的应用，我们需要不同的nginx配置时，只需要挂载不同的configmap即可。常见的应用场景为：将nginx作为代理服务器来使用的情况。

3.应用代码更新

解决办法：持久化存储Openshift是建立在Kubernetes的基础上的，而K8S支持十多种存储方式，如：NFS，GlusterFS，CephFS，RBD，HostPath等。这里使用NFS作为持久化存储方案。CentOS系统自带NFS服务。
systemctl start nfs #启动nfs，如果已启动，则不用操作mkdir -p /nfsdata/nginx_appchown nfsnobody:nfsnobody /nfsdata/nginx_appchmod 700 /nfsdata/nginx_app#开放nfs访问的端口iptables -A INPUT -p tcp --dport 111 -j ACCEPTiptables -A INPUT -p udp --dport 111 -j ACCEPTiptables -A INPUT -p tcp --dport 2049 -j ACCEPTiptables -A INPUT -p udp --dport 2049 -j ACCEPT# 配置NFSecho &quot;/nfsdata/nginx_app *(rw,async,no_root_squash)&quot; &gt;&gt; /etc/exportsexportfs -a #加载共享目录配置showmount -e #查看当前可用的共享目录
创建PV持久化存储
# nginx-pv.ymlkind: PersistentVolumeapiVersion: v1metadata:  name: nginx-pvspec:  capacity:    storage: &quot;5Gi&quot;  accessModes:    - &quot;ReadWriteMany&quot;  nfs:    path: &quot;/nfsdata/nginx_app&quot;    server: &quot;192.168.0.4&quot;
创建PV
oc create -f nginx-pv.yml
创建PVC
kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nginx-app-pvc  namespace: nginx-projectspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: &quot;5Gi&quot;
将创建的PVC挂载到DeploymentConfig中需要放的应用的目录下，同时注意该目录也应该是与ConfigMap中的server的root目录一致。
spec:  containers:    - image: &#x27;harbor.apps.example.com/public/nginx:1.14&#x27;      imagePullPolicy: IfNotPresent      name: nginx-demo      volumeMounts:        - mountPath: /etc/nginx/conf.d          name: nginx-config-hgj4i          readOnly: true        - mountPath: /appdata          name: appdata-pvc  volumes:    - name: nginx-config-hgj4i      configMap:        defaultMode: 420        name: nginx-config          - name: appdata-pvc      persistentVolumeClaim:        claimName: nginx-app-pvc
将对应的NFS挂载到开发或者部署的服务器上，同时将代码拷入共享存储，此时代码自动同步到的应用中。实现了代码同步。
综上，我们实现了使用同一个Nginx镜像，实现了在Openshift上自定义nginx配置及应用代码的部署。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift+Jenkins+zalenium+HtmlReporter实现自动化测试</title>
    <url>/posts/b3188e017b83.html</url>
    <content><![CDATA[上篇介绍了如果在openshift上部署zalenium及python对应用功能进行测试的方法。该篇介绍如何通过Jenkins的Slave容器配合上篇中的zalenium实现自动化功能测试，并生成测试报告。虽然只是一个小例子，但是麻雀虽小，五脏俱全。同时所有的操作及工具都建立在Openshift上。下图为各工具之间的关系：  
###具体操作：


启动Jenkins需要添加环境变量JENKINS_JAVA_OVERRIDES &#x3D;&gt; -Dhudson.model.DirectoryBrowserSupport.CSP=该环境变量使得HtmlReporter页面展示正常



Jenkins系统管理-&gt;系统设置中创建新的Kubernetes Pod Template基础镜像jenkins-slave-python-centos7:3.10，Dockerfile安装需要的python依赖包

 # DockerfileFROM informaticsmatters/jenkins-slave-python-centos7RUN pip install selenium -i https://pypi.douban.com/simple/

  


代码中添加自动化测试脚本



# -*- coding: utf-8 -*-import unittestfrom selenium import webdriverfrom selenium.webdriver.remote.remote_connection import RemoteConnectionfrom HTMLTestRunner import HTMLTestRunnerclass SeleniumTestCase(unittest.TestCase):    def setUp(self):        remoteconnection = RemoteConnection(&#x27;http://zalenium.example.com/wd/hub&#x27;,                                            keep_alive=False,                                            resolve_ip=False)        self.driver = webdriver.Remote(command_executor=remoteconnection,                                       desired_capabilities=&#123;                                           &#x27;browserName&#x27;: &quot;chrome&quot;,                                           &#x27;video&#x27;: &#x27;True&#x27;,                                           &#x27;platform&#x27;: &#x27;LINUX&#x27;,                                           &#x27;platformName&#x27;: &#x27;LINUX&#x27;                                       &#125;)        self.driver.implicitly_wait(30)        self.driver.maximize_window()    def test_login_test_case(self):        self.driver.get(&quot;https://devpf.example.com&quot;)        username_input = self.driver.find_element_by_id(&#x27;username&#x27;)        password_input = self.driver.find_element_by_id(&#x27;password&#x27;)        login_button = self.driver.find_element_by_id(&#x27;login_btn&#x27;)        username_input.clear()        username_input.send_keys(&#x27;panxiaohua&#x27;)        password_input.clear()        password_input.send_keys(&#x27;12345678&#x27;)        login_button.click()        assert not None is self.driver.find_element_by_id(&#x27;content&#x27;), &#x27;Error Happends&#x27;    def tearDown(self):        self.driver.quit()if __name__ == &#x27;__main__&#x27;:    suite = unittest.TestSuite()    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(SeleniumTestCase))    with open(&#x27;report.html&#x27;, &#x27;w&#x27;) as f:        runner = HTMLTestRunner(stream=f, title=&#x27;Test Report&#x27;, verbosity=2)        runner.run(suite)


Jenkins Job中完成部署后，执行测试脚本



cd testspython test_urls.py
  


导出自动测试报表



  


最后结果展示



  
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-3scale简单介绍</title>
    <url>/posts/3e4333ef6f0f.html</url>
    <content><![CDATA[API经济3scale做了什么？关注指标
支持应用数据
完整用例的数量
用户数量
资金价值
开发速度
部署速度
迁移工作量
已有问题

完整的API管理不仅仅只是创建与发布，还需要配套全方位的运维
创建与发布API
度量和计费
安全 &amp; 认证
API文档Portal
扩展性 &amp; 集中策略
API监控
版本控制
生命周期
配置 &amp; 告警
API测试

3scale架构（可容器化部署）
实验内容github.com&#x2F;hgueere
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-All-In-One一键部署工具上线</title>
    <url>/posts/006740db2f5e.html</url>
    <content><![CDATA[Openshift All In One工具制作的初衷：
工作中为了测试各种情况，比如备份恢复等，经常需要部署一套全新的Openshift环境，虽然镜像与安装包都是现成，但是部署过程中还是很容易出错，毕竟还是有些复杂的。竟然部署过程是一致的，那就干脆用脚本化，一键到位得了。
Openshift3.9部署手册，这是我之前整理的一篇单机部署Openshift 3.9的手册，有一些朋友看了后，按照上面的操作还是会遇到一些问题，毕竟步骤有那么多，差了一步，就很容易就会失败。有了这个脚本工具后，想要部署测试Openshift的朋友就很容易去部署测试了，而不是在部署这一步就放弃了。
有些厂商提供了Openshift的解决方案，但是做支持的厂商朋友并不太熟悉容器平台环境，也想在自己的环境下部署一套Openshift来测试，最后往往困难重重，从展望到放弃。

做个自动化工具，让所有关心Openshift&#x2F;K8s的朋友就可以跳过部署的步骤，快速进入到Paas这个神奇的世界，去了解到自己真正关心的内容。
进入正题
什么是正题？直接上工具git地址OpenshiftOneClick：https://gitee.com/xhua/OpenshiftOneClick

开发工具： Ansible + Shell


安装步骤
准备一台主机&#x2F;虚拟机(CentOS 7.4以上, CPU &gt; 2core, Memory &gt; 4G)

将一键部署脚本拷贝到主机上

cd到openshift-oneclick-allinone目录


  cd openshift-oneclick-allinone

运行部署(必须切到root账号)

sudo su/bin/bash deploy_openshift.sh

本地绑定hosts

# HOSTNAME 默认为os39.test.it.example.com&lt;ip&gt; os39.test.it.example.com

浏览器访问

# HOSTNAME 默认为os39.test.it.example.comhttps://os39.test.it.example.com:8443
默认账号：admin默认密码：admin
结果展示
用本地的Vagrant创建的Centos 7.4的虚拟机（VirtualBox）
配置2核4G


工具说明
该工具具有一定的可配置性

查看config.yml文件
CHANGEREPO: trueHOSTNAME: os39.test.it.example.comChange_Base_Registry: falseHarbor_Url: harbor.apps.it.example.comFULL_INSTALL: false
   说明:

CHANGEREPO: 是否使用files&#x2F;all.repo替换系统默认repo源
HOSTNAME：安装Openshift的主机的hostname，也是集群的访问域名
Change_Base_Registry：是否使用私有镜像仓库
Harbor_Url：私有镜像仓库地址，Change_Base_Registry为True时有效
FULL_INSTALL：是否全量安装（包括日志，监控等）如果选择全量安装(FULL_INSTALL&#x3D;true)，请保证主机内存有16G.


如果执行过程中出现异常，不用怕。工具是基于ansible的，幂等性。

重新运行deploy_openshift.sh就OK了。
sudo su/bin/bash deploy_openshift.sh


有什么疑问和改进的想法，欢迎留言。

# 报错 ERROR! | failed expects hostvars is a dictIn /etc/ansible/playbooks/openshift-ansible/roles/openshift_master_certificates/tasks/main.yml around line 63 I simply changed the with_items clause from- &quot;&#123;&#123; hostvars[inventory_hostname] | certificates_to_synchronize &#125;&#125;&quot;to- &quot;&#123;&#123; hostvars[inventory_hostname][&#x27;ansible_facts&#x27;] | certificates_to_synchronize &#125;&#125;&quot;and everything progressed.
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-Build构建详解</title>
    <url>/posts/420369a1c884.html</url>
    <content><![CDATA[Build是什么Build是Openshift容器平台上将输入转化为输出的过程，通常情况下是将代码转化为镜像的过程。
Build的配置叫BuildConfig，它将定义构建策略及构建参数。设置的构建策略和参数确定了Build的构建过程。
Openshift构建有四种主要策略

Docker 构建
源代码构建（S2i)
自定义构建
Pipeline构建可以实现复杂的工作流

Docker构建、S2i构建、Pipeline构建是默认支持的。
不同的构建有六种源

Git
Dockerfile
Binary
Image
Input secrets
External artifacts

其中Binary和Git不能同时使用。 Dockerfile和Image可以单独使用，也可以与Git或Binary一起使用。 如果使用Binary作为BuildConfig.spec.source.type，只能通过命令行工具oc start-build来构建。
BuildConfig实例
kind: &quot;BuildConfig&quot;apiVersion: &quot;v1&quot;metadata:  name: &quot;ruby-sample-build&quot; spec:  runPolicy: &quot;Serial&quot;   triggers:     -      type: &quot;GitHub&quot;      github:        secret: &quot;secret101&quot;    - type: &quot;Generic&quot;      generic:        secret: &quot;secret101&quot;    -      type: &quot;ImageChange&quot;  source:     git:      uri: &quot;https://github.com/openshift/ruby-hello-world&quot;  strategy:     sourceStrategy:      from:        kind: &quot;ImageStreamTag&quot;        name: &quot;ruby-20-centos7:latest&quot;  output:     to:      kind: &quot;ImageStreamTag&quot;      name: &quot;origin-ruby-sample:latest&quot;  postCommit:       script: &quot;bundle exec rake test&quot;




属性
说明



metadata.name
BuildConfig名


spec.runPolicy
运行策略，默认是顺序运行


spec.triggers
触发器，这个可以触发新的构建


spec.source
定义了构建的源。 源类型确定输入的主要来源，可以是Git，指向代码存储库位置，Dockerfile，从内联Dockerfile构建，或Binary，以接受二进制有效负载。 可以同时拥有多个源，有关详细信息，请参阅每种源类型的文档。


spec.strategy
策略部分描述了用于执行构建的构建策略。 您可以在此处指定Source，Docker或Custom策略。


spec.output
成功构建容器映像后，它将被推送到输出部分中描述的存储库中。


spec.postCommit
postCommit部分定义了一个可选的构建钩子。


postCommit支持多种不同方式的配置
script: 默认会使用&#x2F;bin&#x2F;sh -ic来运行内容
command: 如果镜像没有&#x2F;bin&#x2F;sh，则使用command
args: 给Entrypoint传递参数，镜像必须支持传入参数
例子：
postCommit:  script: &quot;bundle exec rake test --verbose&quot;  postCommit:  command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;bundle exec rake test --verbose&quot;]  postCommit:  args: [&quot;bundle&quot;, &quot;exec&quot;, &quot;rake&quot;, &quot;test&quot;, &quot;--verbose&quot;]postCommit:  script: &quot;bundle exec rake test $1&quot;  args: [&quot;--verbose&quot;]  postCommit:  command: [&quot;bundle&quot;, &quot;exec&quot;, &quot;rake&quot;, &quot;test&quot;]  args: [&quot;--verbose&quot;]

通过命令行设置
$ oc set build-hook bc/mybc \    --post-commit \    --command \    -- bundle exec rake test --verbose$ oc set build-hook bc/mybc --post-commit --script=&quot;bundle exec rake test --verbose&quot;

基础的构建操作启动一个Build
$ oc start-build &lt;buildconfig_name&gt;$ #使用--from-build启动$ oc start-build --from-build=&lt;build_name&gt;$ #添加--follow参数，在终端输出日志$ oc start-build &lt;buildconfig_name&gt; --follow$ #给构建传入环境变量$ oc start-build &lt;buildconfig_name&gt; --env=&lt;key&gt;=&lt;value&gt;

您可以通过直接推送源代码来启动构建，而不是依赖于Git源代码拉取或Dockerfile构建构建，这可能是Git或SVN工作目录的内容，也就是您想要的一组预构建二进制工件。 部署或单个文件。 这可以通过为start-build命令指定以下选项之一来完成：



参数
说明



–from-dir&#x3D;
指定将归档并用作构建的二进制输入的目录。


–from-file&#x3D;
指定将成为构建源中唯一文件的单个文件。 该文件放在空目录的根目录中，其文件名与提供的原始文件相同。


–from-repo&#x3D;
指定本地存储库的路径，以用作构建的二进制输入。 添加–commit选项以控制用于构建的分支，标记或提交。


 将任何这些选项直接传递给构建时，内容将流式传输到构建并覆盖当前构建源设置。
$ # 将本地Git仓库的v2分支/tag的内容作为存档并启动构建$ oc start-build hello-world --from-repo=../hello-world --commit=v2

取消一个构建
$ oc cancel-build &lt;build_name&gt;$ #取消多个构建$ oc cancel-build &lt;build1_name&gt; &lt;build2_name&gt; &lt;build3_name&gt;$ #取消通过buildconfig构建的所有构建$ oc cancel-build bc/&lt;buildconfig_name&gt;$ #取消指定状态的构建$ oc cancel-build bc/&lt;buildconfig_name&gt;  --state=&lt;state&gt;

删除构建
$ #删除BuildConfig，同时删除之前的Build$ oc delete bc &lt;BuildConfigName&gt;$ #删除BuildConfig，但保留之前的Build$ oc delete --cascade=false bc &lt;BuildConfigName&gt;

查看Build的详情
$ oc describe build &lt;build_name&gt;

查看Build的日志
$ oc logs -f build/&lt;build_name&gt;$ #查看最新的一个Build的日志$ oc logs -f bc/&lt;buildconfig_name&gt;$ #查看指定版本的Build日志$ oc logs --version=&lt;number&gt; bc/&lt;buildconfig_name&gt;

设置Build的日志级别
要启用更详细的输出，请在BuildConfig中将BUILD_LOGLEVEL环境变量作为sourceStrategy或dockerStrategy的一部分传递：
sourceStrategy:...  env:    - name: &quot;BUILD_LOGLEVEL&quot;      value: &quot;2&quot; 

Build的输入构建输入需要提供输入源

内联Dockerfile定义
从现有图像中提取的内容
Git仓库
本地输入
输入Secret和ConfigMaps
额外的文件

不同的输入可以组合成一个构建。 由于内联Dockerfile优先，它可以覆盖另一个输入提供的名为Dockerfile的任何其他文件。 二进制（本地）输入和Git仓库是不能同时使用。
当您不希望构建期间使用的某些资源或凭据在构建生成的最终应用程序映像中可用时，或者想要使用在Secret资源中定义的值时，输入机密很有用。 外部工件可用于提取其他文件，这些文件不可用作其他构建输入类型之一。

构造工作目录，并将所有输入内容放在工作目录中。 例如，输入Git存储库被克隆到工作目录中，并且使用目标路径将从输入镜像指定的文件复制到工作目录中。

构建过程将目录更改为contextDir（如果已定义）。

内联Dockerfile（如果有）将写入当前目录。

来自当前目录的内容将提供给构建过程，以供Dockerfile。 这意味着构建将忽略驻留在contextDir之外的任何输入内容。


以下例子中设置了多个输入源
source:  git:    uri: https://github.com/openshift/ruby-hello-world.git   images:  - from:      kind: ImageStreamTag      name: myinputimage:latest      namespace: mynamespace    paths:    - destinationDir: app/dir/injected/dir       sourcePath: /usr/lib/somefile.jar  contextDir: &quot;app/dir&quot;   dockerfile: &quot;FROM centos:7\nRUN yum install -y httpd&quot; 


git仓库将会被clone到构建的工作目录中。
myinputimage:latest镜像中的&#x2F;usr&#x2F;lib&#x2F;somefile.jar文件，将会被拷贝到工作目录中的app&#x2F;dir&#x2F;injected&#x2F;dir文件夹下
构建的工作目录将变为原工作目录下的&#x2F;app&#x2F;dir
具有此内容的Dockerfile将在 &#x2F; app &#x2F; dir中创建，覆盖具有该名称的任何现有文件。

Dockerfile源当提供dockerfile值时，该字段的内容将作为名为Dockerfile的文件写入磁盘。 这是在处理完其他输入源之后完成的，因此如果输入源存储库在根目录中包含Dockerfile，则它将被此内容覆盖。
该字段的典型用途是为Docker策略提供Dockerfile
该字段是定义在BuildConfig中的spec下
source:  dockerfile: &quot;FROM centos:7\nRUN yum install -y httpd&quot; 

镜像源可以通过图像向构建过程提供其他文件。 输入图像的引用方式与定义From和To图像目标的方式相同。 这意味着可以引用容器图像和图像流标记。 与映像一起使用时，必须提供一个或多个路径对，以指示要复制映像的文件或目录的路径以及将它们放置在构建上下文中的目标。
源路径可以是指定图像中的任何绝对路径。 目标必须是相对目录路径。 在构建时，将加载映像，并将指示的文件和目录复制到构建过程的上下文目录中。 这是克隆源存储库内容（如果有）的目录。 如果源路径以&#x2F;结尾。 然后将复制目录的内容，但不会在目标上创建目录本身。
该字段是定义在BuildConfig中的spec下
source:  git:    uri: https://github.com/openshift/ruby-hello-world.git  images:   - from:       kind: ImageStreamTag      name: myinputimage:latest      namespace: mynamespace    paths:     - destinationDir: injected/dir       sourcePath: /usr/lib/somefile.jar   - from:      kind: ImageStreamTag      name: myotherinputimage:latest      namespace: myothernamespace    pullSecret: mysecret     paths:    - destinationDir: injected/dir      sourcePath: /usr/lib/somefile.jar

Git源设置了Git源后，将从指定的Git源clone下代码
如果设置了dockerfile源，那么它将会覆盖掉代码中目录下的Dockerfile文件
git源字段也是BuildConfig中的spec下
source:  git:     uri: &quot;https://github.com/openshift/ruby-hello-world&quot;    ref: &quot;master&quot;  contextDir: &quot;app/dir&quot;   dockerfile: &quot;FROM openshift/ruby-22-centos7\nUSER example&quot; 

Git源设置中还可以指定代码
source:  git:    uri: &quot;https://github.com/openshift/ruby-hello-world&quot;    httpProxy: http://proxy.example.com    httpsProxy: https://proxy.example.com    noProxy: somedomain.com, otherdomain.com

Git源设置下载的用户及密码
Builder pods需要访问定义为构建源的任何Git存储库。 源克隆密钥用于为构建器窗格提供通常无法访问的访问权限，例如具有自签名或不可信SSL证书的私有存储库或存储库。

.gitconfig配置文件
Basic Authentication
SSH Key Authentication
Trusted Certificate Authorities

使用builder服务帐户运行构建，该帐户必须能够访问所使用的任何源克隆密钥。 使用以下命令授予访问权限：
$ oc secrets link builder mysecret

说明：默认情况下是不需要主动绑定builder与secret的，除非在masterconfig中的serviceAccountConfig.limitSecretReferences设置为true。
自动将源克隆密码添加到构建配置中
apiVersion: &quot;v1&quot;kind: &quot;BuildConfig&quot;metadata:  name: &quot;sample-build&quot;spec:  output:    to:      kind: &quot;ImageStreamTag&quot;      name: &quot;sample-image:latest&quot;  source:    git:      uri: &quot;https://github.com/user/app.git&quot;    sourceSecret:      name: &quot;basicsecret&quot;  strategy:    sourceStrategy:      from:        kind: &quot;ImageStreamTag&quot;        name: &quot;python-33-centos7:latest&quot;

secret构建
$ oc create secret generic &lt;secret_name&gt; \    --from-literal=username=&lt;user_name&gt; \    --from-literal=password=&lt;password&gt; \    --type=kubernetes.io/basic-auth$ oc create secret generic &lt;secret_name&gt; \    --from-literal=password=&lt;token&gt; \    --type=kubernetes.io/basic-auth    $ oc create secret generic &lt;secret_name&gt; \    --from-file=ssh-privatekey=&lt;path/to/ssh/private/key&gt; \    --type=kubernetes.io/ssh-auth

本地源从本地文件系统到构建器的流内容称为二进制类型构建。 对于此类构建，BuildConfig.spec.source.type的对应值为Binary。
此源类型的独特之处在于它仅基于您对oc start-build的使用而被利用。
在star-build构建时需要指定以下参数之一



参数
说明



–from-dir&#x3D;
指定将归档并用作构建的二进制输入的目录。


–from-file&#x3D;
指定将成为构建源中唯一文件的单个文件。 该文件放在空目录的根目录中，其文件名与提供的原始文件相同。


–from-repo&#x3D;
指定本地存储库的路径，以用作构建的二进制输入。 添加–commit选项以控制用于构建的分支，标记或提交。


–from-archive
指定的压缩包文件将发送到构建器，在构建器上下文目录中将其解压缩。 此选项与–from-dir的行为相同; 只要这些选项的参数是目录，就会首先在主机上创建压缩文件。



如果您的BuildConfig已经定义为binary类型，那它将会被客户端发送的内容给替代。

如果您的BuildConfig定义了Git源类型，则会动态禁用Git源类型，因为Binary和Git是互斥的，并且提供给构建器的二进制流中的数据优先。


您可以将带有HTTP或HTTPS架构的URL传递给–from-file和–from-archive，而不是文件名。 将-from-file与URL一起使用时，构建器映像中文件的名称由Web服务器发送的Content-Disposition标头确定，如果标头不存在，则由URL路径的最后一个组件确定。 不支持任何形式的身份验证，也无法使用自定义TLS证书或禁用证书验证。
使用oc new-build --binary = true时，该命令可确保强制执行与二进制构建关联的限制。 生成的BuildConfig将具有Binary的源类型，这意味着为此BuildConfig运行构建的唯一有效方法是使用oc start-build和其中一个–from选项来提供必需的二进制数据。
dockerfile和contextDir源选项对二进制构建具有特殊含义。
dockerfile可以与任何二进制构建源一起使用。 如果使用dockerfile且二进制流是存档，则其内容将作为存档中任何Dockerfile的替换Dockerfile。 如果dockerfile与–from-file参数一起使用，并且file参数名为dockerfile，则dockerfile中的值将替换二进制流中的值。
 在二进制流封装提取的存档内容的情况下，contextDir字段的值被解释为存档中的子目录，并且如果有效，则构建器在执行构建之前更改为该子目录。
本地源构建的一个实例：

创建本地文件夹，存放代码
$ mkdir myapp$ cd myapp

在目录下创建一个Dockerfile文件
FROM centos:centos7EXPOSE 8080COPY index.html /var/run/web/index.htmlCMD cd /var/run/web &amp;&amp; python -m SimpleHTTPServer 8080

在目录下添加一个index.html文件
&lt;html&gt;  &lt;head&gt;    &lt;title&gt;My local app&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;    &lt;h1&gt;Hello World&lt;/h1&gt;    &lt;p&gt;This is my local application&lt;/p&gt;  &lt;/body&gt;&lt;/html&gt;

创建一个buildConfig
$ oc new-build --strategy docker --binary --docker-image centos:centos7 --name myapp

开始一个构建，使用当前文件夹
$ oc start-build myapp --from-dir . --follow

根据生成的镜像部署应用
$ oc new-app myapp$ oc expose svc/myapp

查看当前应用
$ oc get route myapp

https://docs.openshift.com/dedicated/dev_guide/dev_tutorials/binary_builds.html#binary-builds-local-code-changes
导入Secret与ConfigMap 在某些情况下，构建操作需要凭据或其他配置数据来访问相关资源，但不希望将该信息放置在源代码管理中。 您可以为此目的定义输入密钥并输入ConfigMaps。
例如：
$ #创建configmap$ oc create configmap settings-mvn --from-file=settings.xml=&lt;path/to/settings.xml&gt;$ # 创建Secret$ oc create secret generic secret-mvn --from-file=id_rsa=&lt;path/to/.ssh/id_rsa&gt;
将ConfigMap与Secret加入到构建目录
source:  git:    uri: https://github.com/wildfly/quickstart.git  contextDir: helloworld  configMaps:    - configMap:        name: settings-mvn  secrets:    - secret:        name: secret-mvn

要在新的构建中添加
$ oc new-build \    openshift/wildfly-101-centos7~https://github.com/wildfly/quickstart.git \    --context-dir helloworld --build-secret “secret-mvn” \    --build-config-map &quot;settings-mvn&quot;
指定文件保存的工作目录下的子目录
source:  git:    uri: https://github.com/wildfly/quickstart.git  contextDir: helloworld  configMaps:    - configMap:        name: settings-mvn      destinationDir: &quot;.m2&quot;  secrets:    - secret:        name: secret-mvn      destinationDir: &quot;.ssh&quot;

在新建构建时指定
$ oc new-build \    openshift/wildfly-101-centos7~https://github.com/wildfly/quickstart.git \    --context-dir helloworld --build-secret “secret-mvn:.ssh” \    --build-config-map &quot;settings-mvn:.m2&quot;

setting.xml将会被放在.&#x2F;.m2目录下
id_rsa文件将会放在.&#x2F;.ssh目录下
Build输出Build构建好的镜像输出可以是ImageStreamTag，也可以是外部的镜像仓库
spec:  output:    to:      kind: &quot;ImageStreamTag&quot;      name: &quot;sample-image:latest&quot;

或者
spec:  output:    to:      kind: &quot;DockerImage&quot;      name: &quot;my-registry.mycompany.com:5000/myimages/myimage:tag&quot;

Docker和Source策略，在构建好的镜像中会写入以下环境变量



名称
说明



OPENSHIFT_BUILD_NAME
构建名


OPENSHIFT_BUILD_NAMESPACE
构建的namespace


OPENSHIFT_BUILD_SOURCE
构建的代码地址


OPENSHIFT_BUILD_REFERENCE
构建的代码所在的分支


OPENSHIFT_BUILD_COMMIT
构建的代码的Commit


输出的镜像ImageStreamTag有以下label



名称
说明



io.openshift.build.commit.author
代码提交的作者


io.openshift.build.commit.date
代码提交的时间


io.openshift.build.commit.id
代码提交的Commit ID


io.openshift.build.commit.message
代码提交的Message


io.openshift.build.commit.ref
代码构建所在的分支


io.openshift.build.source-location
构建的地址


也可以通过设置imageLabels给镜像添加新的lable
spec:  output:    to:      kind: &quot;ImageStreamTag&quot;      name: &quot;my-image:latest&quot;    imageLabels:    - name: &quot;vendor&quot;      value: &quot;MyCompany&quot;    - name: &quot;authoritative-source-url&quot;      value: &quot;registry.mycompany.com&quot;

镜像输出到外部的镜像仓库，需要添加外部镜像仓库的权限
您可以使用.docker &#x2F; config.json文件为私有容器注册表提供有效凭据。 这允许您将输出图像推送到私有容器映像注册表，或从需要身份验证的私有容器映像注册表中提取构建器映像。
默认情况下，.docker&#x2F;config.json文件位于主目录中，格式如下：
auths:  https://index.docker.io/v1/:     auth: &quot;YWRfbGzhcGU6R2labnRib21ifTE=&quot;     email: &quot;user@example.com&quot; 

在Openshift平台上，可以将它保存在Secret中，供build时调用

根据本地的.docker&#x2F;config.json文件创建Secret
$ oc create secret generic dockerhub \    --from-file=.dockerconfigjson=&lt;path/to/.docker/config.json&gt; \    --type=kubernetes.io/dockerconfigjson

将该secret授权给builder，一般这一步不需要，默认是已经授过权的
$ oc secrets link builder dockerhub

将pushSecret字段添加到BuildConfig的输出部分，并将其设置为您创建的秘密的名称，在上面的示例中为dockerhub：
spec:  output:    to:      kind: &quot;DockerImage&quot;      name: &quot;private.registry.com/org/private-image:latest&quot;    pushSecret:      name: &quot;dockerhub&quot;

或者使用oc命令
$ oc set build-secret --push bc/sample-build dockerhub

通过指定pullSecret字段从私有容器映像注册表中提取构建器容器映像，该字段是构建策略定义的一部分：
strategy:  sourceStrategy:    from:      kind: &quot;DockerImage&quot;      name: &quot;docker.io/user/private_repository&quot;    pullSecret:      name: &quot;dockerhub&quot;

或者使用oc命令
$ oc set build-secret --pull bc/sample-build dockerhub

Build策略s2i策略强制更新镜像
默认情况下，通过ImageStreamTag构建时，如果构建容器所在的主机上已存在镜像，就会直接使用该镜像，除非添加forcePull:true属性
strategy:  sourceStrategy:    from:      kind: &quot;ImageStreamTag&quot;      name: &quot;builder-image:latest&quot;     forcePull: true 

增量构建
S2I可以执行增量构建，这意味着它可以重用先前构建的图像中的工件。 要创建增量构建，请创建一个BuildConfig，并对策略定义进行以下修改
strategy:  sourceStrategy:    from:      kind: &quot;ImageStreamTag&quot;      name: &quot;incremental-image:latest&quot;     incremental: true 

替换默认的构建脚本
strategy:  sourceStrategy:    from:      kind: &quot;ImageStreamTag&quot;      name: &quot;builder-image:latest&quot;    scripts: &quot;http://somehost.com/scripts_directory&quot; 

Docker策略 Dockerfile的FROM指令将替换为BuildConfig的from：
strategy:  dockerStrategy:    from:      kind: &quot;ImageStreamTag&quot;      name: &quot;debian:latest&quot;

默认情况下，Docker构建使用位于BuildConfig.spec.source.contextDir字段中指定的上下文根的Dockerfile（名为Dockerfile）。
dockerfilePath字段允许构建使用不同的路径来定位Dockerfile，相对于BuildConfig.spec.source.contextDir字段。 它可以是除默认Dockerfile之外的其他文件名（例如，MyDockerfile），或者是子目录中Dockerfile的路径（例如，dockerfiles &#x2F; app1 &#x2F; Dockerfile）
strategy:  dockerStrategy:    dockerfilePath: dockerfiles/app1/Dockerfile

Docker构建通常重用在执行构建的主机上找到的缓存层。 将noCache选项设置为true会强制构建忽略缓存层并重新运行Dockerfile的所有步骤：
strategy:  dockerStrategy:    noCache: true

默认情况下，通过ImageStreamTag构建时，如果构建容器所在的主机上已存在镜像，就会直接使用该镜像，除非添加forcePull:true属性
strategy:  dockerStrategy:    from:      kind: &quot;ImageStreamTag&quot;      name: &quot;debian:latest&quot;     forcePull: true 

构建添加环境变量
dockerStrategy:...  env:    - name: &quot;HTTP_PROXY&quot;      value: &quot;http://myproxy.net:5187/&quot;

构建参数
dockerStrategy:...  buildArgs:    - name: &quot;foo&quot;      value: &quot;bar&quot;



自定义策略根据镜像构建
strategy:  customStrategy:    from:      kind: &quot;DockerImage&quot;      name: &quot;openshift/sti-image-builder&quot;

为了允许从容器内部运行Docker命令和构建容器映像，必须将构建容器绑定到可访问的套接字。 为此，请将exposeDockerSocket选项设置为true：
strategy:  customStrategy:    exposeDockerSocket: true

强制更新镜像
strategy:  customStrategy:    forcePull: true 

构建添加环境变量
customStrategy:...  env:    - name: &quot;HTTP_PROXY&quot;      value: &quot;http://myproxy.net:5187/&quot;

Pipeline流水线策略通过两种方式提供jenkinsfile文件

在BuildConfig配置文件中写
kind: &quot;BuildConfig&quot;apiVersion: &quot;v1&quot;metadata:  name: &quot;sample-pipeline&quot;spec:  strategy:    jenkinsPipelineStrategy:      jenkinsfile: |-        node(&#x27;agent&#x27;) &#123;          stage &#x27;build&#x27;          openshiftBuild(buildConfig: &#x27;ruby-sample-build&#x27;, showBuildLogs: &#x27;true&#x27;)          stage &#x27;deploy&#x27;          openshiftDeploy(deploymentConfig: &#x27;frontend&#x27;)        &#125;

在git代码文件中添加jenkinsfile文件
kind: &quot;BuildConfig&quot;apiVersion: &quot;v1&quot;metadata:  name: &quot;sample-pipeline&quot;spec:  source:    git:      uri: &quot;https://github.com/openshift/ruby-hello-world&quot;  strategy:    jenkinsPipelineStrategy:      jenkinsfilePath: some/repo/dir/filename 

可选的jenkinsfilePath字段指定要使用的文件的名称，相对于源contextDir。 如果省略了contextDir，则默认为存储库的根。 如果省略jenkinsfilePath，则默认为Jenkinsfile。


环境变量
jenkinsPipelineStrategy:...  env:    - name: &quot;FOO&quot;      value: &quot;BAR&quot;

Build构建的高级用法设置构建时对资源进行限制(cpu&#x2F;memory)apiVersion: &quot;v1&quot;kind: &quot;BuildConfig&quot;metadata:  name: &quot;sample-build&quot;spec:  resources:    limits:      cpu: &quot;100m&quot;       memory: &quot;256Mi&quot; 

设置构建的超时时间spec:  completionDeadlineSeconds: 1800

构建pod与Node绑定，只在指定的Node上执行构建apiVersion: &quot;v1&quot;kind: &quot;BuildConfig&quot;metadata:  name: &quot;sample-build&quot;spec:  nodeSelector:    key1: value1    key2: value2

Build链有一些构建，比如说Java应用，在构建的过程中会产生大量的中间文件与缓存，及一些构建的工具包，这些资源在运行时是不需要的，但它们会导致最终的镜像包很大。所以运行时的镜像中只需要存在构建的结果，而不需要构建过程中的文件与工作包。这就需要将构建拆分成两部分，第一部分执行构建，生成最终的制品文件，第二部分将制品文件放在可运行环境中进行运行。

apiVersion: v1kind: BuildConfigmetadata:  name: artifact-buildspec:  output:    to:      kind: ImageStreamTag      name: artifact-image:latest  source:    git:      uri: https://github.com/openshift/openshift-jee-sample.git    type: Git  strategy:    sourceStrategy:      from:        kind: ImageStreamTag        name: wildfly:10.1        namespace: openshift    type: Source

将上一个构建的输出，作为新的构建的输入
apiVersion: v1kind: BuildConfigmetadata:  name: image-buildspec:  output:    to:      kind: ImageStreamTag      name: image-build:latest  source:    type: Dockerfile    dockerfile: |-      FROM jee-runtime:latest      COPY ROOT.war /deployments/ROOT.war    images:    - from:         kind: ImageStreamTag        name: artifact-image:latest      paths:       - sourcePath: /wildfly/standalone/deployments/ROOT.war        destinationDir: &quot;.&quot;  strategy:    dockerStrategy:      from:         kind: ImageStreamTag        name: jee-runtime:latest    type: Docker  triggers:  - imageChange: &#123;&#125;    type: ImageChange

Build清理只保留构建成功的2个历史构建，和2个失败的历史构建
apiVersion: &quot;v1&quot;kind: &quot;BuildConfig&quot;metadata:  name: &quot;sample-build&quot;spec:  successfulBuildsHistoryLimit: 2   failedBuildsHistoryLimit: 2 
参考文章二进制文件构建Pipeline流水线构建BuildConfig
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-F5集成（总结）——与Router方案对比</title>
    <url>/posts/ed13f1158553.html</url>
    <content><![CDATA[这篇文章来自9月份自己在F5年度会议上分享的PPT，感谢过程中帮忙一起联调的小伙伴。PPT也分享出来，地址如下：openshift与F5的联合解决方案Openshift-F5集成（南北流量走F5）Openshift-F5集成（续）——实现灰度发布
Openshift的基础概念Openshift简介红帽® OpenShift 是一款性能强大的开源企业级PaaS产品。不仅是企业级的Kubernetes，可以构建、部署与管理容器应用，还提供从开发到投入生产的整个应用生命周期内使用的完整解决方案，帮助客户享受快速创新带来的收益，同时保持企业级平台的稳定性、可靠性和安全性。Openshift支持多种环境下部署，无论是在企业内部，公共云，或是托管环境中。
Openshift Pod
Pod是Openshift调度的最小单元
一个Pod包含一个或多个容器
Pod内的容器共享网络，IP不固定

实例：
apiVersion: v1kind: Podmetadata:  name: MyApp       labels:    app: MyApp    spec:  containers:  - name: myapp    image: app/myapp:latest    ports:    - containerPort: 9376
Openshift Service
Service是一个内部负均衡器，能将流量代理转发到一组pod中
Service能提供一个稳定可靠的内部IP
Service标识的一组pod可以任意伸缩，而不影响业务

实例：
apiVersion: v1kind: Servicemetadata:  name: MyAppService     spec:  selector:                      app: MyApp  clusterIP: 172.30.136.123     ports:  - port: 9376                   protocol: TCP    targetPort: 9376
Openshift Router与RouteRouter组件是解决外部访问Openshift集群中应用的一种解决方案

Router节点上运行一个特殊定制的Haproxy 
Route为Openshift中的资源对象，配置对应的Service及域名
Service更新会自动更新Haproxy的配置


Openshift Router的不足
Router无法支持灰度发布 (可以引入Nginx&#x2F;Haproxy实现灰度发布)
对外所有南北流量都走Router，随着流量增加，Router的压力也不断增加
生产环境中，面对突发流量，Router的可靠性与稳定性是一个挑战
全站SSL的性能压力如何解决

F5与Openshift集成架构与实践F5与Openshift集成架构
F5与Openshift中的Service集成
通过对应的应用配置ConfigMap（Openshift的配置资源）为每个ConfigMap自动创建对应的Virtual Server并与Pool映射


实例：
kind: ConfigMapapiVersion: v1metadata:  name: myApp.vs  labels:    f5type: virtual-serverdata:  schema: &quot;f5schemadb://bigip-virtual-server_v0.1.7.json&quot;  data: |    &#123;      &quot;virtualServer&quot;: &#123;        &quot;backend&quot;: &#123;          &quot;servicePort&quot;: 9376,          &quot;serviceName&quot;: &quot;myAppService&quot;          &#125;]        &#125;,       &quot;frontend&quot;: &#123;          &quot;virtualAddress&quot;: &#123;            &quot;port&quot;: 8080,            &quot;bindAddr&quot;: &quot;99.248.82.220&quot;          &#125;,          &quot;partition&quot;: &quot;openshift&quot;,          &quot;balance&quot;: &quot;round-robin&quot;,          &quot;mode&quot;: &quot;http&quot;        &#125;      &#125;
F5与Openshift中的Route集成
F5代替了Openshift中的Router组件的功能
Bigip-ctl启动配置中添加参数:  –manage-routes&#x3D;true,  –route-vserver-addr&#x3D;99.248.82.220
Bigip-ctl一启动就会在F5上创建好Virtual Server:80与Virtual Server:443


实例：
apiVersion: route.openshift.io/v1kind: Routemetadata:  labels:    name: f5-test  name: f5-test  annotations:    virtual-server.f5.com/balance: least-connections-nodespec:  host: f5-test.example.com  port:    targetPort: 8080  to:    kind: Service    name: f5-test
F5与Openshift实现灰度发布
手动创建Virtual Server 80(testroute)与443(testroute_https)
Bigip-ctl启动配置中添加参数:  –manage-routes&#x3D;true, –route-http-vserver&#x3D;testroute, –route-https-vserver&#x3D;testroute_https, –route-vserver-addr&#x3D;99.248.82.220
手动创建iRule规则应用到Virtual Server实例：

when HTTP_REQUEST &#123; if &#123; [HTTP::host] equals &quot;test1.apps.openshift.com&quot; &#125;&#123;   if &#123;[IP::addr [IP::client_addr] equals 192.168.100.23/32 ]&#125; &#123;  pool /f5-openShift/openshift_testapp_myapp-v2  &#125; else &#123;   pool /f5-openShift/openshift_testapp_myapp-v1  &#125; &#125;&#125;
使用F5实现灰度发布的不足
自定义iRule无法通过控制器自动下发，所以需要在F5端手动创建VServer及添加iRule来实现灰度发布

F5与Openshift的集成与Openshift的Router可以并存F5与Router节点是可以并存，同时为应用提供对外服务，并不会产生冲突。

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-F5集成（续）——实现灰度发布</title>
    <url>/posts/3fb7f39636aa.html</url>
    <content><![CDATA[上篇：Openshift-F5集成（南北流量走F5）中介绍了如何实现使用F5替换掉Openshift中的Route，但是它的可控性是弱的。本篇则通过手动创建VS及iRule来实现更强的流量控制，实现识别客户端IP来访问相应的服务。
为什么要使用灰度发布
什么是灰度发布灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。ABtest就是一种灰度发布方式，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。 
灰度发布的价值使用灰度发布可以在产品正式上线前针对特定一些目标用户进行开放，获得这些目标用户的反馈，及早发现问题，修复问题，完善产品的不足。如果发现新的版本价值不大，能够及早更换思路，避免产品直接上线后产生不好的影响。

Openshift Route自带的灰度发布功能
Openshift Route自带的灰度发布，是通过Route下“挂载”两个或两个以上Service，并调整各个Service的权值进行控制流量的分布。
例如应用有两个服务，分别为service-v1和service-v2，其中service-v2为新版本。通过不断放大service-v2的权值，观察用户的反馈，及时发现service-v2中的问题，并完善修复，最终service-v2承载所有service-v1的流量，实现服务的升级。通过这种方式，可以大大地降低service-v2中的问题对客户产生的影响。
Openshift Route对Service分流使用非常方便，一些普通的业务完全可以使用这个特性达到测试的目的。但是它的简单也带来了一些不足，就是它只能对请求进行概率地划分流量，并不能定向到用户。
例如，以下需求Openshift Route目前还无法实现。产品新版本正式发布前，我们希望对产品进行一些测试，只允许指定的一批用户或者一些网段的ip下的用户才能访问新版本。

F5与Openshift集成实现灰度发布
流量到达F5时，F5会优先对请求进行iRule下的匹配检查，定向到对应的Pool
如果iRule下未匹配，则会控制vs下绑定的Polices规则进行匹配
在上篇中，我们知道Openshift上的F5控制器会自动在F5上生成Polices规则，来满足Openshift Route的功能。那么只需要将它与自定义的iRule结合就能够实现既满足服务的分流，又能控制用户对服务的定向访问。

F5与Openshift集成配置与部署（实现灰度发布）
准备工作（详细见上篇：Openshift-F5集成（南北流量走F5））


创建新的HostSub Openshift
创建一个VXLAN profile F5
创建一个VXLAN Tunnel F5
在每个F5设备VXLAN中创建Self IP F5
在当前主F5设备的VXLAN下创建Floating IP F5
创建一个新的Partition F5
创建访问F5 BIG-IP的私钥 Openshift
创建RBAC认证 Openshift

说明:以上操作具体步骤参考上篇

手动创建VS（HTTP、HTTPS） F5


Local Traffic -&gt; Virtual Servers 选中指定的Partition，新建VS
HTTP
Name：VS名字
Destination Address/Mask：VS的IP地址
Service Port：HTTP
HTTP Profile：http
Source Address Translation：Auto Map


HTTPS
Name：VS名字
Destination Address/Mask：VS的IP地址
Service Port：HTTPS
HTTP Profile：http
SSL Profile (Client)：&#x2F;Common&#x2F;clientssl
Source Address Translation：Auto Map
设置VS中的cccl-whitelist为1 F5





目的：修改cccl-whitelist的值为1，是为了防止当openshift创建控制器时，route模式下将VS原本的配置覆盖掉
tmshcd /f5-openShift  (openshift所在的partition)modify ltm virtual testroute metadata add &#123; cccl-whitelist &#123; value 1 &#125; &#125;modify ltm virtual testroute_https metadata add &#123; cccl-whitelist &#123; value 1 &#125; &#125;

创建F5控制器 Openshift


对应每台F5设备创建一个Deployment
Deployment中的 –bigip-url 为设备的IP
Deployment中的 –bigip-partition为之前F5下创建的Partition,Openshift
Deployment中的–route-http-vserver为手动创建的HTTP VS
Deployment中的–route-https-vserver为手动创建的HTTPS VS
Deployment中的–route-label为给Controller打的标签（对于一组F5不需要配置，多组F5通过它打Label，并在Route中设置label f5type:label来指定使用的F5）

apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: f5-bigip-ctlr-01spec:  replicas: 1  template:    metadata:      name: k8s-bigip-ctlr      labels:        app: k8s-bigip-ctlr    spec:      # Name of the Service Account bound to a Cluster Role with the required      # permissions      serviceAccountName: bigip-ctlr      containers:        - name: k8s-bigip-ctlr          # replace the version as needed          image: &quot;f5networks/k8s-bigip-ctlr:1.5.1&quot;          env:            - name: BIGIP_USERNAME              valueFrom:                secretKeyRef:                  # Replace with the name of the Secret containing your login                  # credentials                  name: bigip-login                  key: username            - name: BIGIP_PASSWORD              valueFrom:                secretKeyRef:                  # Replace with the name of the Secret containing your login                  # credentials                  name: bigip-login                  key: password          command: [&quot;/app/bin/k8s-bigip-ctlr&quot;]          args: [            # See the k8s-bigip-ctlr documentation for information about            # all config options            # http://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/latest            &quot;--bigip-username=$(BIGIP_USERNAME)&quot;,            &quot;--bigip-password=$(BIGIP_PASSWORD)&quot;,            &quot;--bigip-url=192.168.200.82&quot;,            &quot;--bigip-partition=OpenShift&quot;,            &quot;--pool-member-type=cluster&quot;,            &quot;--openshift-sdn-name=/Common/openshift_vxlan&quot;,            &#x27;--manage-routes=true&#x27;            &#x27;--route-http-vserver=testroute&#x27;            &#x27;--route-https-vserver=testroute_https&#x27;            ]---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: f5-bigip-ctlr-02spec:  replicas: 1  template:    metadata:      name: k8s-bigip-ctlr      labels:        app: k8s-bigip-ctlr    spec:      # Name of the Service Account bound to a Cluster Role with the required      # permissions      serviceAccountName: bigip-ctlr      containers:        - name: k8s-bigip-ctlr          # replace the version as needed          image: &quot;f5networks/k8s-bigip-ctlr:1.5.1&quot;          env:            - name: BIGIP_USERNAME              valueFrom:                secretKeyRef:                  # Replace with the name of the Secret containing your login                  # credentials                  name: bigip-login                  key: username            - name: BIGIP_PASSWORD              valueFrom:                secretKeyRef:                  # Replace with the name of the Secret containing your login                  # credentials                  name: bigip-login                  key: password          command: [&quot;/app/bin/k8s-bigip-ctlr&quot;]          args: [            # See the k8s-bigip-ctlr documentation for information about            # all config options            # http://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/latest            &quot;--bigip-username=$(BIGIP_USERNAME)&quot;,            &quot;--bigip-password=$(BIGIP_PASSWORD)&quot;,            &quot;--bigip-url=192.168.200.83&quot;,            &quot;--bigip-partition=OpenShift&quot;,            &quot;--pool-member-type=cluster&quot;,            &quot;--openshift-sdn-name=/Common/openshift_vxlan&quot;,            &#x27;--manage-routes=true&#x27;            &#x27;--route-http-vserver=testroute&#x27;            &#x27;--route-https-vserver=testroute_https&#x27;            ]

给vs手动绑定Policies F5


Openshift F5控制器创建好后，在F5上会自动创建两条Policies, 分别为：openshift_insecure_routes、openshift_secure_routes。
openshift_insecure_routes为HTTP应用服务
openshift_secure_routes为HTTPS应用服务。


创建应用（Project名为testapp，Service名为f5-nginx-v1与f5-nginx-v2）

oc new-project testappoc new-app harbor.example.com/public/nginx:1.14 --name=f5-nginx-v1 --allow-missing-imagesoc expose dc/f5-test-v1 --port=8080oc expose svc/f5-test-v1 test1.apps.openshift.comoc new-app harbor.example.com/public/nginx:1.14 --name=f5-nginx-v2 --allow-missing-imagesoc expose dc/f5-test-v2 --port=8080

创建iRule,并绑定到VS F5

说明：请求域名test1.apps.openshift.com时，如果客户端IP为192.168.100.23，则访问testapp项目下的f5-nginx-v2服务，否则访问testapp项目下的f5-nginx-v1服务注意：iRule规则需要在Common的Partition下创建
when HTTP_REQUEST &#123; if &#123; [HTTP::host] equals &quot;test1.apps.openshift.com&quot; &#125;&#123;  log local0.info [HTTP::host]   if &#123;[IP::addr [IP::client_addr] equals 192.168.100.23/32 ]&#125; &#123;  log local0.info &quot;enter 2 pool before&quot;  log local0.info [HTTP::host]  pool /f5-openShift/openshift_testapp_f5-nginx-v2  log local0.info &quot;enter 2 pool later&quot;  &#125; else &#123;  log local0.info &quot;enter 3&quot;   pool /f5-openShift/openshift_testapp_f5-nginx-v1  &#125; &#125;&#125;

测试访问服务

本地(192.168.100.23)与另一台非192.168.100.23的机器上绑定hosts
VS的IP地址 test1.apps.openshift.com
再访问test1.apps.openshift.com，查看页面显示，访问不同的Service。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-F5集成（南北流量走F5）</title>
    <url>/posts/099cf7b5c58f.html</url>
    <content><![CDATA[使用F5与Openshift集成目的外部流量访问应用时，通过F5 BIG-IP硬件设备直接代理到集群中的Pod。这样做的好处，很明显。

使用硬件负载均衡器替换掉软件负载均衡器，提高性能。
F5有更灵活的配置，可以实现更复杂的流量控制

Openshift操作Openshift通过BIG-IP Controller来控制BIG-IP设备。由于Openshift是基于Kubernetes的，所以它们使用同一个Controller（k8s-bigip-ctlr）。BIG-IP Controller为集群中的应用配置BIG-IP对象，提供南北流量的服务。
BIG-IP Controller有两种方式来使用F5 BIG-IP设备

为Openshift中的Service提供代理流量
为Openshift中的Route提供代理流量

为Openshift中的Service提供代理流量_(不介绍具体部署操作)_这种方式，我们测试下来发现，需要为对外提供服务的Service绑定到F5的不同端口，同时外部访问应用时需要指定端口号。如：app1.openshift.example.com:8000, app2.openshift.example.com:8001，app3.openshift.example.com:8002，其中端口号不能重复。

这种方式在真正使用时不能满足业务需求，除非在F5的前面再加一层代理，这又增加了架构的复杂性。
理想的状态是，所以的App应用的HTTP请求都访问F5的80端口，而HTTPS的请求访问F5的443端口，根据请求的域名路由到对应的Pool中（每个Pool是一个Service下的所有Pod的列表）。
很高兴告诉大家，第二种方式能够满足上面的需求

为Openshift中的Route提供代理流量使用BIG-IP作为Openshift的Router，能实现以下功能：

为Services创建BIG-IP本地流量规则
提供HTTP&#x2F;HTTPS路由
为Route资源添加BIG-IP健康检查

本文只介绍，如何使用F5提供HTTP&#x2F;HTTPS路由。这也是最核心的部分。

部署环境版本：  2台F5：v13  192.168.200.82 192.168.200.83  Openshift集群： v3.9.1


创建新的HostSub Openshift

# hostsubnet.ymlapiVersion: v1kind: HostSubnetmetadata:  name: f5-bigip-node01  annotations:    pod.network.openshift.io/fixed-vnid-host: &quot;0&quot;    pod.network.openshift.io/assign-subnet: &quot;true&quot;# provide a name for the node that will serve as BIG-IP&#x27;s entry into the clusterhost: f5-bigip-node01# The hostIP address will be the BIG-IP interface address routable to the# OpenShift Origin nodes.# This address is the BIG-IP VTEP in the SDN&#x27;s VXLAN.hostIP: 192.168.200.82---apiVersion: v1kind: HostSubnetmetadata:  name: f5-bigip-node02  annotations:    pod.network.openshift.io/fixed-vnid-host: &quot;0&quot;    pod.network.openshift.io/assign-subnet: &quot;true&quot;# provide a name for the node that will serve as BIG-IP&#x27;s entry into the clusterhost: f5-bigip-node02# The hostIP address will be the BIG-IP interface address routable to the# OpenShift Origin nodes.# This address is the BIG-IP VTEP in the SDN&#x27;s VXLAN.hostIP: 192.168.200.83---apiVersion: v1kind: HostSubnetmetadata:  name: f5-bigip-float  annotations:    pod.network.openshift.io/fixed-vnid-host: &quot;0&quot;    pod.network.openshift.io/assign-subnet: &quot;true&quot;# provide a name for the node that will serve as BIG-IP&#x27;s entry into the clusterhost: f5-bigip-float# The hostIP address will be the BIG-IP interface address routable to the# OpenShift Origin nodes.# This address is the BIG-IP VTEP in the SDN&#x27;s VXLAN.hostIP: 192.168.200.84
oc create -f hostsubnet.yml
查看执行结果
[root@master01 ~]# oc get hostsubnet NAME                                 HOST                                 HOST IP         SUBNET          EGRESS IPSf5-bigip-float                       f5-bigip-float                       192.168.200.84   10.128.6.0/23   []f5-bigip-node01                      f5-bigip-node01                      192.168.200.82   10.129.6.0/23   []f5-bigip-node02                      f5-bigip-node02                      192.168.200.83   10.130.4.0/23   []master01.example.com   master01.example.com   192.168.200.1     10.130.0.0/23   []master02.example.com   master02.example.com   192.168.200.2     10.128.0.0/23   []master03.example.com   master03.example.com   192.168.200.3     10.128.2.0/23   []node01.example.com     node01.example.com     192.168.200.21    10.129.0.0/23   []node02.example.com     node02.example.com     192.168.200.22    10.131.0.0/23   []router01.example.com   router01.example.com   192.168.200.11    10.129.2.0/23   []router02.example.com   router02.example.com   192.168.200.12    10.130.2.0/23   []

创建一个VXLAN profile F5

在F5的TMOS终端，创建一个多点模式的vxlan
create /net tunnels vxlan openshift_vxlan flooding-type multipoint

创建一个VXLAN Tunnel F5

Local Address使用vip： 192.168.200.84Secondary Address使用F5的设备IP：node1 192.168.200.82， node2 192.168.200.83在F5 Node1上的TMOS创建Tunnel
create /net tunnels tunnel &lt;float_tun_name&gt; key 0 profile openshift_vxlan local-address 192.168.200.84 secondary-address 192.168.200.82 traffic-group traffic-group-1
在F5 Node2上的TMOS创建Tunnel
create /net tunnels tunnel &lt;float_tun_name&gt; key 0 profile openshift_vxlan local-address 192.168.200.84 secondary-address 192.168.200.83 traffic-group traffic-group-1

在每个F5设备VXLAN中创建Self IP F5

IP为设备对应在Openshift的HostSubnet下的一个IP（只要在对应的HostSubnet下就OK）在F5 Node1中创建Self IP
create /net self 10.129.6.82/14 allow-service none vlan openshift_vxlan
在F5 Node2中创建Self IP
create /net self 10.130.4.83/14 allow-service none vlan openshift_vxlan

在当前主F5设备的VXLAN下创建Floating IP F5

如：当前主F5设备为node1，那Floating IP为主设备所在的HostSubnet下。
create /net self 10.128.6.84/14 allow-service none traffic-group traffic-group-1 vlan openshift_vxlan


创建一个新的Partition F5

create auth partition OpenShift

创建访问F5 BIG-IP的私钥 Openshift

oc create secret generic bigip-login --from-literal=username=admin --from-literal=password=admin -n kube-system

创建RBAC认证 Openshift

# cluster-role.ymlapiVersion: v1kind: ServiceAccountmetadata:  name: bigip-ctlr  namespace: kube-system---# For use in OpenShift clustersapiVersion: v1kind: ClusterRolemetadata:  annotations:    authorization.openshift.io/system-only: &quot;true&quot;  name: system:bigip-ctlr  namespace: kube-systemrules:- apiGroups: [&quot;&quot;, &quot;extensions&quot;]  resources: [&quot;nodes&quot;, &quot;services&quot;, &quot;endpoints&quot;, &quot;namespaces&quot;, &quot;ingresses&quot;, &quot;routes&quot; ]  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups: [&quot;&quot;, &quot;extensions&quot;]  resources: [&quot;configmaps&quot;, &quot;events&quot;, &quot;ingresses/status&quot;]  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;, &quot;patch&quot; ]- apiGroups: [&quot;&quot;, &quot;extensions&quot;]  resources: [&quot;secrets&quot;]  resourceNames: [&quot;bigip-login&quot;]  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]---apiVersion: v1kind: ClusterRoleBindingmetadata:  name: bigip-ctlr-role  namespace: kube-systemuserNames:- system:serviceaccount:kube-system:bigip-ctlrsubjects:- kind: ServiceAccount  name: bigip-ctlrroleRef:  name: system:bigip-ctlr
oc create -f cluster-role.yml

部署BIG-IP Controller Openshift


对应每台F5设备创建一个Deployment
Deployment中的 –bigip-url 为设备的IP
Deployment中的 –bigip-partition为之前F5下创建的Partition,Openshift
Deployment中的 –route-vserver-addr 为F5对外提供服务的IP

#deployment.ymlapiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: f5-bigip-ctlr-01spec:  replicas: 1  template:    metadata:      name: k8s-bigip-ctlr      labels:        app: k8s-bigip-ctlr    spec:      # Name of the Service Account bound to a Cluster Role with the required      # permissions      serviceAccountName: bigip-ctlr      containers:        - name: k8s-bigip-ctlr          # replace the version as needed          image: &quot;f5networks/k8s-bigip-ctlr:1.5.1&quot;          env:            - name: BIGIP_USERNAME              valueFrom:                secretKeyRef:                  # Replace with the name of the Secret containing your login                  # credentials                  name: bigip-login                  key: username            - name: BIGIP_PASSWORD              valueFrom:                secretKeyRef:                  # Replace with the name of the Secret containing your login                  # credentials                  name: bigip-login                  key: password          command: [&quot;/app/bin/k8s-bigip-ctlr&quot;]          args: [            # See the k8s-bigip-ctlr documentation for information about            # all config options            # http://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/latest            &quot;--bigip-username=$(BIGIP_USERNAME)&quot;,            &quot;--bigip-password=$(BIGIP_PASSWORD)&quot;,            &quot;--bigip-url=192.168.200.82&quot;,            &quot;--bigip-partition=OpenShift&quot;,            &quot;--pool-member-type=cluster&quot;,            &quot;--openshift-sdn-name=/Common/openshift_vxlan&quot;,            &#x27;--manage-routes=true&#x27;,            &#x27;--route-vserver-addr=192.168.200.80&#x27;            ]      imagePullSecrets:        # Secret containing the BIG-IP system login credentials        - name: bigip-login---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: f5-bigip-ctlr-02spec:  replicas: 1  template:    metadata:      name: k8s-bigip-ctlr      labels:        app: k8s-bigip-ctlr    spec:      # Name of the Service Account bound to a Cluster Role with the required      # permissions      serviceAccountName: bigip-ctlr      containers:        - name: k8s-bigip-ctlr          # replace the version as needed          image: &quot;f5networks/k8s-bigip-ctlr:1.5.1&quot;          env:            - name: BIGIP_USERNAME              valueFrom:                secretKeyRef:                  # Replace with the name of the Secret containing your login                  # credentials                  name: bigip-login                  key: username            - name: BIGIP_PASSWORD              valueFrom:                secretKeyRef:                  # Replace with the name of the Secret containing your login                  # credentials                  name: bigip-login                  key: password          command: [&quot;/app/bin/k8s-bigip-ctlr&quot;]          args: [            # See the k8s-bigip-ctlr documentation for information about            # all config options            # http://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/latest            &quot;--bigip-username=$(BIGIP_USERNAME)&quot;,            &quot;--bigip-password=$(BIGIP_PASSWORD)&quot;,            &quot;--bigip-url=192.168.200.83&quot;,            &quot;--bigip-partition=OpenShift&quot;,            &quot;--pool-member-type=cluster&quot;,            &quot;--openshift-sdn-name=/Common/openshift_vxlan&quot;,            &#x27;--manage-routes=true&#x27;,            &#x27;--route-vserver-addr=192.168.200.80&#x27;            ]      imagePullSecrets:        - name: bigip-login
oc create -f deployment.yml
查看BIG-IP Controller的部署进度
[root@master01 ~]# oc get podNAME                                READY     STATUS    RESTARTS   AGEf5-bigip-ctlr-01-7f44695b97-lmwdh   1/1       Running   0          17hf5-bigip-ctlr-02-54df674f58-j2x26   1/1       Running   0          17h
当BIG-IP Controller POD启动后，在F5上会自动创建两个virtual servers

“ose-vserver” 为HTTP请求
“https-ose-vserver” 为HTTPS请求这两个virtual servers是Openshift下所有的Route共用的。


创建新的应用F5-Test，并创建HTTP Route Openshift


使用自己镜像仓库中的镜像：harbor.example.com&#x2F;public&#x2F;nginx:1.14
该镜像的制作方法及说明参考：Openshfit上用ConfigMap来自定义Nginx配置

oc new-project f5-testoc new-app harbor.example.com/public/nginx:1.14 --name=f5-test --allow-missing-imagesoc expose dc/f5-test --port=8080
创建Route
apiVersion: route.openshift.io/v1kind: Routemetadata:  labels:    name: f5-test  name: f5-test  annotations:    virtual-server.f5.com/balance: least-connections-nodespec:  host: f5-test.example.com  port:    targetPort: 8080  to:    kind: Service    name: f5-test
本地绑定hosts
192.168.200.80 f5-test.example.com
浏览器访问http://f5-test.example.com，即能看到应用页面。

创建HTTPS Route（仍然使用上面的应用） Openshift

apiVersion: route.openshift.io/v1kind: Routemetadata:  annotations:    virtual-server.f5.com/balance: least-connections-node  labels:    app: f5-test  name: f5-test-2spec:  host: f5-tes-2t.example.com  tls:    insecureEdgeTerminationPolicy: Allow    termination: edge  to:    kind: Service    name: f5-test
本地绑定hosts
192.168.200.80 f5-test-2.example.com
浏览器访问https://f5-test.example.com，即能看到应用页面。

说明


Openshit上创建Service后，F5会自动创建新的Pool，Pool里的资源即为Service下的Pod
请求到达F5后，F5根据请求的域名，找到对应的Pool，请求直接到达Pod。不会请求集群里的Route。


参考资料


F5与Openshift中的Route集成
F5与Openshift中的Service集成

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-GitLab安装及使用Nodeport支持ssh访问</title>
    <url>/posts/d5eceb7414d3.html</url>
    <content><![CDATA[
部署Gitlab
创建gitlab项目

oc new-project gitlab

创建cicd serviceaccount

$ oc create serviceaccount cicd -n gitlab

导入Gitlab模板

wget https://gitee.com/xhua/OpenshiftOneClick/raw/3.11/openshift-templates/gitlab-template.yamloc create -f openshift-template.json -n openshift

创建持久化存储（如果没有pv的情况下）

$ cat gitlab-pv.yamlapiVersion: v1items:- apiVersion: v1  kind: PersistentVolume  metadata:    creationTimestamp: null    name: gitlabdata-volume  spec:    accessModes:    - ReadWriteMany    capacity:      storage: 50Gi    claimRef:      apiVersion: v1      kind: PersistentVolumeClaim      name: gitlab-data      namespace: gitlab    nfs:      path: /mnt/gitlabdata      server: 192.168.0.13    persistentVolumeReclaimPolicy: Retain  status: &#123;&#125;- apiVersion: v1  kind: PersistentVolume  metadata:    creationTimestamp: null    name: gitlabpostgresql-volume  spec:    accessModes:    - ReadWriteMany    capacity:      storage: 10Gi    claimRef:      apiVersion: v1      kind: PersistentVolumeClaim      name: gitlab-postgresql      namespace: gitlab    nfs:      path: /mnt/gitlabpostgresql      server: 192.168.0.13    persistentVolumeReclaimPolicy: Retain  status: &#123;&#125;- apiVersion: v1  kind: PersistentVolume  metadata:    creationTimestamp: null    name: gitlabredisdata-volume  spec:    accessModes:    - ReadWriteMany    capacity:      storage: 50Gi    claimRef:      apiVersion: v1      kind: PersistentVolumeClaim      name: gitlab-redis-data      namespace: gitlab    nfs:      path: /mnt/gitlabredisdata      server: 192.168.0.13    persistentVolumeReclaimPolicy: Retain  status: &#123;&#125;- apiVersion: v1  kind: PersistentVolume  metadata:    creationTimestamp: null    name: gitlabetc-volume  spec:    accessModes:    - ReadWriteMany    capacity:      storage: 50Gi    claimRef:      apiVersion: v1      kind: PersistentVolumeClaim      name: gitlab-etc      namespace: gitlab    nfs:      path: /mnt/gitlabetc      server: 192.168.0.13    persistentVolumeReclaimPolicy: Retain  status: &#123;&#125;$ oc create gitlab-pv.yaml

给gitlab 容器使用root用户的权限

$  oc adm  policy add-scc-to-user anyuid -z cicd -n gitlab

在Openshift上创建gitlab应用


设置gitlab安装配置（自定义）



通过访问Route访问gitlab在访问的机器上hosts文件中配置Router Host与Ip的对应用户名（root） 密码（根据自定义配置中设定）


使用Nodeport让gitlab服务支持ssh访问
Openshift上的服务最常使用的是Route来对外提供服务。但是Route只支持Http协议，而对于Gitlab通过ssh访问的方式，得通过TCP协议。所以可以使用NodePort向外提供服务。


创建NodePort (30022-&gt;gitlab 22)

$ cat gitlab-nodeport.yamlapiVersion: v1kind: Servicemetadata:  name: gitlab-nodeport  namespace: gitlab  labels:    name: gitlab-nodeportspec:  type: NodePort  ports:    - port: 22      nodePort: 30022      name: ssh  selector:    app: gitlab-ce    deploymentconfig: gitlab-ce$ oc create -f gitlab-nodeport.yaml

将本机的公钥拷贝到gitlab网站的ssh key管理


3.客户端clone代码
git clone ssh://git@gitlab.apps.it.example.com:30022/root/test.git# 或者git clone ssh://git@192.168.1.x:30022/root/test.git ##192.168.1.x为集群中任意Node的ip
注意：因为Nodeport使用的不是ssh默认的22端口，在clone时必须在前缀使用ssh://，同时在git服务后添加:NodePort端口号4. 结果展示
[root@gitlab ~]# git clone ssh://git@gitlab.apps.it.example.com:30022/root/test.gitCloning into &#x27;test&#x27;...Warning: Permanently added &#x27;[gitlab.apps.it.example.com]:30022,[192.168.1.3]:30022&#x27; (ECDSA) to the list of known hosts.remote: Counting objects: 12, done.remote: Compressing objects: 100% (4/4), done.remote: Total 12 (delta 0), reused 0 (delta 0)Receiving objects: 100% (12/12), done.
参考资源https://docs.gitlab.com/ee/install/openshift_and_gitlab&#x2F;index.html
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-HPA（Horizontal-Pod-Autoscaler）自动伸缩过程及算法</title>
    <url>/posts/551f7c289dcf.html</url>
    <content><![CDATA[1、HPA介绍HPA（Horizontal Pod Autoscaler）是Openshift中的一个非常重要的对象，它定义了系统如何根据收集对应的Pod的状态（CPU&#x2F;Memory）对DeploymentConfig、ReplicationController对象进行扩容与缩容。

HPA依赖于收集到的Pod资源的使用状态，所以要使HPA生效，Openshift必须安装好cluster metrics应用。
被监控的pod必须设置好了spec.containers.resources.requests属性，HPA才能正常工作。
仅支持CPU&#x2F;Memory使用率的判断，如果自定义监控项，只能使用经验值，不能使用使用率。
支持对象：DeploymentConfig、ReplicationController、Deployment、Replica Set。

2. HPA伸缩过程及算法
HPA进行伸缩过程


收集该HPA控制下所有Pod最近的cpu使用情况（CPU utilization）
对比在扩容条件里记录的cpu限额（CPUUtilization）
调整实例数（必须要满足不超过最大&#x2F;最小实例数）
每隔30s做一次自动扩容的判断说明：


CPU utilization的计算方法是用cpu usage（最近一分钟的平均值，通过heapster可以直接获取到）除以cpu request（这里cpu request就是我们在创建容器时制定的cpu使用核心数）得到一个平均值，这个平均值可以理解为：平均每个Pod CPU核心的使用占比。
最重要的步骤为3，这里即为HPA的算法，计算当前需要启动几个Pod
HPA进行伸缩算法



分为三种情况：

普通情况下启动Pod数量计算方式

TargetNumOfPods = ceil(sum(CurrentPodsCPUUtilization) / Target)
说明：

ceil()表示取大于或等于某数的最近一个整数

例子：我们有一个集群实例数是3 pods，同时Pod的cpu资源的Request为1.4。cpu限额，即Target是CPU使用率为80%，当cpu的使用量CurrentPodsCPUUtilization为1.1,1.4，1.3时，要扩容成多少个呢？
ceil（（1.1+1.4+1.3）/1.4/0.8）= 4 
所以扩容成四个实例。

实例刚启动时及刚完成扩容&#x2F;缩容，会有一段冷却时间由于启动实例时cpu的使用度会陡增，所以自动扩容会等待一段时间以收集准确的运行时监控数据。每次扩容后冷却3分钟才能再次进行扩容，而缩容则要等5分钟后。这是因为自动扩容使用保守的方法，尽可能满足pods业务的正常使用，所以扩容的优先级要大于缩容。

当前Pod Cpu使用率与目标使用率接近时，不会触发扩容当满足以下条件才会真正触发扩容&#x2F;缩容：


avg(CurrentPodsConsumption) / Target &gt;1.1 或 &lt;0.9
这是为了避免出现频繁的扩容缩容。扩容条件的相对与绝对度量例子：我们有一个集群实例数是3 pods，同时Pod的cpu资源的Request为1.5。cpu限额，即Target是CPU使用率为80%，当cpu的使用量CurrentPodsCPUUtilization为1.1,1.4，1.3时，会不会发生扩容，要扩容成多少个呢？
ceil（（1.1+1.4+1.3）/1.5/0.8）= 4 
按照我们1的说法，它再添加一个pod。但是我们再来算下当前Pod使用率与目标使用率情况。
(1.1 + 1.4 + 1.3)/3/1.5 = 0.84444 #当前Pod CPU平均使用率0.84444 / 0.8 = 1.055555 &lt; 1.1 #当前Pod CPU平均使用率与目标CPU使用率比
综上：1.0555 &lt; 1.1，当前HPA并不会发生扩容，所以最终Pod数仍然是3个。
实战
为 dc&#x2F;nginx-demo 创建一个 HPA （最小为1个pod，最多为3个pod，cpu使用率目标值为80%）

oc autoscale dc/nginx-demo--min=1 --max=3 --cpu-percent=80
查看当前hpa状态
[root@demo ~]# oc delete hpa hpa-resource-metrics-memory horizontalpodautoscaler &quot;hpa-resource-metrics-memory&quot; deleted[root@demo ~]# oc describe hpa nginx-demo Name:                                                  nginx-demoNamespace:                                             testmysqlLabels:                                                &lt;none&gt;Annotations:                                           &lt;none&gt;CreationTimestamp:                                     Wed, 06 Jun 2018 10:36:57 +0800Reference:                                             DeploymentConfig/nginx-demoMetrics:                                               ( current / target )  resource cpu on pods  (as a percentage of request):  0% (0) / 80%Min replicas:                                          1Max replicas:                                          3Conditions:  Type            Status  Reason            Message  ----            ------  ------            -------  AbleToScale     True    ReadyForNewScale  the last scale time was sufficiently old as to warrant a new scale  ScalingActive   True    ValidMetricFound  the HPA was able to succesfully calculate a replica count from cpu resource utilization (percentage of request)  ScalingLimited  True    TooFewReplicas    the desired replica count is more than the maximum replica countEvents:           &lt;none&gt;

为dc&#x2F;nginx-demo创建一个HPA(最小为1个pod，最多为3个pod，memory使用率目标值50%）

与CPU使用率作为目标值不同，memory使用率不能使用oc autoscale命令来创建，只能通过yaml文件来创建。
# hpa-memory.ymlapiVersion: autoscaling/v2beta1kind: HorizontalPodAutoscalermetadata:  name: hpa-resource-metrics-memory spec:  scaleTargetRef:    apiVersion: apps.openshift.io/v1    kind: DeploymentConfig    name: nginx-demo  minReplicas: 1   maxReplicas: 3   metrics:  - type: Resource    resource:      name: memory      targetAverageUtilization: 50 
oc create -f hpa-memory.yml
查看当前hpa状态
[root@demo~]# oc describe hpa hpa-resource-metrics-memory Name:                                                     hpa-resource-metrics-memoryNamespace:                                                testmysqlLabels:                                                   &lt;none&gt;Annotations:                                              &lt;none&gt;CreationTimestamp:                                        Wed, 06 Jun 2018 10:28:59 +0800Reference:                                                DeploymentConfig/nginx-demoMetrics:                                                  ( current / target )  resource memory on pods  (as a percentage of request):  1% (1347584) / 50%Min replicas:                                             1Max replicas:                                             3Conditions:  Type            Status  Reason              Message  ----            ------  ------              -------  AbleToScale     True    ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale  ScalingActive   True    ValidMetricFound    the HPA was able to succesfully calculate a replica count from memory resource utilization (percentage of request)  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable rangeEvents:  Type     Reason          Age              From                       Message  ----     ------          ----             ----                       -------  Warning  FailedGetScale  5m (x6 over 8m)  horizontal-pod-autoscaler  no matches for apps/, Kind=DeploymentConfig  Warning  FailedGetScale  4m (x3 over 5m)  horizontal-pod-autoscaler  no matches for apps/, Kind=ReplicationController  Warning  FailedGetScale  3m               horizontal-pod-autoscaler  replicationcontrollers/scale.autoscaling &quot;nginx-demo&quot; not found
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-Jenkins共享并支持pipeline</title>
    <url>/posts/4dad4d371af1.html</url>
    <content><![CDATA[
诉求
使用Openshift的pipeline构建流水线，在Openshift上统一管理
使用一个公共的Jenkins，而不需要每个Project下都创建一个新的jenkins。以节约资源
在创建新的项目时，尽量少地改动完成以上的需求

问题openshift默认的BuildConfig如果设置为jenkinsPipelineStrategy策略，将会在当前project下查找jenkins服务，如果没有的话，将会使用master-config中设置的默认jenkins模板所在位置在当前project下创建一个新的jenkins应用，便使用该应用执行相关的pipeline。每个项目都会创建一个新的jenkins。
解决思路
禁用Openshfit默认的JenkinsPipeline机制，不在当前项目下面自动创建Jenkins
在创建一个新的project时，创建名为jenkins的service，同时将它指向公共的Jenkins服务。
同时为了在当前Project界面下能够跳转到jenkins的界面，再创建一个jenkins Route，支持跳转到jenkins进行查看运行日志与过程
项目要被jenkins控制，需要让公共jenkins具有操作当前项目的权限，所以还需要创建system:serviceaccount:jenkins:jenkins编辑当前Project的权限

解决方法
部署Jenkins作为公共的Jenkins,例如部署在jenkins project下
创建Project新建模板

$ oc adm create-bootstrap-project-template --name=bootstrap-project-template -o yaml &gt; project-template.yaml
在以上导出的配置中添加默认的jenkins Service&#x2F;Route&#x2F;RoleBinding
- apiVersion: v1  kind: Service  metadata:    name: jenkins    namespace: $&#123;PROJECT_NAME&#125;  selector: &#123;&#125;  spec:    externalName: jenkins.jenkins.svc.cluster.local    type: ExternalName- apiVersion: route.openshift.io/v1  kind: Route  metadata:    name: jenkins    namespace: $&#123;PROJECT_NAME&#125;  spec:    host: jenkins-jenkins.apps.example.com    to:      kind: Service      name: jenkins- apiVersion: authorization.openshift.io/v1  kind: RoleBinding  metadata:    name: edit    namespace: $&#123;PROJECT_NAME&#125;  roleRef:    name: edit  subjects:  - kind: ServiceAccount    name: jenkins    namespace: jenkins  userNames:  - system:serviceaccount:jenkins:jenkins
结果展示:

将Openshift的Master中添加以下设置
设置Project默认模板为default/bootstrap-project-template
关闭JenkinsPipeline默认在project下创建的机制



 ...projectConfig:  defaultNodeSelector: node-role.kubernetes.io/compute=true  projectRequestMessage: &#x27;&#x27;  projectRequestTemplate: &#x27;default/bootstrap-project-template&#x27;...jenkinsPipelineConfig:  autoProvisionEnabled: false  templateNamespace: openshift  templateName: jenkins-ephemeral  serviceName: jenkins

在Jenkins中的系统管理-&gt;系统设置中的OpenShift Jenkins Sync中的Namespace中添加 ${PROJECT_NAME},之间是以空格分隔的.

结果展示:在Testx中创建一个简单的PipelineConfigBuild进行测试
apiVersion: build.openshift.io/v1kind: BuildConfigmetadata:  name: test-pipelinex  namespace: testxspec:  strategy:    jenkinsPipelineStrategy:      jenkinsfile: |-        pipeline &#123;          agent any          stages &#123;            stage(&quot;测试&quot;)&#123;              steps &#123;                echo &quot;打印一下&quot;              &#125;            &#125;          &#125;        &#125;    type: JenkinsPipeline
在Testx项目下创建以上的BuildConfig,前提是testx项目名已在jenkins中的系统设置中的OpenShift Jenkins Sync中添加Testx项目下的PipelineJenkins可以使用Jenkins项目下的Jenkins服务来执行Pipeline
参考文章DevOps的核心内功心法：OpenShift中实现共享Jenkins
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-Network-QoS——Pod网络控制</title>
    <url>/posts/bfe4fac12931.html</url>
    <content><![CDATA[
Pod网络（速）控制的必要性高速公路上，当流量大时，如果汽车仍然不限制速度的话，将会很容易发生车祸，我们都会自觉地减速缓慢通过，只有减速才能安全行驶。在平台的集群中也是一样，一台主机上会有大量容器运行，容器相当于高速速上的汽车，对外的网络通信都使用主机出口这条高速路，如果某（几）个容器突然访问流量大增，而且没有作任何网络限速，会占用了主机的网络，严重影响其它容器的网络，进而影响其它业务。
前提
Openshift打开多租户网络模式修改&#x2F;etc&#x2F;origin&#x2F;master&#x2F;master-config.yaml将networkPluginName设置为redhat/openshift-ovs-multitenant

...  hostSubnetLength: 9  networkPluginName: redhat/openshift-ovs-multitenant  serviceNetworkCIDR: 172.30.0.0/16...

如果已存在的集群，切换网络策略，请参考Openshift网络插件动态切换

为Pod添加网络限速标记kind: PodapiVersion: v1metadata:  name: nginx annotations:   kubernetes.io/ingress-bandwidth: 1M   kubernetes.io/egress-bandwidth: 1Mspec:    containers:      - image: nginx        name: nginx
说明: 

kubernetes.io&#x2F;ingress-bandwidth设置的是 (出端口）下行的网速限制
kubernetes.io&#x2F;egress-bandwidth设置的是 (入端口）上行的网速限制
网络限制单位必须是M，实际单位对应的是Mb

为DeploymentConfig添加限速标记kind: DeploymentConfigmetadata:  labels:    app: nginx  name: nginx  namespace: testspec:  replicas: 1  selector:    deploymentconfig: nginx  template:    metadata:      annotations:        kubernetes.io/egress-bandwidth: 0.5M        kubernetes.io/ingress-bandwidth: 0.5M      labels:        app: nginx        deploymentconfig: nginx    spec:      containers:        - image: nginx          name: nginx
说明:

因为限速是面向Pod的所以需要要Pod对应的template中添加网络上下行速度限制。

测试(上行与下行都限制为0.5M)
Pod访问外网

[root@demo ~]# oc rsh op-java-sample-13-7bmj7sh-4.2$ wget https://xxxx.com/xx.zip--2018-07-10 08:31:26--  https://xxxx.com/xx.zipResolving xxxx.com (xxxx.com)... 117.211.167.14Connecting to xxxx.com (xxxx.com)|117.211.167.14|:443... connected.HTTP request sent, awaiting response... 200 OKLength: unspecified [application/zip]Saving to: &#x27;xx.zip.2&#x27;  14% [               &lt;=&gt;                                                 ] 211,857     57.2KB/s           
说明:下载速度为57.2KB&#x2F;s，恰好是被限的500Kb

外部访问Pod

[root@demo ~]# wget http://10.131.1.32:8080/20180416.db--2018-07-10 16:50:02--  http://10.131.1.32:8080/20180416.dbConnecting to 10.131.1.32:8080... connected.HTTP request sent, awaiting response... 200 OKLength: 10698784 (10M) [application/octet-stream]Saving to: ‘20180416.db.1’ 9% [               &lt;=&gt;                                                 ]1,056,888   58.3KB/s  eta 4m 10s 
说明10.131.1.32为Pod在集群下的IP，从主机访问Pod的服务下载文件，速度为58.3KB&#x2F;s，恰好是被限的500Kb

同一个Poroject下的Pod间访问

sh-4.2$ wget http://10.131.1.32:8080/20180416.db--2018-07-10 08:54:50--  http://10.131.1.32:8080/20180416.dbConnecting to 10.131.1.32:8080... connected.HTTP request sent, awaiting response... 200 OKLength: 10698784 (10M) [application/octet-stream]Saving to: &#x27;20180416.db&#x27; 13% [======================&gt;                                       ] 1,480,482   57.6KB/s  eta 47s
说明:

Pod间网络访问也会受到Pod网络配置的控制
同时在测试过程中发现，刚开始测试时网络是很高的，但是3-5s后会降到被限制的网速

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-Nginx-Router替换默认Haproxy-Router</title>
    <url>/posts/8b2b11346a0d.html</url>
    <content><![CDATA[
什么是Router
Router在Openshift集群的一个不可非常重要的组件，它作为外部请求访问集群内部资源的入口，为Openshift上的应用提供边缘负载均衡。
Router可以为应用提供HTTP和websocket流量的负载均衡，同时支持HTTPS连接。Openshift上有一个特殊的资源叫Route，通过它可以方便地配置Router。
Openshift集群默认使用Haproxy应用作为Router的实现，它通过容器的形式运行在相应的Node上，同时Router Pod网络使用的宿主机的网络，即hostNetwork=true。
除了Haproxy,我们还可以使用Nginx来实现Router，这也是本文的重点。
不管是Haproxy还是Nginx方案都是使用了软件负载均衡器，还可以使用F5等硬件负载均衡器来替换Router，达到性能的提升。


Nginx Router与默认Router比较
如何替换Openshift默认Router卸载当前Router
用system:admin登录集群

$ oc login -u system:admin

选择default项目

$ oc project default

备份现有的Router

$ oc get -o yaml service/router dc/router clusterrolebinding/router-router-role serviceaccount/router &gt; default-router-backup.yaml

删除当前Router

$ oc delete -f default-router-backup.yaml
安装Nginx-Router使用镜像xhuaustc/nginx-openshift-router:1.15部署nginx router
$ oc adm router router --images=xhuaustc/nginx-openshift-router:1.15  --type=&#x27;&#x27; --selector=&#x27;node-role.kubernetes.io/infra=true&#x27;
添加Stub Status页面的查看权限在Router Pod所部署的结点开通1936端口的访问权限
$ sudo iptables -I OS_FIREWALL_ALLOW -p tcp -m tcp --dport 1936 -j ACCEPT 
在浏览器下查看以下链接$ROUTER_IP:1936/stub_status
添加Prometheus监控支持
运行如下命令，添加nginx exporter

$ oc patch dc/router -p &#x27;spec:  template:    spec:      containers:      - image: nginx/nginx-prometheus-exporter:0.1.0        name: nginx-prometheus-exporter        ports:        - name: prometheus          containerPort: 9113        args:          - -web.listen-address          - :9113          - -nginx.scrape-uri          - http://127.0.0.1:1936/stub_status&#x27;

在router service添加exporter想着的标注

$ oc annotate service router --overwrite prometheus.io/port=9113 prometheus.io/scrape=true
此时openshift集群中的prometheus就可以发现nginx exporter的数据
卸载Nginx Router和卸载之前默认的Router一样，只需要将对应的Object删除即可
$ oc delete service/router dc/router clusterrolebinding/router-router-role serviceaccount/router
Nginx Router性能测试测试环境： 

Openshift 3.11 网络插件为ovs-subnet
openshift集群物理环境配置为：3 master + 2 Router Node + 2 Computer Node。每台物理机的配置都是cpu 48核&#x2F;内存 384G 网卡为10Gbps
访问方式，在ab测试机器上，直接在&#x2F;etc&#x2F;hosts中绑定 应用域名 与 Router1的IP
应用为一个Nginx页面，页面大小为3.2KB，Pod数为10
Router节点系统配置

net.ipv4.tcp_max_syn_backlog = 65536net.core.netdev_max_backlog =  36768net.core.somaxconn = 36768 net.core.wmem_default = 8588608net.core.rmem_default = 8588608net.core.rmem_max = 16877216net.core.wmem_max = 16877216 net.ipv4.tcp_synack_retries = 2net.ipv4.tcp_syn_retries = 2 net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.tcp_max_orphans = 3376800net.ipv4.ip_local_port_range = 1024  65535

Router环境变量配置
WORKER_RLIMIT_NOFILE: 65535
KEEPALIVE_REQUESTS: 10000000
WORKER_PROCESSES: 8
WORKER_CPU_AFFINITY: 10000000  01000000  00100000  00010000  00001000  00000100  00000010  00000001 
ROUTER_MAX_CONNECTIONS: 80000


Route配置Annotations nginx.router.openshift.io/keepalive: 300

测试工具： wrk测试命令行：wrk  -t 40 -c 5000 -d 30s  http://xx.com/index.html



Router类型
Type
页面大小
RPS
超时数量




|Svc | 4KB | 58901| 3062&#x2F;2364808F5 | Router| 4KB | 29848 | 15854&#x2F;898446Haproxy | Router| 4KB | 32313| 3702&#x2F;972665Nginx | Router| 4KB | 33943| 3535&#x2F;1021704
|Svc | 500B | 222033 |  42&#x2F;6683188F5 | Router| 500B | 115435| 10829&#x2F;3473097Haproxy | Router| 500B | 48245| 7484&#x2F;1452092Nginx| Router| 500B | 166592| 2479&#x2F;5014470


测试结果说明
F5所在的测试环境与Haproxy&#x2F;Nginx Router的环境有些不一样，F5在使用千M网连接着集群，F5作部分参考
SVC实际为iptables作负载均衡，所以理论上性能是最好的，测试结果也验证了这一点
对于4KB的页面，Haproxy&#x2F;Nginx结果相关不大
对于500B的页面，Nginx的性能明显高于其它测试环境
就性能而言，Nginx相对于默认的Router确实有提高

后序还会对配置做进一步完善优化，到时对测试数据再作更新。说实话，对Nginx Router还是很期待的。
参考文献OpenShift Ecosystem: Implementing the NGINX Proxy Model on Red Hat OpenShiftIntroducing NGINX and NGINX Plus Routers for OpenShift Container Platform 3.10NGINX Plus Router for OpenShift nginxinc&#x2F;nginx-openshift-routernginx-openshift-router&#x2F;docs&#x2F;nginx-oss-router-install.md
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-Route证书HTTPS配置</title>
    <url>/posts/5a77edf030a8.html</url>
    <content><![CDATA[后台应用为http服务Termination Type:  EdgeInsecure Traffic:     Allow
后台应用为https服务Termination Type:  PassthroughInsecure Traffic:     None
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-Service不仅仅能负载Pod，它还能负载VM等外部服务</title>
    <url>/posts/7fa076a49782.html</url>
    <content><![CDATA[
Service是Openshift最核心的概念，它可以为一组具有相同功能的Pod提供一个统一的入口，并且将请求负载均衡分发到后端的各个Pod应用上。同时Service在集群下的IP是不变的，保证了请求的可靠性。
Service仅仅只能负载一组Pod吗？No。它还可以对接VM的服务，甚至是物理机的服务。
Service负载Pod



apiVersion: v1kind: Servicemetadata:  name: nginxspec:  selector:    app: nginx  type: NodePort  ports:    - port: 8080      name: nginx      protocol: TCP      targetPort: 8080      nodePort: 38080

selector：Label选择器，将选择指定Label的Pod作为被负载Pod
type：Service的类型，指定Service的访问方式，默认为ClusterIP。
ClusterIP：虚拟的服务IP地址，该IP可以被集群下的Pod访问
NodePort：使用宿主机的端口，通过访问任意Node的对应端口，就能访问Service的端口
LoadBalancer：使用外部负载均衡器完成到服务的负载分发，需要在.spec.status.loadBalancer指定外部负载均衡器的IP地址，同时定义nodePort和clusterIP。


port：Service监听的端口号
targetPort：需要转发到后端Pod的端口号
nodePort：当type&#x3D;NodePort时，指定映射到物理机的端口（范围30000-32767）
Service负载集群外部Service（可以是VM，物理机等）




创建一个无Label Selector的Service（即无法选择后端的Pod，系统不会自动创建Endpoint，需要手动创建Endpoint）

apiVersion: v1kind: Servicemetadata:  name: nginx-outspec:  ports:    - port: 80      name: nginx-out      protocol: TCP      targetPort: 8080  type: ClusterIP

创建Endpoint指向后端服务（该Endpoint的name必须与Service的name相同）

kind: EndpointsapiVersion: v1metadata:  name: nginx-outsubsets:  - addresses:      - ip: 192.168.0.6    ports:      - port: 8080
集群中的pod访问Service nginx-out:80，请求最终会被指向192.168.0.6:8080。

实战：将外部的Jenkins用Openshift的Service接管，并创建Route来访问Jenkins服务


Jenkins服务： 192.168.0.6:8080
route的Hostname：jenkins.apps.openshift.com


创建project jenkins

oc new-project jenkins

创建jenkinsvm Service

apiVersion: v1kind: Servicemetadata:  name: jenkinsvmspec:  ports:    - port: 8080      name: jenkinsvm      protocol: TCP      targetPort: 8080  type: ClusterIP

创建Endpoints

kind: EndpointsapiVersion: v1metadata:  name: nginx-outsubsets:  - addresses:      - ip: 192.168.0.6    ports:      - port: 8080

创建Route

apiVersion: v1kind: Routemetadata:  name: jenkinsspec:  host: jenkins.apps.openshift.com  port:     targetPort: jenkinsvm  to:    kind: Service    name: jenkinsvm    weight: 100

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-Web-Console自定义界面</title>
    <url>/posts/5b04eaf3f6ae.html</url>
    <content><![CDATA[
什么是Openshift WebConsole
Openshift WebConsole是Openshift提供的一个web界面端，类似到Kubernetes的Dashboard。很多操作都可以在WebConsole端去处理与显示。
部署完成Openshift后，会有一个openshift-web-console的project，在这个project中有一个openshift-web-console应用，该应用便是Openshift WebConsole。

自定义Openshift WebConsole原理Openshift WebConsole前端使用的是Google的AngularJS技术，并且它在设置的时候提供了扩展的入口。WebConsole在应用启动时会去读取configmap(webconsole-config)中的配置，而webconsole-config的配置中可以设置导入的自定义css与js文件。
......extensions:  properties: &#123;&#125;  scriptURLs:  - https://www.example.com/webconsole.js  stylesheetURLs:  - https://www.example.com/webconsole.css......
需要注意的是，自定义的css与js文件链接必须是https链接。在webconsole网站运行时，会去Load自定义的js与css，所有的自定义配置都设置在自定义的css与js文件中，达到最终希望达到的效果。
各类具体操作
修改icon logo

在css文件中添加如下内容
#header-logo &#123;  background-image: url(&quot;https://www.example.com/images/logo.png&quot;);  width: 300px;  height: 40px;&#125;

项目左侧导航目录汉化Project页面中左侧导航栏的显示是放在window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION这个全局变量中，对条目的汉化只需要在js中重新赋值即可

(function() &#123;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[0].label=&quot;概览&quot;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[1].label=&quot;应用&quot;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[2].label=&quot;构建&quot;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[3].label=&quot;资源&quot;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[4].label=&quot;存储&quot;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[5].label=&quot;监控&quot;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[6].label=&quot;商店&quot;&#125;());

项目左侧导航栏添加新目录


添加一个单独的menu菜单

(function() &#123;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION.push(&#123;	  label: &quot;Dashboard&quot;, 	  iconClass: &quot;fa fa-dashboard&quot;, 	  href: &quot;/dashboard&quot;	&#125;);&#125;());

添加一个带有子目录的菜单

(function() &#123;	window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION.splice(2, 0, &#123; 	  label: &quot;Git&quot;,	  iconClass: &quot;fa fa-code&quot;,	  secondaryNavSections: [ 	    &#123;	      items: [	        &#123;	          label: &quot;Branches&quot;,	          href: &quot;/git/branches&quot;,	          prefixes: [	            &quot;/git/branches/&quot;	          ]	        &#125;	      ]	    &#125;,	    &#123;	      header: &quot;Collaboration&quot;,	      items: [	        &#123;	          label: &quot;Pull Requests&quot;,	          href: &quot;/git/pull-requests&quot;,	          prefixes: [	            &quot;/git/pull-requests/&quot;	          ]	        &#125;	      ]	    &#125;	  ]	&#125;);&#125;());

顶部右侧导航栏添加新目录

右侧顶部栏的APP入口默认是隐藏的，给它赋值后
(function() &#123;	window.OPENSHIFT_CONSTANTS.APP_LAUNCHER_NAVIGATION = [	&#123;	  title: &quot;Sharing Videos&quot;,	  iconClass: &quot;fa fa-video-camera&quot;,	  href: &quot;https://yun.baidu.com/s/1xIwYILHQebEHZOcW4yvsAw&quot;,	  tooltip: &quot;一键部署Openshift相关视频&quot;	&#125;];&#125;());

Categories中给自定义模板添加图标


css文件中新建icon的样式

.icon-nexus3&#123;  background-image: url(https://www.example.com/nexus3.png);  width: 80px;  height: 80px;  background-size: 100% 100%;&#125;

在对应的模板文件中指定图标样式

apiVersion: v1kind: Templatelabels:  template: nexus3-templatemetadata:  name: nexus3  annotations:    description: Sonatype Nexus 3 template    tags: ci,nexus,jenkins    iconClass: icon-nexus3objects:  ......

设置特色应用导航页

什么是特色应用导航页，看图展示
我们可以把常用的一些特殊应用入口放在Catalog页面，方便管理与使用
(function() &#123;	window.OPENSHIFT_CONSTANTS.SAAS_OFFERINGS = [&#123;	  title: &quot;Dashboard&quot;,                         // The text label	  icon: &quot;fa fa-dashboard&quot;,                    // The icon you want to appear	  url: &quot;http://example.com/dashboard&quot;,        // Where to go when this item is clicked	  description: &quot;Open application dashboard.&quot;  // Short description	&#125;, &#123;	  title: &quot;System Status&quot;,	  icon: &quot;fa fa-heartbeat&quot;,	  url: &quot;http://example.com/status&quot;,	  description: &quot;View system alerts and outages.&quot;	&#125;, &#123;	  title: &quot;Manage Account&quot;,	  icon: &quot;pficon pficon-user&quot;,	  url: &quot;http://example.com/account&quot;,	  description: &quot;Update email address or password.&quot;	&#125;];&#125;());


修改登录页面


获取登录模板

oc adm create-login-template &gt; /etc/origin/master/login.html

保留默认登录模板form结构的前提下，修改&#x2F;etc&#x2F;origin&#x2F;master&#x2F;login.html页面
将login.html的路径添加到&#x2F;etc&#x2F;origin&#x2F;master&#x2F;master-config.yaml文件中

oauthConfig:  ...  templates:    login: /etc/origin/master/login.html

重启master api

master-restart api

浏览器访问页面


参考文章Openshit官方文档：自定义web console
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift-gitlab-redmine-testlink集成</title>
    <url>/posts/c982647841f2.html</url>
    <content><![CDATA[
Gitlab:使用Git作为代码管理工具，并在此基础上搭建起来的web服务。Redmine:项目管理和bug跟踪工具。TestLink:基于web的测试用例管理系统，主要功能是测试用例的创建、管理和执行，并且还提供了一些简单的统计功能。Redmine作为项目问题及bug收集汇总的地方，gitlab及testlink通过调用redmine的api进行对问题进行更新。那第它们之间是怎么配置的呢？下面就一起来配置下。

Redmine部署与插件安装相关配置在github项目：https://github.com/xhuaustc/redmine-openshift

Dockerfile中更新了docker-entrypoint.sh文件，更新了数据库的编码（mysql: utf8mb4 )
创建镜像


$ docker build -t harbor.local.com/public/redmine .$ docker push harbor.local.com/public/redmine

在openshift上创建新的项目

创建工程
$ oc new-project redmine
允许redmine项目中的pod使用root用户运行
创建工程
$ oc adm policy add-scc-to-user anyuid -z  default -n redmine
在新建的redmine工程中基于模板文件redmine-mysql-ephemeral.yaml在openshift上创建应用。注意：

需要更新redmine的Deployment中的template.spec.containers[0].image为刚刚自己创建的镜像harbor.local.com&#x2F;public&#x2F;redmine
创建应用时添加环境变量REDMINE_PLUGINS_MIGRATE为1
添加环境变量GITLAB_USERNAME及GITLAB_PASSWORD。这两个参数用于redmine的gitlab_callback插件拉取代码。

将git中的plugins目录文件导入到名字为redmine-file-data的PVC中
Testlink安装基础镜像php56：https://github.com/sclorg/s2i-php-container/tree/master/5.6
Testlink代码：https://github.com/xhuaustc/testlink-code.git
Mysql数据库：创建Data Stores(Mysql)
Gitlab（9.3.11）安装当前gitlab使用9.3.11版本，主要是因为9.4以上的gitlab版本与redmine集成有兼容问题。
对应的模板文件路径：https://gitlab.com/gitlab-org/omnibus-gitlab/blob/9.3.11+ce.0/docker/openshift-template.json
安装部署如Openshift上安装Gitlab
至此完成了gitlab+redmine+testlink的安装，接下来就是通过配置，使它们之间能够完成问题的管理。
Gitlab + redmine + Testlink集成一、准备环境创建项目：gitlab项目test， redmine项目test， testlink项目test二、设置redmine。1、管理 -&gt; 插件 -&gt; Redmine GitLab Hook plugin配置

2、开启api设置管理 -&gt; 配置 -&gt; API

3、版本库设置管理 -&gt; 配置 -&gt; 版本库

记录版本库管理网页服务API密钥：例如l1D2z8DJXiL4lFNSl2O4
三、gitlab配置
1、在test代码库中，Settings -&gt; Integrations
添加webhook：http://redmine-redmine.apps.local/gitlab_hook?project_id&#x3D;test&amp;key&#x3D;l1D2z8DJXiL4lFNSl2O4

2、开启Redmine:Settings -&gt; Integrations -&gt; Redmine
四、配置testlink
1、查看redmine API访问key点击我的账号，右边栏显示key
2、配置testlink的代码追踪设置testlink的语言为简体中文点击项目主页中的：缺陷跟踪系统管理添加一个redmine的追踪

其中apikey就是redmine下用户对应的Key.
通过以上集成配置后的世界是怎样的呢？
创建测试case


redmine上创建bug issue
在testlink上同步redmine的issue id
testlink将自动同步问题内容


开发人员提交代码


开发人员使用约定格式的commit msg提交代码
状态会自动同步redmine，并反映到testlink
测试人员根据testlink上的信息，对相关问题进行测试验证


testlink上添加评论


测试人员testlink上添加评论
redmine对应的issue下会自动同步评论

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift3-9上部署Prometheus+Grafana实现集群的监控与告警</title>
    <url>/posts/a45bfc874439.html</url>
    <content><![CDATA[Openshift从3.9版本开始官方支持Prometheus和Grafana
从官方部署脚本 openshift-ansible 中的目录playbooks下的列表 中可以看到相关部署脚本openshift-prometheusopenshift-grafana

既然官方支持，部署安装就十分简单了。
部署安装Prometheus
&#x2F;etc&#x2F;ansible&#x2F;hosts添加配置

[OSEv3:vars]openshift_hosted_prometheus_deploy=true

执行安装

ansible-playbook  playbooks/openshift-prometheus/config.yml

打开宿主机的9100端口（kubernetes-nodes-exporter）

ansible all -m shell -a &#x27;iptables -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 9100 -j ACCEPT; service iptables save&#x27;

完成

查看下资源状态

prometheus会默认部署在新创建的project中：openshift-metrics1个statefulset资源：prometheus1个DaemonSet资源：prometheus-node-exporter4个Service资源：alertmanager 、alerts、prometheus、prometheus-node-exporter3个route资源：alertmanager、alerts、prometheus3个ServiceAccount：prometheus、prometheus-node-exporter、prometheus-reader 

部署安装Grafana注意，3.9-release分支下的grafana安装脚本有问题，请使用最新master分支下的脚本进行安装

检查节点的label
oc get node -l node-role.kubernetes.io/infra=true
如果没有node-role.kubernetes.io&#x2F;infra&#x3D;true标签的Node节点的话，需要给Node配置标签

oc label node nodename1 node-role.kubernetes.io/infra=true


&#x2F;etc&#x2F;ansible&#x2F;hosts添加配置

[OSEv3:vars]grafana_namespace=openshift-grafanagrafana_user=grafanagrafana_password=grafanagrafana_datasource_name=&quot;default&quot;grafana_prometheus_namespace=&quot;openshift-metrics&quot;grafana_prometheus_sa=prometheusgrafana_node_exporter=falsegrafana_graph_granularity=&quot;2m&quot;

执行安装

ansible-playbook playbooks/openshift-grafana/config.yml

完成

查看下资源状态

1个deployment：grafana1个service: grafana1个route: grafana1个serviceaccount: grafana1个configmap:grafana-config 

最终效果图

补充：grafana admin密码丢失，如何重置？
$ grafana-cli admin reset-admin-password --homepath &quot;/usr/share/grafana&quot; admin
prometheus中配置内存使用率(node_memory_requests_total是我自己写的exporter监控数据)
sum(node_memory_requests_total) by(node_name) / sum(label_replace(machine_memory_bytes, &#x27;node_name&#x27;, &quot;$1&quot;, &quot;instance&quot;, &quot;(.*)&quot; )) by (node_name)
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift3-9部署手册</title>
    <url>/posts/1f94b61afe3b.html</url>
    <content><![CDATA[说明：本文主要介绍通过Ansible来部署Openshift 3.9
一、准备系统准备


节点类型
说明



Masters
物理主机或者虚拟机 系统：Fedora 21, CentOS 7.3, 7.4或者7.5 最少4vCPU最少16GB内存&#x2F;var&#x2F;最少40GB空间 &#x2F;usr&#x2F;local&#x2F;bin最少1GB空间容器临时目录最少1GB空间 &nbsp;


Nodes
物理主机或者虚拟机 系统：Fedora 21, CentOS 7.3, 7.4或者7.5 NetworkManager版本1.0以上最少1vCPU最少8GB内存&#x2F;var&#x2F;最少15GB空间 &#x2F;usr&#x2F;local&#x2F;bin最少1GB空间容器临时目录最少1GB空间 &nbsp;


额外的etcd节点
最少20GB用来存储etcd数据 &nbsp;


注：在安装时可以通过ansible_inventory的配置忽略以上系统要求扩展：对于生产部署时，Master的配置要求计算规则如下：每1000个pods需要额外的1核CPU和1.5GB内存。因此如果要满足支持2000个pods的话，Master节点需要在最低配置2核CPU和16GB内存的基础上再加2核CPU和3GB内存，共4核CPU 19GB内存。
安装准备
关闭防火墙及selinux
 systemctl disable firewalldsystemctl stop firewalldsed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/&quot; /etc/selinux/configsetenforce 0
更改yum源 base74 、 openshift-3.9 、 epel 、 updates 和 extras。
  #/etc/yum.repos.d/all.repo[base]name=CentOS-$releasever - Basebaseurl=http://mirrors.ustc.edu.cn/centos/$releasever/os/$basearch/gpgcheck=0[updates]name=CentOS-$releasever - Updatesbaseurl=http://mirrors.ustc.edu.cn/centos/$releasever/updates/$basearch/gpgcheck=0[extras]name=CentOS-$releasever - Extrasbaseurl=http://mirrors.ustc.edu.cn/centos/$releasever/extras/$basearch/gpgcheck=0[openshift-3.9]name=Openshift 3.9baseurl=http://mirrors.ustc.edu.cn/centos/$releasever/paas/$basearch/openshift-origin39/gpgcheck=0[epel]name=Centos EPELbaseurl=http://mirrors.ustc.edu.cn/epel/7/$basearch/gpgcheck=0
 清除缓存
 yum makecache

二、安装
安装需要的软件包
 yum install vim git ansible wget java-1.8.0-openjdk httpd-tools python-passlib docker -y
下载openshift ansible部署脚本
 git clone https://github.com/openshift/openshift-ansible.git -b release-3.9
禁用ansible脚本中的指定repo
 sed -i &#x27;s/enabled=1/enabled=0/g&#x27; ./roles/openshift_repos/templates/CentOS-OpenShift-Origin.repo.j2
设置hostsname a. 在 &#x2F;etc&#x2F;hosts 添加ip映射
# /etc/hosts
192.168.2.3 openshift

 b. 更新本机hostname
 hostnamectl set-hostname --static openshift


设置本地ssh无密钥登录
 ssh-keygen -t rsa
 ssh-copy-id -i ~/.ssh/id_rsa.pub root@openshift
 #或将id_rsa.pub内容添加到~/.ssh/authorized_keys中


配置ansible hosts
   [OSEv3:children]mastersnodesetcdnfs[OSEv3:vars]ansible_ssh_user=rootopenshift_deployment_type=origindeployment_type=originopenshift_release=v3.9  #如果使用自己的镜像的话/etc/sysconfig/docker中会添加ADD_REGISTRY=&#x27;--add-registry harbor.apps.com&#x27;#oreg_url=harbor.apps.com/openshift/origin-$&#123;component&#125;:$&#123;version&#125;#system_images_registry=harbor.apps.com#openshift_examples_modify_imagestreams=true#openshift_docker_additional_registries=harbor.apps.com#openshift_service_catalog_image_prefix=harbor.apps.com/openshift/origin-#openshift_metrics_image_prefix=harbor.apps.com/openshift/origin-#openshift_logging_image_prefix=harbor.apps.com/openshift/origin-#ansible_service_broker_image_prefix=harbor.apps.com/openshift/origin-#ansible_service_broker_etcd_image_prefix=harbor.apps.com/openshift/origin-#openshift_metrics_image_version=v3.9   openshift_enable_service_catalog=falsetemplate_service_broker_install=falseansible_service_broker_install=falseopenshift_master_identity_providers=[&#123;&#x27;name&#x27;: &#x27;htpasswd_auth&#x27;, &#x27;login&#x27;:&#x27;true&#x27;,&#x27;challenge&#x27;: &#x27;true&#x27;, &#x27;kind&#x27;: &#x27;HTPasswdPasswordIdentityProvider&#x27;&#125;]openshift_master_htpasswd_file=/etc/origin/master/htpasswdopenshift_enable_unsupported_configurations=Trueopenshift_docker_options=&quot;-l warn --ipv6=false --insecure-registry=0.0.0.0/0 --registry-mirror=https://docker.mirrors.ustc.edu.cn --log-opt max-size=1M --log-opt max-file=3&quot;openshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storageopenshift_master_default_subdomain=apps.openshiftopenshift_metrics_install_metrics=trueopenshift_hosted_metrics_public_url=https://hawkular-metrics.apps.openshift/hawkular/metricsopenshift_logging_install_logging=trueopenshift_hosted_etcd_storage_kind=nfsopenshift_hosted_etcd_storage_nfs_options=&quot;*(rw,root_squash,sync,no_wdelay)&quot;openshift_hosted_etcd_storage_nfs_directory=/nfs-data openshift_hosted_etcd_storage_volume_name=etcd-vol2 openshift_hosted_etcd_storage_access_modes=[&quot;ReadWriteOnce&quot;]openshift_hosted_etcd_storage_volume_size=1Gopenshift_hosted_etcd_storage_labels=&#123;&#x27;storage&#x27;: &#x27;etcd&#x27;&#125;ansible_service_broker_image_prefix=registry.access.redhat.com/openshift3/ose-ansible_service_broker_registry_url=registry.access.redhat.comansible_service_broker_registry_user=&lt;user_name&gt;ansible_service_broker_registry_password=&lt;password&gt;ansible_service_broker_registry_organization=&lt;organization&gt;[masters]openshift[etcd]openshift[nfs]openshift[nodes]openshift openshift_node_labels=&quot;&#123;&#x27;region&#x27;: &#x27;infra&#x27;, &#x27;zone&#x27;:&#x27;default&#x27;&#125;&quot; openshift_schedulable=true
如果要修改为自己的镜像仓库的话，还需要更改几个yaml文件
 # roles/openshift_web_console/defaults/main.yml（去掉docker.io/）openshift_web_console_image_dict:    origin:        prefix: &quot;openshift/origin-&quot;        version: &quot;&#123;&#123; openshift_image_tag &#125;&#125;&quot;        image_name: &quot;web-console&quot;
执行安装脚本
   ansible-playbook playbooks/prerequisites.ymlansible-playbook playbooks/deploy_cluster.yml
创建管理员账号
   htpasswd -b /etc/origin/master/htpasswd admin adminoc adm policy add-cluster-role-to-user cluster-admin admin

三、展示


参考文章 Openshift 3.9官方高级安装手册
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift上使用shell-operator创建自己的operator</title>
    <url>/posts/b35eec06e235.html</url>
    <content><![CDATA[shell operator是由Falnt公司开发并开源的。Flant公司是一家致力于提供解决所有基础设施问题的解决方案的公司。他们自称是提供DevOps-as-a-Service的服务。
需求
一个简单的任务：监听项目创建与删除事件，并发出告警。
一种简单的解决方法：定时脚本cron，每隔一段时间（如1min)获取对Openshift集群的所有项目，并将它与上次获取的结果值进行比较，得到新创建的项目及删除的项目该方法的缺点：
不及时
性能差，很多时候并没有操作项目，但仍然需要不断执行脚本
如果1min内即创建了新项目，又把这个项目删除了，则无法监测到


另一种解决方法：事件驱动，即订阅来自Kubernetes对象的事件，如果有对Project操作就触发告警。很明显这种方法解决了定时任务的所有问题。
该很么做呢？会不会非常复杂。不会。使用shell-operator项目就可以非常简单地实现。shell-operator项目地址：https://github.com/flant/shell-operator

  
实现部骤
创建shell-operator项目

$ oc new-project shell-operator

为该项目创建serviceAccount monitor-namespaces-acc，将给它授予获取全局namespace的权限

$ cat &lt;&lt; EOF | oc create -f ----apiVersion: v1kind: ServiceAccountmetadata:  name: monitor-namespaces-acc---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  name: monitor-namespacesrules:- apiGroups: [&quot;&quot;]  resources: [&quot;namespaces&quot;]  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]EOF$ oc adm policy add-cluster-role-to-user monitor-namespaces -z monitor-namespaces-acc

创建一个configmap，其中data中的内容为hook脚本

$ cat &lt;&lt; EOF | oc create -f -apiVersion: v1data:  shell-hook.sh: |-    #!/usr/bin/env bash    if [[ $1 == &quot;--config&quot; ]] ; then      cat &lt;&lt;EOF      &#123;&quot;onKubernetesEvent&quot;:[        &#123;          &quot;name&quot;:&quot;OnCreateDeleteNamespace&quot;,          &quot;kind&quot;: &quot;namespace&quot;,          &quot;event&quot;:[&quot;add&quot;, &quot;delete&quot;]        &#125;,        &#123;          &quot;name&quot;:&quot;OnModifiedNamespace&quot;,          &quot;kind&quot;: &quot;namespace&quot;,          &quot;event&quot;:[&quot;update&quot;],          &quot;jqFilter&quot;: &quot;.metadata.labels&quot;        &#125;        ]      &#125;    EOF    else      bindingName=$(jq -r &#x27;.[0].binding&#x27; $BINDING_CONTEXT_PATH)      resourceEvent=$(jq -r &#x27;.[0].resourceEvent&#x27; $BINDING_CONTEXT_PATH)      resourceName=$(jq -r &#x27;.[0].resourceName&#x27; $BINDING_CONTEXT_PATH)      if [[ $bindingName == &quot;OnModifiedNamespace&quot; ]] ; then        echo &quot;Namespace $resourceName labels were modified&quot;      else        if [[ $resourceEvent == &quot;add&quot; ]] ; then          echo &quot;Namespace $resourceName was created&quot;        else          echo &quot;Namespace $resourceName was deleted&quot;        fi      fi    fikind: ConfigMapmetadata:  name: hooksEOF

运行shell-operator应用

$ cat &lt;&lt; EOF | oc create -f -apiVersion: apps.openshift.io/v1kind: DeploymentConfigmetadata:  labels:    run: shell-operator  name: shell-operatorspec:  replicas: 1  selector:    run: shell-operator  template:    metadata:      labels:        run: shell-operator    spec:      serviceAccount: monitor-namespaces-acc      containers:        - image: &#x27;flant/shell-operator:latest-alpine3.9&#x27;          imagePullPolicy: IfNotPresent          name: shell-operator          volumeMounts:            - mountPath: /hooks              name: hooks-no934      volumes:        - configMap:            defaultMode: 511            name: hooks          name: hooks-no934  triggers:    - type: ConfigChangeEOF
说明：

应用启动使用monitor-namespaces-accserviceAccount
将configmap内容挂载到deployment应用的&#x2F;hooks目录中
挂载文件需要给可执行权限defaultMode: 511

  
验证
创建一个project&#x2F;删除该project

$ oc new-project  operator-test$ oc delete project operator-test

查看shell-operator的日志

  
额外补充Shell Operator支持绑定三种hook触发类型

onStartuponStartup类型只有一个参数：”onStartup”设置绑定顺序

&#123;&quot;onStartup&quot;:10&#125;

scheduleschedule绑定用于周期性运行，支持秒级粒度定义计划

&#123;  &quot;schedule&quot;: [   &#123;&quot;name&quot;:&quot;every 10 min&quot;,    &quot;crontab&quot;:&quot;0 */10 * * * *&quot;,    &quot;allowFailure&quot;:true   &#125;,   &#123;&quot;name&quot;:&quot;Every Monday at 8:05&quot;,    &quot;crontab&quot;:&quot;0 5 8 * * 1&quot;    &#125;  ]&#125;

onKubernetesEvent监听Kubernetes事件促发

&#123;  &quot;onKubernetesEvent&quot;: [  &#123;&quot;name&quot;:&quot;Execute on changes of namespace labels&quot;,   &quot;kind&quot;: &quot;namespace&quot;,   &quot;event&quot;:[&quot;update&quot;],   &quot;jqFilter&quot;:&quot;.metadata.labels&quot;  &#125;]&#125;
将镜像带的kubectl命令行替换成oc命令行$ cat Dockerfile-ocFROM ubuntu:18.04ADD ./oc /bin/ocADD ./shell-operator /RUN apt-get update &amp;&amp; \    apt-get install -y ca-certificates wget jq &amp;&amp; \    rm -rf /var/lib/apt/lists &amp;&amp; \    chmod +x /bin/oc &amp;&amp; \    mkdir /hooksWORKDIR /ENV SHELL_OPERATOR_WORKING_DIR /hooksENTRYPOINT [&quot;/shell-operator&quot;]CMD [&quot;start&quot;]$ docker build -f Dockerfile-oc -t docker.io/xhuaustc/shell-operator-oc:latest-3.11 .
其中oc从镜像openshift&#x2F;origin-cli中导出，而shell-operator从镜像flant&#x2F;shell-operator中导出最终镜像保存在：docker.io&#x2F;xhuaustc&#x2F;shell-operator-oc:latest-3.11
总结
以上是使用configmap的方式向operator-shell注入自定义的钩子代码，非常灵活，最原始的operator-shell就能够满足各种不种的需求，十分方便。
shell-operator项目为我们自定义operator提供了一种非常便利的方式。同时它不仅仅支持bash，也可以支持python，需要在镜像中安装python包。

$ cat DockerfileFROM flant/shell-operator:latestRUN apk --no-cache add python
钩子代码的环境使用python
#!/usr/bin/env python

有了这个监控后，就可以非常方便地对Openshift&#x2F;Kubernetes的资源进行控制，想像空间可以很大。
例子：有些项目的应用创建有先后关系，就可以方便地使用shell operator进行编排
例子：不同项目 dev/sit/uat 对不同的用户组 dev/test/ops 会有不同的权限 view/admin/image-puller ，就可以使用项目名格式给不同用户组授予不同的权限


与CRD结合，真正构建自己的operator，想像空间就变得更大了
shell-operator项目地址：https://github.com/flant/shell-operator

参考文章Shell-operator：用于简化Kubernetes operator的创建
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift上搭建Jenkins-Gitlab-Sonarqube自动构建</title>
    <url>/posts/84360199a57a.html</url>
    <content><![CDATA[#Jenkins系统配置详情
安装插件列表：Git Parameter Plug-In
Gitlab Hook Plugin
SonarQube Scanner for Jenkins
JaCoCo plugin
Maven Integration plugin
Jenkins配置详情：Jenkins-&gt;系统管理-&gt;系统配置
SonarQube servers配置

Name:  SonarqubeServer
Server URL: http://sonarqube-sonarqube.apps.test.openshift.com
Server version: 5.3 or higher
Server authentication token: $TOKEN(从SonarQube Server上创建）
Jenkins-&gt;系统管理-&gt;Global Tool Configuration
Maven Configuration

Default settings provider: Use default maven settings
Default global settings provider: Global settings file on filesystem
File Path: &#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;maven&#x2F;settings.xml

JDK配置

JDK别名：jdk8
JAVA_HOME:  &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-1.8.0

SonarQube Scanner配置

#Openshift+Gitlab+Jenkins实现自动构建（区分分支）主要知识点：
1、jenkins的插件gitlab Hook plugin
2、gitlab的WebHook配置
1、在Jenkins上安装Gitlab Hook Plugin插件。
插件所在网址：https://github.com/elvanja/jenkins-gitlab-hook-plugin2、gitlab上配置WebHook
在gitlab project中进入Webhooks配置，在URL栏填如下格式的链接即可：”http://your_server:port/gitlab/build_now/$job_name“
在sonarqube上创建新的project进行代码扫描在jenkins的job的构建中添加sona scanner的配置。执行jenkins的job后就可以在sonar网站上看到代码的扫描结果。如下图：
SonarQube Scanner Name:  SonarScanner
自动安装： True
Install from maven Central:  SonarQube Scanner 3.0.3.778

Maven配置

Maven Name:  MbcloudMaven
自动安装： True
Install From Apache:  3.5.2
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift上搭建NFS-StorageClass，别再说存储使用不方便</title>
    <url>/posts/2dea11d3dfea.html</url>
    <content><![CDATA[动态存储是什么Openshift持久化存储（PV）有两种，一种是静态的，另一种是动态。

静态存储：需要管理员手动创建PV，供PVC挂载调用
动态存储：通过一个称作 Storage Class的对象由存储系统根据PVC的要求自动创建。

StorageClass是什么
StorageClass是Openshfit中的一个资源对象，它主要用于描述请求的存储，并提供按需传递动态预配置存储的参数的方法。 
StorageClass对象还可以用作控制不同级别的存储和对存储的访问的管理机制。
有了StorageClass后，管理员无需手动创建PV。Openshift的使用者在创建PVC时只需要指定StorageClass,会自动按照对应的StorageClass配置，调用对应的Dynamic provisioning来创建需要的存储

没有StorageClass时代，如何使用NFS每次需要手动创建PV，一句话：麻烦。
StorageClass时代来了一次配置，永久自动，无需手动创建PV，一句话：方便。
NFS Provisioner原理  

新建PVC时，指定为默认驱动，或者指定storageclass为nfs storage
运行nfs client provisioner的pod会根据配置，在共享的NFS目录下创建新的文件夹，同时创建新的PV指向该文件夹
将新建的PVC与2中新建的PV关联，完成PVC的创建
该PVC就可以被调用的Pod挂载了。

NFS StorageClass具体配置步骤
准备NFS服务

$ yum install nfs -y$ mkdir -p /nfsdata/share$ chown nfsnobody:nfsnobody /nfsdata/share$ chmod 700 /nfsdata/share$ #开放nfs访问的端口$ iptables -A INPUT -p tcp --dport 111 -j ACCEPT$ iptables -A INPUT -p udp --dport 111 -j ACCEPT$ iptables -A INPUT -p tcp --dport 2049 -j ACCEPT$ iptables -A INPUT -p udp --dport 2049 -j ACCEPT$ # 配置NFS$ echo &quot;/nfsdata/share *(rw,async,no_root_squash)&quot; &gt;&gt; /etc/exports$ exportfs -a #加载共享目录配置$ showmount -e #查看当前可用的共享目录$ # 启动NFS$ systemctl restart nfs

确定Provisioner安装的project（默认为default）如果使用default project的话

$ oc project default
如果希望将它部署在自定义的project中，则新建project
$ oc new-project nfs-provisoner

如果安装的project不是default的话，需要更改配置rbac.yaml,再设置权限

$ cat rbac.yamlkind: ServiceAccountapiVersion: v1metadata:  name: nfs-client-provisioner---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: nfs-client-provisioner-runnerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumeclaims&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]  - apiGroups: [&quot;storage.k8s.io&quot;]    resources: [&quot;storageclasses&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;events&quot;]    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: run-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: defaultroleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisionerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;endpoints&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    # replace with namespace where provisioner is deployed    namespace: defaultroleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io$ NAMESPACE=`oc project -q`$ sed -i&#x27;&#x27; &quot;s/namespace:.*/namespace: $NAMESPACE/g&quot; ./deploy/rbac.yaml$ oc create -f deploy/rbac.yaml$ oc adm policy add-scc-to-user hostmount-anyuid system:serviceaccount:$NAMESPACE:nfs-client-provisioner

更新deploy&#x2F;deployment.yaml，设置NFS Server的配置

$ cat &lt;&lt; EOF | oc create -f -kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: nfs-client-provisionerspec:  replicas: 1  strategy:    type: Recreate  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: docker.io/xhuaustc/nfs-client-provisioner:latest          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              value: fuseim.pri/ifs            - name: NFS_SERVER              value: &lt;YOUR NFS SERVER HOSTNAME&gt;            - name: NFS_PATH              value: /nfsdata/share      volumes:        - name: nfs-client-root          nfs:            server: &lt;YOUR NFS SERVER HOSTNAME&gt;            path: /nfsdata/shareEOF

创建storageclass

$ cat &lt;&lt; EOF | oc create -f -apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: managed-nfs-storage  annotations:    storageclass.kubernetes.io/is-default-class: &quot;true&quot; # 设置该storageclass为PVC创建时默认使用的存储机制provisioner: fuseim.pri/ifs # 匹配deployment中的环境变量&#x27;PROVISIONER_NAME&#x27;parameters:  archiveOnDelete: &quot;true&quot; # &quot;false&quot; 删除PVC时不会保留数据，&quot;true&quot;将保留PVC数据reclaimPolicy: DeleteEOF

NFS StorageClass使用
创建PVC

$ cat &lt;&lt; EOF | oc create -f -apiVersion: v1kind: PersistentVolumeClaimmetadata:  annotations:    volume.beta.kubernetes.io/storage-class: managed-nfs-storage    volume.beta.kubernetes.io/storage-provisioner: fuseim.pri/ifs  name: testpvcspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 1GiEOF
如果storageclass中设置了storageclass.kubernetes.io/is-default-class: &quot;true&quot; ，可以更简单地创建PVC
$ cat &lt;&lt; EOF | oc create -f -apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: hello-pvcspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 1GiEOF

查看PVC

$ oc get pvNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                            STORAGECLASS          REASON    AGEpvc-fb952566-4bed-11e9-9007-525400ad3b43   1Gi        RWO            Delete           Bound      test/hello-pvc                 managed-nfs-storage             5m$ oc get pvchello-pvc   Bound     pvc-fb952566-4bed-11e9-9007-525400ad3b43   1Gi        RWO            managed-nfs-storage   4m

如果storageclass中设置了archiveOnDelete: &quot;true&quot; ，在删除PVC时，会将数据目录归档

$ ls /nfsdata/sharetest-hello-pvc-pvc-fb952566-4bed-11e9-9007-525400ad3b43$ oc delete pvc hello-pvc$ ls /nfsdata/sharearchived-test-hello-pvc-pvc-fb952566-4bed-11e9-9007-525400ad3b43$ #数据目录被改名为以archived开头的文件夹，同时删除了对应的PV和PVC
总结有了NFS StorageClass后，创建存储就非常简单方便了。Openshift NFS动态存储代码 https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client
引用自：https://mp.weixin.qq.com/s/HgDCDgYjkX5en7ORNeG0yA
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift上部署OpenLDAP实战：为账号一统</title>
    <url>/posts/4362c292014b.html</url>
    <content><![CDATA[  
Openshift上部署OpenLDAP也并不仅仅只是为了部署，更重要的是要用它来为各种应用统一账号管理。比如GitLab，Gogs, Nexus, Sonarqube等等，同时结合之前的Openshift使用OpenLDAP做账号管理。理想的世界是，一套Openshift，各种应用，它们的用户认证都在一套OpenLDAP上，同时这套OpenLDAP也是非常方便地部署在Openshift上。
为账号一统的第一步，就是我们需要部署OpenLDAP，这也是该篇主题。以下是之前分享的几篇与OpenLDAP有关的文章。
CentOS上搭建双主高可用OpenLDAP ServerOpenshift使用OpenLDAP作为统一用户认证CentOS上OpenLDAP Server使用cn&#x3D;config方式配置
之前分享的与Openldap相关的几篇文章中，它的部署都是在虚拟机中进行的。那么在Openshift上能否完成部署呢，如果可以的话，那么那些复杂的部署我们都可以将它封装到镜像中去，岂不是更方便？那就一起来操作吧。
应用要求点
方便自定义域及管理员用户名与密码 通过环境变量及secret进行自定义配置
对pod添加健康检查，如果发现问题，主动重启恢复监听389端口
数据持久化，pod重启后，该pod自动恢复数据持久化目录为：数据目录 /var/lib/ldap, 配置目录  /etc/openldap

以上是在Openshift运行Openldap的最基本的要求了，如果更苛刻点，需要添加Openldap的监控、Openldap的日志审计、多pod高可用方案等。
制作镜像如果不希望自己构建，可以直接使用构建好的镜像：docker.io&#x2F;xhuaustc&#x2F;openldap-2441-centos7:latest步骤如下：

基础镜像centos:centos7
安装openldap相关应用：openldap, openldap-clients, openldap-servers
部署：根据环境变量更新openldap的配置文件，并初始化基础结构，最后启动openldap 运行：启动openldap

说明,部署与运行的区别在于该应用是否是第一次启动。如果是第一次启动的话，相关配置还未更新。在第一次配置更新后，在持久化文件夹中创建一个新文件，来标记已完成配置更新。以后的每次启动都会检查该文件，如果存在，则不作初始化操作，而直接运行openldap
对应的Dockerfile参考：https://github.com/openshift/openldap/blob/master/2.4.41/Dockerfile
FROM centos:centos7# OpenLDAP server image for OpenShift Origin## Volumes:# * /var/lib/ldap/data - Datastore for OpenLDAP# * /etc/openldap/     - Config directory for slapd# Environment:# * $OPENLDAP_ADMIN_PASSWORD         - OpenLDAP administrator password# * $OPENLDAP_DEBUG_LEVEL (Optional) - OpenLDAP debugging level, defaults to 256MAINTAINER Steve Kuznetsov &lt;skuznets@redhat.com&gt;LABEL io.k8s.description=&quot;OpenLDAP is an open source implementation of the Lightweight Directory Access Protocol.&quot; \      io.k8s.display-name=&quot;OpenLDAP 2.4.41&quot; \      io.openshift.expose-services=&quot;389:ldap,636:ldaps&quot; \      io.openshift.tags=&quot;directory,ldap,openldap,openldap2441&quot; \      io.openshift.non-scalable=&quot;true&quot;# Add defaults for configCOPY ./contrib/config /opt/openshift/configCOPY ./contrib/lib /opt/openshift/lib# Add startup scriptsCOPY run-*.sh /usr/local/bin/COPY contrib/*.ldif /usr/local/etc/openldap/COPY contrib/*.schema /usr/local/etc/openldap/COPY contrib/DB_CONFIG /usr/local/etc/openldap/# Install OpenLDAP Server, give it permissionst to bind to low portsRUN yum install -y openldap openldap-servers openldap-clients &amp;&amp; \    yum clean all -y &amp;&amp; \    setcap &#x27;cap_net_bind_service=+ep&#x27; /usr/sbin/slapd &amp;&amp; \    mkdir -p /var/lib/ldap &amp;&amp; \    chmod a+rwx -R /var/lib/ldap &amp;&amp; \    mkdir -p /etc/openldap &amp;&amp; \    chmod a+rwx -R /etc/openldap &amp;&amp; \    mkdir -p /var/run/openldap &amp;&amp; \    chmod a+rwx -R /var/run/openldap &amp;&amp; \    chmod -R a+rw /opt/openshift # Set OpenLDAP data and config directories in a data volumeVOLUME [&quot;/var/lib/ldap&quot;, &quot;/etc/openldap&quot;]# Expose default ports for ldap and ldapsEXPOSE 389 636CMD [&quot;/usr/local/bin/run-openldap.sh&quot;]
对应的部署及运行脚本参考：https://github.com/openshift/openldap/blob/master/2.4.41/run-openldap.sh但是有所修改，如果用root用户启动时，会先初始化openldap配置。原来的脚本如果用root用户启动时，会报错，而用非root用户启动，则自定义配置无法生效。
#!/bin/bash# Reduce maximum number of number of open file descriptors to 1024# otherwise slapd consumes two orders of magnitude more of RAM# see https://github.com/docker/docker/issues/8231ulimit -n 1024OPENLDAP_ROOT_PASSWORD=$&#123;OPENLDAP_ROOT_PASSWORD:-admin&#125;OPENLDAP_ROOT_DN_PREFIX=$&#123;OPENLDAP_ROOT_DN_PREFIX:-&#x27;cn=Manager&#x27;&#125;OPENLDAP_ROOT_DN_SUFFIX=$&#123;OPENLDAP_ROOT_DN_SUFFIX:-&#x27;dc=example,dc=com&#x27;&#125;OPENLDAP_DEBUG_LEVEL=$&#123;OPENLDAP_DEBUG_LEVEL:-256&#125;# Only run if no config has happened fully beforeif [ ! -f /etc/openldap/CONFIGURED ]; then    user=`id | grep -Po &quot;(?&lt;=uid=)\d+&quot;`    if (( user == 0 ))    then        # We are root, we can use user input!        # Bring in default databse config        cp /usr/local/etc/openldap/DB_CONFIG /var/lib/ldap/DB_CONFIG        if [ -f /opt/openshift/config/slapd.d/cn\=config/olcDatabase\=\&#123;0\&#125;config.ldif ]        then            # Use provided default config, get rid of current data            rm -rf /var/lib/ldap/*            rm -rf /etc/openldap/*            # Bring in associated default database files            mv -f /opt/openshift/lib/* /var/lib/ldap            mv -f /opt/openshift/config/* /etc/openldap        else            # Something has gone wrong with our image build            echo &quot;FAILURE: Default configuration files from /contrib/ are not present in the image at /opt/openshift.&quot;            exit 1        fi        # start the daemon in another process and make config changes        slapd -h &quot;ldap:/// ldaps:/// ldapi:///&quot; -d $OPENLDAP_DEBUG_LEVEL &amp;        for ((i=30; i&gt;0; i--))        do            ping_result=`ldapsearch 2&gt;&amp;1 | grep &quot;Can.t contact LDAP server&quot;`            if [ -z &quot;$ping_result&quot; ]            then                break            fi            sleep 1        done        if [ $i -eq 0 ]        then            echo &quot;slapd did not start correctly&quot;            exit 1        fi        # Generate hash of password        OPENLDAP_ROOT_PASSWORD_HASH=$(slappasswd -s &quot;$&#123;OPENLDAP_ROOT_PASSWORD&#125;&quot;)        # Update configuration with root password, root DN, and root suffix        sed -e &quot;s OPENLDAP_ROOT_PASSWORD $&#123;OPENLDAP_ROOT_PASSWORD_HASH&#125; g&quot; \            -e &quot;s OPENLDAP_ROOT_DN $&#123;OPENLDAP_ROOT_DN_PREFIX&#125; g&quot; \            -e &quot;s OPENLDAP_SUFFIX $&#123;OPENLDAP_ROOT_DN_SUFFIX&#125; g&quot; /usr/local/etc/openldap/first_config.ldif |            ldapmodify -Y EXTERNAL -H ldapi:/// -d $OPENLDAP_DEBUG_LEVEL        # add test schema        ldapadd -Y EXTERNAL -H ldapi:/// -f /usr/local/etc/openldap/testPerson.ldif -d $OPENLDAP_DEBUG_LEVEL        # add useful schemas        ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldif -d $OPENLDAP_DEBUG_LEVEL        ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldif -d $OPENLDAP_DEBUG_LEVEL        # load memberOf and refint modules        ldapadd -Y EXTERNAL -H ldapi:/// -f /usr/local/etc/openldap/load_modules.ldif -d $OPENLDAP_DEBUG_LEVEL        # configure memberOf module        ldapadd -Y EXTERNAL -H ldapi:/// -f /usr/local/etc/openldap/configure_memberof.ldif -d $OPENLDAP_DEBUG_LEVEL        # configure refint module        ldapadd -Y EXTERNAL -H ldapi:/// -f /usr/local/etc/openldap/configure_refint.ldif -d $OPENLDAP_DEBUG_LEVEL        # extract dc name from root DN suffix        dc_name=$(echo &quot;$&#123;OPENLDAP_ROOT_DN_SUFFIX&#125;&quot; | grep -Po &quot;(?&lt;=^dc\=)[\w\d]+&quot;)        # create base organization object        sed -e &quot;s OPENLDAP_SUFFIX $&#123;OPENLDAP_ROOT_DN_SUFFIX&#125; g&quot; \            -e &quot;s FIRST_PART $&#123;dc_name&#125; g&quot; \            usr/local/etc/openldap/base.ldif |            ldapadd -x -D &quot;$OPENLDAP_ROOT_DN_PREFIX,$OPENLDAP_ROOT_DN_SUFFIX&quot; -w &quot;$OPENLDAP_ROOT_PASSWORD&quot;        # stop the daemon        pid=$(ps -A | grep slapd | awk &#x27;&#123;print $1&#125;&#x27;)        kill -2 $pid || echo $?        # ensure the daemon stopped        for ((i=30; i&gt;0; i--))        do            exists=$(ps -A | grep $pid)            if [ -z &quot;$&#123;exists&#125;&quot; ]            then                break            fi            sleep 1        done        if [ $i -eq 0 ]        then            echo &quot;slapd did not stop correctly&quot;            exit 1        fi    else        # We are not root, we need to populate from the default bind-mount source        if [ -f /opt/openshift/config/slapd.d/cn\=config/olcDatabase\=\&#123;0\&#125;config.ldif ]        then            # Use provided default config, get rid of current data            rm -rf /var/lib/ldap/*            rm -rf /etc/openldap/*            # Bring in associated default database files            mv -f /opt/openshift/lib/* /var/lib/ldap            mv -f /opt/openshift/config/* /etc/openldap        else            # Something has gone wrong with our image build            echo &quot;FAILURE: Default configuration files from /contrib/ are not present in the image at /opt/openshift.&quot;            exit 1        fi    fi    # Test configuration files, log checksum errors. Errors may be tolerated and repaired by slapd so don&#x27;t exit    LOG=`slaptest 2&gt;&amp;1`    CHECKSUM_ERR=$(echo &quot;$&#123;LOG&#125;&quot; | grep -Po &quot;(?&lt;=ldif_read_file: checksum error on \&quot;).+(?=\&quot;)&quot;)    for err in $CHECKSUM_ERR    do        echo &quot;The file $&#123;err&#125; has a checksum error. Ensure that this file is not edited manually, or re-calculate the checksum.&quot;    done    rm -rf /opt/openshift/*    touch /etc/openldap/CONFIGUREDfi# Start the slapd serviceexec slapd -h &quot;ldap:/// ldaps:///&quot; -d $OPENLDAP_DEBUG_LEVEL
说明，本来我是准备按照之前部署经验自己写一个镜像构建，但是写到一半的时候，发现整个逻辑与openshift官方提供的差不多，所以就直接用官方的了。从脚本中可以看到管理员(一般为cn&#x3D;Manager)密码数据库管理员(一般为cn&#x3D;config)密码是一样的，都是通过环境变量OPENLDAP_ROOT_PASSWORD配置。



变量名
说明
默认值



OPENLDAP_ROOT_PASSWORD
OpenLDAP olcRootPW 密码
admin


OPENLDAP_ROOT_DN_SUFFIX
OpenLDAP olcSuffix 域前缀
dc&#x3D;example,dc&#x3D;com


OPENLDAP_ROOT_DN_PREFIX
OpenLDAP olcRootDN 前缀
cn&#x3D;Manager


OPENLDAP_DEBUG_LEVEL
OpenLDAP服务日志级别
256


使用openshift&#x2F;openldap进行构建镜像操作
$ git clone https://github.com/openshift/openldap.git$ cd openldap$ export SKIP_SQUASH=1 &amp;&amp; make build    #构建出的镜像名为：openshift/openldap-2441-centos7:latest
部署Openldap两种方法实现部署：第一种. 使用openshift的client工具部署
$ #创建项目$ oc new-project openldap --display-name=OpenLDAP $ #创建单独的serviceaccount为方便以root用户启动pod$ oc create serviceaccount openldap  $  #为openldap serviceaccount赋予以root启动pod的权限$ oc adm policy add-scc-to-user anyuid -z openldap$ #创建secret，设置openldap的域与密码$ oc create secret generic openldap --from-literal=OPENLDAP_ROOT_PASSWORD=admin --from-literal=OPENLDAP_ROOT_DN_SUFFIX=dc=fcloudy,dc=com $ # 部署应用（默认会是以default serviceaccount运行）$ oc new-app  --docker-image=xhuaustc/openldap-2441-centos7 --name=openldap $ #为应用设置环境变量，即为openldap的域与密码$ oc env --from=secret/openldap dc/openldap $ #为应用设置以openldap serviceaccount启动$ cat &lt;&lt; EOF | oc apply -f -apiVersion: apps.openshift.io/v1kind: DeploymentConfigmetadata:  name: openldapspec:  template:    spec:      serviceAccount: openldapEOF
当然以上配置没有添加健康检查，以及持久化是使用的emptyDir。这个可以后序自己补充。
第二种：使用yaml配置文件部署，主要是DeploymentConfig的配置。添加了健康检查
$ cat &lt;&lt; EOF | oc create -f -apiVersion: apps.openshift.io/v1kind: DeploymentConfigmetadata:  labels:    app: openldap  name: openldapspec:  replicas: 1  selector:    app: openldap    deploymentconfig: openldap  template:    metadata:      labels:        app: openldap        deploymentconfig: openldap    spec:      containers:        - env:            - name: OPENLDAP_ROOT_DN_SUFFIX              valueFrom:                secretKeyRef:                  key: OPENLDAP_ROOT_DN_SUFFIX                  name: openldap            - name: OPENLDAP_ROOT_PASSWORD              valueFrom:                secretKeyRef:                  key: OPENLDAP_ROOT_PASSWORD                  name: openldap          image: &#x27;xhuaustc/openldap-2441-centos7:latest&#x27;          imagePullPolicy: Always          livenessProbe:            failureThreshold: 3            initialDelaySeconds: 3            periodSeconds: 10            successThreshold: 1            tcpSocket:              port: 389            timeoutSeconds: 1          name: openldap          ports:            - containerPort: 389              protocol: TCP            - containerPort: 636              protocol: TCP          readinessProbe:            failureThreshold: 3            initialDelaySeconds: 3            periodSeconds: 10            successThreshold: 1            tcpSocket:              port: 389            timeoutSeconds: 1          name: openldap          ports:            - containerPort: 389              protocol: TCP            - containerPort: 636              protocol: TCP          volumeMounts:            - mountPath: /etc/openldap              name: openldap-volume-1            - mountPath: /var/lib/ldap              name: openldap-volume-2      volumes:        - emptyDir: &#123;&#125;          name: openldap-volume-1        - emptyDir: &#123;&#125;          name: openldap-volume-2  triggers:    - type: ConfigChange
测试验证按照以上部署的配置，管理员账号为：cn=Manager,dc=fcloudy,dc=com，密码为：admin使用LDAPBrowser连接
  
  
最后一步，做成模板将资源导出为模板文件
$ oc export dc,svc,secret  --as-template=openldap &gt; openldap-template.yaml
在此基础上手动编辑，最后得到可用的模板文件
apiVersion: template.openshift.io/v1kind: Templatemessage: |-  OpenLDAP服务将会根据指定的参数创建metadata:  annotations:    description: |-      OpenLDAP服务将会根据指定的参数创建  name: openldapobjects:- apiVersion: apps.openshift.io/v1  kind: DeploymentConfig  metadata:    labels:      app: openldap    name: openldap  spec:    replicas: 1    selector:      app: openldap      deploymentconfig: openldap    strategy:      type: Rolling    template:      metadata:        labels:          app: openldap          deploymentconfig: openldap      spec:        containers:        - env:          - name: OPENLDAP_ROOT_DN_SUFFIX            valueFrom:              secretKeyRef:                key: OPENLDAP_ROOT_DN_SUFFIX                name: openldap          - name: OPENLDAP_ROOT_PASSWORD            valueFrom:              secretKeyRef:                key: OPENLDAP_ROOT_PASSWORD                name: openldap          - name: OPENLDAP_ROOT_DN_PREFIX            valueFrom:              secretKeyRef:                key: OPENLDAP_ROOT_DN_PREFIX                name: openldap          - name: OPENLDAP_DEBUG_LEVEL            valueFrom:              secretKeyRef:                key: OPENLDAP_DEBUG_LEVEL                name: openldap          image: xhuaustc/openldap-2441-centos7:latest          imagePullPolicy: IfNotPresent          name: openldap          ports:          - containerPort: 389            protocol: TCP          - containerPort: 636            protocol: TCP          volumeMounts:          - mountPath: /etc/openldap            name: openldap-volume-1          - mountPath: /var/lib/ldap            name: openldap-volume-2        restartPolicy: Always        serviceAccount: openldap        serviceAccountName: openldap        volumes:        - emptyDir: &#123;&#125;          name: openldap-volume-1        - emptyDir: &#123;&#125;          name: openldap-volume-2    triggers:    - type: ConfigChange- apiVersion: v1  kind: Service  metadata:    labels:      app: openldap    name: openldap  spec:    ports:    - name: 389-tcp      port: 389      protocol: TCP      targetPort: 389    - name: 636-tcp      port: 636      protocol: TCP      targetPort: 636    selector:      app: openldap      deploymentconfig: openldap    sessionAffinity: None    type: ClusterIP- apiVersion: v1  stringData:    OPENLDAP_ROOT_DN_SUFFIX: $&#123;OPENLDAP_ROOT_DN_SUFFIX&#125;    OPENLDAP_ROOT_DN_PREFIX: $&#123;OPENLDAP_ROOT_DN_PREFIX&#125;    OPENLDAP_ROOT_PASSWORD: $&#123;OPENLDAP_ROOT_PASSWORD&#125;    OPENLDAP_DEBUG_LEVEL: $&#123;OPENLDAP_DEBUG_LEVEL&#125;  kind: Secret  metadata:    name: openldap  type: Opaqueparameters:- description: Openldap Root DN Suffix  displayName: Openldap Root DN Suffix  name: OPENLDAP_ROOT_DN_SUFFIX  value: dc=example,dc=com- description: Openldap Root DN Manager Prefix  displayName: Openldap Root DN Manager Prefix  name: OPENLDAP_ROOT_DN_PREFIX  value: cn=Manager- description: Openldap Root Password  displayName: Openldap Root Password  name: OPENLDAP_ROOT_PASSWORD  from: &#x27;[a-zA-Z0-9]&#123;16&#125;&#x27;  generate: expression- description: Openldap Debug Level  displayName: Openldap Debug Level  name: OPENLDAP_DEBUG_LEVEL  value: &quot;256&quot;
在Openshift上将模板导入，完成后，创建Openldap就非常简单了，看截图。
  
ps：OpenLDAP的整个构建及部署过程可以作为Openshift的一种类型案例。当然该案例中没有使用PV、PVC做持久化。相信对Openshift有所了解的，在这个基础上做持久化应该不在话下。
千里之行，始于足下。完成OpenLDAP的部署，这只是一个开始，账号一统还有相当多的工作要去做。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift上部署awx——ansible-tower的社区版</title>
    <url>/posts/a5dfd7cd034e.html</url>
    <content><![CDATA[Ansible的大名，对做运维开发的工程师来说没有不知道的，它是开源的配置管理工具，也是最流行的自动化运维工具之一。Ansible tower是Ansible的图形界面化平台，它是RedHat公司的商业应用，也就是说Ansible tower是付费的。既然是RedHat的付费应用，可想而知，它具有很强大、实用的功能，另外它应该会有一个免费的社区版本。对，那就是awx。Ansible Tower的社区版，功能跟Ansible Tower一样强大，开源免费。
部署安装Awx安装文档：https://github.com/ansible/awx/blob/devel/INSTALL.md

clone下awx的代码

$ git clone https://github.com/ansible/awx.git

Openshift集群中创建部署awx的project

$ oc new-project awx

awx的project下创建PVC，默认pvc名为postgresql

$ cat &lt;&lt; EOF | oc create -f -apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: postgresqlspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 10GiEOF

配置awx/installer/inventory

...# Openshift Install# Will need to set -e openshift_password=developer -e docker_registry_password=$(oc whoami -t)#           or set -e openshift_token=TOKENopenshift_host=127.0.0.1:8443 # openshift masteropenshift_project=awx # awx部署的projectopenshift_user=developer # openshift集群登录用户名openshift_skip_tls_verify=Falseopenshift_pg_emptydir=False...admin_user=admin # awx登录用户名admin_password=password # awx登录密码

执行部署awx脚本

$ cd awx/installer/$ ansible-playbook -i inventory install.yml -e openshift_password=developer  -e docker_registry_password=$(oc whoami -t)

访问查看

  
参考资料https://bhirsch.org/2017/09/08/exploring-a-containerized-awx/#contenthttps://github.com/ansible/awx
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift中Elasticsearch索引自动清理工具：Curator</title>
    <url>/posts/d5518b697fca.html</url>
    <content><![CDATA[Openshift 3.11版本Openshift 3.10版本参考文档Openshift官方文档curator配置
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift中的Prometheus-Operator如何添加新的应用监控</title>
    <url>/posts/d483810a9169.html</url>
    <content><![CDATA[

prometheus使用的配置文件在openshift-monitoring/secret/prometheus-k8s中。有添加ServiceMonitor时会动态更新该文件
为system:serviceaccount:openshift-monitoring:prometheus-k8s添加应用所在project的view权限

$ oc policy add-role-to-user view system:serviceaccount:openshift-monitoring:prometheus-k8s -n myproject

查看prometheus的配置，查看对于servicemonitor的过滤器

$ oc get prometheus k8s -o yaml...  serviceMonitorSelector:    matchExpressions:    - key: k8s-app      operator: Exists

serviceMonitorSelector表示，只有设置了k8s-app的Label的serviceMonitor的状态才能被该prometheus监听到


添加servicemonitor 按照3的说明，serviceMonitor必须设置k8s-appLabel

apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: my-cluster-kafka  namespace: openshift-monitoring  labels:    k8s-app: prometheusspec:  selector:    matchLabels:      strimzi.io/name: my-cluster-kafka-bootstrap  namespaceSelector:    any: true  endpoints:  - port: metrics

selector：匹配的exporter的service的Label
namespaceSelector：匹配exporter所在的namespace。any为匹配所有的namespace
endpoints：对应为exporter服务port名

注意：经过实验serviceMonitor必须创建在prometheus相同的project下，默认为openshift-monitoring另外如果OpenShift网络开启的是多租房&#x2F;NetworkPolicy模式，需要确保prometheus访问exporter节点的网络通畅。
原理部分可参考：官方手册 Prometheus Cluster Monitoring，包含对Openshift集群etcd的监控的详细操作使用Prometheus Operator優雅的監控Kubernetes基于 Prometheus 的集群监控
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift-Kubernetes中易理解错误的问题汇总</title>
    <url>/posts/74b7faefed79.html</url>
    <content><![CDATA[
###1. readiness健康检查一直失败，Pod会不会重启？
 不会，一直处于Running状态，但Not Ready。
2. Pod重启时，Pod的容器是重启，还是重新创建？ POD容器不动，应用容器是重新创建。POD IP保持不变，但应用容器中的临时文件会丢失。
###3. 如果更改Deployment的配置后，马上将配置恢复回去，应用Pod是否会触发重新部署？不会，恢复回去后不会创建新的RS。
4. master_cluster_hostname与master_cluster_public_hostname的区别master_cluster_hostname是集群内部组件使用的域名master_cluster_public_hostname是集群外部使用的域名###5. 对于高可用集群中，3个master节点是完全相同的吗？openshift
不是，虽然它们的运行态是完全一致的，但是在OpenShift的运维脚本中是有区分的，如集群扩容时。有一台是主master（inventory中的第一台），它会单独保存集群的证书及一些配置。
###6. 对于statefulset类型的工作负载，只能逐个启动吗？
并不是，statefulset类型的workload默认确实是逐个启动，但是可以通过更改它的启动策略来实现并行启动。spec.podManagementPolicy值默认为OrderedReady，如果设置为Parallel，将会并行启动和删除Pod。
###7. statefulset类型的工作负载的多个副本，能共用一块共享存储吗？
当然可以，在statefulset的编排中不使用volumeTemplate，而是与deployment资源一样设置共享存储就可以了。
###8. 如果应用存活检查&#x2F;就绪检查未通过的情况下，是否能顺利地进行应用更新？
不一定，这个取决于应用的升级策略：滚动升级，maxSurge与maxUnavailable的配置。如果应用是一个副本，maxSurge与maxUnavailable的值为默认值25%，将无法关闭旧版本应用。

maxSurge：升级过程中最多可以比原先设置多出的POD数量。可以是具体个数，也可以百分比（进一法取整）
maxUnavailable：升级过程中最多有多少个POD处于无法提供服务的状态。可以是具体个数，也可以百分比（去尾法取整）

###9. 如果应用做了多副本，而且部署在不同的节点上，即使宿主机发生宕机，对业务也不会有影响，正确吗？
这个结论并不正确。因为作为内部负载均衡的Service并不能马上感知应用的问题，所以不会马上主动剔除故障应用，同时Service一般使用（ocp）iptables作为代理方式，它没有使用失败则使用别的节点的机制，所以在一段时间内请求会被发送到问题节点。

默认情况如下：

40s检测后，流量会从负载均衡中剔除。
5分钟之内完成重启，podname不变，pod的ip会更新。
如果5分钟后node节点仍然没有恢复，集群会将问题pod重新调度。


###10. NFS作为持久化卷挂载在应用中，如果NFS服务故障后恢复，容器是否会自动重新挂载存储服务？是的。
11. 在Docker的服务器上，关闭&#x2F;重启iptables，上面的容器网络依然正常吗？异常。docker安装完成后，会自动接管iptables或者firewalld，在docker run的时候，会自动往iptables里加入规则；所以当iptables重启后会丢失，只有再重启docker就好了的原因。在使用Systemd 时， firewalld 会在 Docker 之前启动，但是如果你在 Docker 启动之后再启动或者重启 firewalld ，就需要重启 Docker 进程了。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift中集群ElasticSearch的运维</title>
    <url>/posts/34e946b3354d.html</url>
    <content><![CDATA[大家都知道，Openshift集成了EFK(Elasticsearch + Fluentd + Kibana)作为集群的日志管理平台。我们该怎样去维护ElasticSearch.


找到ES所在的pod


oc get pod -l component=es -o name -n logging | cut -d/ -f2


查看所有ES索引


oc exec $es-pod-name -- curl -s --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca https://localhost:9200/_cat/indices?v


清理所有3月份的ES索引


oc exec $es-pod-name -- curl -s --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XDELETE https://localhost:9200/*.2018.03.*
curl可以加-k这样就不需要--cacert参数


打开Kibana，显示错误Elasticsearch is still initializing the kibana index解决办法：删除kibana的索引，重新进入kibana


oc exec $es-pod-name -- curl -s --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XDELETE https://localhost:9200/.kibana


确认ES集群的工作状态


$ oc exec $es-pod-name -c elasticsearch -- health


停止ES节点分片平衡


$ oc exec $es-pod-name -c elasticsearch -- curl --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XPUT https://localhost:9200/_cluster/settings -d &#x27;&#123;&quot;transient&quot;: &#123;&quot;cluster.routing.allocation.enable&quot;:&quot;none&quot;&#125;&#125;&#x27;

6.开启ES节点分片平衡

$ oc exec $es-pod-name -c elasticsearch -- curl --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XPUT https://localhost:9200/_cluster/settings -d &#x27;&#123;&quot;transient&quot;: &#123;&quot;cluster.routing.allocation.enable&quot;:&quot;all&quot;&#125;&#125;&#x27;


ES扩容


OpenShift自带的日志搜索引擎ES服务的扩容
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift之NetApp-SolidFire集成及测试</title>
    <url>/posts/82cffe541eda.html</url>
    <content><![CDATA[
什么是NetApp SolidFire?NetApp公司是一家存储和数据管理公司,主要提供软件，系统和服务来管理和存储数据，包括其专有的Data ONTAP操作系统。NetApp公司主要向全球数据密集型企业提供统一存储解决方案其 Data ONTAP是全球首屈一指的存储操作系统，公司存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。——(来自搜狗百科)SolidFire成立于2010年，是一家全闪存阵列的存储厂商，其存储控制器基于标准的x86服务器，最大可扩展到100个节点，2015年12月，SolidFire被NetApp收购；2017年6月，NetApp基于SolidFire推出超融合一体机。SolidFire提供分布式块存储，类似于ceph rbd，非常灵活，支持动态扩缩容，具有良好的性能。同时具有很多企业特性：如快照，组快照，丰富的API，非常灵活的QOS配置等。

什么是Trident?NetApp是CNCF的金牌会员，它开发的Trident是一款开源存储配置程序和流程编排程序。在没有Trident的环境下，K8s&#x2F;Openshift环境要使用NetApp存储，就需要，手动在NetApp控制台上创建volume，并设置创建PV，再创建PVC。这些过程需要在两个平台切换操作，而且很麻烦。部署了Trident后，配置好相应的storageclass，K8s&#x2F;Openshift平台就可以直接通过storageclass动态自动创建PVC。K8s&#x2F;Openshift平台通过Trident控制器调用NetApp设备的API从而达到控制NetApp设备目的，如创建volume，并自动创建PV，及PVC，进而让Pod能够使用，此过程是自动的，对平台使用者是无感知的。Trident本身也是以Pod的形式在K8s&#x2F;Openshift平台上运行的。

Openshift上部署与使用Trident准备工作
用system:admin登录集群

$ oc login -u system:admin

集群能够访问SolidFire机器的MVIP(管理VIP)及SVIP(存储SVIP)

$ telnet $MVIP 443$ telnet $SVIP 3260

安装基本包

$ ansible all -m package -a &#x27;name=lsscsi,iscsi-initiator-utils,sg3_utils,device-mapper-multipath state=present&#x27;$ ansible all -m shell -a &#x27;mpathconf --enable --with_multipathd y&#x27;$ ansible all -m service -a &#x27;name=iscsid enabled=true state=started&#x27;$ ansible all -m service -a &#x27;name=multipathd enabled=true state=started&#x27;$ ansible all -m service -a &#x27;name=iscsi enabled=true state=started&#x27;
部署
下载安装文件，并解压

$ wget https://github.com/NetApp/trident/releases/download/v18.10.0/trident-installer-18.10.0.tar.gz$ tar -xf trident-installer-18.10.0.tar.gz$ cd trident-installer

配置安装backend.json文件

$ cp sample-input/backend-solidfire.json setup/backend.json# 修改里面的配置$ cat setup/backend.json&#123;    &quot;version&quot;: 1,    &quot;storageDriverName&quot;: &quot;solidfire-san&quot;,    &quot;Endpoint&quot;: &quot;https://&#123;&#123;用户名&#125;&#125;:&#123;&#123;密码&#125;&#125;@&#123;&#123;管理VIP&#125;&#125;/json-rpc/11.0&quot;,    &quot;SVIP&quot;: &quot;&#123;&#123;存储VIP&#125;&#125;:3260&quot;,    &quot;TenantName&quot;: &quot;trident&quot;,    &quot;UseCHAP&quot;: true,    &quot;InitiatorIFace&quot;: &quot;default&quot;,    &quot;Types&quot;: [&#123;&quot;Type&quot;: &quot;Bronze&quot;, &quot;Qos&quot;: &#123;&quot;minIOPS&quot;: 1000, &quot;maxIOPS&quot;: 2000, &quot;burstIOPS&quot;: 4000&#125;&#125;,              &#123;&quot;Type&quot;: &quot;Silver&quot;, &quot;Qos&quot;: &#123;&quot;minIOPS&quot;: 4000, &quot;maxIOPS&quot;: 6000, &quot;burstIOPS&quot;: 8000&#125;&#125;,              &#123;&quot;Type&quot;: &quot;Gold&quot;, &quot;Qos&quot;: &#123;&quot;minIOPS&quot;: 6000, &quot;maxIOPS&quot;: 8000, &quot;burstIOPS&quot;: 10000&#125;&#125;]&#125;

创建trident project

$ oc new-project trident

安装检查

$ ./tridentctl install --dry-run -n trident
这个步骤会模拟安装过程进行执行一遍，并会删除所有资源。通过模拟对整个环境进行全面的检测。以下是执行的日志
[root@master02 trident-installer]# ./tridentctl install --dry-run -n trident -dDEBU Initialized logging.                          logLevel=debugDEBU Running outside a pod, creating CLI-based client. DEBU Initialized Kubernetes CLI client.            cli=oc flavor=openshift namespace=trident version=1.11.0+d4cacc0DEBU Validated installation environment.           installationNamespace=trident kubernetesVersion=DEBU Deleted Kubernetes configmap.                 label=&quot;app=trident-installer.netapp.io&quot; namespace=tridentDEBU Namespace exists.                             namespace=tridentDEBU Deleted Kubernetes object by YAML.           DEBU Deleted installer cluster role binding.      DEBU Deleted Kubernetes object by YAML.           DEBU Deleted installer cluster role.              DEBU Deleted Kubernetes object by YAML.           DEBU Deleted installer service account.           DEBU Removed security context constraint user.     scc=privileged user=trident-installerDEBU Created Kubernetes object by YAML.           INFO Created installer service account.            serviceaccount=trident-installerDEBU Created Kubernetes object by YAML.           INFO Created installer cluster role.               clusterrole=trident-installerDEBU Created Kubernetes object by YAML.           INFO Created installer cluster role binding.       clusterrolebinding=trident-installerINFO Added security context constraint user.       scc=privileged user=trident-installerDEBU Created Kubernetes configmap from directory.  label=&quot;app=trident-installer.netapp.io&quot; name=trident-installer namespace=trident path=/root/trident-installer/setupINFO Created installer configmap.                  configmap=trident-installerDEBU Created Kubernetes object by YAML.           INFO Created installer pod.                        pod=trident-installerINFO Waiting for Trident installer pod to start.  DEBU Trident installer pod not yet started, waiting.  increment=280.357322ms message=&quot;pod not yet started (Pending)&quot;DEBU Trident installer pod not yet started, waiting.  increment=523.702816ms message=&quot;pod not yet started (Pending)&quot;DEBU Trident installer pod not yet started, waiting.  increment=914.246751ms message=&quot;pod not yet started (Pending)&quot;DEBU Trident installer pod not yet started, waiting.  increment=1.111778662s message=&quot;pod not yet started (Pending)&quot;DEBU Pod started.                                  phase=SucceededINFO Trident installer pod started.                namespace=trident pod=trident-installerDEBU Getting logs.                                 cmd=&quot;oc --namespace=trident logs trident-installer -f&quot;DEBU Initialized logging.                          logLevel=debugDEBU Running in a pod, creating API-based client.  namespace=tridentDEBU Initialized Kubernetes API client.            cli=oc flavor=openshift namespace=trident version=v1.11.0+d4cacc0DEBU Validated installation environment.           installationNamespace=trident kubernetesVersion=v1.11.0+d4cacc0DEBU Parsed requested volume size.                 quantity=2GiDEBU Dumping RBAC fields.                          ucpBearerToken= ucpHost= useKubernetesRBAC=trueDEBU Namespace exists.                             namespace=tridentDEBU PVC does not exist.                           pvc=tridentDEBU PV does not exist.                            pv=tridentINFO Starting storage driver.                      backend=/setup/backend.jsonDEBU config: &#123;&quot;Endpoint&quot;:&quot;https://admin:root1234@99.248.106.82/json-rpc/11.0&quot;,&quot;InitiatorIFace&quot;:&quot;default&quot;,&quot;SVIP&quot;:&quot;99.248.82.55:3260&quot;,&quot;TenantName&quot;:&quot;trident&quot;,&quot;Types&quot;:[&#123;&quot;Qos&quot;:&#123;&quot;burstIOPS&quot;:4000,&quot;maxIOPS&quot;:2000,&quot;minIOPS&quot;:1000&#125;,&quot;Type&quot;:&quot;Bronze&quot;&#125;,&#123;&quot;Qos&quot;:&#123;&quot;burstIOPS&quot;:8000,&quot;maxIOPS&quot;:6000,&quot;minIOPS&quot;:4000&#125;,&quot;Type&quot;:&quot;Silver&quot;&#125;,&#123;&quot;Qos&quot;:&#123;&quot;burstIOPS&quot;:10000,&quot;maxIOPS&quot;:8000,&quot;minIOPS&quot;:6000&#125;,&quot;Type&quot;:&quot;Gold&quot;&#125;],&quot;UseCHAP&quot;:true,&quot;storageDriverName&quot;:&quot;solidfire-san&quot;,&quot;version&quot;:1&#125; DEBU Storage prefix is absent, will use default prefix. DEBU Parsed commonConfig: &#123;Version:1 StorageDriverName:solidfire-san BackendName: Debug:false DebugTraceFlags:map[] DisableDelete:false StoragePrefixRaw:[] StoragePrefix:&lt;nil&gt; SerialNumbers:[] DriverContext: LimitVolumeSize:&#125; DEBU Initializing storage driver.                  driver=solidfire-sanDEBU Configuration defaults                        Size=1G StoragePrefix= UseCHAP=trueDEBU Parsed into solidfireConfig                   DisableDelete=false StorageDriverName=solidfire-san Version=1DEBU Decoded to &amp;&#123;CommonStorageDriverConfig:0xc42064e0a0 TenantName:trident EndPoint:https://admin:root1234@99.248.106.82/json-rpc/11.0 SVIP:99.248.82.55:3260 InitiatorIFace:default Types:0xc4206d26e0 LegacyNamePrefix: AccessGroups:[] UseCHAP:true DefaultBlockSize:0 SolidfireStorageDriverConfigDefaults:&#123;CommonStorageDriverConfigDefaults:&#123;Size:1G&#125;&#125;&#125; DEBU Set default block size.                       defaultBlockSize=512DEBU Using SF API version from config file.        version=11.0DEBU Initializing SolidFire API client.            cfg=&quot;&#123;trident https://admin:root1234@99.248.106.82/json-rpc/11.0  99.248.82.55:3260 default 0xc4206d26e0  [] 512 map[]&#125;&quot; endpoint=&quot;https://admin:root1234@99.248.106.82/json-rpc/11.0&quot; svip=&quot;99.248.82.55:3260&quot;ERRO Error detected in API response.               ID=637 code=500 message=xUnknownAccount name=xUnknownAccountDEBU Account not found, creating.                  error=&quot;device API error: xUnknownAccount&quot; tenantName=tridentDEBU Created account.                              accountID=0 tenantName=tridentDEBU SolidFire driver initialized.                 AccountID=2 InitiatorIFace=defaultDEBU Using CHAP, skipped Volume Access Group logic.  AccessGroups=&quot;[]&quot; SVIP=&quot;99.248.82.55:3260&quot; UseCHAP=true driver=solidfire-sanDEBU Added pool for SolidFire backend.             attributes=&quot;map[media:&#123;Offers: ssd&#125; IOPS:&#123;Min: 1000, Max: 2000&#125; snapshots:&#123;Offer:  true&#125; clones:&#123;Offer:  true&#125; encryption:&#123;Offer:  false&#125; provisioningType:&#123;Offers: thin&#125; backendType:&#123;Offers: solidfire-san&#125;]&quot; backend=solidfire_99.248.82.55 pool=BronzeDEBU Added pool for SolidFire backend.             attributes=&quot;map[clones:&#123;Offer:  true&#125; encryption:&#123;Offer:  false&#125; provisioningType:&#123;Offers: thin&#125; backendType:&#123;Offers: solidfire-san&#125; media:&#123;Offers: ssd&#125; IOPS:&#123;Min: 4000, Max: 6000&#125; snapshots:&#123;Offer:  true&#125;]&quot; backend=solidfire_99.248.82.55 pool=SilverDEBU Added pool for SolidFire backend.             attributes=&quot;map[snapshots:&#123;Offer:  true&#125; clones:&#123;Offer:  true&#125; encryption:&#123;Offer:  false&#125; provisioningType:&#123;Offers: thin&#125; backendType:&#123;Offers: solidfire-san&#125; media:&#123;Offers: ssd&#125; IOPS:&#123;Min: 6000, Max: 8000&#125;]&quot; backend=solidfire_99.248.82.55 pool=GoldDEBU Storage driver initialized.                   driver=solidfire-sanINFO Storage driver loaded.                        driver=solidfire-sanINFO Dry run completed, no problems found.        DEBU Received EOF from pod logs.                   container= pod=trident-installerINFO Waiting for Trident installer pod to finish. DEBU Pod finished.                                 phase=SucceededINFO Trident installer pod finished.               namespace=trident pod=trident-installerDEBU Deleted Kubernetes pod.                       label=&quot;app=trident-installer.netapp.io&quot; namespace=tridentINFO Deleted installer pod.                        pod=trident-installerDEBU Deleted Kubernetes configmap.                 label=&quot;app=trident-installer.netapp.io&quot; namespace=tridentINFO Deleted installer configmap.                  configmap=trident-installerINFO In-cluster installation completed.           DEBU Deleted Kubernetes object by YAML.           INFO Deleted installer cluster role binding.      DEBU Deleted Kubernetes object by YAML.           INFO Deleted installer cluster role.              DEBU Deleted Kubernetes object by YAML.           INFO Deleted installer service account.           INFO Removed security context constraint user.     scc=privileged user=trident-installer


正式安装

$ ./tridentctl install -n trident
该步骤是真正的执行。会创建serviceaccount, clusterrolebinding,configmap配置，trident-install pod（该pod在部署完trident deployment后会删除）等， 并会创建一个pv与trident pvc进行初始化操作，最终会创建trident deployment，完成trident的安装。

trident的安装支持自定义一些配置。
–etcd-image可指定etcd的镜像（默认是quay.io&#x2F;coreos&#x2F;etcd，下载会比较慢）
–trident-image指定trident的镜像
–volume-size指定trident持久存储的大小（默认为2GiB）
–volume-name指定volume名字（默认是etcd-vol）
–pv指定pv名字（默认是trident）
–pvc指定pvc名字（默认是trident）
–generate-custom-yaml将使用的所有配置进行导出到一个setup文件夹，不会对集群做任何操作
–use-custom-yaml安装setup下的所有yaml文件进行部署trident

以下是执行的日志
[root@master02 trident-installer]# ./tridentctl install -n trident INFO Created installer service account.            serviceaccount=trident-installerINFO Created installer cluster role.               clusterrole=trident-installerINFO Created installer cluster role binding.       clusterrolebinding=trident-installerINFO Added security context constraint user.       scc=privileged user=trident-installerINFO Created installer configmap.                  configmap=trident-installerINFO Created installer pod.                        pod=trident-installerINFO Waiting for Trident installer pod to start.  INFO Trident installer pod started.                namespace=trident pod=trident-installerINFO Starting storage driver.                      backend=/setup/backend.jsonINFO Storage driver loaded.                        driver=solidfire-sanINFO Starting Trident installation.                namespace=tridentINFO Created service account.                     INFO Created cluster role.                        INFO Created cluster role binding.                INFO Added security context constraint user.       scc=anyuid user=tridentINFO Created PVC.                                 INFO Controller serial numbers.                    serialNumbers=&quot;4BZXJB2,85Q8JB2,4BXXJB2,4BXTJB2&quot;INFO Created iSCSI CHAP secret.                    secret=trident-chap-solidfire-99-248-82-55-tridentINFO Created PV.                                   pv=tridentINFO Waiting for PVC to be bound.                  pvc=tridentINFO Created Trident deployment.                  INFO Waiting for Trident pod to start.            INFO Trident pod started.                          namespace=trident pod=trident-57ccdff48f-gtflxINFO Waiting for Trident REST interface.          INFO Trident REST interface is up.                 version=18.10.0INFO Trident installation succeeded.              INFO Waiting for Trident installer pod to finish. INFO Trident installer pod finished.               namespace=trident pod=trident-installerINFO Deleted installer pod.                        pod=trident-installerINFO Deleted installer configmap.                  configmap=trident-installerINFO In-cluster installation completed.           INFO Deleted installer cluster role binding.      INFO Deleted installer cluster role.              INFO Deleted installer service account.           INFO Removed security context constraint user.     scc=privileged user=trident-installer

添加第一个backend执行完install后，trident并不会安装之前配置的backend，需要另外再单独添加。（个人觉得netapp这点考虑得有点多余，因为dry-run的时候已经对backend.json作了检查了，直接install将它添加上岂不是更方便）

$ ./tridentctl -n trident create backend -f setup/backend.json$ ./tridentctl -n trident get backend+------------------------+----------------+--------+---------+|          NAME          | STORAGE DRIVER | ONLINE | VOLUMES |+------------------------+----------------+--------+---------+| solidfire_99.248.82.55 | solidfire-san  | true   |       0 |+------------------------+----------------+--------+---------+

添加基本的storageclass将sample-input&#x2F;storage-class-basic.yaml.templ中的__BACKEND_TYPE__用指定的backend中的STORAGE DRIVER值替换（此例中为solidfire-san）

$ cat sample-input/storage-class-basic.yaml.templapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: basicprovisioner: netapp.io/tridentparameters:  backendType: &quot;__BACKEND_TYPE__&quot;$ sed &quot;s/__BACKEND_TYPE__/solidfire-san/&quot; sample-input/storage-class-basic.yaml.templ | oc create -f -

根据backend中的Type创建对应的storageclass

$ cat storage-class-gold.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: gold  annotations:    storageclass.kubernetes.io/is-default-class: &quot;true&quot;provisioner: netapp.io/tridentparameters:  storagePools: &quot;solidfire_99.248.82.55:Gold&quot; # solidfire_99.248.82.55为backend name;Gold为指定的Type$ oc create -f storage-class-gold.yaml
查看当前的storageclass
$ oc get scNAME             PROVISIONER         AGEbasic            netapp.io/trident   2hgold (default)   netapp.io/trident   1h
使用：创建PVC
创建第一个PVC

$ cat test-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata:  annotations:    volume.beta.kubernetes.io/storage-class: gold    volume.beta.kubernetes.io/storage-provisioner: netapp.io/trident    trident.netapp.io/reclaimPolicy: &quot;Retain&quot;  name: testpvc  namespace: testspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 1Gi$ oc create -f test-pvc.yaml
PVC创建的说明:

volume.beta.kubernetes.io&#x2F;storage-class为10，11步创建的storageclass
volume.beta.kubernetes.io&#x2F;storage-provisioner指定为netapp的trident
trident.netapp.io&#x2F;reclaimPolicy指定创建PV的reclaimPolicy,默认为”Delete”,支持”Delete”和”Retain”，不支持”Recycle”
accessModes因SolidFire是块存储，只支持ReadWriteOnce

SolidFire功能测试快照恢复数据创建快照


基于快照创建新的PVC指定快照，创建新的存储

查看新建的volume的IQN

基于新的volume创建PV
$ cat test-clone-pv.yamlapiVersion: v1kind: PersistentVolumemetadata:  annotations:    pv.kubernetes.io/provisioned-by: netapp.io/trident    volume.beta.kubernetes.io/storage-class: gold  name: test-dd-testxx-volumespec:  accessModes:  - ReadWriteOnce  capacity:    storage: 100Gi  iscsi:    chapAuthDiscovery: true    chapAuthSession: true    fsType: ext4    iqn: iqn.2010-01.com.solidfire:fs69.test-dd-testxx-volume.169    iscsiInterface: default    lun: 0    secretRef:      name: trident-chap-solidfire-99-248-82-55-trident      namespace: trident    targetPortal: 99.248.82.55:3260  persistentVolumeReclaimPolicy: Delete  storageClassName: gold$ oc create -f test-clone-pv.yaml
创建pvc使用手动创建的pv
$ cat test-clone-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: test111x  namespace: test-ddspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 100Gi
组快照组快照与快照类似，不同之处，它把多个存储卷在同一时间的数据做快照，从而避免数据不一致的情况。同时在恢复的时候，也同时将备份时刻的数据进行恢复。

克隆已有的pvc数据添加annotations配置trident.netapp.io&#x2F;cloneFromPVC: test-pvc,创建新的pvc基于已有的PVC test-pvc
apiVersion: v1kind: PersistentVolumeClaimmetadata:  annotations:    trident.netapp.io/cloneFromPVC: test-pvc  name: test-clone-pvc  namespace: test-ddspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 100Gi  storageClassName: gold
SolidFire性能测试测试环境说明

openshift 3.11物理机部署：3 Masters   4 Nodes
SolidFire 4台Node：型号SF9605 每台Node上为10块SSD盘，每个Node的IOPS为5w，集群最高IOPS 20w
每块PV存储设置为gold类型storageclass：{&quot;Type&quot;: &quot;Gold&quot;, &quot;Qos&quot;: {&quot;minIOPS&quot;: 6000, &quot;maxIOPS&quot;: 8000, &quot;burstIOPS&quot;: 10000}}

dd测试# 测试命令$ dd if=/dev/zero of=/data/dd.test bs=4k count=200000 oflag=direct

单个pod，单个pv作dd命令测试创建deployment进行测试

$ cat test0-pvc.yamlkind: PersistentVolumeClaimmetadata:  annotations:    volume.beta.kubernetes.io/storage-class: gold    volume.beta.kubernetes.io/storage-provisioner: netapp.io/trident  name: test0  namespace: test-ddspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 10Gi$ oc create -f test0-pvc.yaml ## 创建测试的存储$ cat dd.yamlapiVersion: apps.openshift.io/v1kind: DeploymentConfigmetadata:  labels:    run: ddtest  name: ddtestspec:  replicas: 1  selector:    run: ddtest  strategy:    type: Recreate  template:    metadata:      labels:        run: ddtest    spec:      containers:      - command:        - /bin/bash        - &#x27;-c&#x27;        - |          #/bin/bash          dd if=/dev/zero of=/data/out.test1 bs=4k count=200000 oflag=direct        image: tools/iqperf:latest        imagePullPolicy: Always        name: ddtest        volumeMounts:        - mountPath: /data          name: volume-spq10      volumes:      - name: volume-spq10        persistentVolumeClaim:          claimName: test0  triggers:  - type: ConfigChange$ oc create -f dd.yaml
在webconsole上查看日志如下
200000+0 records in200000+0 records out819200000 bytes (819 MB) copied, 68.8519 s, 11.9 MB/s
NetApp的管理平台上查看集群IO状态，如图（只需要看11:32时间以后部分）

IOPS为29082. 1个pod，1个pv，8个dd进程将1中的deploymentconfig中的command内容更新为：
...- command:  - &#x27;/bin/bash&#x27;  - &#x27;-c&#x27;  - |    #/bin/bash    for i in &#123;1..8&#125;    do       dd if=/dev/zero of=/data/dd.test$i bs=4k count=200000 oflag=direct &amp;    done        sleep 1000000...


IOPS为10000
额外补充

1个pod，1个pv，50-&gt;150个dd进程，bs&#x3D;512，在netapp控制台将volume的max iops设置为200000，最终测试结果，该volume的写iops最大为40000。也就是该版本solidfire下，单个volume最大写iops为4w。
同样方法测试读，单个volume最大达到写iops为7.5w。$ dd if=/data/out.test1 of=/dev/zero bs=512 count=20000000 iflag=direct


8个pod，8个pv同时使用dd命令测试创建statefulset，设置volumeClaimTemplates批量创建存储

$ cat dd-statefulset.yamlapiVersion: apps/v1kind: StatefulSetmetadata:  name: testdd  namespace: test-ddspec:  serviceName: testdd  replicas: 8  selector:    matchLabels:      app: testdd  template:    metadata:      labels:        app: testdd    spec:      terminationGracePeriodSeconds: 10      containers:        - name: testdd          containers:          - command:            - /bin/bash            - &#x27;-c&#x27;            - |              #!/bin/bash              dd if=/dev/zero of=/data/out.test1 bs=4k count=2000000 oflag=direct          image: &#x27;harbor.apps.it.mbcloud.com/tools/iqperf:latest&#x27;          imagePullPolicy: Always          name: testdd          image: &#x27;tools/dd:latest&#x27;          volumeMounts:            - name: data              mountPath: /data  volumeClaimTemplates:    - metadata:        name: data      spec:        accessModes:          - ReadWriteOnce        storageClassName: gold        resources:          requests:            storage: 100Gi


IOPS为338834. 8个pod，8个pv同时每个pod启用8个dd进程，共64个dd进程测试更改3中statefulset的command命令如下：
...- command:  - &#x27;/bin/bash&#x27;  - &#x27;-c&#x27;  - |    #/bin/bash    for i in &#123;1..8&#125;    do       dd if=/dev/zero of=/data/dd.test$i bs=4k count=200000 oflag=direct &amp;    done        sleep 1000000...


IOPS为76832达到了gold Type下设置的IOPS上限5. 50个pod，50个pv同时每个pod启用3个dd进程，共150个dd进程测试

此时单个PV存储的详情
IOPS为205545达到了gold Type下设置的IOPS上限综合结果如下：



pod数
pv数
dd进程数
IOPS



1
1
1
2908


1
1
8
10000


8
8
8
33883


8
8
64
76832


50
50
150
205545


数据库测试测试工具mydbtest测试配置
$ mysql -uapp -h172.30.213.17 -papp app -e &quot;create table t_mytest(col1 int);&quot;$ cat test.confoptionname apploop 20000user app/app@172.30.213.17:3306:appdeclarea int 10 30000begin#select * from t_mytest where col1 = :a; # 查询insert into t_mytest set col1 = :a;  # 插入end
执行测试过程
./mydbtest_64.bin query=test.conf  degree=40
执行结果
# 插入数据2019-01-23 17:59:35 Total  tran=20000=312/s, qtps=40000=624/s, ela=64046 ms, avg=3202 usSummary: SQL01 exec=800000, rows=0=0/e, avg=65 usSummary: SQL02 exec=800000, rows=800000=100/e, avg=3135 usSummary: exec=12307/s, qtps=24615/s# 创建完索引后，读数据（参考意义不大）2019-01-23 17:56:31 Total  tran=20000=3835/s, qtps=40000=7670/s, ela=5203 ms, avg=260 usSummary: SQL01 exec=800000, rows=22668078=2833/e, avg=174 usSummary: SQL02 exec=800000, rows=0=0/e, avg=69 usSummary: exec=133333/s, qtps=266666/s
插入的qtps为24615&#x2F;s,性能不错。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift之服务网格Istio</title>
    <url>/posts/866b0c24d8f1.html</url>
    <content><![CDATA[
什么是服务网格？服务网络就是指构成应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A&#x2F;B 测试、金丝雀发布、限流、访问控制和端到端认证等。
Istio是什么？Istio 是一个用来连接、管理和保护微服务的开放平台。Istio 提供一种简单的方式来为已部署的服务建立网络，该网络具有负载均衡、服务间认证、监控等功能，而不需要对服务的代码做任何改动。想要让服务支持 Istio，只需要在您的环境中部署一个特殊的 sidecar，使用 Istio 控制平面功能配置和管理代理，拦截微服务之间的所有网络通信。
Istio能做什么？Istio 提供了一个完整的解决方案，通过为整个服务网格提供行为洞察和操作控制来满足微服务应用程序的多样化需求。它在服务网络中统一提供了许多关键功能：

流量管理。控制服务之间的流量和API调用的流向，使得调用更可靠，并使网络在恶劣情况下更加健壮。
服务身份和安全。为网格中的服务提供可验证身份，并提供保护服务流量的能力，使其可以在不同可信度的网络上流转。
策略执行。将组织策略应用于服务之间的互动，确保访问策略得以执行，资源在消费者之间良好分配。可以通过通过配置网格而不是修改应用程序代码来完成策略的更改。
遥测：了解服务之间的依赖关系，以及它们之间流量的本质和流向，从而提供快速识别问题的能力。

Istio的基本架构Istio 服务网格逻辑上分为数据平面和控制平面。

Envoy（数据平面Proxy）
Mixer(负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据。)
Pilot(Envoy sidecar 提供服务发现功能，为智能路由（例如 A&#x2F;B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。)
Citadel(通过内置身份和凭证管理可以提供强大的服务间和最终用户身份验证。)


什么是Red Hat OpenShift服务网格？
Red Hat OpenShift服务网格是一个平台，它提供了对服务网格的行为监控和操作，提供了连接、保护和监控微服务应用程序的统一方式。
基于开源Istio项目，Red Hat OpenShift服务网格在现有的应用程序上添加了一个透明层，而不需要对服务代码进行任何更改。通过在整个环境中部署一个特殊的sidecar proxy，拦截微服务之间的所有网络通信，通过使用控制平面的配置，管理服务网格，可以达到服务的网络控制。
Red Hat OpenShift Service Mesh能够简单地实现服务发现、负载均衡、服务间的认证、故障恢复、统计、监控，同时也提供了复杂的功能，包括A&#x2F;B测试、金丝雀发布、速率限制、访问控制和端到端认证。
当前最新版本的Openshift Service Mesh为 Technology Preview 7（技术预览版7）。它增了3scale Istio Adapter，同时相关软件服务版本为Istio 1.11、Kiali 0.13.x、Jaeger 1.9.0

Red Hat OpenShift Service Mesh当前问题
Red Hat OpenShift Service Mesh不支持网络多租户
Red Hat OpenShift Service Mesh不支持IPV6
istio-init容器需要有privileged权限，或者至少需要root并具有NET管理功能。因为istio-init需要配置Pod中的iptable规则来拦截网络连接。

安装Red Hat OpenShift Service MeshOpenShift Service Mesh准备环境
Openshift 3.11
oc命令行工具与Openshift集群版本匹配
执行oc命令时，必须使用集群管理员账号
配置所有Node节点系统参数
安装的版本为0.7.0

$ cat /etc/sysctl.d/99-elasticsearch.conf vm.max_map_count = 262144$ sysctl vm.max_map_count=262144

相关镜像

registry.access.redhat.com/openshift-istio-tech-preview/istio-operator:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/openshift-ansible:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/citadel:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/proxyv2:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/pilot:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/mixer:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/galley:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/sidecar-injector:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/proxy-init:0.7.0registry.access.redhat.com/openshift-istio-tech-preview/kiali:0.13.0registry.access.redhat.com/distributed-tracing-tech-preview/jaeger-elasticsearch:5.6.10registry.access.redhat.com/distributed-tracing-tech-preview/jaeger-agent:1.9.0registry.access.redhat.com/distributed-tracing-tech-preview/jaeger-collector:1.9.0registry.access.redhat.com/distributed-tracing-tech-preview/jaeger-query:1.9.0docker.io/grafana/grafana:5.4.0docker.io/prom/prometheus:v2.3.1
开始安装
创建CRD文件istio-installation.yaml

$ cat istio-installation.yamlapiVersion: &quot;istio.openshift.com/v1alpha1&quot;kind: &quot;Installation&quot;metadata:  name: &quot;istio-installation&quot;spec:  deployment_type: openshift  istio:    authentication: true    community: false    prefix: registry.access.redhat.com/openshift-istio-tech-preview/    version: 0.7.0  jaeger:    prefix: registry.access.redhat.com/distributed-tracing-tech-preview/    version: 1.9.0    elasticsearch_memory: 1Gi  kiali:    username: username    password: password    prefix: registry.access.redhat.com/openshift-istio-tech-preview/    version: 0.13.0  launcher:    openshift:      user: user      password: password    github:      username: username      token: token    catalog:      filter: booster.mission.metadata.istio      branch: v71      repo: https://github.com/fabric8-launcher/launcher-booster-catalog.git  threeScale:    enabled: false    prefix: registry.access.redhat.com/openshift-istio-tech-preview/    version: 0.2.0    adapter:      listenAddr: 3333      logLevel: info      logJSON: true      reportMetrics: true      metricsPort: 8080      cacheTTLSeconds: 300      cacheRefreshSeconds: 180      cacheEntriesMax: 1000      cacheRefreshRetries: 1      allowInsecureConn: false      clientTimeoutSeconds: 10
注意metadata.name必须是istio-installationspec.launcher.openshift.user和spec.launcher.openshift.password为openshift集群对应的用户名和密码spec.launcher.github.username和spec.launcher.github.token为github对应的账号信息以上是最全的安装，当前也有最简安装的配置
$ cat istio-installation.yamlapiVersion: &quot;istio.openshift.com/v1alpha1&quot;kind: &quot;Installation&quot;metadata:  name: &quot;istio-installation&quot;

安装istio operator相关文件准备相关的配置文件在github上https://github.com/Maistra/openshift-ansible/tree/maistra-0.7/istio相关文件配置列表如下cr-full.yamlcr-kiali.yamlcr-minimal.yamlistio_community_operator_template.yamlistio_product_operator_template.yamlmaster-config.patch
安装istio operator

$ oc new-project istio-operator$ oc new-app -f istio_product_operator_template.yaml --param=OPENSHIFT_ISTIO_MASTER_PUBLIC_URL=&lt;master public url&gt; --param=OPENSHIFT_ISTIO_PREFIX=registry.access.redhat.com/openshift-istio-tech-preview/

OPENSHIFT_ISTIO_MASTER_PUBLIC_URL为openshift集群对外的master地址，例如：https://master.example.com:8443
OPENSHIFT_ISTIO_PREFIX为openshift istio的镜像前缀，如果将准备中的镜像都导入到了自己的私有仓库中，就可以使用它换成自己的镜像仓库地址


验证operator安装是否成功

$ oc logs -n istio-operator $(oc -n istio-operator get pods -l name=istio-operator --output=jsonpath=&#123;.items..metadata.name&#125;)time=&quot;2019-02-11T05:28:25Z&quot; level=info msg=&quot;Go Version: go1.9.4&quot;time=&quot;2019-02-11T05:28:25Z&quot; level=info msg=&quot;Go OS/Arch: linux/amd64&quot;time=&quot;2019-02-11T05:28:25Z&quot; level=info msg=&quot;operator-sdk Version: 0.0.5+git&quot;time=&quot;2019-02-11T05:28:25Z&quot; level=info msg=&quot;Metrics service istio-operator created&quot;time=&quot;2019-02-11T05:28:25Z&quot; level=info msg=&quot;Watching resource istio.openshift.com/v1alpha1, kind Installation, namespace istio-operator, resyncPeriod 0&quot;time=&quot;2019-02-11T05:33:10Z&quot; level=info msg=&quot;Installing istio for Installation istio-installation&quot;
有类似如上的输出就说明安装好了。5. 部署控制平面CONTROL PLANE
oc create -f istio-installation.yaml -n istio-operator
其中istio-installation.yaml为第一步创建的文件。该步骤将创建istio-system项目，并调用operator中的ansible任务完成istio的安装。可以通过观察名为openshift-ansible-istio-installer-job的pod的状态，来确认安装的进度，如果它的状态为completed,说明安装完成。
$ oc get pods -n istio-systemNAME                                          READY     STATUS      RESTARTS   AGE3scale-istio-adapter-7df4db48cf-sc98s         1/1       Running     0          13selasticsearch-0                               1/1       Running     0          29sgrafana-c7f5cc6b6-vg6db                       1/1       Running     0          33sistio-citadel-d6d6bb7bb-jgfwt                 1/1       Running     0          1mistio-egressgateway-69448cf7dc-b2qj5          1/1       Running     0          1mistio-galley-f49696978-q949d                  1/1       Running     0          1mistio-ingressgateway-7759647fb6-pfpd5         1/1       Running     0          1mistio-pilot-7595bfd696-plffk                  2/2       Running     0          1mistio-policy-779454b878-xg7nq                 2/2       Running     2          1mistio-sidecar-injector-6655b6ffdb-rn69r       1/1       Running     0          1mistio-telemetry-dd9595888-8xjz2               2/2       Running     2          1mjaeger-agent-gmk72                            1/1       Running     0          25sjaeger-collector-7f644df9f5-dbzcv             1/1       Running     1          25sjaeger-query-6f47bf4777-h4wmh                 1/1       Running     1          25skiali-7cc48b6cbb-74gcf                        1/1       Running     0          17sopenshift-ansible-istio-installer-job-fbtfj   0/1       Completed   0          2mprometheus-5f9fd67f8-r6b86                    1/1       Running     0          1m$ oc get pods -n devexNAME                          READY     STATUS    RESTARTS   AGEconfigmapcontroller-1-8rr6w   1/1       Running   0          1mlauncher-backend-2-2wg86      1/1       Running   0          1mlauncher-frontend-2-jxjsd     1/1       Running   0          1m

配置master-config，使得应用能够自动注入istio-proxy sidecar在master-config.yaml文件所在目录下新建文件master-config.patch，内容见下面的代码块

$ cd /etc/origin/master/$ cat master-config.patchadmissionConfig:  pluginConfig:    MutatingAdmissionWebhook:      configuration:        apiVersion: apiserver.config.k8s.io/v1alpha1        kubeConfigFile: /dev/null        kind: WebhookAdmission    ValidatingAdmissionWebhook:      configuration:        apiVersion: apiserver.config.k8s.io/v1alpha1        kubeConfigFile: /dev/null        kind: WebhookAdmission$ cp -p master-config.yaml master-config.yaml.prepatch$ oc ex config patch master-config.yaml.prepatch -p &quot;$(cat master-config.patch)&quot; &gt; master-config.yaml$ /usr/local/bin/master-restart api &amp;&amp; /usr/local/bin/master-restart controllers

创建自动注入istio-proxy的deployment，需要满足以下两个条件


所在project下的容器需要privileged与anyuid的scc

oc adm policy add-scc-to-user anyuid -z &lt;service account&gt; -n &lt;namespace&gt;oc adm policy add-scc-to-user privileged -z &lt;service account&gt; -n &lt;namespace&gt;
默认为default为需要istio控制的project

deployment中添加annotations sidecar.istio.io/inject: &quot;true&quot;例如

apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: sleepspec:  replicas: 1  template:    metadata:      annotations:        sidecar.istio.io/inject: &quot;true&quot;      labels:        app: sleep    spec:      containers:      - name: sleep        image: tutum/curl        command: [&quot;/bin/sleep&quot;,&quot;infinity&quot;]        imagePullPolicy: IfNotPresent

卸载Red Hat OpenShift Service Mesh
删除自定义资源

$ oc delete -n istio-operator installation istio-installation

删除operator

$ oc process -f istio_product_operator_template.yaml | oc delete -f -
升级Red Hat OpenShift Service Mesh目前Red Hat OpenShift Service Mesh不支持升级，如果需要使用新版本的Service Mesh，只能通过先卸载再安装的方式。
实战Bookinfo目前Istio的测试例子几本都是用Bookinfo这个应用，它也是istio项目官方的测试例子，同样Red Hat Openshift Service Mesh也是用这个例子来做演练。这个Bookinfo应用包含有4个微服务，分别是：

productpage，该页面会去调用details和reviews服务
details，该服务获取书的信息
reviews，该服务包括对书的评价，它会调用rating服务
ratings，该服务包括书的评分同时reviews服务有三个版本：v1版本不调用 ratings服务v2版本，调用ratings服务，同时给评星显示五个黑色的星星v3版本，调用ratings服务，同时给评星显示五个红色的星星

安装Bookinfo应用
为bookinfo应用单独创建一个project

$ oc new-project myproject

为myproject项目的容器默认添加privileged与anyuid scc

$ oc adm policy add-scc-to-user anyuid -z default -n myproject$ oc adm policy add-scc-to-user privileged -z default -n myproject

在myproject项目下部署应用

$ oc apply -n myproject -f https://raw.githubusercontent.com/Maistra/bookinfo/master/bookinfo.yaml

为bookinfo创建ingress gateway，使得外部能够访问到bookinfo服务

$ oc apply -n myproject -f https://raw.githubusercontent.com/Maistra/bookinfo/master/bookinfo-gateway.yaml

获得ingress gateway的请求域名

$ oc get route -n istio-system istio-ingressgateway -o jsonpath=&#x27;&#123;.spec.host&#125;&#x27;istio-ingressgateway-istio-system.apps.example.com

浏览器中访问：http://istio-ingressgateway-istio-system.apps.example.com/productpage




至此，Bookinfo应用已成功部署好了。
添加额外的规则
不启用mutual TLS

$ curl -o destination-rule-all.yaml https://raw.githubusercontent.com/istio/istio/release-1.0/samples/bookinfo/networking/destination-rule-all.yaml$ oc apply -f destination-rule-all.yaml$ cat destination-rule-all.yamlapiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:  name: productpagespec:  host: productpage  subsets:  - name: v1    labels:      version: v1---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:  name: reviewsspec:  host: reviews  subsets:  - name: v1    labels:      version: v1  - name: v2    labels:      version: v2  - name: v3    labels:      version: v3---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:  name: ratingsspec:  host: ratings  subsets:  - name: v1    labels:      version: v1  - name: v2    labels:      version: v2  - name: v2-mysql    labels:      version: v2-mysql  - name: v2-mysql-vm    labels:      version: v2-mysql-vm---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:  name: detailsspec:  host: details  subsets:  - name: v1    labels:      version: v1  - name: v2    labels:      version: v2---

启用mutual TLS

$ curl -o destination-rule-all-mtls.yaml https://raw.githubusercontent.com/istio/istio/release-1.0/samples/bookinfo/networking/destination-rule-all-mtls.yaml$ oc apply -f destination-rule-all-mtls.yaml$ cat destination-rule-all-mtls.yamlapiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:  name: productpagespec:  host: productpage  trafficPolicy:    tls:      mode: ISTIO_MUTUAL  subsets:  - name: v1    labels:      version: v1---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:  name: reviewsspec:  host: reviews  trafficPolicy:    tls:      mode: ISTIO_MUTUAL  subsets:  - name: v1    labels:      version: v1  - name: v2    labels:      version: v2  - name: v3    labels:      version: v3---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:  name: ratingsspec:  host: ratings  trafficPolicy:    tls:      mode: ISTIO_MUTUAL  subsets:  - name: v1    labels:      version: v1  - name: v2    labels:      version: v2  - name: v2-mysql    labels:      version: v2-mysql  - name: v2-mysql-vm    labels:      version: v2-mysql-vm---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:  name: detailsspec:  host: details  trafficPolicy:    tls:      mode: ISTIO_MUTUAL  subsets:  - name: v1    labels:      version: v1  - name: v2    labels:      version: v2---

列出所有的目标规则

$ oc get destinationrules -o yaml
删除Bookinfo应用
下载清除Bookinfo的脚本代码

$ curl -o cleanup.sh https://raw.githubusercontent.com/Maistra/bookinfo/master/cleanup.sh &amp;&amp; chmod +x ./cleanup.sh

执行cleanup.sh

$ ./cleanup.shnamespace ? [default] myproject

确认清除完成

$ oc get virtualservices -n myprojectNo resources found.$ oc get gateway -n myprojectNo resources found.$ oc get pods -n myprojectNo resources found.
RedHat Openshift Service Mesh扩展组件**RedHat Openshift Service Mesh的全量安装，还包括以下组件：分布式追踪Jaeger、Istio可视化工具Kiali、Prometheus监控及Grafana监控展示。下面分别介绍各个组件的功能与使用。请确保上一步安装的Bookinfo应用正常访问。

分布式追踪JaegerJaeger是Uber开源的分布式跟踪系统，现在已经成为CNCF的开源项目，其灵感来源于Google的Dapper和twitter的Zipkin，从2016年开始该系统在Uber内部得到了广泛的应用。 Jaege在设计上参考了Google的Dapper。

访问并刷新几次bookinfo页面:http://istio-ingressgateway-istio-system.apps.example.com/productpage
打开jaeger页面

$ oc get route -n istio-system jaeger-query -o jsonpath=&#x27;&#123;.spec.host&#125;&#x27;jaeger-query-istio-system.apps.example.com

浏览器访问https://jaeger-query-istio-system.apps.example.com
jaeger页面的左侧Service选择productpage，并点击Find Traces按钮，将在右侧显示轨迹列表

5. 点击列表中的一条轨迹，将打开该轨迹的详细视图


从上图中可以看出，该追踪轨迹是由多个嵌套块组成。每个嵌套都对应一个Bookinfo服务的调用，所有的这些都是为了响应/productpage请求而执行的。
该请求总共耗时46.23ms,详情服务（detail）耗时1.76ms，评论服务（reviews）耗时32.28ms，评分服务（ratings）耗时17.41ms。
每个请求都是由调用客户端与服务端组成，例如：productpage details.myproject.svc.cluster.local:9080，即表示productpage服务调用了details服务。

KialiKiali为网格管理和可观察性提供了良好的用户体验的可视化工具。Kiali 提供以下功能：

服务拓扑图
分布式跟踪
指标度量收集和图标
配置校验
健康检查和显示
服务发现


查看Kiali域名地址

$ oc get route -n istio-system kiali -o jsonpath=&#x27;&#123;.spec.host&#125;&#x27;kiali-istio-system.apps.example.com

浏览器访问 https://kiali-istio-system.apps.example.com

使用安装istio时的istio-installation.yaml中配置的kiali的用户名和密码登录，如：username，password3. OverView页面
可查看各project的运行状况。4. Graph页面在Graph页面上选择Namespace为myproject，可查看bookinfo这个项目的微服务关系图。


在Graph页面中能够看到一个包含所有微服务的图表，在页面上可以方便看到这些服务的相互交互关系。
点击Legend按钮，可以查看每个节点图标代表的含义
鼠标悬停在某个节点上，能够看到该节点相关的调用关系
点击节点，展示该节点相关调用关系的详情


Service页面Service页面可以查看项目的微服务列表，及服务相关的额外信息，如：健康状态、请求错误率。



点击具体服务，可查看服务的详情信息


Istio Config 页面Istio Config页面，可以查看当前运行的Istio配置信息，包括熔断器，规则、网关、故障注入、路由、路由规则、虚拟服务。



点击具体的信息条目查看详情
只能查看，不能修改


分布式追踪器链接分布式追踪器链接,跳转到对应的Jaeger页面

Prometheus监控Prometheus是一个开源的服务工具包，可以按指定的时间间隔从配置的目标中获取监控数据，同时它还支持告警。经常使用Grafana来展示相关数据。

查看Prometheus页面地址

$ oc get route -n istio-system prometheus -o jsonpath=&#x27;&#123;.spec.host&#125;&#x27;prometheus-istio-system.apps.example.com

浏览器访问http://prometheus-istio-system.apps.example.com


Grafana监控展示Grafana是一个开源的监控展示工具，它可以与很多不同的数据源连接，如Mysql, InfluxDB,Prometheus等。图表丰富美观，同时也有告警功能。

查看Grafana页面地址

$ oc get route -n istio-system grafana -o jsonpath=&#x27;&#123;.spec.host&#125;&#x27;grafana-istio-system.apps.example.com

浏览器访问http://grafana-istio-system.apps.example.com



选择Istio Mesh Dashboard查看网格服务监控情况



在Istio Mesh Dashboard页面下点击相关Service，查看对应Service的监控详情



RedHat Openshift Service Mesh Grafana默认的监控图表，不仅仅只有应用服务的监控，同时也有Istio相关组件的监控(Mixer,Pilot)。所有的监控图表如下：


Istio Galley Dashboard
Istio Mesh Dashboard
Istio Performance Dashboard
Istio Service Dashboard
Istio Workload Dashboard
Mixer Dashboard
Pilot Dashboard

总结
Istio 目前是在服务网格的产品中最为火热的产品，目前最新版本为1.1.0
随着服务网格越来越火热，相关的周边应用也不断出现，分布式追踪工具Jaeger，Istio配置管理工具Kiali及监控Prometheus与Grafana也让服务网格更加的完整，可靠。
与之前Openshift 3.10版本手动安装Istio相比，在Openshift 3.11版本通过Istio-operator安装部署，更加方便也更加稳定。

参考文章Openshift官方文档之：Service Mesh
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift使用OpenLDAP作为统一用户认证</title>
    <url>/posts/3069f6ea7b88.html</url>
    <content><![CDATA[
背景Openshift默认的用户认证是使用HTPasswd，之前的部署方式也都是使用了HTPasswd的方式。其实Openshift官方默认支持LDAP协议，可以很容易地将OpenLDAP与Openshift进行集成，使用OpenLDAP集中管理用户。
环境
centos 7.4
openshift 3.10
openldap 2.4.44

openldap安装与使用介绍Openshift集群3.9升级到3.10
用户账号配置及验证登录
Openshift中配置验证方式为LDAPPasswordIdentityProvider

LDAP URL说明：
ldapurl     = scheme &quot;://&quot; [hostport] [&quot;/&quot;[dn [&quot;?&quot; [attributes] [&quot;?&quot; [scope][&quot;?&quot; [filter] [&quot;?&quot; extensions]]]]]]scheme   = &quot;ldap&quot;hostport  = hostport from Section 5 of [RFC 1738]dn           = distinguishedName from Section 3 of [1]attributes = attrdesc *(&quot;,&quot; attrdesc)scope      = &quot;base&quot; / &quot;one&quot; / &quot;sub&quot;filter     = filter from Section 4 of [4]     extensions = extension *(&quot;,&quot; extension)    extension  = [&quot;!&quot;] extype [&quot;=&quot; exvalue]    extype     = token / xtoken    exvalue    = LDAPString from section 4.1.2 of [2]    token      = oid from section 4.1 of [3]    xtoken     = (&quot;X-&quot; / &quot;x-&quot;) token&lt;/pre&gt;# 例子：# ldap://host.com:6666/o=UniversityofMichigan,c=US??sub?(cn=BabsJensen)# ldap://host.com:6666/o=UniversityofMichigan,c=US?mail??(sn=MBU)



scope类型
说明



base
表示把基准DN作为搜索对象。例如：cn&#x3D;yanzong,ou&#x3D;Ops,dc&#x3D;shuyun,dc&#x3D;com 的基准DN是dc&#x3D;shuyun,dc&#x3D;com


one
表示把基准DN的第一层作为搜索对象，如上个例子中的ou&#x3D;Ops为搜索对象。


sub
表示把基准DN及以下的整棵树都作为搜索对象。


使用person对象中的mail作为登录的用户名，同时使用dn作为用户的id,用户的显示名为person对象中的cn，同时只允许带有sn=MBC的用户登录
# /etc/origin/master/master-config.yamloauthConfig:  identityProviders:  - challenge: true    login: true    mappingMethod: claim    name: my_ldap_provider    provider:      apiVersion: v1      attributes:        email:        - mail        id:        - dn        name:        - cn        preferredUsername:        - uid      bindDN: &#x27;&#x27;      bindPassword: &#x27;&#x27;      ca: &#x27;&#x27;      insecure: true      kind: LDAPPasswordIdentityProvider      url: ldap://192.168.0.2:389/ou=users,dc=example,dc=com?mail??(sn=MBC)
重启openshift服务
# master-restart controllers api# master-restart controllers controllers

在openldap中创建对应的用户

创建用户密码
# slappasswd -s test&#123;SSHA&#125;5rMM/3f8Ki13IyarGTtwzieoTu7KMgwc
使用创建的密码及用户信息添加openldap账号
$ cat users.ldifdn: ou=users,dc=example,dc=comobjectClass: organizationalUnitou: usersdn: cn=Michael,ou=users,dc=example,dc=comobjectClass: personobjectClass: organizationalPersonobjectClass: inetOrgPersoncn: Michaelsn: MBCdisplayName: Michael MBCmail: michael@example.comuserPassword: &#123;SSHA&#125;5rMM/3f8Ki13IyarGTtwzieoTu7KMgwc
至此就可以通过：用户名michael@example.com，密码：test进行登录openshift。登录成功后，openshift会自动创建一个普通用户，用户id为cn=Michael,ou=users,dc=example,dc=com
解决了登录，是否就完全OK了呢?openshift与openldap的集成就到此为止了吗？当然不是。用户组配置及同步
在openldap中创建对应的组对象，并将用户Michael设置为组成员

$ cat groups.ldifdn: ou=groups,dc=example,dc=comobjectClass: organizationalUnitou: groupsdn: cn=admins,ou=groups,dc=example,dc=comobjectClass: groupOfNamescn: adminsowner: cn=Manager,dc=example,dc=comdescription: Administrators Groupmember: cn=Michael,ou=users,dc=example,dc=com

openshift同步openldap上的组与用户信息

$ cat rfc2307_config.yamlkind: LDAPSyncConfigapiVersion: v1url: ldap://192.168.0.2:389 insecure: truerfc2307:    groupsQuery:        baseDN: &quot;ou=groups,dc=example,dc=com&quot;        scope: sub        derefAliases: never        filter: (objectclass=groupOfNames)    groupUIDAttribute: dn    groupNameAttributes: [ cn ]    groupMembershipAttributes: [ member ]    usersQuery:        baseDN: &quot;ou=users,dc=example,dc=com&quot;        scope: sub        derefAliases: never        pageSize: 0    userUIDAttribute: dn    userNameAttributes: [ dn ]    tolerateMemberNotFoundErrors: true    tolerateMemberOutOfScopeErrors: true
执行同步
oc adm  groups sync --sync-config=rfc2307_config.yaml --confirm

结果展示

[root@allinone ~]# oc get group NAME      USERSadmins    cn=Michael,ou=users,dc=example,dc=com[root@allinone ~]#[root@allinone ~]# oc get userNAME                                     UID                                    FULL NAME   IDENTITIEScn=Michael,ou=users,dc=example,dc=com    bf612e04-b592-11e8-8841-5254501db2dc   Michael     my_ldap_provider:cn=Michael,ou=users,dc=example,dc=com
通过给admins这个group授权，admins组下面的成员也都具有了对应的权限，实现了权限的管理。 
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift各组件Master-Node-Etcd-Router-Registry证书维护</title>
    <url>/posts/d190629849e1.html</url>
    <content><![CDATA[Openshift集群正常运行过程中，各个组件:Master、Node、Etcd、Router、Registry之前相互通信交互，它们之间都是通过加密协议通信。那么问题来了，对于tls证书是有有效期的，突然有一天，证书过期了怎么办？集群是不是就无法正常运行了呢？现在我们就来看下，怎么能够让加密证书持续有效。

安装时，将证书有效期设置为很长，100年够不够
快速查看当前集群所有证书的有效期
证书过期了，我们该如何进行更新证书

安装时指定证书的有效期默认情况下，etcd证书、openshift证书的有效期为5年，kubelet证书、私有镜像仓库registry证书、Route证书的有效期为2年。在集群安装时可以通过设置ansible&#x2F;hosts中的参数来指定证书的有效期
[OSEv3:vars]openshift_hosted_registry_cert_expire_days=730openshift_ca_cert_expire_days=1825openshift_node_cert_expire_days=730openshift_master_cert_expire_days=730etcd_ca_default_days=1825

查看当前集群所有证书的有效期
确保ansible&#x2F;hosts中的参数有如下信息

openshift_is_atomic=falseansible_distribution=centos

检查

$ ansible-playbook playbooks/openshift-checks/certificate_expiry/easy-mode.yaml$ #执行完成后可在roles/openshift_certificate_expiry/defaults/main.yml$ #中的openshift_certificate_expiry_html_report_path$ #所在路径下（默认是/tmp/cert-expiry-report.html）查看所有证书的过期时间
它将会展示出所有Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书的过期时间

更新证书更新证书方法可以只针对Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书中的一种进行更新，也可以全部进行更新。

确保ansible&#x2F;hosts中的参数有如下信息

openshift_master_cluster_hostname=master.example.comopenshift_master_cluster_public_hostname=master.example.com

重新生成证书进行更新


全部一次性更新  $ ansible-playbook playbooks/redeploy-certificates.yml

只更新master CA证书$ ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml
只更新etcd CA证书$ ansible-playbook playbooks/openshift-etcd/redeploy-ca.yml
只更新master Certificates证书$ ansible-playbook playbooks/openshift-master/redeploy-certificates.yml
只更新etcd Certificates证书ansible-playbook playbooks/openshift-etcd/redeploy-certificates.yml
只更新node Certificates证书ansible-playbook playbooks/openshift-node/redeploy-certificates.yml
只更新私有镜像仓库Rgistry Certificates证书ansible-playbook playbooks/openshift-hosted/redeploy-registry-certificates.yml
只更新Router Certificates证书ansible-playbook playbooks/openshift-hosted/redeploy-router-certificates.yml
只更新etcd Certificates证书ansible-playbook playbooks/openshift-etcd/redeploy-certificates.yml




使用自定义Master CA证书


安装时使用自定义证书
将证书的写在inventory的配置参数中 $ cat /etc/ansible/hosts...[OSEv3.vars]...openshift_master_ca_certificate=&#123;&#x27;certfile&#x27;: &#x27;&lt;/path/to/ca.crt&gt;&#x27;, &#x27;keyfile&#x27;:   &#x27;&lt;/path/to/ca.key&gt;&#x27;&#125;...
执行正常部署 $ ansible-playbook  playbooks/deploy_cluster.yml


已运行集群，更新自定义证书
同上面的1步骤，将证书的写在inventory的配置参数中
运行更新Master CA证书playbook $ ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml



更新完成后遇到的问题
allinone的集群下更新所有证书时，在重启docker那一步中，卡住了。
Router重启一直报错。解决办法：删除secret router-crt证书，让它自动更新。

EFK证书更新相关文件：https://docs.openshift.com/container-platform/3.9/install_config&#x2F;aggregate_logging.html#fluentd-redeploy-certs
通过重新部署EFK来更新证书。

删除旧证书

$ rm -r /etc/origin/logging

确保在inventory文件中设置好了EFK证书相关的配置
执行EFK重新部署脚本

$ cd openshift-ansible$ ansible-playbook playbooks/openshift-logging/config.yml
该命令执行会出现如下错误信息
RUNNING HANDLER [openshift_logging_elasticsearch : Checking current health for &#123;&#123; _es_node &#125;&#125; cluster] ***Friday 14 December 2018 07:53:44 +0000 (0:00:01.571) 0:05:01.710 *******[WARNING]: Consider using the get_url or uri module rather than running curl.If you need to use command because get_url or uri is insufficient you can addwarn=False to this command task or set command_warnings=False in ansible.cfg toget rid of this message.fatal: [ec2-34-207-171-49.compute-1.amazonaws.com]: FAILED! =&gt; &#123;&quot;changed&quot;: true, &quot;cmd&quot;: [&quot;curl&quot;, &quot;-s&quot;, &quot;-k&quot;, &quot;--cert&quot;, &quot;/tmp/openshift-logging-ansible-3v1NOI/admin-cert&quot;, &quot;--key&quot;, &quot;/tmp/openshift-logging-ansible-3v1NOI/admin-key&quot;, &quot;https://logging-es.openshift-logging.svc:9200/_cluster/health?pretty&quot;], &quot;delta&quot;: &quot;0:00:01.024054&quot;, &quot;end&quot;: &quot;2018-12-14 02:53:33.467642&quot;, &quot;msg&quot;: &quot;non-zero return code&quot;, &quot;rc&quot;: 7, &quot;start&quot;: &quot;2018-12-14 02:53:32.443588&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: []&#125;RUNNING HANDLER [openshift_logging_elasticsearch : Set Logging message to manually restart] ***Friday 14 December 2018 07:53:46 +0000 (0:00:01.557) 0:05:03.268 *******

通过删除以下pod，来完成密钥的刷新

$ oc delete pod --all -n openshift-logging
补充
OpenShift 3.10版本起删除了单独更新Node证书的脚本。
OpenShift 3.11版本前更新证书会重启Docker；而在OpenShift 3.11后期版本优化了更新证书过程，不再重启Docker。
如果集群证书已经过期了怎么办？一旦集群证书过期，OpenShift中的各组件的交互都会报错，包括WebConsole。此时一定要及时更新集群的证书。但是如果此时直接运行更新证书的脚本，它会检查证书是否过期，如果过期会报错，并终止更新。此时需要在ansible inventory文件中添加变量：openshift_certificate_expiry_fail_on_warn=flase，再运行更新证书脚本即可。如果ca证书也需要更新的话，需要额外添加变量：openshift_redeploy_openshift_ca=true，再运行更新证书脚本即可。

参考文档官方更新Openshfit证书安装时指定证书的过期时间
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift实现Etcd一键备份与一键恢复脚本</title>
    <url>/posts/eb02cfa8b57d.html</url>
    <content><![CDATA[
一键备份etcd脚本
[root@master01 ~]# cat backup_etcd.sh#!/bin/bashexport ETCD_POD_MANIFEST=&quot;/etc/origin/node/pods/etcd.yaml&quot;export ETCD_EP=$(grep https $&#123;ETCD_POD_MANIFEST&#125; | cut -d &#x27;/&#x27; -f3)oc login -u system:adminexport ETCD_POD=$(oc get pods -n kube-system | grep -o -m 1 &#x27;\S*etcd\S*&#x27;)oc project kube-systemoc exec $&#123;ETCD_POD&#125; -c etcd -- /bin/sh -c &quot;ETCDCTL_API=3 etcdctl --cert /etc/etcd/peer.crt --key /etc/etcd/peer.key --cacert /etc/etcd/ca.crt --endpoints $ETCD_EP snapshot save /var/lib/etcd/snapshot.db&quot;today_date=$(date +%Y%m%d)mkdir -p /backup/$&#123;today_date&#125;/etcdmv /var/lib/etcd/snapshot.db /backup/$&#123;today_date&#125;/etcd/snapshot.dbls /backup/$&#123;today_date&#125;/etcd/echo &quot;success backup etcd&quot;
一键恢复etcd脚本
[root@master01 ~]# cat restore_etcd.sh #!/bin/bashsnapshot_file_dir=$1if [ $# -lt 1 ]then	echo &quot;Please input snapshot file path&quot;	exit 2fiexport ETCD_POD_MANIFEST=&quot;/etc/origin/node/pods/etcd.yaml&quot;mv $&#123;ETCD_POD_MANIFEST&#125; .rm -rf /var/lib/etcd## 获取etcd相关初始化配置项ETCD_CONFIG_FILE=&quot;/etc/etcd/etcd.conf&quot;etcd_data_dir=$(grep ^ETCD_DATA_DIR= $ETCD_CONFIG_FILE|cut -d= -f2)etcd_name=$(grep ^ETCD_NAME= $ETCD_CONFIG_FILE|cut -d= -f2)etcd_initial_cluster=$(grep ^ETCD_INITIAL_CLUSTER= $ETCD_CONFIG_FILE|awk -F&#x27;ETCD_INITIAL_CLUSTER=&#x27; &#x27;&#123;print $2&#125;&#x27;)etcd_initial_cluster_token=$(grep ^ETCD_INITIAL_CLUSTER_TOKEN= $ETCD_CONFIG_FILE|cut -d= -f2)etcd_initial_advertise_peer_urls=$(grep ^ETCD_INITIAL_ADVERTISE_PEER_URLS= $ETCD_CONFIG_FILE|cut -d= -f2)## 恢复etcd数据export ETCDCTL_API=3etcdctl snapshot restore $snapshot_file_dir --data-dir $etcd_data_dir --name $etcd_name --initial-cluster &quot;$etcd_initial_cluster&quot; --initial-cluster-token &quot;$etcd_initial_cluster_token&quot; --initial-advertise-peer-urls $etcd_initial_advertise_peer_urls --skip-hash-check=truechown -R etcd.etcd /var/lib/etcd/restorecon -Rv /var/lib/etcdmv etcd.yaml $ETCD_POD_MANIFEST

一键整理etcd数据，解决证书问题
[root@master01 ~]# cat reset.sh #!/bin/bashoc login -u system:adminprojects=$(oc get projects | awk &#x27;&#123;print $1&#125;&#x27; | grep -v kube-system|grep -v NAME)for project in $(echo $projects)do  oc delete secret $(oc get secret -n $project | grep token | awk &#x27;&#123;print $1&#125;&#x27;) -n $project  oc delete pod $(oc get pod -n $project | grep -v NAME | awk &#x27;&#123;print $1&#125;&#x27;) -n $project --force --grace-period=0done
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift开启Calico-BGP-与-OVS性能PK</title>
    <url>/posts/169c43378e6a.html</url>
    <content><![CDATA[
Openshift网络方案选择
大家都知道K8S在网络插件选择上有很多种，默认的是Flannel，但是它的性能一般，互联网中使用最多的是Calico BGP，因为它的性能非常好。
而对于Openshift，官方只支持ovs一种网络方案，同时RedHat也表示ovs在Openshift平台上运行是最合适的。但是ovs的网络性能怎样呢？因为ovs方案对数据需要进行加包，解包的过程，性能肯定是会受影响的。同时经过实测，在万兆网络中的损耗近50%，虽然在绝大部分场景下ovs已经够用了，但是但是跟几乎无损耗的Calico BGP比起来还是逊色不少。
很庆幸，Openshift虽然官方不作Calico网络方案的支持，但还是很体贴地把它加入到了Openshift的安装脚本中，从而让大家都能方便地使用Calico网络方案，包括IPIP及BGP方案。

安装步骤
在ansible hosts中设置关闭openshift默认的sdn方案，开启calico方案&#x2F;etc&#x2F;ansible&#x2F;hosts

[OSEv3:vars]os_sdn_network_plugin_name=cniopenshift_use_calico=trueopenshift_use_openshift_sdn=false

设置Calico网络配置openshift-ansible&#x2F;roles&#x2F;calico&#x2F;defaults&#x2F;main.yaml

calico_ip_autodetection_method: &quot;first-found&quot;ip_pools:  apiVersion: projectcalico.org/v3  kind: IPPoolList  items:  - apiVersion: projectcalico.org/v3    kind: IPPool    metadata:      name: default-ipv4-ippool    spec:      cidr: &quot;&#123;&#123; openshift_cluster_network_cidr &#125;&#125;&quot;      ipipMode: Always  #默认是为Always，为IPIP模式      natOutgoing: true      nodeSelector: &quot;all()&quot;
配置说明（正确开启calico bgp网络的关键）：calico_ip_autodetection_method
calico_ip_autodetection_method: &quot;interface=eth0&quot;# 默认为“first-found”，如果各主机网络设备名不一样，可以使用正则# calico_ip_autodetection_method: &quot;interface=(eth0|eth1)&quot;
spec.ipipMode
ipipMode: Always  #默认是为Always，为IPIP模式；Never为开启BGP模式
完整配置
---cni_conf_dir: &quot;/etc/cni/net.d/&quot;cni_bin_dir: &quot;/opt/cni/bin/&quot;calico_url_policy_controller: &quot;quay.io/calico/kube-controllers:v3.5.0&quot;calico_node_image: &quot;quay.io/calico/node:v3.5.0&quot;calico_cni_image: &quot;quay.io/calico/cni:v3.5.0&quot;calicoctl_image: &quot;quay.io/calico/ctl:v3.5.0&quot;calico_upgrade_image: &quot;quay.io/calico/upgrade:v1.0.5&quot;calico_ip_autodetection_method: &quot;interface=eth0&quot;# 默认为“first-found”，如果各主机网络设备名不一样，可以使用正则# calico_ip_autodetection_method: &quot;interface=(eth0|eth1)&quot;use_calico_etcd: False# Configure the IP Pool(s) from which Pod IPs will be chosen.ip_pools:  apiVersion: projectcalico.org/v3  kind: IPPoolList  items:  - apiVersion: projectcalico.org/v3    kind: IPPool    metadata:      name: default-ipv4-ippool    spec:      cidr: &quot;&#123;&#123; openshift_cluster_network_cidr &#125;&#125;&quot;      ipipMode: Never #默认是为Always，为IPIP模式；Never为开启BGP模式      natOutgoing: true      nodeSelector: &quot;all()&quot;# Options below are only valid for legacy Calico v2 installations,# and have been superceded by options above for Calico v3.calico_ipv4pool_ipip: &quot;always&quot;

正常执行Openshift安装脚本

$ ansible-playbook playbooks/prerequisites.yml$ ansible-playbook playbooks/deploy_cluster.yml

查看网络

[root@master1 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000    link/ether 52:54:fc:dd:fc:ed brd ff:ff:ff:ff:ff:ff    inet 192.168.0.3/24 brd 192.168.0.255 scope global dynamic eth0       valid_lft 86262sec preferred_lft 86262sec    inet6 fe80::248:584e:2626:2269/64 scope link        valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN     link/ether 02:42:46:89:5d:d0 brd ff:ff:ff:ff:ff:ff    inet 172.17.0.1/16 scope global docker0       valid_lft forever preferred_lft forever4: cali252a8913dc3@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP     link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0    inet6 fe80::ecee:eeff:feee:eeee/64 scope link        valid_lft forever preferred_lft forever5: cali6d8bb449db0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP     link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1    inet6 fe80::ecee:eeff:feee:eeee/64 scope link        valid_lft forever preferred_lft forever6: cali9efe4d704f6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP     link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2    inet6 fe80::ecee:eeff:feee:eeee/64 scope link        valid_lft forever preferred_lft forever[root@master1 ~]# ip routedefault via 192.168.0.1 dev eth0 proto static metric 100 10.128.113.64/26 via 192.168.0.7 dev eth0 proto bird 10.128.141.128/26 via 192.168.0.4 dev eth0 proto bird 10.129.8.0/26 via 192.168.0.9 dev eth0 proto bird 10.129.182.192/26 via 192.168.0.8 dev eth0 proto bird 10.129.200.0/26 via 192.168.0.6 dev eth0 proto bird 10.130.193.128/26 via 192.168.0.10 dev eth0 proto bird blackhole 10.131.9.192/26 proto bird 10.131.9.206 dev cali252a8913dc3 scope link 10.131.9.207 dev cali6d8bb449db0 scope link 10.131.9.208 dev cali9efe4d704f6 scope link 10.131.42.192/26 via 192.168.0.11 dev eth0 proto bird 10.131.148.0/26 via 192.168.0.5 dev eth0 proto bird 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.3 metric 100 

说明：如果要部署路由反射（RR）模式，可参考OpenShift支持Calico BGP 路由反射（RR）模式
网络性能测试测试环境为公有云平台上的虚拟机###iperf测试Pod吞吐量
测试方法与步骤
部署iperf服务端

$ oc new-project test$ oc run iperf-server --image=registry.dcs.cmbchina.cn:9443/tools/iperf3 -- -s$ oc get pod -o wideNAME                   READY     STATUS    RESTARTS   AGE       IP            NODEiperf-server-1-r6z2x   1/1       Running   0          3m        10.131.2.76  node1

部署iperf客户端

$ oc run iperf-client --image=registry.dcs.cmbchina.cn:9443/tools/iperf3 -n project-e --command -- sleep 10000$ oc get pod -o wide | grep qperfNAME                   READY     STATUS    RESTARTS   AGE       IP            NODEiperf-client-3-gtr2l   1/1       Running   0          2h        10.130.0.70   node2qperf-server-1-xxmhz   1/1       Running   0          4h        10.128.2.59    node1

iperf3客户端测试iperf3(pod)吞吐量

$ oc rsh iperf-client-3-gtr2l  $ iperf3 -c 10.131.2.76 
测试结果ovs网络方案测试结果
Connecting to host 10.130.0.51, port 5201[  4] local 10.129.0.50 port 42924 connected to 10.130.0.51 port 5201[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd[  4]   0.00-1.00   sec   282 MBytes  2.36 Gbits/sec  1406    638 KBytes       [  4]   1.00-2.00   sec   326 MBytes  2.74 Gbits/sec  2484    797 KBytes       [  4]   2.00-3.00   sec   324 MBytes  2.71 Gbits/sec  2136    692 KBytes       [  4]   3.00-4.00   sec   314 MBytes  2.63 Gbits/sec  3907    744 KBytes       [  4]   4.00-5.00   sec   323 MBytes  2.71 Gbits/sec  1539    811 KBytes       [  4]   5.00-6.00   sec   323 MBytes  2.71 Gbits/sec  1996    685 KBytes       [  4]   6.00-7.00   sec   318 MBytes  2.67 Gbits/sec  1085    891 KBytes       [  4]   7.00-8.00   sec   286 MBytes  2.40 Gbits/sec  2534    744 KBytes       [  4]   8.00-9.00   sec   336 MBytes  2.82 Gbits/sec  1856    793 KBytes       [  4]   9.00-10.00  sec   256 MBytes  2.14 Gbits/sec  2256    452 KBytes       - - - - - - - - - - - - - - - - - - - - - - - - -[ ID] Interval           Transfer     Bandwidth       Retr[  4]   0.00-10.00  sec  3.01 GBytes  2.59 Gbits/sec  21199             sender[  4]   0.00-10.00  sec  3.01 GBytes  2.59 Gbits/sec                  receiveriperf Done.
calico bgp网络方案测试结果
Connecting to host 10.129.8.3, port 5201[  4] local 10.130.193.131 port 46222 connected to 10.129.8.3 port 5201[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd[  4]   0.00-1.00   sec   735 MBytes  6.17 Gbits/sec  204    655 KBytes       [  4]   1.00-2.00   sec   914 MBytes  7.67 Gbits/sec  353    818 KBytes       [  4]   2.00-3.00   sec  1.01 GBytes  8.70 Gbits/sec    0   1.44 MBytes       [  4]   3.00-4.00   sec  1.02 GBytes  8.76 Gbits/sec  465   1.87 MBytes       [  4]   4.00-5.00   sec  1.02 GBytes  8.79 Gbits/sec  184   2.20 MBytes       [  4]   5.00-6.00   sec  1.03 GBytes  8.81 Gbits/sec  596   1.33 MBytes       [  4]   6.00-7.00   sec  1012 MBytes  8.49 Gbits/sec   17   1.28 MBytes       [  4]   7.00-8.00   sec  1.02 GBytes  8.79 Gbits/sec   46   1.31 MBytes       [  4]   8.00-9.00   sec  1.01 GBytes  8.69 Gbits/sec   87   1.26 MBytes       [  4]   9.00-10.00  sec  1.02 GBytes  8.73 Gbits/sec  133   1.21 MBytes       - - - - - - - - - - - - - - - - - - - - - - - - -[ ID] Interval           Transfer     Bandwidth       Retr[  4]   0.00-10.00  sec  9.73 GBytes  8.36 Gbits/sec  2085             sender[  4]   0.00-10.00  sec  9.73 GBytes  8.36 Gbits/sec                  receiveriperf Done.




网络方案
传输数据量
传输速率



ovs方案
3.01 GB
2.59 Gb


calico bgp方案
9.73 GB
8.36 Gb


qperf测试网络带宽与延时测试方法与步骤
部署qperf服务端

$ oc run qperf-server --image=registry.dcs.cmbchina.cn:9443/tools/qperf$ oc get pod -o wideNAME                   READY     STATUS    RESTARTS   AGE       IP            NODEqperf-server-1-xxmhz   1/1       Running   0          4h        10.128.2.59    node1

部署qperf客户端

$ oc run qperf-client --image=registry.dcs.cmbchina.cn:9443/tools/qperf --command -- sleep 10000$ oc get pod -o wide -n project-e | grep qperfNAME                   READY     STATUS    RESTARTS   AGE       IP            NODEqperf-client-2-7jmvb   1/1       Running   0          4h        10.130.2.224   node2qperf-server-1-xxmhz   1/1       Running   0          4h        10.128.2.59    node1

qperf客户端测试qperf(pod)带宽与延时

$ oc rsh qperf-client-2-7jmvb  $ qperf 10.128.2.59 -t 10 -oo msg_size:8:256K:*2 tcp_bw tcp_lat
测试结果ovs网络方案qperf测试结果
tcp_bw:    bw  =  15 MB/sectcp_bw:    bw  =  26.4 MB/sectcp_bw:    bw  =  40.7 MB/sectcp_bw:    bw  =  59.5 MB/sectcp_bw:    bw  =  76.1 MB/sectcp_bw:    bw  =  194 MB/sectcp_bw:    bw  =  239 MB/sectcp_bw:    bw  =  256 MB/sectcp_bw:    bw  =  258 MB/sectcp_bw:    bw  =  262 MB/sectcp_bw:    bw  =  259 MB/sectcp_bw:    bw  =  250 MB/sectcp_bw:    bw  =  272 MB/sectcp_bw:    bw  =  291 MB/sectcp_bw:    bw  =  272 MB/sectcp_bw:    bw  =  282 MB/sectcp_lat:    latency  =  34.2 ustcp_lat:    latency  =  34.3 ustcp_lat:    latency  =  33.9 ustcp_lat:    latency  =  33.4 ustcp_lat:    latency  =  34.1 ustcp_lat:    latency  =  34.1 ustcp_lat:    latency  =  34.2 ustcp_lat:    latency  =  34.8 ustcp_lat:    latency  =  46.3 ustcp_lat:    latency  =  56 ustcp_lat:    latency  =  86.5 ustcp_lat:    latency  =  133 ustcp_lat:    latency  =  219 ustcp_lat:    latency  =  435 ustcp_lat:    latency  =  733 ustcp_lat:    latency  =  1.27 ms
calico bgp网络方案qperf测试结果
tcp_bw:    bw  =  17 MB/sectcp_bw:    bw  =  32.1 MB/sectcp_bw:    bw  =  39.4 MB/sectcp_bw:    bw  =  81.7 MB/sectcp_bw:    bw  =  141 MB/sectcp_bw:    bw  =  297 MB/sectcp_bw:    bw  =  703 MB/sectcp_bw:    bw  =  790 MB/sectcp_bw:    bw  =  845 MB/sectcp_bw:    bw  =  708 MB/sectcp_bw:    bw  =  830 MB/sectcp_bw:    bw  =  884 MB/sectcp_bw:    bw  =  768 MB/sectcp_bw:    bw  =  787 MB/sectcp_bw:    bw  =  749 MB/sectcp_bw:    bw  =  780 MB/sectcp_lat:    latency  =  95.8 ustcp_lat:    latency  =  71.5 ustcp_lat:    latency  =  69.1 ustcp_lat:    latency  =  69.6 ustcp_lat:    latency  =  72.7 ustcp_lat:    latency  =  84 ustcp_lat:    latency  =  93.3 ustcp_lat:    latency  =  86.3 ustcp_lat:    latency  =  145 ustcp_lat:    latency  =  139 ustcp_lat:    latency  =  158 ustcp_lat:    latency  =  171 ustcp_lat:    latency  =  198 ustcp_lat:    latency  =  459 ustcp_lat:    latency  =  593 ustcp_lat:    latency  =  881 us



包大小
ovs方案带宽
calico bgp方案带宽
ovs方案时延
calico bgp方案时延



msg_size
ovs tcp_bw
calico bgp tcp_bw
ovs tcp_lat
calico bgp tcp_lat


8bytes
15 MB&#x2F;sec
17 MB&#x2F;sec
34.2 us
95.8 us


16bytes
26.4 MB&#x2F;sec
32.1 MB&#x2F;sec
34.4 us
71.5 us


32bytes
40.7 MB&#x2F;sec
39.4 MB&#x2F;sec
33.9 us
69.1 us


64bytes
59.5MB&#x2F;sec
81.7 MB&#x2F;sec
33.4 us
69.6 us


128bytes
76.1 MB&#x2F;sec
141 MB&#x2F;sec
34.1 us
72.7 us


256bytes
194 MB&#x2F;sec
297 MB&#x2F;sec
34.1 us
84 us


512bytes
239 MB&#x2F;sec
703 MB&#x2F;sec
34.2 us
93.3 us


1KiB
256 MB&#x2F;sec
790 MB&#x2F;sec
34.8 us
86.3 us


2KiB
258 MB&#x2F;sec
845 MB&#x2F;sec
46.3 us
145 us


4KiB
262 MB&#x2F;sec
708 MB&#x2F;sec
56 us
139 us


8KiB
259 MB&#x2F;sec
830 MB&#x2F;sec
86.5 us
158 us


16KiB
250 MB&#x2F;sec
884 MB&#x2F;sec
133 us
171 us


32KiB
272 MB&#x2F;sec
768 MB&#x2F;sec
219 us
198 us


64KiB
291 MB&#x2F;sec
787 MB&#x2F;sec
435 us
459 us


128KiB
272 MB&#x2F;sec
749 MB&#x2F;sec
733 us
593 us


256KiB
282 MB&#x2F;sec
780 MB&#x2F;sec
1.27 ms
881 us


结果总结从测试的数据中可以看到对于小包传输，Calico BGP的优势并不明显，同时它的网络延时甚至会更高,而对于大包传输，Calico BGP网络方案明显好于ovs方案。


文章已结束，以下并没有内容了。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift排错技巧——From-Redhat大客户交流会</title>
    <url>/posts/b7b877349e97.html</url>
    <content><![CDATA[
今天参加Redhat大客户交流会，有一个主题是Redhat的小伙伴介绍Openshift的排错技巧。这个还是很值得参考的，于是将内容发在了这里，以便更多的小伙伴能够从中受益。当然Redhat小伙伴列出的也只是一部分，同时有些排错细节PPT中也并没有具体写，这篇中我就不扩展了。以下为PPT的内容。
OpenShift排错技巧
环境基本信息收集
日志等级
应用程序
OC客户端排错
镜像仓库
网络
路由
Installer
DNS
Etcd

日志等级openshift service log:&#x2F;etc&#x2F;origin&#x2F;master&#x2F;master.env #同时作用于API和ControllersDEBUG_LOGLEVEL&#x3D;4
&#x2F;etc&#x2F;sysconfig&#x2F;atomic-openshift-nodeOPTIONS&#x3D;–loglevel&#x3D;4
错误等级

0 - Errors and warning only
2 - Normal information
4 - Debugging-level information
6 - API-level debugging information (request &#x2F; response)
8 - Body-level API debugging information

Docker Log level&#x2F;etc&#x2F;sysconfig&#x2F;docker –log-level&#x3D;debug
Etcd Log level# source &#x2F;etc&#x2F;etcd&#x2F;etcd.conf# curl –cert $ETCD_PEER_CERT_FILE –key $ETCD_PEER_KEY_FILE –cacert $ETCD_TRUSTED_CA_FILE $ETCD_ADVERTISE_CLIENT_URLS&#x2F;config&#x2F;local&#x2F;log -XPUT -d ‘{“Level”:”DEBUG”}’
Get Log# &#x2F;usr&#x2F;local&#x2F;bin&#x2F;master-logs etcd etcd &gt; $(hostname)-etcd.log 2&gt;&amp;1
OpenShift Builder Pod LogsBUILD_LOGLEVEL in BC&#x2F;env ## BC的环境变量中设置
应用日志三个不同阶段： build errors, deployment errors 和applications errors
Build Errors# oc logs bc&#x2F;
Deployment Errors# oc get status -o wide -n # oc get events -o wide -n 
Application Errors# oc logs pod&#x2F; -p# oc debug pod&#x2F;deploy a version of this pod without liveness and rediness probes as well as setting the entrypoint to the shell.
OC客户端oc客户端日志等级# oc whoami –loglevel&#x3D;8
Setting this value between 6 and 8 will provide extensive logging
API requests being send (loglevel 6)headers (loglevel 7)responses received (loglevel 8)
OpenShift Registry健康检查基本的健康检查、保证Registry正常运行并且正常响应其对应的service地址
# RegistryAddr&#x3D;$(oc get svc docker-registry -n default -o jsonpath&#x3D;{.spec.clusterIP}:{.spec.ports[0].port})
# curl -vk https:&#x2F;&#x2F;$RegistryAddr&#x2F;healthz
测试镜像仓库docker login -u openshift -p $(oc whoami -t) :docker pull&#x2F;tag&#x2F;push
如果使用存储# oc rsh $(oc get pods -o name -l docker-registry -n default)
OpenShift NetworkingDebugging External Access to an HTTP ServiceDebugging Node to Node NetworkingDebugging Local Networking
使用NetWorking Diagnostics Tool检查网络状况https://docs.openshift.com/container-platform/3.11/admin_guide/sdn_troubleshooting.html
OpenShift Routing分段检查 定位问题 curl pod &#x2F; svc
$ oc logs dc&#x2F;router -n default$ oc get dc&#x2F;router -o yaml default$ oc get route  -n $ oc get endpoints –all-namespaces$ oc exec -it $ROUTER_POD – ls -la$ oc exec -it $ROUTER_POD – find &#x2F;var&#x2F;lib&#x2F;haproxy -regex  “.(.map|config.|.json)“ -print -exec cat {} ; &gt; haproxy_configs_and_maps
router log
Router健康状态查看http://admin:@:1936&#x2F;haproxy_stats
enable access log to syslog server
OpenShift InstallerOpenShift Ansible Playbooks
# ansible-playbook  -vvv | tee ansible.logs
如果在某一个task上失败，可以访问github上的源码查找对应task具体操作步骤：Access the Git Hub Install Repo: openshift&#x2F;openshift-ansible
OpenShift DNSDnsmasq 是一个小型的DNS缓存服务器。它可以根据缓存来响应DNS查询或将其转发到外部真实的DNS服务器上，它安装在每个节点上。
Skydns是一个建立在ETCD之上的DNS服务器，它嵌入在节点的进程中，主要负责相应内部service的解析。
NetworkManager会启动origin dispatcher &#x2F;etc&#x2F;NetworkManager&#x2F;dispatcher.d&#x2F;99-origin-dns.sh以配置&#x2F;etc&#x2F;resolv.conf和一些其他文件
NetworkManager请确定NetworkManger服务正常运行请查看&#x2F;etc&#x2F;NetworkManager&#x2F;dispatch.d&#x2F;99-origin-dns.sh为可执行请确定&#x2F;etc&#x2F;resolv.conf文件包含主机的私网IP，并且有正确的search域。&#x2F;etc&#x2F;resolv.conf是由NetworkManager服务生成的
请检查dnsmasq服务是否oksystemctl status dnsmasq -l
OpenShift Etcd设置etcd变量# source &#x2F;etc&#x2F;etcd&#x2F;etcd.conf# export ETCDCTL_API&#x3D;3
Set endpoint variable to include all etcd endpoints# ETCD_ALL_ENDPOINTS&#x3D;$(etcdctl –cert&#x3D;$ETCD_PEER_CERT_FILE –key $ETCD_PEER_KEY_FILE –cacert $ETCD_TRUSTED_CA_FILE –endpoints&#x3D;$ETCD_LISTEN_CLIENT_URLS –write-out&#x3D;fields member list  | awk ‘&#x2F;ClientURL&#x2F;{printf”%s%s”, sep, $3; sep&#x3D;”,”}’)
check health of etcd# etcdctl –cert&#x3D;$ETCD_PEER_CERT_FILE –key $ETCD_PEER_KEY_FILE –cacert $ETCD_TRUSTED_CA_FILE –endpoints&#x3D;$ETCD_LISTEN_CLIENT_URLS –write-out&#x3D;table endpoint status
# etcdctl –cert&#x3D;$ETCD_PEER_CERT_FILE –key $ETCD_PEER_KEY_FILE –cacert $ETCD_TRUSTED_CA_FILE –endpoints&#x3D;$ETCD_LISTEN_CLIENT_URLS –write-out&#x3D;table endpoint health
最佳实践推荐Red Hat OpenShift Container Platform Life Cycle Policyhttps://access.redhat.com/support/policy/updates/openshift
OpenShift Container Platform Tested Integrations supported configurationhttps://access.redhat.com/articles/2176281
不推荐

Master节点和Infra节点混用
外部负载均衡和openshift节点混用
单独升级某个组件版本
service ip

排错指南推荐Troubleshooting OpenShift Container Platform: Cluster Metricshttps://access.redhat.com/articles/2448341
Troubleshooting OpenShift Container Platform 3.x: Aggregating Container Logginghttps://access.redhat.com/articles/3136551
Troubleshooting OpenShift Container Platform: Middleware Containershttps://access.redhat.com/articles/3135421
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift模板之项目创建模板（资源限制）</title>
    <url>/posts/65cec275d806.html</url>
    <content><![CDATA[Openshift创建project时，可以按照设定的模板来创建，这就省去了很多初始化的工作。###文件内容(template.yaml)
apiVersion: v1kind: Templatemetadata:  creationTimestamp: null  name: project-requestobjects:- apiVersion: v1  kind: Project  metadata:    annotations:      openshift.io/description: $&#123;PROJECT_DESCRIPTION&#125;      openshift.io/display-name: $&#123;PROJECT_DISPLAYNAME&#125;      openshift.io/requester: $&#123;PROJECT_REQUESTING_USER&#125;    creationTimestamp: null    name: $&#123;PROJECT_NAME&#125;  spec: &#123;&#125;  status: &#123;&#125;- apiVersion: v1  kind: ResourceQuota  metadata:    name: $&#123;PROJECT_NAME&#125;-quota  spec:    hard:      pods: 10      requests.cpu: 4000m      requests.memory: 8Gi      resourcequotas: 1      requests.storage: 25Gi      persistentvolumeclaims: 5- apiVersion: v1  kind: LimitRange  metadata:     name: $&#123;PROJECT_NAME&#125;-limits    creationTimestamp: null  spec:     limits:       -         type: Container        max:           cpu: 4000m          memory: 1024Mi        min:           cpu: 10m          memory: 5Mi        default:           cpu: 4000m          memory: 1024Mi        defaultRequest:          cpu: 100m          memory: 512Mi- apiVersion: v1  groupNames:  - system:serviceaccounts:$&#123;PROJECT_NAME&#125;  kind: RoleBinding  metadata:    creationTimestamp: null    name: system:image-pullers    namespace: $&#123;PROJECT_NAME&#125;  roleRef:    name: system:image-puller  subjects:  - kind: SystemGroup    name: system:serviceaccounts:$&#123;PROJECT_NAME&#125;  userNames: null- apiVersion: v1  groupNames: null  kind: RoleBinding  metadata:    creationTimestamp: null    name: system:image-builders    namespace: $&#123;PROJECT_NAME&#125;  roleRef:    name: system:image-builder  subjects:  - kind: ServiceAccount    name: builder  userNames:  - system:serviceaccount:$&#123;PROJECT_NAME&#125;:builder- apiVersion: v1  groupNames: null  kind: RoleBinding  metadata:    creationTimestamp: null    name: system:deployers    namespace: $&#123;PROJECT_NAME&#125;  roleRef:    name: system:deployer  subjects:  - kind: ServiceAccount    name: deployer  userNames:  - system:serviceaccount:$&#123;PROJECT_NAME&#125;:deployer- apiVersion: v1  groupNames: null  kind: RoleBinding  metadata:    creationTimestamp: null    name: admin    namespace: $&#123;PROJECT_NAME&#125;  roleRef:    name: admin  subjects:  - kind: User    name: $&#123;PROJECT_ADMIN_USER&#125;  userNames:  - $&#123;PROJECT_ADMIN_USER&#125;parameters:- name: PROJECT_NAME- name: PROJECT_DISPLAYNAME- name: PROJECT_DESCRIPTION- name: PROJECT_ADMIN_USER- name: PROJECT_REQUESTING_USER

将它导入模板oc create -f template.yaml -n default

将它设为Project默认配置#/etc/origin/master/master-config.yaml...projectConfig:  projectRequestTemplate: &quot;default/project-request&quot;  ...

重启Master，使配置生效sudo systemctl restart atomic-openshift-master

验证oc new-project tem-test

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift生产环境部署配置事项</title>
    <url>/posts/6a6e96f6251d.html</url>
    <content><![CDATA[1. 主机配置推荐
master 16核 32GB 网卡带宽不低于1Gb。CPU x86_64架构，核数和主机数线性递增，每增加一台主机增加0.1核。5台主机4.5核，总的核数为4+0.1 * 主机数内存与主机数线性递增，每增一台主机增加200M内存，5台8G，总的内存数为7+0.2 * 主机数
node 40核 256GB  网卡带宽不低于1Gb根据应用场景估算

2. 磁盘目录挂载
master磁盘格式：xfs ftype&#x3D;1&#x2F; : 10GB&#x2F;var&#x2F;log ：50GB&#x2F;var&#x2F;lib&#x2F;docker：100GB 做raid高可用&#x2F;var&#x2F;lib&#x2F;etcd [ssd]：20GB 做raid高可用&#x2F;var ：50GB 可根据实际进行调整，主要emptyDir的存储在&#x2F;var&#x2F;lib&#x2F;origin目录下
node磁盘格式：xfs ftype&#x3D;1&#x2F; : 10GB&#x2F;var&#x2F;log ：50GB&#x2F;var&#x2F;lib&#x2F;docker：100GB 做raid高可用&#x2F;var ：50GB 可根据实际进行调整，主要emptyDir的存储在&#x2F;var&#x2F;lib&#x2F;origin目录下

mkfs.xfs -n ftype=1 /path/to/your/device
说明：xfs文件格式，docker overlay2存储设备必须设置ftype=1。
3. 关闭swapswapoff -acat /etc/fstab ## 注释掉swap
4. 打开seLinux enabledsed -i &#x27;s/SELINUX=disabled/SELINUX=permissive/&#x27; /etc/selinux/config
5. 设置系统参数$ cat /etc/sysctl.conf # 禁用整个系统所有接口的IPv6net.ipv6.conf.all.disable_ipv6 = 1vm.swappiness = 0net.netfilter.nf_conntrack_max = 1000000$ lsmod | grep conntrack || modprobe ip_conntrack$ sysctl -w net.netfilter.nf_conntrack_max=1000000$ sysctl -p /etc/sysctl.conf
6. 更改resolve.conf$ cat /etc/resolv.confsearch cluster.localnameserver 192.168.0.2
7. 时间同步$ ansible all -m package -a &#x27;name=chrony state=present&#x27;## chronyd服务端配置$ cat /etc/chrony.confserver 55.15.226.193 iburstallow 55.15.226.0/24local stratum 10
强制同步时间
## chrony客户端配置chronyc sources -vsystemctl stop chronydchronyd -q &#x27;pool 55.15.226.193 iburst&#x27;

8.创建docker 用户组groupadd docker
将普通用户添加到docker用户组
usermod -aG docker $&#123;USER&#125;
9. docker设置&#x2F;etc&#x2F;sysconfig&#x2F;docker-storageDOCKER_STORAGE_OPTIONS&#x3D;”–storage-driver overlay2 “&#x2F;etc&#x2F;sysconfig&#x2F;dockerOPTIONS&#x3D;” –log-opt max-size&#x3D;1M –log-opt max-file&#x3D;3 –live-restore&#x3D;true “
设置docker与kubelet的cgroup driver为systemd。OpenShift默认安装就是设置的systemd，而社区版的kubelet默认是cgroupfs，需要注意。。
10. 网卡配置
配置网卡多队列：ethtool -l eth0查看网卡多队列Combined数
NetworkManager, 是一个提供网络检测和配置网络的工具，在Node节点需要使用它来自动配置节点的dnsmasq作为默认的网络入口。
网络设备的配置中**&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth***默认NM_CONTROLLED是被设置为yes,如果它被设置为no，那么NetworkManager应用将不会去自动创建dnsmasq相关的配置，所以此时需要手动配置dnsmasq。

添加文件
$ cat /etc/dnsmasq.d/origin-upstream-dns.confserver=192.168.0.2$ cat /etc/origin/node/resolv.confnameserver 192.168.0.2

参考 install-config-network-using-firewalld11. 多网卡管理网：集群间组件通信，Node与Master节点通信网络业务网：应用间网络通信，pod间网络通信存储网：与存储设备网络通信还可以将与外部镜像仓库的网络也考虑进去每个网络，使用两张网卡做bond，提高网络性能及可用性。其中管理网与业务网必须互通，否则部分组件服务将不可用。
12.外部节点相关组件
时间同步服务（chronyd）
DNS(dnsmasq)
镜像仓库(docker-distribution)
负载均衡器（Haproxy）

13. 外部镜像仓库授权将私有镜像仓库的CA文件拷贝到镜像仓库所在服务器的&#x2F;etc&#x2F;pki&#x2F;ca-trust&#x2F;source&#x2F;anchors&#x2F;目录下
$ ansible all -m copy -a &#x27;src=registry.crt dest=/etc/pki/ca-trust/source/anchors/registry.crt&#x27;$ update-ca-trust 
为OpenShift节点设置默认的登录信息
$ # 在/etc/ansible/hosts中添加认证用户oreg_auth_user=&quot;&lt;用户名&gt;&quot;oreg_auth_password=&quot;&lt;密码&gt;&quot;$ oc login &lt;镜像仓库url&gt; -u &lt;用户名&gt;$ ansible -m copy -a &#x27;src=/root/.docker dest=/var/lib/origin&#x27; all$ ansible -m service -a &#x27;name=origin-node state=restarted&#x27; all
14. 内核优化（openshift安装会自动配置）$ ansible all -m package -a &#x27;name=tuned state=present&#x27;$ ansible all -m service -a &#x27;name=tuned state=started enabled=true&#x27;$ ansible all -m shell -a &#x27;tuned-adm profile throughput-performance&#x27;
15. ansible设置reserved。OpenShift官方推荐规则通常，它需要保留5％-10％的节点资源来保护节点，越高越安全。AWS的规则：内存预留值（AWS）:
Reserved memory &#x3D; 255MiB + 11MiB * MAX_POD_PER_INSTANCE
CPU预留值（AWS）:

6% of the first core
1% of the next core (up to 2 cores)
0.5% of the next 2 cores (up to 4 cores)
0.25% of any cores above 4 cores

GKE的规则：内存预留值（GKE）:

255 MiB of memory for machines with less than 1 GB of memory
25% of the first 4GB of memory
20% of the next 4GB of memory (up to 8GB)
10% of the next 8GB of memory (up to 16GB)
6% of the next 112GB of memory (up to 128GB)
2% of any memory above 128GB

CPU预留值（GKE）::

6% of the first core
1% of the next core (up to 2 cores)
0.5% of the next 2 cores (up to 4 cores)
0.25% of any cores above 4 cores

例子：2 vCPU and 7.5GB
Allocatable memory = 0.25 * 4 (first 4GB) + 0.2 * 3.5 (remaining 3.5GB)Allocatable CPU = 0.06 * 1 (first core) + 0.01 * 1 (second core)
Azure的规则：内存预留值（Azure）:

255 MiB of memory for machines with less than 1 GB of memory
25% of the first 4GB of memory
20% of the next 4GB of memory (up to 8GB)
10% of the next 8GB of memory (up to 16GB)
6% of the next 112GB of memory (up to 128GB)
2% of any memory above 128GB

CPU预留值（Azure）:



核数 core
预留 millicores



1
60


2
100


4
140


8
180


16
260


32
420


64
740


另外：



Google和亚马逊产品的hard eviction threshold 为100MB，而AKS则为750MB。



[OSEv3:vars]# 节点配置低的话可参考openshift_node_kubelet_args=&#123;&#x27;pods-per-core&#x27;: [&#x27;10&#x27;], &#x27;max-pods&#x27;: [&#x27;250&#x27;], &#x27;image-gc-high-threshold&#x27;: [&#x27;85&#x27;], &#x27;image-gc-low-threshold&#x27;: [&#x27;80&#x27;], &#x27;system-reserved&#x27;:[&#x27;cpu=200m&#x27;, &#x27;memory=1G&#x27;], &#x27;kube-reserved&#x27;:[&#x27;cpu=200m&#x27;,&#x27;memory=1G&#x27;]&#125;# 节点配置高的话可参考 openshift_node_kubelet_args=&#123;&#x27;pods-per-core&#x27;: [&#x27;10&#x27;], &#x27;max-pods&#x27;: [&#x27;250&#x27;], &#x27;image-gc-high-threshold&#x27;: [&#x27;85&#x27;], &#x27;image-gc-low-threshold&#x27;: [&#x27;80&#x27;], &#x27;system-reserved&#x27;:[&#x27;cpu=500m&#x27;, &#x27;memory=1G&#x27;], &#x27;kube-reserved&#x27;:[&#x27;cpu=1&#x27;,&#x27;memory=2G&#x27;]&#125;
16. 配置集群对master控制台的public域名证书及应用Route路由服务的域名证书openshift_master_cluster_hostname=master.example.comopenshift_master_cluster_public_hostname=master_public.example.comopenshift_master_default_subdomain=apps.example.comopenshift_master_named_certificates=[&#123;&quot;certfile&quot;: &quot;/data/cert/master_public.example.com.crt&quot;, &quot;keyfile&quot;: &quot;/data/cert/master_public.example.com.key&quot;, &quot;names&quot;: [&quot;master_public.example.com&quot;], &quot;cafile&quot;: &quot;/data/cert/example.com_ca.crt&quot;&#125;]openshift_master_overwrite_named_certificates=trueopenshift_hosted_router_certificate=&#123;&quot;certfile&quot;: &quot;/data/cert/apps.example.com.crt&quot;, &quot;keyfile&quot;: &quot;/data/cert/apps.example.com.key&quot;, &quot;cafile&quot;: &quot;/data/cert/example.com_ca.crt&quot;&#125;
其中各证书的文件名不要使用与Master组件默认的名字重复，否则会覆盖掉组件间的自签证书。
另外可以自签证书生成长有效期的相关证书。自签证书步骤如下：

根证书创建

$ openssl genrsa -out ca.key 2048$ openssl req -new -x509 -days 36500 -key ca.key -out ca.crt -subj &quot;/C=CN/ST=shanxi/L=taiyuan/O=cn/OU=test/CN=example.com&quot;$ #或者 openssl req -new -x509 -days 36500 -key ca.key -out ca.crt 手动输入配置

创建证书并使用根证书签发

$ openssl genrsa -out app.key 2048$ openssl req -new -key app.key -out app.csr$ openssl x509 -req -in app.csr -CA ca.crt -CAkey ca.key -out app.crt -days 3650  -CAcreateserial

使用 Openssl 工具查看证书信息

$ openssl x509 -in signed.crt -noout -dates$ openssl x509 -in signed.crt -noout -subject$ openssl x509 -in signed.crt -noout -text
17. 添加集群自动审批证书签发请求OpenShift 3.11中默念Node的证书有效期为1年，满1年后会自动更新证书。更新证书时，该节点会向集群发送证书签发请求，批准之后才能继续添加到集群。
[OSEv3:vars]openshift_master_bootstrap_auto_approve=true
说明：对于已经部署好的集群可以通过执行ansible-playbook来配置
# ansible-playbook -vvv openshift-ansible/playbooks/openshift-master/enable_bootstrap.yml -e openshift_master_bootstrap_auto_approve=true

18. ansible中设置Docker存储type及Docker与etcd额外磁盘[OSEv3:vars]# Docker setup for extra disks on nodescontainer_runtime_docker_storage_setup_device=/dev/vdbcontainer_runtime_docker_storage_type=overlay2openshift_node_local_quota_per_fsgroup=512Mi[masters:vars]container_runtime_extra_storage=[&#123;&#x27;device&#x27;: &#x27;/dev/vdc&#x27;, &#x27;path&#x27;: &#x27;/var/lib/origin/openshift.local.volumes&#x27;, &#x27;options&#x27;: &#x27;gquota&#x27;, &#x27;filesystem&#x27;: &#x27;xfs&#x27;, &#x27;format&#x27;: &#x27;True&#x27;&#125;, &#123;&#x27;device&#x27;: &#x27;/dev/vdd&#x27;, &#x27;path&#x27;: &#x27;/var/lib/etcd&#x27;, &#x27;hosts&#x27;: &#x27;masters&#x27;, &#x27;filesystem&#x27;: &#x27;xfs&#x27;, &#x27;format&#x27;: &#x27;True&#x27;&#125;][nodes:vars]container_runtime_extra_storage=[&#123;&#x27;device&#x27;: &#x27;/dev/vdc&#x27;, &#x27;path&#x27;: &#x27;/var/lib/origin/openshift.local.volumes&#x27;, &#x27;options&#x27;: &#x27;gquota&#x27;, &#x27;filesystem&#x27;: &#x27;xfs&#x27;, &#x27;format&#x27;: &#x27;True&#x27;&#125;]
19. 设置日志自动归档
journal日志归档  减少/var/log/journal的日志，设置/etc/systemd/journald.conf

  $ cat /etc/systemd/journald.conf  [Journal] Storage=persistent Compress=yes#Seal=yes#SplitMode=uid SyncIntervalSec=1s RateLimitInterval=1s RateLimitBurst=10000 SystemMaxUse=1G SystemKeepFree=20% SystemMaxFileSize=10M#RuntimeMaxUse=#RuntimeKeepFree=#RuntimeMaxFileSize= MaxRetentionSec=3days MaxFileSec=1day ForwardToSyslog=False#ForwardToKMsg=no#ForwardToConsole=no ForwardToWall=False#TTYPath=/dev/console#MaxLevelStore=debug#MaxLevelSyslog=debug#MaxLevelKMsg=notice#MaxLevelConsole=info#MaxLevelWall=emerg$ systemctl restart systemd-journald
或者部署时更新以下文件内容(openshift 3.9以上)roles/openshift_node/defaults/main.yml
...journald_vars_to_replace:- &#123; var: Storage, val: persistent &#125;- &#123; var: Compress, val: yes &#125;- &#123; var: SyncIntervalSec, val: 1s &#125;- &#123; var: RateLimitInterval, val: 1s &#125;- &#123; var: RateLimitBurst, val: 10000 &#125;- &#123; var: SystemMaxUse, val: 1G &#125;- &#123; var: SystemKeepFree, val: 20% &#125;- &#123; var: SystemMaxFileSize, val: 10M &#125;- &#123; var: MaxRetentionSec, val: 3days &#125;- &#123; var: MaxFileSec, val: 1day &#125;- &#123; var: ForwardToSyslog, val: no &#125;- &#123; var: ForwardToWall, val: no &#125;...

message日志归档只收集warning以上的日志/etc/rsyslog.conf

$ cat /etc/rsyslog.conf*.warning;mail.none;authpriv.none;cron.none  /var/log/messages
将message日志只保留最近三天的日志
$ cat /etc/logrotate.d/syslog/var/log/cron/var/log/messages&#123;  daily  rotate 3  sharedscripts  postrotate     /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true  endscript&#125;
如果要设置普通用户可查看&#x2F;var&#x2F;log&#x2F;messages文件，需要在&#x2F;etc&#x2F;rsyslog.conf配置的前面添加messages文件可读权限
$umask 0000
20. 定时清理主机上退出的容器、未被使用的卷和未被使用的镜像（发布非常频繁时需要）$ cat /usr/bin/prune_docker.sh#!/bin/bashdocker container prune -f # 删除所有退出状态的容器docker volume prune -f # 删除未被使用的数据卷docker image prune -f # 删除 dangling 或所有未被使用的镜像
做为定时任务定期作清理
$ crontab -l0 0 * * * /usr/bin/prune_docker.sh &gt;&gt; /var/log/prune_docker.log 2&gt;&amp;1
21. 定时清理私有镜像仓库（发布非常频繁时需要）$ cat &gt; /usr/bin/cleanregistry.sh &lt;&lt;EOF#!/bin/bashoc login -u admin -p passwordoc adm prune builds --orphans --keep-complete=25 --keep-failed=5 --keep-younger-than=60m --confirmoc adm prune deployments --orphans --keep-complete=25 --keep-failed=10 --keep-younger-than=60m --confirm#oc rollout latest docker-registry -n default#sleep 20oc adm prune images --keep-younger-than=400m --confirmEOF$ crontab -l0 0 * * * /usr/bin/cleanregistry.sh &gt;&gt; /var/log/cleanregistry.log 2&gt;&amp;1
22. 注释掉origin-accounting.conf文件中的DefaultIOAccounting$ cat /etc/systemd/system.conf.d/origin-accounting.conf[Manager]DefaultCPUAccounting=yesDefaultMemoryAccounting=yes# systemd v230 or newer# DefaultIOAccounting=yes# Deprecated, remove in futureDefaultBlockIOAccounting=yes
23. Pod与Service网段规划
集群的Service网段
集群的Pod网段
根据主机的配置规划好每台主机上Pod的网段

24. Router环境变量优化ROUTER_THREADS 设置为CPU核数ROUTER_MAX_CONNECTIONS 默认值是20000
25. Router设置默认503页面（服务不存在）设置页面HTML，覆盖&#x2F;var&#x2F;lib&#x2F;haproxy&#x2F;conf&#x2F;error-page-503.http文件补充：Openshift自定义Router配置
26. 计算节点优化配置
MTU值：通常的以太网设置为1450，在巨型帧以太网中设置为8950node的配置文件中

networkConfig:  mtu: 1450

开启并行拉取镜像，提升效率。node的配置文件中

kubeletArguments:  serialize-image-pulls:  - &quot;false&quot;

容器清理：通过kubelet自动清理退出的容器node的配置文件中

kubeletArguments:  minimum-container-ttl-duration:    - &quot;10s&quot;  maximum-dead-containers-per-container:    - &quot;1&quot;  maximum-dead-containers:    - &quot;20&quot;
minimum-container-ttl-duration: 容器可以进行垃圾收集的最低时长。 默认值为0，表示不限制。 可以使用单位后缀来指定此设置的值，例如h表示小时，m表示分钟，s表示秒。maximum-dead-containers-per-container：每个pod容器要保留的实例数。 预设值为1。maximum-dead-containers：节点中死容器总数的最大值。 默认值为-1，表示无限制。
27. 证书的有效期设置更长年限（100年）核心步骤是：

部署时设置主要ocp组件的过期时间，&#x2F;etc&#x2F;ansible&#x2F;hosts
更新部署脚本中所有生成证书的地方，设置长年限的过期时间

具体操作参考笔者之前的文章：OpenShift部署时如何延长组件证书的有效期

参考文章：linux journalctl 命令配置 logrotate 的终极指导Allocatable memory and CPU in Kubernetes NodesOpenShift容器云平台建设之部署前准备企业级容器云平台建设之功能汇总
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift的网络策略networkpolicy</title>
    <url>/posts/7f967ecf9c2f.html</url>
    <content><![CDATA[开启networkpolicy
创建新集群时，在ansible的hosts的参数列表中添加os_sdn_network_plugin_name配置

[OSEv3:vars]os_sdn_network_plugin_name=&#x27;redhat/openshift-ovs-networkpolicy

如果已存在的集群，切换网络策略，请参考Openshift网络插件动态切换
ovs-networkpolicy网络策略下，pod也支持qos网络流量控制。详情请阅读：Openshift Network QoS——Pod网络控制

说明：在Openshift容器平台只支持部分k8s networkpolicy v1版本特性，所以egress协议类型，IPBlock和podSelector与namespaceSelector的组合都不支持。
NetworkPolicy配置规则样例：
apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: test-network-policy  namespace: testspec:  podSelector:    matchLabels:      role: db  policyTypes:  - Ingress  ingress:  - from:    - namespaceSelector:        matchLabels:          project: myproject    - podSelector:        matchLabels:          role: frontend    ports:    - protocol: TCP      port: 6379

podSelector: pod选择器，指定该networkpolicy对指定pod产生作用。默认当前project下的所有pod
policyTypes: Openshift只支持Ingress协议类型
ingress[].from[].namespaceSelector：namespace选择器，带有指定label的namespace允许访问受控制的pod
ingress[].from[].podSelector：pod选择器，指定同一个project下带有指定label的pod允许访问受控制的pod
ingress[].ports：受控制的端口
一个project下可以有多条NetworkPolicy规则，同时它们是或的关系
每个NetworkPolicy可以有多条ingress策略，同时它们也是或的关系

策略设置案例
默认策略

在没有设置任何NetworkPolicy策略时，pod之间的网络与openshift-ovs-subnet一样，都是可以互相访问的

对所有pod网络隔离

  
kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:  name: deny-by-defaultspec:  podSelector:  ingress: []

只允许在同一个project下的所有pod可访问

  
kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:  name: allow-same-namespacespec:  podSelector:  ingress:  - from:    - podSelector: &#123;&#125;

只允许同一个project下的指定pod可访问

  
kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:  name: allow-same-namespacespec:  podSelector:  ingress:  - from:    - podSelector:          matchLabels:              type: blue

只允许指定的namespace下的pod可访问

  
oc label namespace project-b name=project-b
kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:  name: allow-test-namespacespec:  podSelector:  ingress:  - from:    - namespaceSelector:        matchLabels:          name: project-b

 公开指定label的pod的HTTP和HTTPS端口

  
kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:  name: allow-http-and-httpsspec:  podSelector:    matchLabels:      type: red  ingress:  - ports:    - protocol: TCP      port: 80    - protocol: TCP      port: 443

NetworkPolicy与Routers网络打通

在ovs-multitenant模式下，router所在的default project对所有project中的pod都具有访问权限，但是这点在networkpolicy策略中并不适用。如果某个需要公开的服务设置了networkpolicy策略，那么也需要将它向router pod公开。

一种方法是将需要公开的服务的端口对所有project公开

kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:  name: allow-to-database-servicespec:  podSelector:    matchLabels:      role: database  ingress:  - ports:    - protocol: TCP      port: 5432
该策略不仅允许router能访问该服务，同时也允许所有的pod能够访问该服务。通常这是没有问题的，因为有这种需求的服务是对外开放的。

另一种方法是只对default namespace进行公开

$ oc label namespace default name=default$ cat allow-from-default-namespace.yamlkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:  name: allow-from-default-namespacespec:  podSelector:  ingress:  - from:    - namespaceSelector:        matchLabels:          name: default
给新建的project创建默认策略在openshift project的默认配置中添加如下object
objects:...- apiVersion: networking.k8s.io/v1  kind: NetworkPolicy  metadata:    name: allow-from-same-namespace  spec:    podSelector:    ingress:    - from:      - podSelector: &#123;&#125;- apiVersion: networking.k8s.io/v1  kind: NetworkPolicy  metadata:    name: allow-from-default-namespace  spec:    podSelector:    ingress:    - from:      - namespaceSelector:          matchLabels:            name: default...
总结Openshift的networkpolicy功能只有当前最新k8s的networkpolicy的部分功能。它支持以下两种控制：

同一个project下的指定pod对受控制的pod的访问
不同project下的所有pod对受控制的pod的访问

不支持不同project下的指定pod对受控的pod的访问
参考文章Openshift官方文档：Managing Networking
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift私有仓库爆了，怎么办？？！！</title>
    <url>/posts/9333fbeb9715.html</url>
    <content><![CDATA[背景像大家担心的那样，Openshift私有仓库磁盘爆了，使用率100%!!!使用oc adm prune来作对openshift集群的清理，你敢吗？会不会把需要的镜像也一并删了呢，要是如此，就只能像DBA一样走人了。今天就来探讨下oc adm prune的用法。
先给出答案如你所料，我们最终的清理镜像的办法是oc adm prune images
oc login -u admin -p adminoc adm prune images --keep-younger-than=400m --confirm
注意以上oc adm prune images命令只能在普通用户下执行，无法在system:admin用户下执行，所以必须先登录普通用户，同时用户拥有system:image-pruner权限。
精减镜像命令oc adm prune images的用法为了让镜像不占用过多的磁盘空间，需要对它全精减。很多时候，比如重复地构建镜像，会留下很多不必要的镜像层，而这些镜像层会占用非常多的空间。
oc adm prune images [&lt;options&gt;]

options说明




参数
说明



–all
默认:true，删除除了私有镜像仓库中的多余的镜像外，也删除ImageStream中连接外部镜像的ImageStream。如果只删除私有镜像仓库中的镜像则设置–all&#x3D;false


–certificate-authority
访问私有镜像仓库的证书，openshift中的路径在master节点中的&#x2F;etc&#x2F;origin&#x2F;master&#x2F;registry.crt。如果是在集群中的master节点执行命令，这个参数不需要添加


–confirm
确认执行，如果不加只是进行检查演练


–force-insecure
强制使用不安全的连接，不作证书检查


–keep-tag-revisions&#x3D;
默认:3，为每个ImageStream中的每个Tag保留N个镜像


–keep-younger-than&#x3D;
默认:60m，不清理创建时间低于duration时间内的镜像，也不清理任何创建时间低于duration时间的对象引用的镜像


–prune-over-size-limit
清理同一个项目下超过指定LimitRanges的镜像，该先项不能与–keep-tag-revisions和–keep-younger-than同时使用


–registry-url
私有镜像链接，如果是在集群中的master节点执行命令，这个参数不需要添加



使用--keep-tag-revisions与--keep-younger-than来清理镜像，以下情况对应的镜像不会被清理


创建时间在--keep-younger-than指定的时间以内的所有Pod
创建时间在--keep-younger-than指定的时间以内的所有ImageStream
所有正在运行的Pod
所有状态为pending状态的Pod
所有的replication controllers
所有的build configurations
所有的builds
ImageStream的状态items下最近的--keep-tag-revisions镜像


使用--prune-over-size-limit 清理超过指定Limit的镜像，以下情况下指向的镜像不会清理


所有正在运行的Pod
所有状态为pending状态的Pod
所有的replication controllers
所有的build configurations
所有的builds


注意：以下情况并不会真正删除镜像


手动删除image，这只会删除etcd中的数据，不会删除私有仓库中的镜像

oc delete image &lt;sha256:image-id&gt;

oc adm prune用来清理deployments与builds当然oc adm prune并不仅仅只是用于清理镜像，它还用来清理deployments与builds
oc adm prune deployments/builds [&lt;options&gt;]

options说明




参数
说明



–confirm
确认执行，如果不加只是进行检查演练


–orphans
清理所有的deploymentconfig&#x2F;buildconfig不存在的deployment&#x2F;build


–keep-complete&#x3D;
默认:5，保留最近N个成功的deployment&#x2F;build


–keep-failed&#x3D;
默认：1，保留最近N个出错的deployment&#x2F;build


–keep-younger-than&#x3D;
默认:60m，不清理创建时间低于duration时间内的deployment&#x2F;build



实例：清理deployments&#x2F;builds

oc adm prune deployments --orphans --keep-complete=5 --keep-failed=1 --keep-younger-than=60m --confirmoc adm prune builds --orphans --keep-complete=5 --keep-failed=1 --keep-younger-than=60m --confirm
定时清理，做到永无后患$ cat cleanregistry.sh#!/bin/bashdateoc login -u admin -p passwordoc adm prune builds --orphans --keep-complete=25 --keep-failed=5 --keep-younger-than=60m --confirmoc adm prune deployments --orphans --keep-complete=25 --keep-failed=10 --keep-younger-than=60m --confirmoc adm prune images --keep-younger-than=400m --confirm$ crontab -l0 0 * * * /usr/bin/cleanregistry.sh &gt;&gt; /var/log/cleanregistry.log 2&gt;&amp;1
参考资料Openshift官方文档#pruning_resourcesoc-adm-prune-images命令行介绍
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift管理project【译自官方文档】</title>
    <url>/posts/34851936f0a3.html</url>
    <content><![CDATA[概述在OpenShift容器平台中，项目用于对相关对象进行分组和隔离。作为管理员，您可以授予开发人员对某些项目的访问权限，允许他们创建自己的项目，并授予他们在各个项目中的管理权限。
创建一个新的Project可以允许开发人员创建自己的项目。开发人员可以通过web控制台或者oc new-project命令来创建新的project。
$ oc new-project test-project
预定义projectAPI服务器根据master-config.yaml文件的projectRequestTemplate参数标识的模板自动配置新的项目。如果未定义该参数，API服务器将创建一个默认模板，该模板使用请求的名称创建一个项目，并赋予创建该项目的用户该项目的“admin”角色。
如何创建自定义的project模板呢？
导出当前默认的模板

$ oc adm create-bootstrap-project-template -o yaml &gt; template.yaml

使用文本编辑工具编辑template.yaml文件，对默认的配置进行更新
导入更改后的模板

$ oc create -f template.yaml -n default

修改master-config.yaml文件的projectRequestTemplate参数，指向新建的模板

...projectConfig:  projectRequestTemplate: &quot;default/project-request&quot;  ...
在更改模板文件时，可以使用如下变量



参数
说明



PROJECT_NAME
project名字，必填


PROJECT_DISPLAYNAME
project显示的名字，可以为空


PROJECT_DESCRIPTION
project说明， 可以为空


PROJECT_ADMIN_USER
管理用户的用户名


PROJECT_REQUESTING_USER
请求的用户的用户名


说明：要想调用创建新项目的api，请求的用户必须具有 self-provisioners 权限，默认情况下，所有通过认证的用户都具有该权限。



如何禁用自定义模板？通过设置，也可以阻止通过认证的用户组使用自定义模板创建新项目。

用具有”cluster-admin”权限的用户登录
查看self-provisioners权限的clusterrolebinding使用情况

$ oc  describe clusterrolebinding.rbac self-provisionersName:		self-provisionersLabels:		&lt;none&gt;Annotations:	rbac.authorization.kubernetes.io/autoupdate=trueRole:  Kind:	ClusterRole  Name:	self-provisionerSubjects:  Kind	Name				Namespace  ----	----				---------  Group	system:authenticated:oauth

从system:authenticated:oauth组中删除self-provisioner权限
如果self-provisioners权限只绑定到了system:authenticated:oauth用户组，则运行下面的命令$ oc patch clusterrolebinding.rbac self-provisioners -p &#x27;&#123;&quot;subjects&quot;: null&#125;&#x27;
如果self-provisioners权限不仅绑定到了system:authenticated:oauth用户组，还绑定到了users,groups或者serviceaccounts ，则运行下面的命令$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth


在master-config.yaml文件中配置projectRequestMessage参数，当开发人员创建新的项目时，将会用设置的Message提示他该如何去创建项目

...projectConfig:  ProjectRequestMessage: &quot;To request a project, contact your system administrator at projectname@example.com.&quot;  ...

更新self-provisioners cluster role binding以阻止自动更新角色，以下两种方式都能实现
使用一个命令行

 $ oc patch clusterrolebinding.rbac self-provisioners -p &#x27;&#123; &quot;metadata&quot;: &#123; &quot;annotations&quot;: &#123; &quot;rbac.authorization.kubernetes.io/autoupdate&quot;: &quot;false&quot; &#125; &#125; &#125;&#x27;

使用命令行进行编辑
运行如下命令

$ oc edit clusterrolebinding.rbac self-provisioners
 2.在文件编辑界面下设置rbac.authorization.kubernetes.io&#x2F;autoupdate为false apiVersion: authorization.openshift.io/v1kind: ClusterRoleBindingmetadata:  annotations:    rbac.authorization.kubernetes.io/autoupdate: &quot;false&quot;...




对project设置nodeselectorNodeSelector与Node的Label配合使用，可以用来控制pod的调度。
设置集群级别的默认节点选择器修改master-config.yaml文件中的defaultNodeSelector参数，这个配置将会影响所有没有设置nodeSelector标记的pod
...projectConfig:  defaultNodeSelector: &quot;type=user-node,region=east&quot;...
重启master服务，让配置生效
# master-restart api# master-restart controllers
设置项目级别的默认节点选择器在创建项目时可以指定 –node-selector来设置项目级别的节点选择器。例如，如果您有一个包含多个区域的OpenShift容器平台，您可以使用–node-selector来限制项目，使其只在特定区域的节点上部署pod。
$ oc adm new-project myproject --node-selector=&#x27;type=user-node,region=east&#x27;
一旦执行了以上命令，所有myproject项目的创建的pod都会带上’type&#x3D;user-node,region&#x3D;east’的nodeSelector说明:oc与oc adm两个命令都有 new-project子命令，但是–node-selector设置只有在cluster-admin用户下执行oc adm new-project时才有效使用oc adm new-project创建的新的project将会添加一个annotation，也可以编辑project,修改该annotation
...metadata:  annotations:    openshift.io/node-selector: type=user-node,region=east...
也可以通过命令行来对annotation进行更新
# oc patch namespace myproject -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;openshift.io/node-selector&quot;:&quot;node-role.kubernetes.io/infra=true&quot;&#125;&#125;&#125;&#x27;

如果openshift&#x2F;node-selector设置为””,那么这个项目下的pod并不会设置nodeSelctor
如果开发人员在创建deployment时额外设置了另外的nodeSelector，那么pod在调度的时候会需要同时满足project设置的nodeSelector及deployment的nodeSelector

限制用户创建的project数
通过设置master-config.yaml文件中的ProjectRequestLimit 可以限制用户的project数量。该配置将根据user用户的label进行设置project的最大数量
当前用户的将按照ProjectRequestLimit的设置的selector进行匹配，以第一个匹配为准设置maxProjects
如果ProjectRequestLimit没有设置selector，那么该maxProjects对所有用户生效
如果没有设置maxProjects，那么对于用户不限制project数

例子：以下配置为每个用户设置了2个项目的全局限制，同时为标签为level&#x3D;advanced的用户允许10个项目，为标签为level&#x3D;admin的用户允许无限制的项目。
admissionConfig:  pluginConfig:    ProjectRequestLimit:      configuration:        apiVersion: v1        kind: ProjectRequestLimitConfig        limits:        - selector:            level: admin         - selector:            level: advanced           maxProjects: 10        - maxProjects: 2 
重启master服务，让配置生效
# master-restart api# master-restart controllers

参考文章openshift官方文档：Managing Project
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift结合IaaS的LB与Nginx实现高可用可伸缩外部负载均衡器</title>
    <url>/posts/d2be1d48338a.html</url>
    <content><![CDATA[背景
Openshift在集群部署时需要额外提供一个负载均衡器，该负载均衡器对master api及router haproxy中的80与443服务进行负载，实现高可用。
如果在IaaS上部署Openshift集群的话，我们很自然地会使用IaaS的LB服务，直接对LB作TCP端口负载就解决问题了，既实现了需要的负载均衡，而且还有健康检查，负载高可用，非常方便。
但是我们的Openshift集群并不是部署在IaaS上，而是额外部署。IaaS上的LB无法直接将集群中的服务器作为监听器的后端进行负载，这时我们就需要在Openshift集群下额外独立部署负载均衡器。考虑到保证负载均衡器的高可用性，及监控等，这会是一个比较复杂的过程。
那么我们是否可以结合IaaS上的LB现有的机制，制定一套既方便部署，又高可用的方案呢？答案是肯定的。
该篇就是介绍如何结合IaaS的LB与代理应用Nginx实现Openshift高可用的外部负载均衡

原理
负载均衡器应用，首先大家想到的最多的两款应用是：Nginx与Haproxy，我们选用了Nginx。
将IaaS的LB与Nginx结合的思路也很简单。在IaaS上部署两台主机，上面部署好代理应用，将代理的后端设置为Openshift的服务器，同时将这两台服务器作为IaaS上的LB的监听器的后台。

部署操作
IaaS上资源准备


创建VPC IaaS

创建私网 IaaS

在私网下创建两台主机（instance1, instance2） IaaS

创建一个LB Iaas

搭建应用与配置


LB上创建三个监听器（8443&#x2F;TCP, 443&#x2F;TCP, 80&#x2F;TCP) IaaS

LB的三个监听器的后端都配置为两台Instance IaaS

在instance上安装nginx Instance


[root@i-8 root]# yum install nginx

配置nginx Instanceopenshift route服务器IP: 99.3.1.11、99.3.1.12openshift master服务器IP: 99.3.1.1、99.3.1.2、99.3.1.3

[root@i-8 conf.d]# cat /etc/nginx/nginx.conf...events &#123;    worker_connections 1024;&#125;stream&#123;    include /etc/nginx/conf.d/tcp.conf;&#125;http &#123;...[root@i-8 conf.d]# cat /etc/nginx/conf.d/tcp.confupstream nginx80&#123;    server 99.3.1.11:80;    server 99.3.1.12:80;&#125;server &#123;    listen       80;    proxy_pass nginx80;&#125;upstream nginx443&#123;    server 99.3.1.11:443;    server 99.3.1.12:443;&#125;server &#123;    listen       443;    proxy_pass nginx443;&#125;upstream nginx8443&#123;    server 99.3.1.1:8443;    server 99.3.1.2:8443;    server 99.3.1.3:8443;&#125;server &#123;    listen       8443;    proxy_pass nginx8443;&#125;

启动nginx

[root@i-8 root]# systemctl start nginx[root@i-8 root]# systemctl enable nginx
至此一个可监控、高可用且具有弹性伸缩功能的外部负载均衡器就完成了。
演示方法与之前访问openshift的服务一样，只是将域名解析的IP指向IaaS的LB就可以了。所有服务都能正常访问。同时对于不同的项目，可以通过创建不同的LB，来实现了负载均衡的扩容，Nginx应用也可以通过创建更多的Instance进行扩容。
补充说明感谢有朋友提出了以下问题：“nginx不好用，除非nginx plus，否则一个master api不可用，还是会访问到那个节点。haproxy配置也很简单，可以有健康检查，如果熟悉的话，最好使用haproxy来作为代理应用。”朋友提出的问题确实是一个很严重的问题，我对haproxy确实不太熟悉~~。但是朋友说的这个问题其实不必太担心，因为nginx默认的轮询方式的请求按时间顺序逐一分配到不同的后端服务器，**如果后端某台服务器宕机，故障系统被自动剔除，使用户访问不受影响。**所以nginx自己会对openshift的服务作了检查，来避免朋友提出的问题。我也单独对nginx的这个健康检查作了验证，确实是有效的。再次感谢提出问题的朋友，后序我也找时间使用haproxy作下测试。
Haproxy的配置global    ......    maxconn  20000    nbthread 2    ......frontend f_8443 *:8443    use_backend b_8443    mode  tcpbackend b_8443    mode  tcp    balance  roundrobin    server master1 99.3.1.1:8443    server master2 99.3.1.2:8443    server master3 99.3.1.3:8443frontend f_443 *:443    use_backend b_443    mode  tcpbackend b_443    mode  tcp    balance  roundrobin    server infra1 99.3.1.11:443    server infra2 99.3.1.12:443frontend f_80 *:80    use_backend b_80    mode  tcpbackend b_80    balance  roundrobin    server infra1 99.3.1.11:80    server infra2 99.3.1.12:80
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift网络插件动态切换</title>
    <url>/posts/17fd09f230fb.html</url>
    <content><![CDATA[Openshift 3.11版本Openshift SDN网络插件ovs-subnet 与 ovs-multitenant切换

更改节点上的networkPluginName的配置



master节点：/etc/origin/master/master-config.yaml
node节点：/etc/origin/node/node-config.yaml
将networkPluginName中的redhat/openshift-ovs-subnet换成redhat/openshift-ovs-multitenant,或者反过来。

$ ansible all -m shell -a &#x27;sed -i &quot;s/openshift-ovs-subnet/openshift-ovs-multitenant/g&quot; /etc/origin/node/node-config.yaml&#x27;$ ansible masters  -m shell -a &#x27;sed -i &quot;s/openshift-ovs-subnet/openshift-ovs-multitenant/g&quot; /etc/origin/master/master-config.yaml&#x27;


重启master节点上的origin-master-api和origin-master-controllers服务


$ ansible masters -m shell -a &#x27;master-restart api&#x27; -f 1$ ansible masters -m shell -a &#x27;master-restart controllers&#x27; -f 1


停止节点上的origin-node服务


$ ansible all -m service -a &#x27;name=origin-node state=stopped&#x27;


在所有节点上重启Openshift SDN服务


$ oc delete pod --all -n openshift-sdn


在所有节点上重启origin-node服务


$ ansible all -m service -a &#x27;name=origin-node state=started&#x27;


如果是从openshift sdn插件切换到第三方插件，需要清空将openshit sdn的特性


$ oc delete clusternetwork --all$ oc delete hostsubnets --all$ oc delete netnamespaces --all


检查VNIDs


$ oc get netnamespace
Openshift SDN网络插件ovs-multitenant 切换到 ovs-networkpolicy

下载切换脚本并更改执行权限


$ curl -O https://raw.githubusercontent.com/openshift/origin/master/contrib/migration/migrate-network-policy.sh$ chmod a+x migrate-network-policy.sh


执行脚本，openshift使用cluster-admin权限的用户


$ ./migrate-network-policy.sh

Openshift 3.9版本Openshift SDN网络插件ovs-subnet 与 ovs-multitenant切换

更改节点上的networkPluginName的配置



master节点：/etc/origin/master/master-config.yaml
node节点：/etc/origin/node/node-config.yaml
将networkPluginName中的redhat/openshift-ovs-subnet换成redhat/openshift-ovs-multitenant,或者反过来。

$ ansible all -m shell -a &#x27;sed -i &quot;s/openshift-ovs-subnet/openshift-ovs-multitenant/g&quot; /etc/origin/node/node-config.yaml&#x27;$ ansible masters  -m shell -a &#x27;sed -i &quot;s/openshift-ovs-subnet/openshift-ovs-multitenant/g&quot; /etc/origin/master/master-config.yaml&#x27;


重启master节点上的origin-master-api和origin-master-controllers服务


$ ansible masters -m service -a &#x27;name=origin-master-api state=restarted&#x27; -f 1$ ansible masters -m service -a &#x27;name=origin-master-controllers state=restarted&#x27; -f 1


停止节点上的origin-node服务


$ ansible all -m service -a &#x27;name=origin-node state=stopped&#x27;


在所有节点上重启openvswitch服务


$ ansible all -m service -a &#x27;name=openvswitch state=restarted&#x27;


在所有节点上重启origin-node服务


$ ansible all -m service -a &#x27;name=origin-node state=started&#x27;


如果是从openshift sdn插件切换到第三方插件，需要清空将openshit sdn的特性


$ oc delete clusternetwork --all$ oc delete hostsubnets --all$ oc delete netnamespaces --all


检查VNIDs


$ oc get netnamespace
Openshift SDN网络插件ovs-multitenant 切换到 ovs-networkpolicy

下载切换脚本并更改执行权限


$ curl -O https://raw.githubusercontent.com/openshift/origin/master/contrib/migration/migrate-network-policy.sh$ chmod a+x migrate-network-policy.sh


执行脚本，openshift使用cluster-admin权限的用户


$ ./migrate-network-policy.sh
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift自定义Router成倍提升集群性能</title>
    <url>/posts/02c4b11486c5.html</url>
    <content><![CDATA[为什么要自定义Router
Openshift的Router节点其实就是一个Haproxy容器应用，这个已经不是什么秘密了。我们平常使用时，不需要关心Haproxy的配置，因为它的默认配置完全满足绝大多数情况的需求。
但是它的默认配置并没有把最大的性能发挥出来，这时就需要我们对Haproxy的配置进行自定义了。
如果你需要实现一些新的功能，比如说将Mysql通过Router对外提供服务等

怎样自定义Router
Router Pod根据haproxy-config模板创建haproxy.conf配置文件，默认文件为/var/lib/haproxy/conf/haproxy-config.template
Router Haproxy镜像启动时会根据环境变量TEMPLATE_FILE来选择最终使用的模板文件
向Router Pod中放入自定义的模板文件，并将它的路径传给环境变量TEMPLATE_FILE
模板文件中的相关函数




函数
说明



processEndpointsForAlias(alias ServiceAliasConfig, svc ServiceUnit, action string) []Endpoint
返回所有可用的endpoints，如果action为shuffle，那返回结果会打乱顺序


env(variable, default …string) string
获取环境变量，如果没有环境变量，使用第二个参数的值，如果第二个参数没设置，返回空字符串


matchPattern(pattern, s string) bool
第一个参数是正则表达式，第二个参数为校验字符串，如果第二个参数满足正则规则，则返回True，否则返回False


isInteger(s string) bool
检查字符串是否为数字格式


firstMatch(s string, allowedValues …string) bool
将一个字符串在允许的字符串列表中从左到右匹配，返回最新匹配的字符串


matchValues(s string, allowedValues …string) bool
如果字符串与给定的一组字符串中的字符串有匹配，则返回True,否则返回False


generateRouteRegexp(hostname, path string, wildcard bool) string
生成一个 与主机名和路径匹配的正则表达式


genCertificateHostName(hostname string, wildcard bool) string
为匹配证书生成主机名


isTrue(s string) bool
如果给定的字符串中有’true’则返回True,否则False


自定义Router配置实战
导出Router默认的模板文件

$ oc rsh router-1-pod -n default cat haproxy-config.template &gt; haproxy-config.template

更改haproxy-config.template，添加nbthread支持 3.10以上版本已添加了该变量

...global  nbthread &#123;&#123;env &quot;ROUTER_NBTHREAD&quot; &quot;1&quot;&#125;&#125;  cpu-map auto:1/1-&#123;&#123;env &quot;ROUTER_NBTHREAD&quot; &quot;1&quot;&#125;&#125; 1-&#123;&#123;env &quot;ROUTER_NBTHREAD&quot; &quot;1&quot;&#125;&#125;...
环境变量，3.11版本主要更新以下两个ROUTER_THREADSROUTER_MAX_CONNECTIONS 默认值是20000

创建ConfigMap

$ oc create configmap customrouter --from-file=haproxy-config.template

挂载ConfigMap，设置环境变量

#### *3.9* 版本$ oc volumes dc/router --add --overwrite \    --name=config-volume \    --mount-path=/var/lib/haproxy/conf/custom \    --source=&#x27;&#123;&quot;configMap&quot;: &#123; &quot;name&quot;: &quot;customrouter&quot;&#125;&#125;&#x27;$ oc set env dc/router \    TEMPLATE_FILE=/var/lib/haproxy/conf/custom/haproxy-config.template#### *3.11* 版本$ oc set volumes dc/router --add --overwrite \    --name=config-volume \    --mount-path=/var/lib/haproxy/conf/custom \    --source=&#x27;&#123;&quot;configMap&quot;: &#123; &quot;name&quot;: &quot;customrouter&quot;&#125;&#125;&#x27;$ oc set env dc/router \    TEMPLATE_FILE=/var/lib/haproxy/conf/custom/haproxy-config.template

等待Router自动重启，使用新的配置模板

$ oc rsh router-2-pod -n default cat haproxy.conf
测试结果通过设置环境变量ROUTER_NBTHREAD的值，使用Jmeter对应用进行压力测试



ROUTER_NBTHREAD
Avg(ms)
Min(ms)
Max(ms)
Error
TPS(&#x2F;s)



1
194
0
8383
0
18322


2
133
0
15602
0
31160


4
127
0
7553
0
47049


8
138
0
14259
0
41995


16
163
0
10069
1(0%)
23979


从中可以看出，不同的nbthread对Router容量的影响非常大，当nbthread&#x3D;4时，容量达到默认的nbthread&#x3D;1的2倍。







参考文章：Openshift 自定义Router的haproxy配置Openshift 容器平台负载测试Haproxy多线程Haproxy性能优化
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift部署zalenium(容器化的selenium)及Python自动测试</title>
    <url>/posts/c424b4cf6841.html</url>
    <content><![CDATA[什么是zaleniumzalenium是一个Selenium Grid扩展，用Docker容器动态扩展你的本地网格。它使用docker-selenium在本地运行Firefox和Chrome中的测试。官网地址 zalenium
Openshift上部署zalenium
创建zalenium项目

oc new-project zalenium --display-name=&quot;自动测试Selenium Grid&quot;

创建Service Account

# 创建ClusterRole# oc create -f zalenium-role.json&#123;    &quot;kind&quot;: &quot;ClusterRole&quot;,    &quot;apiVersion&quot;: &quot;v1&quot;,    &quot;metadata&quot;: &#123;        &quot;name&quot;: &quot;zalenium-role&quot;    &#125;,    &quot;rules&quot;: [        &#123;            &quot;verbs&quot;: [                &quot;create&quot;,                &quot;list&quot;,                &quot;get&quot;,                &quot;delete&quot;,                &quot;exec&quot;            ],            &quot;attributeRestrictions&quot;: null,            &quot;apiGroups&quot;: [                &quot;&quot;            ],            &quot;resources&quot;: [                &quot;pods&quot;            ]        &#125;,        &#123;            &quot;verbs&quot;: [                &quot;list&quot;,                &quot;create&quot;,                &quot;delete&quot;,                &quot;get&quot;            ],            &quot;attributeRestrictions&quot;: null,            &quot;apiGroups&quot;: [                &quot;&quot;            ],            &quot;resources&quot;: [                &quot;services&quot;            ]        &#125;,        &#123;            &quot;verbs&quot;: [                &quot;create&quot;,                &quot;get&quot;            ],            &quot;attributeRestrictions&quot;: null,            &quot;apiGroups&quot;: [                &quot;&quot;            ],            &quot;resources&quot;: [                &quot;pods/exec&quot;            ]        &#125;     ]&#125;
oc create -f zalenium-role.jsonoc  create sa zaleniumoc adm policy add-scc-to-user anyuid -z zaleniumoc adm policy add-role-to-user zalenium-role -z zalenium

部署zalenium

oc run zalenium --image=dosel/zalenium \        --env=&quot;ZALENIUM_KUBERNETES_CPU_REQUEST=250m&quot; \        --env=&quot;ZALENIUM_KUBERNETES_CPU_LIMIT=500m&quot; \        --env=&quot;ZALENIUM_KUBERNETES_MEMORY_REQUEST=1Gi&quot; \        --overrides=&#x27;&#123;&quot;spec&quot;: &#123;&quot;template&quot;: &#123;&quot;spec&quot;: &#123;&quot;serviceAccount&quot;: &quot;zalenium&quot;&#125;&#125;&#125;&#125;&#x27; \        -l app=zalenium,role=hub --port=4444 -- \        start --desiredContainers 2 --seleniumImageName elgalu/selenium:latest

创建Service

#创建相应的service# oc create -f zalenium-service.yamlapiVersion: v1kind: Servicemetadata:  creationTimestamp: null  labels:    app: zalenium  name: zaleniumspec:  ports:  - name: 4444-tcp    port: 4444    protocol: TCP    targetPort: 4444  - name: 4445-tcp    port: 4445    protocol: TCP    targetPort: 4445  selector:    app: zalenium    role: hub  sessionAffinity: None  type: ClusterIPstatus:loadBalancer: &#123;&#125;

创建Router

# oc create -f ./zalenium-route.yamlapiVersion: v1kind: Routemetadata:  name: zalenium  annotations:    haproxy.router.openshift.io/timeout: 60sspec:  host: zalenium.example.com  port:    targetPort: 4444-tcp  to:    kind: Servicename: zalenium

访问被管理selenium状态

http://zalenium-zalenium.demo.example.com/grid/admin/live  
Python具体实现自动测试
安装selenium模块

pip install selenium

代码

# -*- coding: utf-8 -*-from selenium import webdriverfrom selenium.webdriver.remote.remote_connection import RemoteConnection# 创建远程连接selenium gridremoteconnection = RemoteConnection(&#x27;http://zalenium-zalenium.demo.example.com/wd/hub&#x27;, keep_alive=False,                                    resolve_ip=False)driver = webdriver.Remote(command_executor=remoteconnection,                          desired_capabilities=&#123;                              &#x27;browserName&#x27;: &quot;chrome&quot;,                              &#x27;video&#x27;: &#x27;True&#x27;,                              &#x27;platform&#x27;: &#x27;LINUX&#x27;,                              &#x27;platformName&#x27;: &#x27;LINUX&#x27;                          &#125;)try:    driver.implicitly_wait(30)    driver.maximize_window()    driver.get(&quot;http://www.baidu.com&quot;)    assert u&#x27;百度一下，你就知道&#x27; in driver.title    kw_input = driver.find_element_by_id(&#x27;kw&#x27;)    su_button = driver.find_element_by_id(&#x27;su&#x27;)    kw_input.clear()   # 输入关键字Openshift    kw_input.send_keys(&#x27;Openshift&#x27;)    su_button.click()finally:    driver.quit()
说明：创建RemoteConnection时需要设置keep_alive=False及设置resolve_ip=False。resolve_ip默认为True，它会解析senelium grip服务器的ip,便通过Ip去访问。我们知道Openshift的应用是无法使用ip来访问的，必须使用域名。所以需要将resolve_ip设置为False。当然，如果我们部署zalenium中的service使用nodeport将4444端口暴露出去的resolve_ip可以使用默认值。

查看自动测试结果

http://zalenium-zalenium.demo.example.com/dashboard  
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift部署测试环境oc-cluster-up</title>
    <url>/posts/49d94ef45774.html</url>
    <content><![CDATA[oc cluster up --base-dir=/root/openshift --public-hostname=master.douhua.com
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift集群3-9升级到3-10</title>
    <url>/posts/e66a2c56ece7.html</url>
    <content><![CDATA[
更新方法与策略：自动升级方法
下载openshift-ansible的脚本代码
  git clone https://github.com/openshift/openshift-ansible.git git checkout  release-3.10
将master上的&#x2F;etc&#x2F;origin&#x2F;master&#x2F;htpasswd备份到&#x2F;root&#x2F;htpasswd

scp master1:/etc/origin/master/htpasswd /root/htpasswd

openshift_master_identity_providers中的filename去掉

# /etc/ansible/hostsopenshift_master_identity_providers=[&#123;&#x27;name&#x27;: &#x27;htpasswd_auth&#x27;, &#x27;login&#x27;:&#x27;true&#x27;,&#x27;challenge&#x27;: &#x27;true&#x27;, &#x27;kind&#x27;: &#x27;HTPasswdPasswordIdentityProvider&#x27;&#125;]openshift_master_htpasswd_file=/root/htpasswd

屏蔽掉引入[gluster]导致的一个升级错误

在playbooks/common/openshift-cluster/upgrades/v3_10/upgrade_control_plane.yml中会去做对glusterfs与glusterfs_registry的比较，3.9之前未必需这些host group所以需要在ansible&#x2F;hosts中需要添加该host group
#/etc/ansible/host[glusterfs][glusterfs_registry]

在ansible&#x2F;hosts中的nodes列表中添加openshift_node_group_name

......master openshift_node_group_name=&#x27;node-config-master&#x27;node openshift_node_group_name=&#x27;node-config-compute&#x27;infra openshift_node_group_name=&#x27;node-config-infra&#x27;

升级默认的node group configmap

# ansible-playbook -i &lt;/path/to/inventory/file&gt;  playbooks/openshift-master/openshift_node_group.yml

检测Python OpenSSL版本

python -c &#x27;import OpenSSL.crypto&#x27;# 如果报错则需要升级Python OpenSSLyum install python2-pippip install -U pyopenssl -i https://pypi.douban.com/simple

关闭etcd目录的selinux检查**【不建议这么做，建议开启selinux】**关闭任务Check selinux label of ‘‘与Make sure the ‘‘ has the proper label

# roles/etcd/tasks/backup/backup.yml...- name: Detecting Atomic Host Operating System  stat:    path: /run/ostree-booted  register: l_ostree_booted#- name: Check selinux label of &#x27;&#123;&#123; etcd_data_dir &#125;&#125;&#x27;#  command: &gt;#    stat -c &#x27;%C&#x27; &#123;&#123; etcd_data_dir &#125;&#125;#  register: l_etcd_selinux_labels##- debug:#    msg: &quot;&#123;&#123; l_etcd_selinux_labels &#125;&#125;&quot;##- name: Make sure the &#x27;&#123;&#123; etcd_data_dir &#125;&#125;&#x27; has the proper label#  command: &gt;#    chcon -t svirt_sandbox_file_t  &quot;&#123;&#123; etcd_data_dir &#125;&#125;&quot;#  when:#  - l_etcd_selinux_labels.rc == 0#  - &quot;&#x27;svirt_sandbox_file_t&#x27; not in l_etcd_selinux_labels.stdout&quot;- name: Generate etcd backup  command: &gt;    &#123;&#123; r_etcd_common_etcdctl_command &#125;&#125; backup --data-dir=&#123;&#123; l_etcd_incontainer_data_dir &#125;&#125;    --backup-dir=&#123;&#123; l_etcd_incontainer_backup_dir &#125;&#125;...

升级集群

# ansible-playbook -i &lt;/path/to/inventory/file&gt;  playbooks/byo/openshift-cluster/upgrades/v3_10/upgrade.yml


升级过程中可能遇到的问题
webconsole自定义兼容问题

webconsole自定义格式是通过在openshift-web-console项目中的webconsole-configConfigMap添加css与js文件路径实现的，3.9版本支持路径不添加引号，而3.10版本必须添加引号。

selinux启动问题

每个节点（包括Master&#x2F;router&#x2F;node）都需要开启selinux。

ceph 无法挂载问题

升级过程中，如果有ceph挂载，则会出现超时，升级完成后，自动修复该问题。

HTPasswdPasswordIdentityProvider方式ansible_hosts文件格式更新问题

3.9版本可以将htpasswd文件路径填写在openshift_master_identity_providers变量中，而3.10版本则需要分开写，如下：
openshift_master_identity_providers=[&#123;&#x27;name&#x27;: &#x27;htpasswd_auth&#x27;, &#x27;login&#x27;:&#x27;true&#x27;,&#x27;challenge&#x27;: &#x27;true&#x27;, &#x27;kind&#x27;: &#x27;HTPasswdPasswordIdentityProvider&#x27;&#125;]openshift_master_htpasswd_file=/root/htpasswd

对于router节点添加了独占配置后，ovs服务无法部署问题

版本3.10将组件使用容器的方式部署，如果router节点设置了taints（目的是为了router节点只给route服务，确保应用的性能）那么node,ovs服务将无法在router节点上部署。这时需要给对应deamonset设置tolerations。目前相关的daemonset有：openshift-node/sync、openshift-sdn/sdn、openshift-sdn/ovs。

升级集群时为了尽可能影响线上业务，可以使用分步升级法

所谓分步升级法，就是先升级Master控制节点，再逐个升级计算节点。

通过执行upgrade_control_plane.yaml 脚本来升级控制平台节点

$ cd /usr/share/ansible/openshift-ansible$ ansible-playbook -i &lt;/path/to/inventory/file&gt; \    playbooks/byo/openshift-cluster/upgrades/v3_11/upgrade_control_plane.yml

通过执行upgrade_nodes.yaml 脚本来升级计算节点

$ cd /usr/share/ansible/openshift-ansible$ ansible-playbook -i &lt;/path/to/inventory/file&gt; \    playbooks/byo/openshift-cluster/upgrades/v3_11/upgrade_nodes.yml \    [-e &lt;customized_node_upgrade_variables&gt;] 
通过-e传入变量的方式来控制升级Node节点的节奏。例如-e openshift_upgrade_nodes_serial=&quot;20%&quot; 表示一次只升级20%的节点-e openshift_upgrade_nodes_serial=&quot;2&quot;     -e openshift_upgrade_nodes_label=&quot;region=group1&quot;表示一次只升级带label为region&#x3D;group1的节点中的2个。-e openshift_upgrade_nodes_drain_timeout=600设置openshift_upgrade_nodes_drain_timeout变量以指定将节点标记为失败之前要等待的时间。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift集群全环境备份</title>
    <url>/posts/ff93f3dfa6fc.html</url>
    <content><![CDATA[
创建集群全环境备份非常有必要，特别是在生产过程中。当集群发生异常崩溃，数据丢失时，备份的数据就派上了用场，利用备份数据可以将之前的环境重新构建出来。
在Openshift平台，我们可以对集群的完整状态备份到外部存储。集群全环境包括：

集群数据文件

etcd数据库

Openshift对象配置

私有镜像仓库存储

持久化卷


我们要定期对集群作备份，以防止数据的丢失。
集群全环境备份并不是万能的，应用自己的数据我们应该保证有单独的备份。
创建Master节点备份在系统基础架构进行更改，都需要对节点做备份。比如说，系统升级，集群升级或者任何重大更新。通过定期备份数据，当集群出现故障时，我们就能使用备份恢复集群。
Master主机上运行着非常重要的服务：API、Controllers。&#x2F;etc&#x2F;origin&#x2F;master目录下存放着许多重要的文件。

API、Controllers服务等的配置文件

安装生成的证书

云提供商提供的配置文件

密钥和其它身份认证文件


另外如果有额外自定义的配置，比如更改日志级别，使用代理等。这些配置文件在&#x2F;etc&#x2F;sysconfig&#x2F;目录下。
Master节点同时也是计算节点，所以备份&#x2F;etc&#x2F;origin整个目录。
备份过程需要在每台Master节点上都运行备份操作

主机配置文件备份
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo mkdir -p $&#123;MYBACKUPDIR&#125;/etc/sysconfig$ sudo cp -aR /etc/origin $&#123;MYBACKUPDIR&#125;/etc$ sudo cp -aR /etc/sysconfig/ $&#123;MYBACKUPDIR&#125;/etc/sysconfig/

注意: &#x2F;etc&#x2F;origin&#x2F;master&#x2F;ca.serial.txt文件只在会安装的ansible inventory hosts中的第一台master主机上创建，如果弃用该台主机时，需要将该文件拷贝到其它的Master主机上。

备份其它重要的文件



File
Description



&#x2F;etc&#x2F;cni&#x2F;*
CNI配置


&#x2F;etc&#x2F;sysconfig&#x2F;iptables
iptables 防火墙配置


&#x2F;etc&#x2F;sysconfig&#x2F;docker-storage-setup
container-storage-setup 命令调用


&#x2F;etc&#x2F;sysconfig&#x2F;docker
docker 应用配置


&#x2F;etc&#x2F;sysconfig&#x2F;docker-network
docker 网络配置


&#x2F;etc&#x2F;sysconfig&#x2F;docker-storage
docker 容器存储配置


&#x2F;etc&#x2F;dnsmasq.conf
dnsmasq 的配置


&#x2F;etc&#x2F;dnsmasq.d&#x2F;*
dnsmasq 的额外配置


&#x2F;etc&#x2F;sysconfig&#x2F;flanneld
flannel 配置文件


&#x2F;etc&#x2F;pki&#x2F;ca-trust&#x2F;source&#x2F;anchors&#x2F;
系统信任的证书


备份以上文件
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo mkdir -p $&#123;MYBACKUPDIR&#125;/etc/sysconfig$ sudo mkdir -p $&#123;MYBACKUPDIR&#125;/etc/pki/ca-trust/source/anchors$ sudo cp -aR /etc/sysconfig/&#123;iptables,docker-*,flanneld&#125; \    $&#123;MYBACKUPDIR&#125;/etc/sysconfig/$ sudo cp -aR /etc/dnsmasq* /etc/cni $&#123;MYBACKUPDIR&#125;/etc/$ sudo cp -aR /etc/pki/ca-trust/source/anchors/* \    $&#123;MYBACKUPDIR&#125;/etc/pki/ca-trust/source/anchors/

如果安装在系统中的应用包被意外删除，也会影响到集群的运行，所以需要备份系统中安装的rpm包列表
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo mkdir -p $&#123;MYBACKUPDIR&#125;$ rpm -qa | sort | sudo tee $MYBACKUPDIR/packages.txt

执行了上面的操作后，备份目录中会有如下文件列表，可将它们压缩在一个文件中进行保存
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo find $&#123;MYBACKUPDIR&#125; -mindepth 1 -type f -printf &#x27;%P\n&#x27;etc/sysconfig/flanneldetc/sysconfig/iptablesetc/sysconfig/docker-networketc/sysconfig/docker-storageetc/sysconfig/docker-storage-setupetc/sysconfig/docker-storage-setup.rpmnewetc/origin/master/ca.crtetc/origin/master/ca.keyetc/origin/master/ca.serial.txtetc/origin/master/ca-bundle.crtetc/origin/master/master.proxy-client.crtetc/origin/master/master.proxy-client.keyetc/origin/master/service-signer.crtetc/origin/master/service-signer.keyetc/origin/master/serviceaccounts.private.keyetc/origin/master/serviceaccounts.public.keyetc/origin/master/openshift-master.crtetc/origin/master/openshift-master.keyetc/origin/master/openshift-master.kubeconfigetc/origin/master/master.server.crtetc/origin/master/master.server.keyetc/origin/master/master.kubelet-client.crtetc/origin/master/master.kubelet-client.keyetc/origin/master/admin.crtetc/origin/master/admin.keyetc/origin/master/admin.kubeconfigetc/origin/master/etcd.server.crtetc/origin/master/etcd.server.keyetc/origin/master/master.etcd-client.keyetc/origin/master/master.etcd-client.csretc/origin/master/master.etcd-client.crtetc/origin/master/master.etcd-ca.crtetc/origin/master/policy.jsonetc/origin/master/scheduler.jsonetc/origin/master/htpasswdetc/origin/master/session-secrets.yamletc/origin/master/openshift-router.crtetc/origin/master/openshift-router.keyetc/origin/master/registry.crtetc/origin/master/registry.keyetc/origin/master/master-config.yamletc/origin/generated-configs/master-master-1.example.com/master.server.crt...[OUTPUT OMITTED]...etc/origin/cloudprovider/openstack.confetc/origin/node/system:node:master-0.example.com.crtetc/origin/node/system:node:master-0.example.com.keyetc/origin/node/ca.crtetc/origin/node/system:node:master-0.example.com.kubeconfigetc/origin/node/server.crtetc/origin/node/server.keyetc/origin/node/node-dnsmasq.confetc/origin/node/resolv.confetc/origin/node/node-config.yamletc/origin/node/flannel.etcd-client.keyetc/origin/node/flannel.etcd-client.csretc/origin/node/flannel.etcd-client.crtetc/origin/node/flannel.etcd-ca.crtetc/pki/ca-trust/source/anchors/openshift-ca.crtetc/pki/ca-trust/source/anchors/registry-ca.crtetc/dnsmasq.confetc/dnsmasq.d/origin-dns.confetc/dnsmasq.d/origin-upstream-dns.confetc/dnsmasq.d/node-dnsmasq.confpackages.txt

将备份的文件进行压缩
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo tar -zcvf /backup/$(hostname)-$(date +%Y%m%d).tar.gz $MYBACKUPDIR$ sudo rm -Rf $&#123;MYBACKUPDIR&#125;

Openshift 已经在openshift-ansible-contrib这个项目中提供了备份脚本backup_master_node.sh
将该脚本代码存放在master主机上，并执行，将会自动运行上面的步骤，对master主机进行备份
$ mkdir ~/git$ cd ~/git$ git clone https://github.com/openshift/openshift-ansible-contrib.git$ cd openshift-ansible-contrib/reference-architecture/day2ops/scripts$ ./backup_master_node.sh -h

创建计算节点备份创建计算节点的备份与Master节点的备份不一样，Master节点上有很多非常重要的文件，所以备份Master节点非常有必要。但是计算节点上一般并不保存运行集群的必要数据，即使计算节点出现了故障，其它节点也能代替它的功能，而不受影响。所以一般不需要备份计算节点，如果有一些特殊的配置必须要备份计算节点，则备份计算节点。
如果计算节点需要备份，那跟Master节点一样，在系统升级，集群升级或者集群有重要变更时都需要对节点做备份，同时也需要定期备份。
计算节点的主要配置文件存放在&#x2F;etc&#x2F;origin&#x2F;和&#x2F;etc&#x2F;origin&#x2F;node目录中。

计算节点服务的配置

安装时生成的证书

云提供商提供的配置文件

密钥和其它身份认证文件


另外如果有额外自定义的配置，比如更改日志级别，使用代理等。这些配置文件在&#x2F;etc&#x2F;sysconfig&#x2F;目录下。
备份过程
对计算节点服务的配置作备份

  $ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo mkdir -p $&#123;MYBACKUPDIR&#125;/etc/sysconfig$ sudo cp -aR /etc/origin $&#123;MYBACKUPDIR&#125;/etc$ sudo cp -aR /etc/sysconfig/origin-node $&#123;MYBACKUPDIR&#125;/etc/sysconfig/


备份其它重要的文件



File
Description



&#x2F;etc&#x2F;cni&#x2F;*
CNI配置


&#x2F;etc&#x2F;sysconfig&#x2F;iptables
iptables 防火墙配置


&#x2F;etc&#x2F;sysconfig&#x2F;docker-storage-setup
container-storage-setup 命令调用


&#x2F;etc&#x2F;sysconfig&#x2F;docker
docker 应用配置


&#x2F;etc&#x2F;sysconfig&#x2F;docker-network
docker 网络配置


&#x2F;etc&#x2F;sysconfig&#x2F;docker-storage
docker 容器存储配置


&#x2F;etc&#x2F;dnsmasq.conf
dnsmasq 的配置


&#x2F;etc&#x2F;dnsmasq.d&#x2F;*
dnsmasq 的额外配置


&#x2F;etc&#x2F;sysconfig&#x2F;flanneld
flannel 配置文件


&#x2F;etc&#x2F;pki&#x2F;ca-trust&#x2F;source&#x2F;anchors&#x2F;
系统信任的证书


备份以上文件
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo mkdir -p $&#123;MYBACKUPDIR&#125;/etc/sysconfig$ sudo mkdir -p $&#123;MYBACKUPDIR&#125;/etc/pki/ca-trust/source/anchors$ sudo cp -aR /etc/sysconfig/&#123;iptables,docker-*,flanneld&#125; \    $&#123;MYBACKUPDIR&#125;/etc/sysconfig/$ sudo cp -aR /etc/dnsmasq* /etc/cni $&#123;MYBACKUPDIR&#125;/etc/$ sudo cp -aR /etc/pki/ca-trust/source/anchors/* \    $&#123;MYBACKUPDIR&#125;/etc/pki/ca-trust/source/anchors/

如果安装在系统中的应用包被意外删除，也会影响到集群的运行，所以需要备份系统中安装的rpm包列表
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo mkdir -p $&#123;MYBACKUPDIR&#125;$ rpm -qa | sort | sudo tee $MYBACKUPDIR/packages.txt

执行了上面的操作后，备份目录中会有如下文件列表，可将它们压缩在一个文件中进行保存
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo find $&#123;MYBACKUPDIR&#125; -mindepth 1 -type f -printf &#x27;%P\n&#x27;etc/sysconfig/origin-nodeetc/sysconfig/flanneldetc/sysconfig/iptablesetc/sysconfig/docker-networketc/sysconfig/docker-storageetc/sysconfig/docker-storage-setupetc/sysconfig/docker-storage-setup.rpmnewetc/origin/node/system:node:app-node-0.example.com.crtetc/origin/node/system:node:app-node-0.example.com.keyetc/origin/node/ca.crtetc/origin/node/system:node:app-node-0.example.com.kubeconfigetc/origin/node/server.crtetc/origin/node/server.keyetc/origin/node/node-dnsmasq.confetc/origin/node/resolv.confetc/origin/node/node-config.yamletc/origin/node/flannel.etcd-client.keyetc/origin/node/flannel.etcd-client.csretc/origin/node/flannel.etcd-client.crtetc/origin/node/flannel.etcd-ca.crtetc/origin/cloudprovider/openstack.confetc/pki/ca-trust/source/anchors/openshift-ca.crtetc/pki/ca-trust/source/anchors/registry-ca.crtetc/dnsmasq.confetc/dnsmasq.d/origin-dns.confetc/dnsmasq.d/origin-upstream-dns.confetc/dnsmasq.d/node-dnsmasq.confpackages.txt

将备份的文件进行压缩
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ sudo tar -zcvf /backup/$(hostname)-$(date +%Y%m%d).tar.gz $MYBACKUPDIR$ sudo rm -Rf $&#123;MYBACKUPDIR&#125;

Openshift 已经在openshift-ansible-contrib这个项目中提供了备份脚本backup_master_node.sh将该脚本代码存放在master主机上，并执行，将会自动运行上面的步骤，对master主机进行备份
$ mkdir ~/git$ cd ~/git$ git clone https://github.com/openshift/openshift-ansible-contrib.git$ cd openshift-ansible-contrib/reference-architecture/day2ops/scripts$ ./backup_master_node.sh -h

备份私服镜像仓库证书如果使用了外部私有镜像仓库，就必须备份所有的外部镜像仓库的证书。
备份过程$ cd /etc/docker/certs.d/$ tar cf /tmp/docker-registry-certs-$(hostname).tar *

备份相关安装文件还原过程集群过程需要完全重新安装，所以需要保存所有相关的文件。包括

ansible playbooks和inventory hosts完整内容
yum源文件

备份应用数据大部分情况下，可以使用oc rsync 命令来对应用数据做备份。这个是通用的备份方案。
不同的存储方案，比如说NFS等，也可以根据存储的不同，使用更方便的备份方案。
同时备份的目录，也根据应用程序的不同而不同。
以下是一个备份jenkins应用的例子。
备份过程
获得jenkins应用数据挂载目录
$ oc get dc/jenkins -o jsonpath=&#x27;&#123; .spec.template.spec.containers[?(@.name==&quot;jenkins&quot;)].volumeMounts[?(@.name==&quot;jenkins-data&quot;)].mountPath &#125;&#x27;/var/lib/jenkins

获取当前运行的应用的pod名字
$ oc get pod --selector=deploymentconfig=jenkins -o jsonpath=&#x27;&#123; .metadata.name &#125;&#x27;jenkins-1-37nux

使用oc rsync对数据进行备份
$ oc rsync jenkins-1-37nux:/var/lib/jenkins /tmp/

备份etcd数据库备份etcd分布式数据库，需要备份etcd的配置文件及数据。备份时既可以使用etcd v2版本也可以使用etcd v3版本API来备份etcd数据
备份过程
备份etcd配置文件

etcd的配置文件在&#x2F;etc&#x2F;etcd目录中，其中包括etcd.conf配置文件，及集群通信所需的证书。这些文件都是在用ansible安装时生成的。
对每个etcd节点备份相关配置文件
$ ssh master-0$ mkdir -p /backup/etcd-config-$(date +%Y%m%d)/$ cp -R /etc/etcd/ /backup/etcd-config-$(date +%Y%m%d)/


备份etcd数据

openshift容器平台为了方便调用etcdctl不同版本，创建了两个别名，etcdctl2和etcdctl3。但是，etcdctl3别名不会向etcdctl命令提供完整的端点列表，因此您必须指定–endpoints选项并列出所有端点。
在做etcd数据备份前，需要先做如下处理。

etcdctl可执行文件必须可用，容器化安装时容器etcd必须可用

确保openshit容器平台的api服务正常运行

确保与etcd集群的2379端口TCP通信正常

确保有etcd集群的请求证书



检查etcd集群的健康状态，可以使用etcdctl2或者etcdctl3

使用etcd v2 api
$ etcdctl2 --cert-file=/etc/etcd/peer.crt \          --key-file=/etc/etcd/peer.key \          --ca-file=/etc/etcd/ca.crt \          --endpoints=&quot;https://master-0.example.com:2379,\          https://master-1.example.com:2379,\          https://master-2.example.com:2379&quot;\          cluster-healthmember 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379cluster is healthy

使用etcd v3 api
$ etcdctl3 --cert=&quot;/etc/etcd/peer.crt&quot; \          --key=/etc/etcd/peer.key \          --cacert=&quot;/etc/etcd/ca.crt&quot; \          --endpoints=&quot;https://master-0.example.com:2379,\            https://master-1.example.com:2379,\            https://master-2.example.com:2379&quot;            endpoint healthhttps://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358mshttps://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173mshttps://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms


查看member列表

使用etcd v2 api
# etcdctl2 member list2a371dd20f21ca8d: name=master-1.example.com peerURLs=https://192.168.55.12:2380 clientURLs=https://192.168.55.12:2379 isLeader=false40bef1f6c79b3163: name=master-0.example.com peerURLs=https://192.168.55.8:2380 clientURLs=https://192.168.55.8:2379 isLeader=false95dc17ffcce8ee29: name=master-2.example.com peerURLs=https://192.168.55.13:2380 clientURLs=https://192.168.55.13:2379 isLeader=true

使用etcd v3 api
# etcdctl3 member list2a371dd20f21ca8d, started, master-1.example.com, https://192.168.55.12:2380, https://192.168.55.12:237940bef1f6c79b3163, started, master-0.example.com, https://192.168.55.8:2380, https://192.168.55.8:237995dc17ffcce8ee29, started, master-2.example.com, https://192.168.55.13:2380, https://192.168.55.13:2379


开始备份etcd数据
  v2版本有etcdctl backup命令，用这个命令可以对etcd集群数据做备份。但是etcdctl v3没有这个命令，但是v3版本有etcdctl snapshot save命令或者直接复制member&#x2F;snap&#x2F;db文件。
  etcdctl backup命令会重写备份中包含的一些元数据，特别是节点ID和集群ID，这意味着在备份中，节点将丢失其以前的标识。 要从备份重新创建群集，需要创建新的单节点群集，然后将其余节点添加到群集。 重写元数据以防止新节点加入现有集群。

如果etcd部署在独立的主机上，使用etcd v2 api备份

通过删除etcd pod yaml文件，停止etcd服务
$ mkdir -p /etc/origin/node/pods-stopped$ mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/

创建etcd数据备份文件夹，复制etcd db文件
$ mkdir -p /backup/etcd-$(date +%Y%m%d)$ etcdctl2 backup \    --data-dir /var/lib/etcd \    --backup-dir /backup/etcd-$(date +%Y%m%d)$ cp /var/lib/etcd/member/snap/db /backup/etcd-$(date +%Y%m%d)

重启主机
$ reboot


如果etcd部署在独立的主机上，使用etcd v3 api

在etcd节点上创建快照snapshot
$ systemctl show etcd --property=ActiveState,SubState$ mkdir -p /backup/etcd-$(date +%Y%m%d)$ etcdctl3 snapshot save /backup/etcd-$(date +%Y%m%d)/db

通过删除etcd pod yaml文件，停止etcd服务
$ mkdir -p /etc/origin/node/pods-stopped$ mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/

创建etcd数据备份文件夹，复制etcd db文件
$ etcdctl2 backup \    --data-dir /var/lib/etcd \    --backup-dir /backup/etcd-$(date +%Y%m%d)

重启主机
$ reboot


如果部署etcd使用的是容器安装，使用etcd v3 api

从etcd pod的配置文件中获取etcd endpoint IP
$ export ETCD_POD_MANIFEST=&quot;/etc/origin/node/pods/etcd.yaml&quot;$ export ETCD_EP=$(grep https $&#123;ETCD_POD_MANIFEST&#125; | cut -d &#x27;/&#x27; -f3)

获得etcd pod名
$ oc login -u system:admin$ export ETCD_POD=$(oc get pods -n kube-system | grep -o -m 1 &#x27;\S*etcd\S*&#x27;)

创建快照snapshot，并将它保存到本地
$ oc project kube-system$ oc exec $&#123;ETCD_POD&#125; -c etcd -- /bin/bash -c &quot;ETCDCTL_API=3 etcdctl \    --cert /etc/etcd/peer.crt \    --key /etc/etcd/peer.key \    --cacert /etc/etcd/ca.crt \    --endpoints &lt;ETCD_EP&gt; \     snapshot save /var/lib/etcd/snapshot.db&quot;





备份项目project项目的备份，涉及导出所有相关的对象，最终使用备份的文件恢复到新的项目中。
备份过程
列出需要备份的所有对象
$ oc get allNAME         TYPE      FROM      LATESTbc/ruby-ex   Source    Git       1NAME               TYPE      FROM          STATUS     STARTED         DURATIONbuilds/ruby-ex-1   Source    Git@c457001   Complete   2 minutes ago   35sNAME                 DOCKER REPO                                     TAGS      UPDATEDis/guestbook         10.111.255.221:5000/myproject/guestbook         latest    2 minutes agois/hello-openshift   10.111.255.221:5000/myproject/hello-openshift   latest    2 minutes agois/ruby-22-centos7   10.111.255.221:5000/myproject/ruby-22-centos7   latest    2 minutes agois/ruby-ex           10.111.255.221:5000/myproject/ruby-ex           latest    2 minutes agoNAME                 REVISION   DESIRED   CURRENT   TRIGGERED BYdc/guestbook         1          1         1         config,image(guestbook:latest)dc/hello-openshift   1          1         1         config,image(hello-openshift:latest)dc/ruby-ex           1          1         1         config,image(ruby-ex:latest)NAME                   DESIRED   CURRENT   READY     AGErc/guestbook-1         1         1         1         2mrc/hello-openshift-1   1         1         1         2mrc/ruby-ex-1           1         1         1         2mNAME                  CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGEsvc/guestbook         10.111.105.84    &lt;none&gt;        3000/TCP            2msvc/hello-openshift   10.111.230.24    &lt;none&gt;        8080/TCP,8888/TCP   2msvc/ruby-ex           10.111.232.117   &lt;none&gt;        8080/TCP            2mNAME                         READY     STATUS      RESTARTS   AGEpo/guestbook-1-c010g         1/1       Running     0          2mpo/hello-openshift-1-4zw2q   1/1       Running     0          2mpo/ruby-ex-1-build           0/1       Completed   0          2mpo/ruby-ex-1-rxc74           1/1       Running     0          2m

将对象配置导出为yaml文件或者json文件

导出为yaml文件
$ oc get -o yaml --export all &gt; project.yaml

导出为json文件
$ oc get -o json --export all &gt; project.json


将 role bindings, secrets, service accounts和 persistent volume claims等导出
$ for object in rolebindings serviceaccounts secrets imagestreamtags podpreset cms egressnetworkpolicies rolebindingrestrictions limitranges resourcequotas pvcs templates cronjobs statefulsets hpas deployments replicasets poddisruptionbudget endpointsdo  oc get -o yaml --export $object &gt; $object.yamldone

说明

列出所有的对象种类
$ oc api-resources --namespaced=true -o name

有些对象的参数中依赖于元数据，或者带有唯一的认证标识。这些对象在恢复时将会受到影响。比如说deploymentconfig中的image指向imagestream时，image将会指向镜像的一个sha256值，在恢复时将无法找到镜像，而导致恢复失败。


备份持久化卷将持久化卷挂载到pod上，再使用oc rsync命令将数据备份到服务器
备份过程
查看pod
$ oc get podsNAME           READY     STATUS      RESTARTS   AGEdemo-1-build   0/1       Completed   0          2hdemo-2-fxx6d   1/1       Running     0          1h

查看pod将pvc挂载到的目录
$ oc describe pod demo-2-fxx6dName:			demo-2-fxx6dNamespace:		testSecurity Policy:	restrictedNode:			ip-10-20-6-20.ec2.internal/10.20.6.20Start Time:		Tue, 05 Dec 2017 12:54:34 -0500Labels:			app=demo			deployment=demo-2			deploymentconfig=demoStatus:			RunningIP:			172.16.12.5Controllers:		ReplicationController/demo-2Containers:  demo:    Container ID:	docker://201f3e55b373641eb36945d723e1e212ecab847311109b5cee1fd0109424217a    Image:		docker-registry.default.svc:5000/test/demo@sha256:0a9f2487a0d95d51511e49d20dc9ff6f350436f935968b0c83fcb98a7a8c381a    Image ID:		docker-pullable://docker-registry.default.svc:5000/test/demo@sha256:0a9f2487a0d95d51511e49d20dc9ff6f350436f935968b0c83fcb98a7a8c381a    Port:		8080/TCP    State:		Running      Started:		Tue, 05 Dec 2017 12:54:52 -0500    Ready:		True    Restart Count:	0    Volume Mounts:      */opt/app-root/src/uploaded from persistent-volume (rw)*      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8mmrk (ro)    Environment Variables:	&lt;none&gt;...omitted...

可以看到将pvc对应在pod中的目录为/opt/app-root/src/uploaded from persistent-volume

oc rsync备份数据
$ oc rsync demo-2-fxx6d:/opt/app-root/src/uploaded ./demo-appreceiving incremental file listuploaded/uploaded/ocp_sop.txtuploaded/lost+found/sent 38 bytes  received 190 bytes  152.00 bytes/sectotal size is 32  speedup is 0.14

一键备份etcd数据脚本一键备份etcd
[root@master01 ~]# cat backup_etcd.sh#!/bin/bashexport ETCD_POD_MANIFEST=&quot;/etc/origin/node/pods/etcd.yaml&quot;export ETCD_EP=$(grep https $&#123;ETCD_POD_MANIFEST&#125; | cut -d &#x27;/&#x27; -f3)oc login -u system:adminexport ETCD_POD=$(oc get pods -n kube-system | grep -o -m 1 &#x27;\S*etcd\S*&#x27;)oc project kube-systemoc exec $&#123;ETCD_POD&#125; -c etcd -- /bin/sh -c &quot;ETCDCTL_API=3 etcdctl --cert /etc/etcd/peer.crt --key /etc/etcd/peer.key --cacert /etc/etcd/ca.crt --endpoints $ETCD_EP snapshot save /var/lib/etcd/snapshot.db&quot;today_date=$(date +%Y%m%d)mkdir -p /backup/$&#123;today_date&#125;/etcdmv /var/lib/etcd/snapshot.db /backup/$&#123;today_date&#125;/etcd/snapshot.dbls /backup/$&#123;today_date&#125;/etcd/echo &quot;success backup etcd&quot;
参考文章Openshift官方文档之集群备份
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift集群全环境恢复</title>
    <url>/posts/558c019ddfc3.html</url>
    <content><![CDATA[
阅读前说明
按照官网提供的文档操作能够恢复etcd数据，但是由于证书的问题，恢复的集群并不能正常使用，需要单独对集群的token进行清理。
官方文档中没有明确恢复etcd集群的部署，经过多次验证，确认了恢复etcd集群需要以下三步：1. 部署1台节点的etcd；2. 在当前1台etcd节点上恢复数据；3. 使用ansible扩容的方式，将etcd节点扩展到3台。
可以更新&#x2F;etc&#x2F;etcd&#x2F;etcd.conf文件更改etcd name，进而解决etcd客户端访问服务器端证书不匹配的问题。
文章最后附上了经过测试认证的根据备份一键恢复etcd集群的脚本。

Openshift集群平台能够使用备份完整恢复集群。Openshift集群全环境备份
在恢复集群之前，请确保对集群做过完成的备份，并重新安装Openshift集群。
恢复Master节点创建Master主机文件的备份后，如果它们被损坏或意外删除，就可以通过这些文件复制回Master主机来恢复文件，然后重新启动受影响的服务。
恢复过程
恢复/etc/origin/master/master-config.yaml文件
$ MYBACKUPDIR=*/backup/$(hostname)/$(date +%Y%m%d)*$ cp /etc/origin/master/master-config.yaml /etc/origin/master/master-config.yaml.old$ cp /backup/$(hostname)/$(date +%Y%m%d)/origin/master/master-config.yaml /etc/origin/master/master-config.yaml$ master-restart api$ master-restart controllers

重启master服务可能会导致停机，此时可以将该主机从负载均衡池中删除，再恢复主机，待恢复完成后，Master服务也起来了，再将它添加到负载均衡池中。

如果因为缺少一些二进制包，而导致无法启动Master服务，那么重新安装缺少的包

获得当前已有的包
$ rpm -qa | sort &gt; /tmp/current_packages.txt

与之前备份的包列表作比较，得到缺少的包
$ diff /tmp/current_packages.txt $&#123;MYBACKUPDIR&#125;/packages.txt&gt; ansible-2.4.0.0-5.el7.noarch

安装缺少的包
$ yum reinstall -y &lt;packages&gt;


恢复系统信任的证书
$ MYBACKUPDIR=*/backup/$(hostname)/$(date +%Y%m%d)*$ sudo cp $&#123;MYBACKUPDIR&#125;/external_certificates/my_company.crt /etc/pki/ca-trust/source/anchors/$ sudo update-ca-trust

恢复计算节点一般计算节点不需要做恢复，但是如果有特殊的重要节点需要恢复的话，与Master节点恢复过程类似。
恢复过程
恢复/etc/origin/node/node-config.yaml文件
$ MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)$ cp /etc/origin/node/node-config.yaml /etc/origin/node/node-config.yaml.old$ cp /backup/$(hostname)/$(date +%Y%m%d)/etc/origin/node/node-config.yaml /etc/origin/node/node-config.yaml$ reboot

如果因为缺少一些二进制包，而导致无法启动Master服务，那么重新安装缺少的包

获得当前已有的包
$ rpm -qa | sort &gt; /tmp/current_packages.txt

与之前备份的包列表作比较，得到缺少的包
$ diff /tmp/current_packages.txt $&#123;MYBACKUPDIR&#125;/packages.txt&gt; ansible-2.4.0.0-5.el7.noarch

安装缺少的包
$ yum reinstall -y &lt;packages&gt;


恢复系统信任的证书
$ MYBACKUPDIR=*/backup/$(hostname)/$(date +%Y%m%d)*$ sudo cp $&#123;MYBACKUPDIR&#125;/external_certificates/my_company.crt /etc/pki/ca-trust/source/anchors/$ sudo update-ca-trust

恢复etcd数据恢复过程
恢复etcd配置文件
用备份中的etcd配置文件替换掉当前集群的配置文件，然后重启服务或者静态Pod。
$  ssh master-0$ cp /backup/yesterday/master-0-files/etcd.conf /etc/etcd/etcd.conf$ restorecon -Rv /etc/etcd/etcd.conf$ systemctl restart etcd.service

恢复etcd数据

根据etcd v2 和 v3数据恢复
该恢复过程必须，在单独的一台主机上恢复数据，再通过扩容的方式加入剩下的主机

通过将pod的yaml文件移出来暂停etcd pod
$ mkdir -p /etc/origin/node/pods-stopped$ mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/$ reboot

清除之前的数据

对当前数据做备份
$ mv /var/lib/etcd /var/lib/etcd.old$ mkdir /var/lib/etcd$ restorecon -Rv /var/lib/etcd/

直接清除当前数据
$ rm -rf /var/lib/etcd


在所有的etcd节点做如下操作，恢复数据
$ cp -R /backup/etcd-xxx/* /var/lib/etcd/$ mv /var/lib/etcd/db /var/lib/etcd/member/snap/db$ chcon -R --reference /backup/etcd-xxx/* /var/lib/etcd/

在每台etcd主机上执行以下操作，强制创建一个新的etcd集群
$ mkdir -p /etc/systemd/system/etcd.service.d/$ echo &quot;[Service]&quot; &gt; /etc/systemd/system/etcd.service.d/temp.conf$ echo &quot;ExecStart=&quot; &gt;&gt; /etc/systemd/system/etcd.service.d/temp.conf$ sed -n &#x27;/ExecStart/s/&quot;$/ --force-new-cluster&quot;/p&#x27; \    /usr/lib/systemd/system/etcd.service \    &gt;&gt; /etc/systemd/system/etcd.service.d/temp.conf$ systemctl daemon-reload$ master-restart etcd

检查错误日志
$ master-logs etcd etcd

检查etcd集群的状态
# etcdctl2 cluster-healthmember 5ee217d17301 is healthy: got healthy result from https://192.168.55.8:2379cluster is healthy

集群默认配置下启动etcd
$ rm -f /etc/systemd/system/etcd.service.d/temp.conf$ systemctl daemon-reload$ master-restart etcd

检查etcd状态，查看member list
$ etcdctl2 cluster-healthmember 5ee217d17301 is healthy: got healthy result from https://192.168.55.8:2379cluster is healthy$ etcdctl2 member list5ee217d17301: name=master-0.example.com peerURLs=http://localhost:2380 clientURLs=https://192.168.55.8:2379 isLeader=true

第一个实例运行后，就可以还原其余的etcd服务器


修复PEERURL参数问题
在恢复数据后，新的etcd集群参数peerurl为localhost而不是ip地址，我们需要将它修改为ip地址

执行etcdctl member list获得member ID
$ etcdctl member list

获得etcd通信的IP
$ ss -l4n | grep 2380

更新对应member的peer地址
$ etcdctl2 member update 5ee217d17301 https://192.168.55.8:2380Updated member with ID 5ee217d17301 in cluster

查看新的peer地址进行校验
$ etcdctl2 member list5ee217d17301: name=master-0.example.com peerURLs=https://*192.168.55.8*:2380 clientURLs=https://192.168.55.8:2379 isLeader=true


根据v3的快照snapshot恢复
如果是使用etcdctl snapshot save的方式备份的snapshot，etcdctl snapshot restore恢复数据时会去校验数据的hash，但是如果直接从数据目录中拷贝出来的就无法校验hash，这时恢复数据时需要加上**–skip-hash-check**
该恢复过程必须，在单独的一台主机上恢复数据，再通过扩容的方式加入剩下的主机

通过将pod的yaml文件移出来暂停etcd pod
$ mkdir -p /etc/origin/node/pods-stopped$ mv /etc/origin/node/pods/* /etc/origin/node/pods-stopped/$ reboot

清除之前的数据
$ rm -rf /var/lib/etcd

使用snapshot restore命令来恢复数据
# etcdctl3 snapshot restore /backup/etcd-xxxxxx/backup.db \  --data-dir /var/lib/etcd \  --name master-0.example.com \  --initial-cluster &quot;master-0.example.com=https://192.168.55.8:2380&quot; \  --initial-cluster-token &quot;etcd-cluster-1&quot; \  --initial-advertise-peer-urls https://192.168.55.8:2380 \  --skip-hash-check=true2017-10-03 08:55:32.440779 I | mvcc: restore compact to 10412692017-10-03 08:55:32.468244 I | etcdserver/membership: added member 40bef1f6c79b3163 [https://192.168.55.8:2380] to cluster 26841ebcf610583c

相关配置从&#x2F;etc&#x2F;etcd&#x2F;etcd.conf获取

给相关文件及目录设置相关的selinux权限
$ restorecon -Rv /var/lib/etcd/

启动etcd服务
$ systemctl start etcd

检查错误日志
$ master-logs etcd etcd


在静态etcd pod恢复etcd

通过将pod的yaml文件移出来暂停etcd pod
$ mv /etc/origin/node/pods/etcd.yaml .

清除之前的数据
$ rm -rf /var/lib/etcd

使用snapshot恢复集群数据
$ export ETCDCTL_API=3$ etcdctl snapshot restore /etc/etcd/backup/etcd/snapshot.db	 --data-dir /var/lib/etcd/	 --name ip-172-18-3-48.ec2.internal	 --initial-cluster &quot;ip-172-18-3-48.ec2.internal=https://172.18.3.48:2380&quot;	 --initial-cluster-token &quot;etcd-cluster-1&quot;	 --initial-advertise-peer-urls https://172.18.3.48:2380	 --skip-hash-check=true

从**$&#x2F;backup_files&#x2F;etcd.conf** 文件中获得相关的参数

给相关文件及目录设置相关的selinux权限
$ restorecon -Rv /var/lib/etcd/

通过将etcd pod的yaml文件恢复到静态pod目录下来重启etcd数据
$ mv etcd.yaml /etc/origin/node/pods/.





使用Ansible添加etcd节点还原etcd数据后，可以使用ansible或者手动的方式对etcd进行扩容。
添加过程
在inventory的hosts中添加[new_etcd]服务器组
[OSEv3:children]mastersnodesetcdnew_etcd ... [OUTPUT ABBREVIATED] ...[etcd]master-0.example.commaster-1.example.commaster-2.example.com[new_etcd] etcd0.example.com 

执行ansible扩容ansible脚本
$ cd /usr/share/ansible/openshift-ansible$ ansible-playbook  playbooks/openshift-etcd/scaleup.yml

将[new_etcd]服务器组的主机移到[etcd]组
[OSEv3:children]mastersnodesetcdnew_etcd... [OUTPUT ABBREVIATED] ...[etcd]master-0.example.commaster-1.example.commaster-2.example.cometcd0.example.com

恢复Openshift集群节点上的服务恢复过程
在每一个Master节点恢复配置文件及重启相关服务
$ cp $&#123;MYBACKUPDIR&#125;/etc/origin/node/pods/* /etc/origin/node/pods/$ cp $&#123;MYBACKUPDIR&#125;/etc/origin/master/master.env /etc/origin/master/master.env$ cp $&#123;MYBACKUPDIR&#125;/etc/origin/master/master-config.yaml.&lt;timestamp&gt; /etc/origin/master/master-config.yaml$ cp $&#123;MYBACKUPDIR&#125;/etc/origin/node/node-config.yaml.&lt;timestamp&gt; /etc/origin/node/node-config.yaml$ cp $&#123;MYBACKUPDIR&#125;/etc/origin/master/scheduler.json.&lt;timestamp&gt; /etc/origin/master/scheduler.json$ master-restart api$ master-restart controllers

在每一个Node节点，恢复配置文件，并重启origin-node服务
$ cp /etc/origin/node/node-config.yaml.&lt;timestamp&gt; /etc/origin/node/node-config.yaml$ systemctl enable atomic-openshift-node$ systemctl start atomic-openshift-node

恢复项目Project恢复项目前，先创建项目，再通过oc create -f命令将项目中的对象恢复。恢复项目时要注意对象的依赖关系，比如说pod依赖configmap资源，就需要先创建configmap。
恢复过程$ oc new-project &lt;projectname&gt;$ oc create -f project.yaml$ oc create -f secret.yaml$ oc create -f serviceaccount.yaml$ oc create -f pvc.yaml$ oc create -f rolebindings.yaml

恢复应用数据与备份应用数据类似，可以使用oc rsync命令来恢复应用数据。
以下是一个利用jenkins应用的备份数据恢复应用的例子。
恢复过程
检查备份数据
$ ls -la /tmp/jenkins-backup/total 8drwxrwxr-x.  3 user     user   20 Sep  6 11:14 .drwxrwxrwt. 17 root     root 4096 Sep  6 11:16 ..drwxrwsrwx. 12 user     user 4096 Sep  6 11:14 jenkins

使用oc rsync恢复应用数据
$ oc rsync /tmp/jenkins-backup/jenkins jenkins-1-37nux:/var/lib

重启应用
$ oc delete pod jenkins-1-37nux

或者使用oc scale命令将pod数调整为0，再调整为1，实现应用的重启
$ oc scale --replicas=0 dc/jenkins$ oc scale --replicas=1 dc/jenkins

恢复持久化卷数据如果应用已挂载了新的PV，那就将该PV原来的数据删除，之后将备份的数据拷贝到对应的目录；如果应用没有挂载PV，那就先挂载一个PV，再恢复数据。
恢复过程
如果没有挂载PV执行创建新的挂载
$ oc set volume dc/demo --add --name=persistent-volume \		--type=persistentVolumeClaim --claim-name=filestore \ --mount-path=/opt/app-root/src/uploaded --overwrite

删除当前PV挂载目录下的数据
$ oc rsh demo-2-fxx6dsh-4.2$ ls */opt/app-root/src/uploaded/*lost+found  ocp_sop.txtsh-4.2$ *rm -rf /opt/app-root/src/uploaded/ocp_sop.txt*sh-4.2$ *ls /opt/app-root/src/uploaded/*lost+found

将之前备份的数据拷贝到对应的目录下
$ oc rsync uploaded demo-2-fxx6d:/opt/app-root/src/

验证应用数据
$ oc rsh demo-2-fxx6dsh-4.2$ *ls /opt/app-root/src/uploaded/*lost+found  ocp_sop.txt

实战演练步骤
部署安装3Master 1etcd及2个Node节点的Openshift集群
使用恢复etcd数据中的根据v3的快照snapshot恢复恢复etcd数据如果此时pod无法正常启动，可以执行以下命令

$ echo &quot;ETCD_FORCE_NEW_CLUSTER=true&quot; &gt;&gt; /etc/etcd/etcd.conf
再重启Pod，待etcd正常运行后，将刚才添加的ETCD_FORCE_NEW_CLUSTER=true从&#x2F;etc&#x2F;etcd&#x2F;etcd.conf文件中删除。3. 按照使用Ansible添加etcd节点中的步骤将1个etcd节点扩容为3个etcd节点4. 清理恢复中的Openshift集群中的token，并重启相关pod，实现Openshift集群的完整恢复。
一键恢复与解决证书问题的脚本一键恢复etcd
[root@master01 ~]# cat restore_etcd.sh #!/bin/bashsnapshot_file_dir=$1if [ $# -lt 1 ]then    echo &quot;Please input snapshot file path&quot;    exit 2fiexport ETCD_POD_MANIFEST=&quot;/etc/origin/node/pods/etcd.yaml&quot;mv $&#123;ETCD_POD_MANIFEST&#125; .rm -rf /var/lib/etcd## 获取etcd相关初始化配置项ETCD_CONFIG_FILE=&quot;/etc/etcd/etcd.conf&quot;etcd_data_dir=$(grep ^ETCD_DATA_DIR= $ETCD_CONFIG_FILE|cut -d= -f2)etcd_name=$(grep ^ETCD_NAME= $ETCD_CONFIG_FILE|cut -d= -f2)etcd_initial_cluster=$(grep ^ETCD_INITIAL_CLUSTER= $ETCD_CONFIG_FILE|awk -F&#x27;ETCD_INITIAL_CLUSTER=&#x27; &#x27;&#123;print $2&#125;&#x27;)etcd_initial_cluster_token=$(grep ^ETCD_INITIAL_CLUSTER_TOKEN= $ETCD_CONFIG_FILE|cut -d= -f2)etcd_initial_advertise_peer_urls=$(grep ^ETCD_INITIAL_ADVERTISE_PEER_URLS= $ETCD_CONFIG_FILE|cut -d= -f2)## 恢复etcd数据export ETCDCTL_API=3etcdctl snapshot restore $snapshot_file_dir --data-dir $etcd_data_dir --name $etcd_name --initial-cluster &quot;$etcd_initial_cluster&quot; --initial-cluster-token &quot;$etcd_initial_cluster_token&quot; --initial-advertise-peer-urls $etcd_initial_advertise_peer_urls --skip-hash-check=truerestorecon -Rv /var/lib/etcdmv etcd.yaml $ETCD_POD_MANIFEST
一键整理etcd数据，解决证书问题
[root@master01 ~]# cat reset.sh #!/bin/bashoc login -u system:adminprojects=$(oc get projects | awk &#x27;&#123;print $1&#125;&#x27; | grep -v kube-system|grep -v NAME)for project in $(echo $projects)do  oc delete secret $(oc get secret -n $project | grep token | awk &#x27;&#123;print $1&#125;&#x27;) -n $project  oc delete pod $(oc get pod -n $project | grep -v NAME | awk &#x27;&#123;print $1&#125;&#x27;) -n $project --force --grace-period=0done
参考文章Openshift官方文档之恢复集群
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift集群升级3-6至3-7</title>
    <url>/posts/dae9f91bc2e9.html</url>
    <content><![CDATA[更新方法与策略：自动升级方法

更新openshift-ansible的脚本代码
  git pull git checkout -b release-3.7
安装需要的包

ansible all -m yum -a &#x27;name=pyOpenSSL state=present&#x27;

关闭Service Catalog和Template Service Broker。3.7 以上版本默认会启用 Service Catalog 和 Template Service Broker

# /etc/ansible/hosts...openshift_enable_service_catalog=falsetemplate_service_broker_install=falseansible_service_broker_install=false...

关闭etcd目录的selinux检查关闭任务Check selinux label of ‘‘与Make sure the ‘‘ has the proper label

# roles/etcd/tasks/backup/backup.yml...- name: Detecting Atomic Host Operating System  stat:    path: /run/ostree-booted  register: l_ostree_booted#- name: Check selinux label of &#x27;&#123;&#123; etcd_data_dir &#125;&#125;&#x27;#  command: &gt;#    stat -c &#x27;%C&#x27; &#123;&#123; etcd_data_dir &#125;&#125;#  register: l_etcd_selinux_labels##- debug:#    msg: &quot;&#123;&#123; l_etcd_selinux_labels &#125;&#125;&quot;##- name: Make sure the &#x27;&#123;&#123; etcd_data_dir &#125;&#125;&#x27; has the proper label#  command: &gt;#    chcon -t svirt_sandbox_file_t  &quot;&#123;&#123; etcd_data_dir &#125;&#125;&quot;#  when:#  - l_etcd_selinux_labels.rc == 0#  - &quot;&#x27;svirt_sandbox_file_t&#x27; not in l_etcd_selinux_labels.stdout&quot;- name: Generate etcd backup  command: &gt;    &#123;&#123; r_etcd_common_etcdctl_command &#125;&#125; backup --data-dir=&#123;&#123; l_etcd_incontainer_data_dir &#125;&#125;    --backup-dir=&#123;&#123; l_etcd_incontainer_backup_dir &#125;&#125;...

执行更新

ansible-playbook -i inventory-hosts playbooks/byo/openshift-cluster/upgrades/v3_7/upgrade.yml

查看当前oc版本及各node版本

oc versionoc get node
升级后发现的问题
RBD的storageclass异常

# 解决方法，storageclass.parameters中添加 imageFormat: &quot;1&quot;apiVersion: storage.k8s.io/v1metadata:   name: ceph-rbd-scParameters:  adminId: admin  adminSecretName: ceph-secret  adminSecretNamespace: kube-system  fsType: ext4  imageFormat: &quot;1&quot;  monitors: 192.168.1.3:6789  pool: rbd  userId: admin  userSecretName: ceph-secretProvisioner: kubernetes.io/rbd
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshif对集群的扩容与缩容</title>
    <url>/posts/b6a18d18133d.html</url>
    <content><![CDATA[
Openshift是一个云平台，它是以集群的方式提供服务。前面已经介绍了，业务都跑在Openshift的Node节点上。随着业务的不断变化，扩展或者消亡，我们的Node提供的服务需求也会不断变化。这时就需要对Node节点进行增删管理。
本篇只介绍CentOS7下管理Node节点。
Openshift使用Ansible playbook来实现扩容与缩容

1. oc命令查看当前Node节点的状态oc get node --show-labels
2. 添加Node节点到已存在的集群
准备好需要添加的主机




节点类型
说明



Nodes
物理主机或者虚拟机 系统：Fedora 21, CentOS 7.3, 7.4或者7.5 NetworkManager版本1.0以上最少1vCPU最少8GB内存&#x2F;var&#x2F;最少15GB空间 &#x2F;usr&#x2F;local&#x2F;bin最少1GB空间容器临时目录最少1GB空间 &nbsp;



设置主机的hostname

hostnamectl --static set-hostname infra1.example.com

集群中的DNS中添加新加主机的域名与ip的解析

#/etc/dnsmasq.d/more.confaddress=/infra1.example.com/192.168.0.8systemctl restart dnsmasq

设置新增加主机的默认DNS

# /etc/resolv.confnameserver 192.168.0.2
补充，如果是使用&#x2F;etc&#x2F;hosts的方式来设置域名与IP的，需要为每台主机添加hosts条目，同时重启dnsmasq。

配置ansible Hosts文件，添加新增的主机

#/etc/ansible/hosts[OSEv3:children]mastersnodesnew_nodes...[new_nodes]infra1.example.com openshift_node_labels=&quot;&#123;&#x27;region&#x27;: &#x27;primary&#x27;, &#x27;zone&#x27;: &#x27;default&#x27;, &#x27;node-role.kubernetes.io/infra&#x27;: &#x27;true&#x27;&#125;&quot;

执行扩容脚本

ansible-playbook playbooks/openshift-node/scaleup.yml

将new_nodes中的主机移到nodes组中移除

#/etc/ansible/hosts[OSEv3:children]mastersnodesnew_nodes...[nodes]infra1.example.com openshift_node_labels=&quot;&#123;&#x27;region&#x27;: &#x27;primary&#x27;, &#x27;zone&#x27;: &#x27;default&#x27;, &#x27;node-role.kubernetes.io/infra&#x27;: &#x27;true&#x27;&#125;&quot;[new_nodes]

给新增的节点配置&#x2F;etc&#x2F;origin&#x2F;node&#x2F;node-config.yaml

kubeletArguments:  system-reserved:  - cpu=200m  - memory=1G  kube-reserved:  - cpu=200m  - memory=1G

重启origin-node服务

systemctl restart origin-node

查看集群中的主机情况进行确认

oc get node --show-labels
3.从集群中移除Node节点
设置需要移除的Node为不可调度

oadm manage-node &lt;node1&gt; --schedulable=false

迁移node上已有的容器

oadm manage-node &lt;node1&gt; --evacuate

在集群中删除指定的node节点

oc delete node infra1.example.com

删除在Ansible hosts文件中的主机配置

...[nodes]

查看集群中的主机情况进行确认

oc get node --show-labels

**[可选]**新建一个hosts文件，作为ansible-playbook的inventory，只需要写需要删除的node节点

[OSEv3:children]nodes [OSEv3:vars]ansible_ssh_user=rootopenshift_deployment_type=origin [nodes]infra1.example.com

**[可选]**执行清理脚本uninstall.yml

ansible-playbook -i hosts openshift-ansible/playbooks/adhoc/uninstall.yml
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshit-新的存储类型LocalVolume该如何使用</title>
    <url>/posts/17a022f7f345.html</url>
    <content><![CDATA[

使用本地盘的方式的有三种：emptyDir、HostPath与Local Volume
Openshift针对Local Volume提供了一个local volume provisioner方便PV的自动创建，但目前它需要创建的PV对应的目录必须是挂载点。否则无法创建PV。
除了自动创建PV的过程外，部分与正常使用Local Volume一样。

手动挂载目录使用Local Volume我们先来看下通过手动的方式创建PV使用Local Volume是如何操作的。
1. 创建StorageClassapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: local-hddprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumer
2. 创建PVapiVersion: v1 kind: PersistentVolume metadata:   name: local-volume-1 spec:   accessModes:   - ReadWriteOnce   capacity:     storage: 1Gi   local:     path: /root/hdd/vol1   nodeAffinity:     required:       nodeSelectorTerms:       - matchExpressions:         - key: kubernetes.io/hostname           operator: In           values:           - node01.example.com   storageClassName: local-hdd
3. 创建PVC使用PVapiVersion: v1 kind: PersistentVolumeClaim metadata:   name: local-pvc spec:   accessModes:   - ReadWriteOnce   resources:     requests:       storage: 1Gi   storageClassName: local-hdd   volumeName: local-volume-1
4. 对于StatefulSet创建volumeClaimTemplatesvolumeClaimTemplates:  - metadata:      name: local-pvc    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      storageClassName: local-hdd      resources:        requests:          storage: 1Gi
使用Local Volume Provisioner方式自动挂载目录使用Local Volume再次说明：目前它需要创建的PV对应的目录必须是挂载点。
1. 准备磁盘&#x2F;分区，并将它挂载到指定目录下，每个Node节点都做一样的挂载点。如/etc/fstab配置如下/dev/sdb1       /mnt/local-storage/hdd/disk1 ext4     defaults 1 2
2. 对挂载点目录设置权限，允许容器进行访问读写$ chcon -R unconfined_u:object_r:svirt_sandbox_file_t:s0 /mnt/local-storage/
3. 创建一个新的Project，为部署Local Volume Provisioner准备$ oc new-project local-storage.
4. 导入主机local volume的目录配置，即创建对应的configmap$ cat  &lt;&lt; EOF | create -f apiVersion: v1kind: ConfigMapmetadata:  name: local-volume-configdata:    storageClassMap: |        local-hdd:            hostDir: /mnt/local-storage/hdd            mountDir: /mnt/local-storage/hdd

local-hdd将为创建的StorageClass的名字hostDir为主机挂载点的父目录，如果该目录下有新的挂载点，将会自动创建新的PVmountDir为Provisioner Pod的挂载点，最好与hostDir一致
5. 准备serviceaccount等权限配置$ oc create serviceaccount local-storage-admin$ oc adm policy add-scc-to-user privileged -z local-storage-admin
6. 安装Local Volume Provisioner$ oc create -f https://raw.githubusercontent.com/openshift/origin/release-3.11/examples/storage-examples/local-examples/local-storage-provisioner-template.yaml$  oc new-app -p CONFIGMAP=local-volume-config \  -p SERVICE_ACCOUNT=local-storage-admin \  -p NAMESPACE=local-storage \  -p PROVISIONER_IMAGE=docker.io/xhuaustc/openshift3-local-storage-provisioner:v3.11 \   local-storage-provisioner
7. 创建对应的storageclass$ cat &lt;&lt; EOF | oc create -f -apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: local-hddprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumerEOF

至此，部署完成。使用方式与手动创建PV一样。新加一块存储也非常容易，只需要在各个Node节点的hostDir目录下创建新挂载点，并更新好权限，Local Volume Provisioner将会自动创建PV以供使用。需要注意的是，PVC的配置yaml中，storageClassName必须写在spec的配置中。而openshift console默认是将storageclass写在metadata-&gt;annotations-&gt;volume.beta.kubernetes.io&#x2F;storage-class: local-hdd，所以通过openshift console创建的pvc是无法使用Local Volume创建的pv的。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Rancher大屏</title>
    <url>/posts/3c9aa01de06e.html</url>
    <content><![CDATA[


]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Service-iptables实现原理</title>
    <url>/posts/6e5f72fcd352.html</url>
    <content><![CDATA[
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>oc的常用命令整理</title>
    <url>/posts/3be580fc375b.html</url>
    <content><![CDATA[
oc process


oc new-app


oc new-build


oc create imagestream


oc run


oc create -f 


oc policy add-role-to-user edit system:serviceaccount:jenkins:jenkins -n dev


oc adm policy add-scc-to-user anyuid -z gitlab -n gitlab


oc new-project


oc set volumes


oc expose


oc start-build


oc rollback


oc rollout


oc rsync


oc rsh


oc set triggers dc&#x2F;flask-app-s2i –remove-all &amp;&amp; oc set triggers dc&#x2F;flask-app-s2i –from-image&#x3D;flask-app-s2i:v1 -c flask-app-s2i &amp;&amp; oc set triggers dc&#x2F;flask-app-s2i –from-config


oc patch NginxLB example-nginxlb -p ‘{“spec”: {“size”: “1”}}’ –type&#x3D;merge


oc serviceaccounts create-kubeconfig  -n 

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>《Prometheus监控实战》读书笔记</title>
    <url>/posts/3ea85377ce64.html</url>
    <content><![CDATA[  
Prometheus是一个开源的监控系统。  
一、监控简介监控不仅仅只是系统的技术指标，还可以是业务指标。确保为客户提供可靠恰当的产品。在生产系统中，监控是必须的，它应该和应用程序一起构建和部署。帮助我们了解系统的环境、进行诊断故障、制定容量计划，为组织提供系统的性能、成本和状态等信息。
监控模式
务必在项目最开始阶段就把监控考虑进去，它和安全性一样，都是应用的核心功能。
根据服务价值设计自上而下的监控系统是一个很好的方式。先是业务逻辑，再是应用程序，最后才是基础设施操作系统。
需要找准监控项，及时发现错误
通过多样化的数据，如查看数据窗口等更智能的技术分析指标与阀值
增加监控的频率
尽可能实现监控系统部署实施自动化

监控机制
探针与内省：监控应用程序的两种方法。探针：在应用程序的外部，查询应用程序的外部特征。如Nagios。内省：查看应用程序的内部内容，经过检测，返回其状态、内部组件等的度量。可用作健康检查接口，由监控工具收集。
拉取与推送：执行监控检查的两种方式。拉取：提取或检查远程应用程序。如Prometheus。推送：应用程序发送事件给监控系统接收。

监控数据类型
指标：软件和硬件组件属性的度量。指标类型：测量型 Gauge、计数型 Counter、直方图 Histogram
日志：应用程序发出的事件（通常是文本类型）。

监控方法论
Brendan Gregg的USE方法 主机级监控USE：使用率、饱和度、错误。针对每个资源（如CPU），检查使用率（如CPU使用百分比）、饱和度（如等待CPU的进程数）和错误（如错误事件的计数）。
Google四个黄金指标 应用程序级监控延迟（如服务请求时间）、流量（如每秒HTTP请求数）、错误（如响应时间走过30ms的请求视为错误）、饱和度（如内存IO）。
RED方法Rate（你的服务所服务的每秒的请求数）、Errors（每秒失败的请求数）、Duration（每个请求所花费的时间，用时间间隔表示）

通知系统通知系统需要关注以下几点：

通知清晰、准确、可操作。人易读。
通知添加上下文，通知应包含组件的其他相关信息。
仅发送有意义的通知。

二、Prometheus是什么？Prometheus灵感来自于谷歌的Borgmon。这由前谷歌SRE Matt T. Proud开发。Proud加入SoundCloud后，两天Julius Volz合作开发了Prometheus。于2015年1月发布。Prometheus主要用于提供近实时的、基于动态云环境和容器的微服务、服务和应用的内省监控。Prometheus专注于当前的事件，大多数监控查询和警报都是从最近（通常为一天内）的数据中生成。Prometheus通过拉取应用程序中暴露的时间序列数据来工作。时间序列数据通常由应用程序本身通过客户端库或exporter的代理来作为HTTP端点暴露。
Prometheus架构Prometheus有一个推送网关，可用于接收少量数据。  
名词解释endpoint：端点，Prometheus抓取的指标来源target：目标，定义执行抓取所需的信息，如链接，身份验证，抓取方式job：一组目标，具有相同角色的目标组
监控资源服务发现
用户提供静态资源列表。
基于文件的发现。例如使用配置管理工具生成在Prometheus中可以自动更新的资源列表。
自动发现。例如，查询consule等数据存储。

聚合和警报Prometheus服务器可以查询和聚合时间序列数据，创建规则来记录常用的查询和聚合。Prometheus可以定义警报规则。为系统配置的在满足条件时触发警报的标准。将警报从Prometheus推送到警报管理器（Alertmanager）的单独服务器。AlertManager可以管理、整合、分发各种警报到不同的目的地。
查询、可视化Prometheus提供了一套内置查询语言PromQL、一个表达式浏览器及浏览数据的图形界面。Prometheus可以与开源DashBoard Grafana集成，同时也支持其他的DashBoard。为了速度与可靠性，Prometheus服务器充分使用内存和SSD磁盘
冗余和高可用性部署Prometheus的高可用模式，可以使用两个以上配置相同的Prometheus服务器来收集时间序列数据，并且生成的警报由Alertmanager集群来处理。
Prometheus数据模型total_website_visits&#123;site=&quot;MegaApp&quot;, location=&quot;NJ&quot;, instance=&quot;webserver:9090&quot;, job=&quot;web&quot;&#125;

指标名称：total_website
标签：site&#x3D;”MegaApp”等，通常都有一个instance标签和job标签标签分为两类：1，插桩标签；2，目标标签插桩标签：来自被监控的资源目标标签：通常与架构相关，由Prometheus在抓取期间和之后添加。
时间序列由名称和标签标识，如果在时间序列中添加或更改标签，Prometheus会将其视为新的时间序列。

三、安装部署Prometheus安装PrometheusLinux上安装Prometheus
下载Prometheus的二进制文件

$ wget https://github.com/prometheus/prometheus/releases/download/v2.14.0/prometheus-2.14.0.linux-amd64.tar.gz

解压文件到相关目录，安装promtool

$ tar -xzf prometheus-2.14.0.linux-amd64.tar.gz$ sudo cp prometheus-2.14.0.linux-amd64/prometheus /usr/local/bin/$ sudo cp prometheus-2.14.0.linux-amd64/promtool /usr/local/bin/

执行prometheus –version查看版本验证安装。

Ansible安装Promethues相关的role路径为：https://github.com/cloudalchemy/ansible-prometheus。
OpenShift上安装PrometheusOpenShift官方部署脚本支持Prometheus的安装。在OpenShift 3.11部署的时候，prometheus默认是安装的，如果要禁止安装则需将inventory中的配置openshift_cluster_monitoring_operator_install设置为false。相关的开源软件地址为：https://github.com/openshift/cluster-monitoring-operator
配置PrometheusPrometheus通过YAML文件来配置。Prometheus的解压的目录中自带有配置文件prometheus.yml。
global:  scrape_interval:     15s # 应用程序或服务抓取数据的时间间隔，默认为1min  evaluation_interval: 15s # Prometheus评估规则（记录规则、警报规则）的频率，默认为1min  # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting:  alertmanagers:  - static_configs:    - targets:      # - alertmanager:9093# 一次性加载规则，并且每隔evaluation_interval时间对规则进行评估&#x27;evaluation_interval&#x27;.rule_files:  # - &quot;first_rules.yml&quot;  # - &quot;second_rules.yml&quot;# 指定Prometheus抓取的所有目标# 下面是监控Prometheus本身scrape_configs:  # job名字将会被加到监控的指标中`job=&lt;job_name&gt;`   - job_name: &#x27;prometheus&#x27;    # metrics_path 默认为 &#x27;/metrics&#x27;    # scheme 默认为 &#x27;http&#x27;.    static_configs:    - targets: [&#x27;localhost:9090&#x27;]
global：全局配置alerting：设置Prometheus的警报。Prometheus支持Alertmanager服务发现功能。rule_files：指定包含记录规则或警报规则的文件列表。scrape_configs：指定Prometheus抓取的所有目标。
运行Prometheus
将配置文件prometheus.yml移到合适的位置

$ sudo mkdir -p /etc/prometheus$ sudo cp prometheus.yml /etc/prometheus/

运行prometheus应用

$ prometheus --config.file &quot;/etc/prometheus/prometheus.yml&quot;

如果发生异常，可以使用promtool来验证配置文件

$ promtool check config prometheus.ymlChecking prometheus.yml    SUCCESS: 0 rule files found
查询数据
查看指定指标值

go_gc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125;  1.6166e-05

过滤一些匹配标签

go_gc_duration_seconds&#123;quantile!=&quot;0.5&quot;&#125;  

聚合数据求和

sum(promhttp_metric_handler_requests_total) # 求和

PromQL通过子名by，按特定维度聚合

sum(promhttp_metric_handler_requests_total) by (job)

转换成速率

sum(rate(promhttp_metric_handler_requests_total[5m])) by (job)
rate()函数计算一定范围内时间序列的每秒平均增长率，适合缓慢变化的计数器（counter）。irate()函数计算指定时间范围内的最近两个数据点来算速率，适合快速变化的计数器（counter）。rate()与irate()函数都必须与计数器一起使用。6. 增加数量
increase(promhttp_metric_handler_requests_total[1h])
指标1小时内增长的值。
容量规划
内存

通过以下查询语句查看样本的收集率。显示最后一分钟添加到数据库的每秒样本率。
rate(prometheus_tsdb_head_samples_appended_total[1m])
查询收集的指标数量，通过以下语句
sum(count by (__name__)(&#123;__name__-\~&quot;\.\+&quot;&#125;))
每个样本大小通常为1到2字节，假设在12小时内每秒收集100000个样本，那么内存使用情况为
100000 * 2 bytes * 43200秒 约为 8.64GB内存
约为 8.64GB内存，还需要考虑查询与记录规则方面内存的使用情况。可以通过查看process_resident_memory_bytes指标查看Prometheus进程的内存使用情况。2. 磁盘
默认情况下，指标会在本地数据库中存储15天。具体时间由命令行选项控制。–storage.tsdb.path： 设置数据存储目录，默认为Prometheus的目录中。–storage.tsdb.retention： 设置时间序列保留期，默认为15天。建议采用SSD作为时间序列数据库的磁盘每秒10万个样本的示例，每个样本磁盘上占用约1~2字节，保留15天数据算，大约需要259GB磁盘。
四、监控主机和容器Prometheus通过使用exporter工具来暴露主机和应用程序的指标。常用的exporter列表可在以下网站上查看。
https://prometheus.io/docs/instrumenting/exporters/
监控主机Node Exporter 是用GO语言编写的，收集各种主机指标数据（CPU&#x2F;内存&#x2F;磁盘等）。同时还有一个textfile收集器，允许导出静态指标。

安装Node ExporterNode Exporter的下载地址如下：

https://github.com/prometheus/node_exporter
下载并解压安装
$ wget https://github.com/prometheus/node_exporter/releases/download/v0.18.1/node_exporter-0.18.1.linux-amd64.tar.gz$ tar -xzf node_exporter-*$ sudo cp node_exporter-*/node_exporter /user/local/bin/

配置Node Exporternode_exporter是通过执行命令时传入参数来配置。

$ node_exporter --help
默认情况下node_exporter在端口9100上运行，路径为&#x2F;metrics。可通过–web.listen-address和–web.telemetry-path来设置
$ node_exporter --web.listen-address=&quot;:9600&quot; --web.telemetry-path=&quot;/node_metrics&quot;
同时可以通过参数控制启动的收集器，默认的收集器可在以下地址中查看
https://github.com/prometheus/node_exporter#enabled-by-default
例如，想要禁用&#x2F;proc&#x2F;net&#x2F;arp统计信息，只需要启动node_exporter时添加--no-collector.arp配置项。3. 配置textfile收集器textfile收集器可以帮助我们暴露自定义指标。node_exporter通过参数--collector.textfile.directory参数指定textfile的目录，它会扫描该目录下的所有文件，提取所有格式为Prometheus指标的字符串，然后暴露它们。
$ mkdir -p /var/lib/node_exporter/textfile_collector$ echo &#x27;metadata&#123;role=&quot;docker_server&quot;, datacenter=&quot;NJ&quot;&#125; 1&#x27; | sudo tee /var/lib/node_exporter/textfile_collector/metadata.prom

启用systemd收集器systemd收集器记录systemd中的服务和系统状态。首先需要通过参数--collector.systemd启用该收集器，同时如果不希望收集所有的服务，只收集部分关键服务。node_exporter在启动时可以使用--collector.systemd.unit-whitelist参数配置。
运行Node Exporter启动node_exporter的实例

$ node_exporter --collector.textfile.director /var/lib/node_exporter/textfile_collector --collector.systemd --collector.systemd.unit-whitelist-&#123;(docker|ssh|rsyslog).service&quot;

抓取Node Exporter配置新作业来抓取Node Exporter导出的数据。prometheus.yml文件中的scrape_configs部分添加job

scrape_configs:  - job_name: &#x27;prometheus&#x27;    static_configs:      - targets: [&#x27;localhost:9090&#x27;]  - job_name: &#x27;node&#x27;    static_configs:      - targets: [&#x27;192.168.0.3:9100&#x27;, &#x27;192.168.0.4:9100&#x27;, &#x27;192.168.0.5:9100&#x27;]
添加了新的job ‘node’，添加了三台主机的node_exporter监控，默认路径为&#x2F;metrics。重启Prometheus服务，将会加载最新的配置，新监控项将会被收集到prometheus数据中。7. 过滤收集器Prometheus提供了限制抓取过来的数据指标的机制。使用params块中的collect[]来指定收集的指标。
scrape_configs:  - job_name: &#x27;node&#x27;    static_configs:      - targets: [&#x27;192.168.0.3:9100&#x27;, &#x27;192.168.0.4:9100&#x27;, &#x27;192.168.0.5:9100&#x27;]    params:      collect[]:        - cpu        - meminfo        - diskstats        - netdev        - netstat        - filefd        - filesystem        - xfs        - systemd
使用以下命令来模拟测试
$ curl -g -X GET http://192.168.0.3:9100/metrics?collect[]=cpu
监控Docker容器推荐使用Google的cAdvisor工具来监控Docker。cAdvisor作为Docker容器运行，单个cAdvisor容器返回针对Docker守护进程和所有正在运行的容器的指标。

运行cAdvisor

$ docker run \-v /:/rootfs:ro \-v /var/run:/var/run:rw \-v /sys:/sys:ro \-v /var/lib/docker/:/var/lib/docker:ro \-v /dev/disk/:/dev/disk:ro \-p 8080:8080 \-d --name=cadvisor \google/cadvisor:latest
通过浏览器访问http://192.168.0.3:8080/containers/可查看  
通过浏览器访问http://192.168.0.3:8080/metrics查看暴露的内置Prometheus指标  

抓取cAdvisor配置新作业来抓取cAdvisor导出的数据。prometheus.yml文件中的scrape_configs部分添加job

scrape_configs:  - job_name: &#x27;prometheus&#x27;    static_configs:      - targets: [&#x27;localhost:9090&#x27;]  - job_name: &#x27;node&#x27;    static_configs:      - targets: [&#x27;192.168.0.3:9100&#x27;, &#x27;192.168.0.4:9100&#x27;, &#x27;192.168.0.5:9100&#x27;]  - job_name: &#x27;docker&#x27;    static_configs:      - targets: [&#x27;192.168.0.3:8080&#x27;, &#x27;192.168.0.4:8080&#x27;, &#x27;192.168.0.5:8080&#x27;]
添加了新的job ‘docker’，添加了三台主机的node_exporter监控，默认路径为&#x2F;metrics。
Prometheus抓取数据的生命周期  
可以看到在Prometheus抓取数据的生命周期中，有两处重新标记指标的部分，一个在抓取前，一个是在抓取后。
标签标签提供了时间序列的维度，可以定义目标，并为时间序列提供上下文。最重要的是，结合指标名称，它们构成了时间序列的标识。更改或者添加标签会创建新的时间序列

标签分类：拓扑标签、模式标签拓扑标签：通过其物理或者逻辑组成来切割服务组件。每个指标都天然带着job和instance两个拓扑标签。instance标签可以标识目标，通常是目标的IP地址和端口，并且来自__address__标签。模式标签：url、error_code或者user之类，允许将拓扑中同一级别的时间序列匹配在一起。如果需要添加额外的标签，可以考虑以下的层次结构  

重新标记控制标签。有两个阶段可以重新标记标签。抓取前的relabel_configs和抓取后的metric_relabel_configs模式。



删除不必要的指标
从指标中删除敏感或不需要的标签
添加、编辑、修改指标的标签值或标签格式


在拉取了cAdvisor的指标数据后，删除container_tasks_state与container_memory_failures_total指标数据

- job_name: &#x27;docker&#x27;   static_configs:     - targets: [&#x27;192.168.0.3:8080&#x27;, &#x27;192.168.0.4:8080&#x27;, &#x27;192.168.0.5:8080&#x27;]   metric_relabel_configs:     - source_labels: [__name__]       separator: &#x27;;&#x27;       regex: &#x27;(container_tasks_state|container_memory_failures_total)&#x27;       action: drop
separator默认值为’;’，多个标签通过separator连接在一起。如果指定多个源标签，可以使用；隔开每个正则表达式。action: drop将会在数据存储之前删除指标，keep则会保留与正则匹配的指标，删除所有其他指标。

正则配置id指标中的数据，并将匹配的值放入新的标签container_id中。

- job_name: &#x27;docker&#x27;   static_configs:     - targets: [&#x27;192.168.0.3:8080&#x27;, &#x27;192.168.0.4:8080&#x27;, &#x27;192.168.0.5:8080&#x27;]   metric_relabel_configs:     - source_labels: [id]       regex: &#x27;/docker/([a-z0-9]+);&#x27;       replacement: &#x27;$1&#x27;       target_label: container_id
没有指定action操作，因为metric_relabel_configs的action默认操作为replace。honor_labels默认值为false，如果target_label的标签已经存在，则会在其前面添加exported_前缀来做区分。

删除标签kernelVersion

- job_name: &#x27;docker&#x27;   static_configs:     - targets: [&#x27;192.168.0.3:8080&#x27;, &#x27;192.168.0.4:8080&#x27;, &#x27;192.168.0.5:8080&#x27;]   metric_relabel_configs:     - regex: &#x27;kernelVersion&#x27;       action: labeldrop
action: labeldrop，删除匹配的标签action: labelkeep，保留匹配的标签，删除所有其他标签
Node Exporter和cAdvisor指标
CPU使用率

100 - avg(irate(node_cpu_seconds_total&#123;job=&quot;node&quot;, mode=&quot;idle&quot;&#125;[5m])) by (instance) * 100

CPU饱和度

node_load1 &gt; on (instance) 2 * count by (instance)(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;)

内存使用率

(node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Cached_bytes + node_memory_Buffers_bytes)) / node_memory_MemTotal_bytes * 100

内存饱和度

1024 * sum by (instance) ((rate(node_vmstat_pgpgin[1m]) + rate(node_vmstat_pgpgout[1m])))

磁盘使用率

(node_fileystem_size_bytes&#123;mountpoint=~&quot;/|/run&quot;&#125; - node_filesystem_free_bytes&#123;mountpoint=~&quot;/|/run&quot;&#125;) / node_filesystem_size_bytes&#123;mountpoint=~&quot;/|/run&quot;&#125; * 100
Prometheus支持预测
predict_linear(node_fileystem_size_bytes&#123;job=&quot;node&quot;&#125;[1h], 4*3600) &lt; 0
使用1个小时窗口数据，预测所有磁盘4个小时后的磁盘会不会用完。6. 服务状态
node_systemd_unit_state&#123;name=&quot;docker.service&quot;, state=&quot;active&quot;&#125;

exporter的可用性和up指标

up &#123;job=&quot;node&quot;, instance=&quot;192.168.0.3:9100&quot;&#125;

textfile收集器metadata指标

metadata&#123;datacenter != &quot;NJ&quot;&#125;node_systemd_unit_state&#123;name=&quot;docker.service&quot;&#125; == 1 and on (instance, job) metadata&#123;datacenter = &quot;SF&quot;&#125;
on：一对一匹配group_left：多对一group_right：一对多详情：https://prometheus.io/docs/prometheus/latest/querying/operators/#vector-matching
查询持久化三种方式使查询持久化

记录规则：根据查询创建新指标
警报规则：从查询生成警报
可视化：使用Grafana等仪表板可视化查询

记录规则prometheus.yml中的配置项evaluation_interval设置为自动计算记录的规则。
global:  scrape_interval: 15s  evaluation_interval: 15s
在prometheus.yml文件的同一文件夹下，创建名为rules的子文件夹，用于保存记录规则。同时在prometheus.yml配置文件中的rule_files块中添加文件rules/node_rules.yml
$ mkdir -p rules &amp;&amp; cd rules$ cat &gt; node_rules.yml &lt;&lt;EOFgroups:- name: node_rules  interval: 10s  rules:  - record: instance:node_cpu:avg_rate5m    expr: 100 - avg(irate(node_cpu_seconds_total&#123;job=&quot;node&quot;, mode=&quot;idle&quot;&#125;[5m])) by (instance) * 100    labels:      metric_type: aggregationEOF
创建了名为”node_rules”的规则组，顺序执行规则，后面的规则可以重用前面的规则。规则组间是并行运行的，因此不建议跨组使用规则。interval：10s 可以设置interval覆盖默认的evaluation_interval值。record：应该仔细为新的时间序列取名，以便快速识别它的含义，一般推荐的格式是： level:metric:operationsexpr：保存生成新时间序列的查询labels：向新序列添加新的标签
可视化 Grafana
安装Grafana

$ wget https://dl.grafana.com/oss/release/grafana-6.5.1-1.x86_64.rpm$ sudo yum localinstall grafana-6.5.1-1.x86_64.rpm
使用ansible role安装：https://galaxy.ansible.com/list#roles/3563使用Docker镜像安装：https://hub.docker.com/search/?q=grafana2. 启动与配置Grafana配置文件位于/etc/grafana/grafana.ini
$ systemctl start grafana-server

浏览器访问http://localhost:3000，默认用户名和密码为admin和admin。Grafana仪表盘实例查看地址：https://grafana.com/grafana/dashboards

五、Prometheus服务发现对于主机数少的情况下，Prometheus可以使用静态配置目标。当规模较大的集群时，就不适用了。服务发现可以通过以下三种机制实现：

配置管理工具生成的文件中接收目标列表
查询API以获取目标列表
使用DNS记录以返回目标列表

基于文件的服务发现适合配置管理工具。Prometheus会按指定时间计划从这些文件重新加载目标，文件格式支持yaml、Json格式，它们包含了目标列表。
- job_name: node  file_sd_configs:    - files:      - targets/nodes/*.json        refresh_interval: 5m- job_name: docker  file_sd_configs:    - files:      - targets/docker/*.yml        refresh_interval: 5m
新建nodes与docker目录，并添加对应的配置文件
$ cd /etc/prometheus$ mkdir -p targets/&#123;nodes,docker&#125;$ cat &gt; targets/nodes/nodes.json &lt;&lt;EOF[&#123;  &quot;targets&quot;: [    &#x27;192.168.0.3:9100&#x27;,     &#x27;192.168.0.4:9100&#x27;,     &#x27;192.168.0.5:9100&#x27;  ]&#125;]EOF$ cat &gt; targets/docker/daemons.yml &lt;&lt;EOF- targets:  - &#x27;192.168.0.3:8080&#x27;  - &#x27;192.168.0.4:8080&#x27;  - &#x27;192.168.0.5:8080&#x27;  labels:    datacenter: NJEOF
基于API的服务发现当前API的服务发现支持以下平台：

AWS EC2
Azure
Consul
Google Compute Cloud
Kubernetes

基于DNS的服务发现- job_name: webapp  dns_sd_configs:    - names: [&#x27;_prometheus._tcp.example.com&#x27;]
Prometheus在查询目标时，通过DNS服务器查找example.com域，然后在该域下搜索名为_prometheus._tcp.example.com的SRV记录，返回该条目中的服务记录。还可以使用DNS服务发现来查询单个A或AAAA记录。
- job_name: webapp  dns_sd_configs:    - names: [&#x27;example.com&#x27;]      type: A      port: 9100
返回example.com域根目录下的所有A记录。
六、Alert Manager报警管理监控是为了了解系统的状态，以便于及时发现问题。当指标出现异常时，我们应该第一时间知道，便及时处理，但是我们又无法实时关注在每个监控指标，这时我们就需要告警机制。没有告警机制的监控，像是摆在家里的花瓶，它很美但用处不大。一个好的警报应该是，在正确的时间、发给正确的人、恰当的量、包含准确（不多也不少）的信息。

适当数量的警报
设置正确的警报优先级
警报应包括适当的上下文

Alertmanager如何工作Prometheus服务器向Alertmanager发送警报，当然Alertmanager也可以接收其他工具的警报。Alertmanager对警报进行去重、分组，然后路由到不同的接收器：email，sms等。

在Prometheus服务上编写警报规则
当指标达到阈值时，会生成警报推送到Alertmanager。
一个或多个Prometheus服务器可以将警报定向到单个Alertmanager下
Alertmanager处理警报，并根据其标签进行路由。

Alertmanager安装、配置、运行
Alertmanager安装

$ wget https://github.com/prometheus/alertmanager/releases/download/v0.20.0-rc.0/alertmanager-0.20.0-rc.0.linux-amd64.tar.gz$ tar -xzf alertmanager-0.20.0-rc.0.linux-amd64.tar.gz$ sudo cp alertmanager-0.20.0-rc.0.linux-amd64/alertmanager /usr/local/bin/$ sudo cp alertmanager-0.20.0-rc.0.linux-amd64/amtool /usr/local/bin/

配置Alertmanagerhttps://prometheus.io/docs/alerting/configuration/

$ sudo mkdir -p /etc/alertmanager$ sudo cat &gt; /etc/alertmanager/alertmanager.yml &lt;&lt;EOFglobal:  smtp_smarthost: &#x27;localhost:25&#x27;  smtp_from: &#x27;alertmanager@example.com&#x27;  smtp_require_tls: falsetemplates:- &#x27;/etc/alertmanager/template/*.tmpl&#x27;route:  receiver: emailreceivers:- name: &#x27;email&#x27;  email_configs:  - to: &#x27;alerts@example.com&#x27;EOF
global：全局设置，为其他模块的默认值template：保存警报模板route：警报根据规则进行匹配，采取相应的操作receivers：接收器列表，每个接收器有唯一的名字及配置。email_configs来指定电子邮件选项，webhook_configs可以扩展Alertmanager的接收器。3. 运行Alertmanager
$ alertmanager --config.file alertmanager.yml
通过浏览器访问alertmanager，http://localhost:9093
Prometheus配置Alertmanager
在prometheus.yml配置中设置alerting模块。

alerting:  alertmanagers:  - static_configs:    -targets:      - alertmanager:9093

监控AlertmanagerAlertmanager服务暴露了自身的相关指标，创建一个Prometheus Job就可以监控Alertmanager

- job_name: &#x27;alertmanager&#x27;  static_configs:    - targets: [&#x27;localhost:9093&#x27;]

添加警报规则与记录规则一样，警报规则在Prometheus服务器配置中加载的规则文件内也使用Yaml语句定义。在prometheus.yml配置文件中的rule_files块中添加文件rules&#x2F;node_alerts.yml在rules目录下创建文件node_alerts.yml来保存节点报警规则。

$ cat &gt; rules/node_alerts.yml &lt;&lt;EOFgroups:- name: node_alerts  rules:  - alert: HighNodeCPU    expr: instance:node_cpu:avg_rate5m &gt; 80    for: 60m    labels:      severity: warning    annotations:      summary: High Node CPU for 1 hour      console: You might want to check the Node Dashboard at http://grafana.example.com/dashboard/db/node-dashboardEOF
alert：规则名expr：触发规则for：控制在触发警报之前测试表达式必须为true的时长labels与annotations：装饰警报
警报有三种状态：Inactive：警报未激活Pending：警报已满足测试表达式条件，但仍在等待for子句中指定的持续时长Firing：警报（如果没有设置for，则一旦触发条件，立刻Firing）
Pending、Firing状态下的警报可以在Prometheus的指标中查看到ALERTS。新的警报与模板示例
groups:- name: node_alerts  rules:  - alert: DiskWillFillIn4Hours    expr: predict_linear(node_fileystem_size_bytes&#123;mountpoing=&quot;/&quot;&#125;[1h], 4*3600) &lt; 0    for: 5m    labels:      severity: critical    annotations:      summary: Disk on &#123;&#123; $labels.instance &#125;&#125; will fill in approximately 4 hours.  - alert: InstanceDown    expr: up&#123;job=&quot;node&quot;&#125; == 0    for: 10m    labels:      severity: critical    annotations:      summary: Host &#123;&#123; $labels.instance &#125;&#125; of &#123;&#123; $labels.job &#125;&#125; is down!  - alert: NodeServiceDown    expr: node_systemd_unit_state&#123;state=&quot;active&quot;&#125; != 1    for: 60s    labels:      severity: critical    annotations:      summary: Service &#123;&#123;$labels.name&#125;&#125; on &#123;&#123; $labels.instance &#125;&#125; is no longer active!      description: Service Down  - alert: InstanceGone    expr: absent(up&#123;job=&quot;node&quot;&#125;)    for: 10s    labels:      severity: critical    annotations:      summary: Host &#123;&#123; $labels.instance &#125;&#125; is no logger reporting!      description: &#x27;OMG Where are my instances&#x27;- name: prometheus_alerts  rules:  - alert: PrometheusConfigReloadFailed    expr: prometheus_config_last_reload_successful == 0    for: 10m    labels:      severity: warning    annotations:      description: Reloading Prometheus configuration has failed on &#123;&#123; $lables.instance &#125;&#125;.  - alert: PrometheusNotConnectedToAlertmanagers    expr: prometheus_notifications_alertmanagers_discovered &lt; 1    for: 10m    labels:      severity: warning    annotations:      description: Prometheus &#123;&#123; $labels.instance &#125;&#125; is not connected to any Alertmanagers.
路由Alertmanager的配置文件alertmanager.yml中添加一些路由配置。
route:  group_by: [&#x27;instance&#x27;]  group_wait: 30s  group_interval: 5m  repeat_interval: 3h  receiver: email  routes:  - match:      severity: critical    receiver: pager  - match_re:      severity: ^(warning|critical)$    receiver: support_teamreceivers:- name: &#x27;email&#x27;  email_configs:  - to: &#x27;alert@example.com&#x27;    send_resolved: true- name: &#x27;support_team&#x27;  email_configs:  - to: &#x27;support@example.com&#x27;- name: &#x27;pager&#x27;  email_configs:  - to: &#x27;alert-pager@example.com&#x27;  pagerduty_configs:  - service_key: TEAFDSFEWS
group_by：对Alertmanager警报指定分组方式，如按照instance来分组group_wait：如果进行分组，Alertmanager会等待group_wait指定的时间，以便在触发报警前查看是否收到该组中的其他报警group_interval：如果发出警报后，Alertmanager收到该分组的下一次评估的新警报后，会等待group_interval时间后再发送新警报，以免警报泛滥repeat_interval：适用于单个警报，等待重新发送相同警报的时间段receiver：默认接收器send_resolved: 恢复后发送通知routes：路由规则。如果需要routers还可以分支。如：
routes:- match:    severity: critical  receiver: pager  routes:  - match:      service: application1    receiver: support_team
通知模板模板目录：&#x2F;etc&#x2F;alertmanager&#x2F;templates
$ cat &gt; /etc/alertmanager/templates/slack.tmpl &lt;&lt;EOF&#123;&#123; define &quot;annotation.summary.text&quot; &#125;&#125;&#123;&#123; .CommonAnnotations.summary&#125;&#125;&#123;&#123;end&#125;&#125;EOF
对应的slack_configs receiver配置
  slack_configs:  - api_url: https://hooks.slack.com/services/ABC123/fsdaf、EXAMPLE  channel: #monitoring  text: &#x27;&#123;&#123;template &quot;annotation.summary.text&quot; .&#125;&#125;
使用模板通知来填充text字段，使用上下文通知。
silence和维护报警静默设置。当服务进行维护时，不需要发出告警，使用silence进行控制。两种方法进行设置：

通过Alertmanger Web控制台
通过amtool命令工具


使用amtool添加silence。默认为1h过期时间，可以指定–expires和–expire-on参数指定更长的时间与窗口

$ amtool --alertmanager.url=http://localhost:9093 silence add alertname=InstancesGone service=application1

使用query命令查询silence列表

$ amtool  --alertmanager.url=http://localhost:9093 silence query

指定silence过期

$ amtool  --alertmanager.url=http://localhost:9093 silence expire $SILENCE_ID

使用正则创建silence

$ amtool  --alertmanager.url=http://localhost:9093 silence add --comment &quot;App1 maintenance&quot; alertname=~&#x27;Instance.*&#x27; service=application1
七、Prometheus高可用性Prometheus通过运行两个配置相同的Prometheus服务器，并且它们同时处于活动状态来实现容错。该配置生成的重复警报交由上游Alertmanager使用其分组功能进行处理。所以一个推荐的做法是关注Alertmanager的高可用，而不是Prometheus服务。通过创建一个Alertmanager集群来实现高可用，所有Prometheus服务器将告警发送到Alertmamanager集群，而Alertmanager负责去除重复数据。
设置Alertmanager集群Alertmanager包含由HashiCorp Memberlist库提供的集群功能。

在多台主机上安装Alertmanager
启动Alertmanager，传入参数–cluster.listen-address第一台主机启动Alertmanager命令如下：

$ alertmanager --config.file alertmanager.yml --cluster.listen-address 192.168.0.3:8001
剩下的主机启动Alertmanager命令如下：
$ alertmanager --config.file alertmanager.yml --cluster.listen-address 192.168.0.4:8001 --cluster.peer 192.168.0.3:8001

在Alertmanager的控制台状态页面&#x2F;status上查看集群状态
为Prometheus配置Alertmanager集群

alerting:  alertmanagers:    - static_configs:      - targets:        - 192.168.0.3:9093        - 192.168.0.4:9093        - 192.168.0.5:9093
三个Alertmanager服务都会收到告警信息，保证告警可达。
可扩展性Prometheus federation API抓取每个工作节点的聚合指标一个很好的例子是：基于区域的主节点和工作节点，基于区域的主节点视为全局的工作节点，然后向全局的主节点进行报告。因为是金字塔级结构，主节点可能会有延时，所以尽量通过工作节点向Alertmanager发送告警。水平分片通常是最后的选择。每个目标都有数万个指标或者大量时间序列。
Prometheus work0配置
global:  external_labels:    worker: 0rule_files:  - &quot;rules/node_rules.yml&quot;scrape_configs:  - job_name: &#x27;node&#x27;    file_sd_configs:    - files:      - targets/nodes/*.json        refresh_interval: 5m    relabel_configs:    - source_labels: [__address__]      modulus: 3      target_label: __tmp_hash      action: hashmod    - source_labels: [__tmp_hash]      regex: ^0$      action: keep
该过程是使用hashmod模块对__address__标签的值对3取Mod，如果余数为0，则保留。同样的方法配置好work1与work2节点。
Prometheus 主节点配置
scrap_configs:- job_name: &#x27;node_workers&#x27;  file_sd_configs:    - files:      - &#x27;targets/workers.json&#x27;      refresh_interval: 5m  honor_labels: true  metrics_path: /federate  params:    &#x27;match[]&#x27;:      - &#x27;&#123;__name__=&quot;^instance:.*&quot;&#125;&#x27;
其中targets&#x2F;workers.json内容为
[&#123;  &quot;targets&quot;: [    &quot;worker0:9090&quot;,    &quot;worker1:9090&quot;,    &quot;worker2:9090&quot;,  ]&#125;]
Prometheus支持使用&#x2F;federate API根据指定的匹配参数来查询服务器指标。
八、日志监控使用mtail作为日志处理工具，它是Google开发的，非常轻巧。它专门用于从应用程序日志中提取要导出到时间序列数据库中的指标。

安装mtail

$ wget https://github.com/google/mtail/releases/download/v3.0.0-rc33/mtail_v3.0.0-rc33_linux_amd64 -O mtail$ chmod 0755 mtail$ sudo cp mtail /usr/local/bin/

使用mtailmtail通过命令进行配置，指定日志文件列表，以及运行的程序目录。每个mtail程序都以.mtail为后缀名。

$ sudo mkdir /etc/mtail$ cat &gt; /etc/mtail/line_count.mtail &lt;&lt;EOFcounter line_count/$/ &#123;  line_count++&#125;EOF
程序定义了一个名为line_count的计数器（计数器 counter，测量型 gauge）。这些计数与测量通过mtail导出到定义的任何目的地。3. 运行mtail
$ sudo mtail --progs /etc/mtail --logs &#x27;/var/log/*.log&#x27; 
–progs：指定mtail程序所在目录–logs：指定日志文件执行后，mtail将在3903端口上启动Web服务（使用–address和–port参数设置IP与端口）。http://localhost:3903/metrics路径可以被Prometheus获取相关监控数据。
处理Web服务器访问日志# LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%&#123;Referer|i\&quot; \&quot;%&#125;User-agent|i\&quot;counter apache_http_requests_total by request_method, http_version, request_statuscounter apache_http_bytes_total by request_method, http_version, request_status/^/ +/(?P&lt;hostname&gt;[0-9A-Za=z\.-]+) / + # %h/(?P&lt;remote_logname&gt;[0-9A-Za=z\.-]+) / + # %l/(?P&lt;remote_username&gt;[0-9A-Za=z\.-]+) / + # %u/(?P&lt;timestamp&gt;\[\d&#123;2&#125;\/\w&#123;3&#125;\/\d&#123;4&#125;:\d&#123;2&#125;:\d&#123;2&#125;:\d&#123;2&#125; (\+|-)\d&#123;4&#125;\]) / + # %u/&quot;(?P&lt;request_method&gt;[A-Z]+) (?P&lt;URI&gt;\S+) (?P&lt;http_version&gt;HTTP\/[0-9\.]+)&quot; / + # \&quot;%r\&quot;/(?P&lt;request_status&gt;\d&#123;3&#125;) / + # %&gt;s/(?P&lt;response_size&gt;\d+) / + # %b/&quot;(?P&lt;referer&gt;\S+)&quot; / + # \&quot;%&#123;Referer&#125;i\&quot;/&quot;(?P&lt;user_agent&gt;[[:print:]]+)&quot;/ + # \&quot;%&#123;User-agent&#125;i\&quot;/$/ &#123;  apache_http_requests_total[$request_method][$http_version][$request_status]++  apache_http_bytes_total[$request_method][$http_version][$request_status] += $response_size&#125;
by：指定要添加到指标的其他维度，它们将会添加到指标的标签中。每个维度包含在[ ]中。运行mtail程序
$ sudo mtail --progs /etc/mtail --logs &#x27;/var/log/apache/*.access&#x27;
解析Rail日志到直方图counter rails_requests_started_totalcounter rails_requests_started by verbcounter rails_requests_completed_totalcounter rails_requests_completed by statuscounter rails_requests_completed_milliseconds_sum by statuscounter rails_requests_completed_milliseconds_count by statuscounter rails_requests_completed_milliseconds_bucket by le,status/^Started (?P&lt;verb&gt;[A-Z]+) ./*/ &#123;  rails_requests_started_total++  rails_requests_started[$verb]++&#125;/^Completed (?P&lt;status&gt;\d&#123;3&#125;) .+ in (?P&lt;request_milliseconds&gt;\d+)ms .*$/ &#123;rails_requests_completed_total++rails_requests_completed[$status]++rails_requests_completed_milliseconds_sum[$status] += $request_millisecondsrails_requests_completed_milliseconds_count[$status] ++$request_milliseconds &lt;= 10&#123;    rails_requests_completed_milliseconds_bucket[&quot;10&quot;][$status] ++  &#125;$request_milliseconds &lt;= 50&#123;  rails_requests_completed_milliseconds_bucket[&quot;50&quot;][$status] ++&#125;&#125;
尽量为每个应用单独部署日志监控，对于K8S上的应用，可以使用sidecar的方式运行mtail来实现日志监控。例子代码：https://github.com/google/mtail/tree/master/examples
九、探针监控使用Blackbox exporter来对外部服务探测监控。

安装Blackbox exporter

$ wget https://github.com/prometheus/blackbox_exporter/releases/download/v0.16.0/blackbox_exporter-0.16.0.linux-amd64.tar.gz$ tar -xzf blackbox_exporter-0.16.0.linux-amd64.tar.gz$ sudo cp blackbox_exporter-0.16.0.linux-amd64/blackbox_exporter /usr/local/bin/

配置Blackbox exporter使用&#x2F;etc&#x2F;prober&#x2F;prober.yml文件配置exporter

$ sudo mkdir -p /etc/prober$ sudo touch /etc/prober/prober.yml$ cat &gt; /etc/prober/prober.yml &lt;&lt;EOFmodules:  http_2xx_check:    prober: http    timeout: 5s    http:      valid_status_codes: []      method: GET  icmp_check:    prober: icmp    timeout: 5s    icmp:      preferred_ip_protocol: &quot;ip4&quot;  dns_examplecom_check:    prober: dns    dns:      preferred_ip_protocol: &quot;ip4&quot;      query_name: &quot;www.example.com&quot;EOF

启动exporter

$ sudo blackbox_exporter --config.file=&quot;/etc/prober/prober.yml&quot;
默认在端口9115下运行服务：http://localhost:9115/metrics4. Prometheus创建作业，调用Blackbox Exporter服务
scrape_configs:  - job_name: &#x27;blackbox&#x27;    metrics_path: /probe    params:      module: [http_2xx_check]  # Look for a HTTP 200 response.    static_configs:      - targets:        - http://prometheus.io    # Target to probe with http.        - https://prometheus.io   # Target to probe with https.        - http://example.com:8080 # Target to probe with http on port 8080.    relabel_configs:      - source_labels: [__address__]        target_label: __param_target      - source_labels: [__param_target]        target_label: instance      - target_label: __address__        replacement: 127.0.0.1:9115 
Blackbox Exporter的配置Job方式与其他作业不同，这的targets是作为Blackbox Exporter检测的目标地址。而Blackbox Exporter的服务地址，通过relabel_configs替换掉__address__标签的方式设置。
十、Pushgateway方式推送监控数据Prometheus主要是基于拉取的架构来运行作业，同时也提供了Pushgateway服务来支持Push方式。Pushgateway位于发送指标的应用程序与Prometheus服务器之间，接收指标，同时作为目标被抓取。Pushgateway只能用作有限的解决方案使用，特别是监控其他无法访问的资源。

安装Pushgateway

$ wget https://github.com/prometheus/pushgateway/releases/download/v1.0.0/pushgateway-1.0.0.linux-amd64.tar.gz$ tar -xzf pushgateway-1.0.0.linux-amd64.tar.gz$ sudo cp pushgateway-1.0.0.linux-amd64/pushgateway /usr/local/bin

配置和运行Pushgateway

$ pushgateway --web.listen-address=&quot;0.0.0.0:9091&quot; --persistence.file=&quot;/tmp/pushgateway_persist&quot; --persistence.interval=5m
–web.listen-address：指定服务端口。pushgateway默认端口是9091. –persistence.file：指标持久化到路径。默认情况下，pushgateway所有指标存储在内存中，如果pushgateway停止服务或者重新启动，所有数据将会丢失。–persistence.interval：指标持久化写入周期。默认5m3. 向pushgateway服务发送指标
$ echo &quot;batchjob1_user_counter 2&quot; | curl --data-binary @- http://localhost:9091/metrics/job/batchjob1/instance/sidekiq_server
将为作业batchjob1添加一个新的指标batchjob1_user_counter{instance=&quot;sidekiq_server&quot;} 2
$ cat &lt;&lt;EOF | curl --data-binary @- http://localhost:9091/metrics/job/batchjob1/instance/sidekiq_server# TYPE batchjob1_user_counter counter# HELP batchjob1_user_counter A metric from BatchJob1.batchjob1_user_counter&#123;job_id=&quot;123ABC&quot;&#125; 2
也可以同时发送多个指标。4. 在pushgateway上查看指标
$ curl http://localhost:9091/metrics

删除pushgateway上的指标

$ curl -x DELETE localhost:9091/metrics/job/batchjob1 #删除job batchjob1下的所有指标$ curl -x DELETE localhost:9091/metrics/job/batchjob1/instance/sidekiq_server # 删除job batchjob1下标签满足instance=sidekiq_server的指标

Promethes上添加Pushgateway Job

- job_name: &#x27;pushgateway&#x27;    honor_labels: true    static_configs:      - targets: [&#x27;localhost:9091&#x27;]
honor_labels设置为true，Prometheus使用Pushgateway上的job和instance标签，否则会在前面加上exported_前缀。
十一、监控OpenShiftOpenShift平台的监控方案默认为Prometheus开源监控方案，它不仅带有一整套完成的监控，而且还预配置了一组告警，以及一组丰富的Grafana仪表盘。
  
可以通过Prometheus Operator方便地创建、配置和管理Prometheus及Alertmanager。从上图可以看到，除了Prometheus与Alertmanager服务，OpenShift的监控方案中还装了node exporter与kube-state-metrics获取集群的状态指标。OpenShift使用ansible安装时，默认会安装的cluster monitoring operator。除非在inventory中指定openshift_cluster_monitoring_operator_install为false。
  
  
扩展资料Awesome Prometheus alertsRich ExporterExporter Operator
]]></content>
      <categories>
        <category>读书笔记</category>
        <category>openshift</category>
      </categories>
      <tags>
        <tag>openshift</tag>
        <tag>读书笔记</tag>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title>一条命令解决Kubernetes更改默认的namespace</title>
    <url>/posts/3c19c6380d6c.html</url>
    <content><![CDATA[
K8S默认是在default的namespace下，但是很多时候我们是在一个新的namespace下部署应用，每个操作都需要指定namespace，非常不方便。那么有没有办法切换namespce呢？如果可以切换的话，那有没有更简单的办法。
最简单的方式切换namespace，赠送一个方便的脚本可以将它alias到一个新的命令”kubectl ns”
$  alias kubectl=&#x27;_kubectl_custom()&#123; if [[ &quot;$1&quot; == &quot;project&quot; &amp;&amp; &quot;$2&quot; != &quot;&quot; ]]; then ccontext=`kubectl config current-context`; kubectl config set-context $ccontext --namespace=$2; elif [[ &quot;$1&quot; == &quot;projects&quot; &amp;&amp; &quot;$2&quot; == &quot;&quot; ]]; then kubectl get ns; elif [[ &quot;$1&quot; == &quot;project&quot; &amp;&amp; &quot;$2&quot; == &quot;&quot; ]]; then project=$(kubectl config get-contexts | sed -n &quot;2p&quot; | awk &quot;&#123;print \$5&#125;&quot;); if [[ &quot;$project&quot; == &quot;&quot; ]]; then echo default; else echo $project; fi; else kubectl $*; fi;&#125;; _kubectl_custom&#x27;$ alias oc=&#x27;kubectl&#x27;
验证
[root@localhost k8s]# kubectl project testContext &quot;kubernetes-admin@kubernetes&quot; modified.[root@localhost k8s]# kubectl get podNAME                                 READY   STATUS    RESTARTS   AGEtest-46mfd                           1/1     Running   0          16h
成功切换了namespace，同时对之前的命令没有任何影响。
相关配置具体操作kubernetes的运行时是通过~&#x2F;.kube&#x2F;config文件的配置来设置的

查看当前cluster config配置

$ kubectl config view

如果没有的话，可以添加cluster配置

$ kubectl config set-cluster kubernetes-cluster --server=https://192.168.1.128:6443

新建context运行时

$ kubectl config set-context &#123;&#123; namespace &#125;&#125; --namespace=&#123;&#123; namespace &#125;&#125; --cluster=kubernetes-cluster --userkubernetes-admin

设置当前运行环境

$ kubectl config use-context &#123;&#123; namespace &#125;&#125;
这时所有操作默认都会在下操作。
直接更改当前context的namespace$ kubectl config set-context --current --namespace=&#123;&#123; namespace &#125;&#125;

补充Openshiftopenshift就非常方便了，把这些操作都进行了封装，在创建project时也会自动把context加入到kubeconfig的配置中，如果需要切换运行环境，只要执行oc project &#123;&#123; projectname &#125;&#125;就好了。
kubernetes也可以使用Ansible进行安装生产集群： kubespray 
Kubernetes添加节点前Token忘了怎么办？
[root@iZ2ze436suxv73x9jtiy2vZ helm-demo-master]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;cda2299d203e26b6499e8283937ffbb6489421ff761569fdf8172d03d9a889d6[root@iZ2ze436suxv73x9jtiy2vZ helm-demo-master]# kubeadm token listTOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPSqyg3si.njhyhixcqd18by2g   23h       2019-07-27T15:13:15+08:00   authentication,signing   The default bootstrap token generated by &#x27;kubeadm init&#x27;.   system:bootstrappers:kubeadm:default-node-token[root@iZ2ze436suxv73x9jtiy2vZ helm-demo-master]# kubeadm join 172.17.3.226:6443 --token qyg3si.njhyhixcqd18by2g --discovery-token-ca-cert-hash sha256:cda2299d203e26b6499e8283937ffbb6489421ff761569fdf8172d03d9a889d6
或者创建新的token
[root@iZ2ze436suxv73x9jtiy2vZ helm-demo-master]# kubeadm token create --print-join-command
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>下载国外资源加速器</title>
    <url>/posts/0e6ee5d84317.html</url>
    <content><![CDATA[axel
https://github.com/axel-download-accelerator/axel
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>云原生简介</title>
    <url>/posts/e57e3c101040.html</url>
    <content><![CDATA[什么是云原生云原生（Cloud Native）的概念由Pivotal的Matt Stine于2013年首次提出。它是Matt Stine根据多年的架构和咨询经验总结出来的一个思想集合，并得到社区的不断完善和补充，包括DevOps、持续交付、微服务、敏捷基础设施和12要素等几大主题。包括根据业务能力对公司进行文化、组织架构的重组与建设、方法论与原则、还有具体的操作工具。将云计算从传统的以资源为中心，改变为以应用为中心，聚焦于业务，从而满足应用快速迭代的要求。
CNCF（Cloud Native Computing Foundation）云原生计算基金会2015年把云原生定义为三个方面

应用容器化
面向微服务架构
应用支持容器的编排调度

随着CNCF中的项目越来越多，原先的定义已经限制了云原生生态的发展，于是更新了定义

云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。
这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。
云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。

#云原生的起因
云原生的意义让产品能够支持快速验证业务模式简化复杂的开发流程、提升研发效率保障产品的高可用性实现大规模弹性伸缩，轻松应对业务爆发
架构技术的升级

可用性
可扩展性，弹性
可管理性开发模式的升级
DevOps模式架构设计的升级
更快的迭代速度
持续可用的服务
弹性伸缩
非功能要求

如何实施云原生
基础设施建设（云容器平台）
约束技术栈，微服务化建设
标准化技术栈构建及构建流程标准化
应用架构必须遵循规范要求

云原生的未来]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>从0开始构建CI-CD持续集成流水线[Openshift]</title>
    <url>/posts/fe3ee29ea046.html</url>
    <content><![CDATA[开始前的话
完成一只大象装进冰箱需要几步？三步： 1. 有一个冰箱；2. 有一只大象；3. 把大象装进冰箱
构建CI&#x2F;CD流水线[Openshift]需要几步？三步： 1. 有Openshift平台；2. 部署CI&#x2F;CD流水线应用；3. 启动流水线

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>企业生产系统迁移至容器云平台的众多难题</title>
    <url>/posts/2970a12120e5.html</url>
    <content><![CDATA[
该篇是twt论坛交流活动《企业生产系统迁移至容器云平台的网络及安全方案探讨—暨红帽容器化混合云解决方案在线发布活动》的总结。原文：http://www.talkwithtrend.com/Article/245665活动地址：http://www.talkwithtrend.com/activity/?id=1459
自从2013年Docker应用开源以来，容器技术飞速发展，它彻底改变了PaaS原先的发展轨迹。2014年Google发布的容器编排应用Kubernetes，经过5年的发展，更是已经成为了现代容器平台的事实标准，容器平台应用于生产环境也已经现实可落地。
与之前的虚拟技术相比，容器技术具有更快、更轻、更强的优势。
更快，体现在容器可以实现秒级启动。
更轻，体现在容器更加轻量，往往一个容器镜像只有几十兆，可以快速在主机之间迁移，而虚拟机镜像往往是几个G。
更强，体现在容器的性能更好。容器是直接调用硬件资源，不存在虚拟化层，所以它的性能接近于原生应用。
容器以应用为中心，能够快速地在各个平台下部署运行，可以很好地满足应用的快速部署。
但是容器运行时使用的是宿主机的内核，在同一台宿主机上的容器也会共享系统的资源，如果没有做好资源的规划及权限的控制，不仅容器之间会互相影响，甚至会让整个系统的受到威胁。那么怎样能够避免这些问题呢？这是在容器平台上生产前必须考虑的问题。
另外，容器的网络也是在容器平台规划时需要考虑的一个重点，我们很多网络安全策略都是基于IP做的规则，但是一般而言同一个镜像运行的不同容器的IP是不一样的，一旦容器被重新调度，之前的安全策略便失效了，而新的容器并没有受到安全策略管理。另外生产中的主机一般会配置多个网络，有业务网，管理网等，那么容器化后是否有办法支持多网络平面呢？
当然容器平台上生产需要考虑的问题不止这一些，比如说容器的调度，存储的规划等等。
但随着容器技术的不断演进，以及企业应用的不断加深，容器平台上生产中遇到的问题也得到了很大程度的突破。此外，容器生态圈中的重要力量——容器平台厂家为更能满足企业客户生产的要求，实现容器平台上生产，也在不断优化各自的产品，推出新的解决方案。
其中，2019年5月红帽推出了RHEL8和第四代OpenShift容器平台，该版本融合了OpenShift与CoreOS两家容器平台大玩家的技术。相对于第三代OpenShift，新一代OpenShift拥有很多新的特性，比如说Operator管理组件的生命周期，添加了多集群管理等。同时网络及安全方面也是OpenShift4.1版本升级的一个重要部分。
为解决容器平台上生产中的诸多问题，本次活动不仅邀请到招银云创专家和某股份制银行专家一起，基于企业生产系统迁移至容器云平台的网络及安全方案进行探讨；并同时邀请红帽专家，围绕OpenShift4的特性，如何帮助企业基于OpenShift实现企业容器化混合云的价值进行答疑。
答疑结束后，对活动中提出的问题进行了分类总结，供大家参考。
容器平台价值容器云和传统的虚拟机比有什么优势呢？
解答1：
优势：
a. 更快，可以实现秒级启动。
b. 更轻，更加轻量，往往一个容器镜像只有几十兆，可以快速在主机之间迁移，而虚拟机镜像往往是几个G。
c. 更强，性能更好。
并且有了Kubernetes容器编排引擎后，应用的水平扩展、服务发现、滚动升级、应用高可用等等特性变得非常容易实现。
基于此容器可以大大降低于低企业成本，资源成本及运维管理成本。当然容器也有较虚拟机不足的地方。
劣势：
a. 容器共用内核，隔离性低于虚拟机，也不够虚拟机安全。
b. 跨平台性低于虚拟机，容器只能运行在linux系统，而虚机可运行在windows&#x2F;Linux&#x2F;Unix系统。
解答2：
拿容器和虚拟机比较更准确，不是容器云
容器优势明显，缺点也很明显
容器轻量，可快速弹性伸缩，适合部署轻量的分布式应用或服务，但带来管理的复杂性，需要借助容器管理调度工具或者实现的容器云平台
标准化，容器引擎使基础设施标准化，容器镜像使应用交付标准化，容器使运维调度管理标准化，容器镜像仓库使分发部署标准化
一致性，容器的标准化使开发、测试、生产环境具备一致性，可以快速构建一致性的环境
维护简单，一个容器通常部署一个服务或实例，而虚拟机通常很多服务和应用，所以其准备、启动、维护都相对简单很多
另外需要强调一点的是，容器并不节省资源，在容器规模达到一定程度，能实现资源分时共享时才能节省资源
解答3：
轻量级，资源利用率高扩展容易，最重要的是和微服务结合的好，当然用了PAAS以后你的蓝绿发布，应用构建，灰度发布都更简单
传统应用是否应该迁移到容器环境，有没有意义，有什么值得参考的迁移方法论？
解答1：
不同应用有不同的要求，比如稳定性、可靠性、可扩展性等，在迁移之前需要明确容器是否满足应用的要求。应用迁云或迁容器云需要遵循相应的原则和方法，难以一概论之，具体的应用需要根据实际确定具体的方案
解答2：
如果是传统应用做容器话的迁移，那没啥方法轮，如果是为服务改造有很多方法论，比如 绞杀模式 （Strangler） 以及 修缮者模式
容器平台架构方案传统架构模式和云与docker模式结合后如何解决网络和SDN的问题？
解答1：
我们采用的方案是，容器建设在虚拟机或者物理机上，集群中的容器使用SDN网络方案，互相间通讯，同时也可以与宿主机同一网络的主机通信。但是集群外部机器要访问集群内部的服务，只能通过route&#x2F;ingress&#x2F;NodePort。
pod与pod之间网络通过networkpolicy做网络隔离，主机与主机之间的使用传统的防火墙来做隔离。
另外macvlan网络方案，可以让pod与宿主机在同一个网络，这种方式下容器间的网络隔离与传统方式一样，每个容器可以当作一个单独的主机。但这种方案生产应用并不常见。
解答2：
目前来看没有什么问题啊，传统网络和SDN的问题本身就可以通过网络厂家的设备来解决，如果你说的是PAAS自身的SDN如何同集群外的服务互联互通，那就是通过router来实现（或者是ingress和egress）。安全域的问题更多的是通过多集群和设计的问题来规避。
在私有云，在容器环境下安全区域怎么划分，是否设置DMZ区域？
解答1：
可以考虑部署多个容器集群来设置不同区域。测试环境、生产环境完全隔离的情况下，可考虑建设DMZ区以实现可能需要的镜像流转等场景。
解答2：
安全组网在容器环境下并不需要改变，按照安全规范实施即可，确保安全优先
解答3：
为了安全，可以部署多个容器集群来设置不同区域。一个集群作为DMZ区，另一个集群作为核心区，还可以有一个互联区的集群。
解答4：
在多个网络区域布署不同的容器集群，DMZ，业务区可以分别多套集群，做为多可用区，提高可用性
容器云是否有和SDN集成的方案？
解答1：
有很多，但是界面或者说管理能力展示的集成需要自己做，也就是openshift 并不会有第三方SDN的管理功能，通常来说只要这个SDN厂家有CNI的呃插件，那么OCP4 就可以和他集成，当然如果需要提供售后支持的话，还需要该SDN厂家和红帽做认证。
解答2：
有啊 比如和neutron集成 你可以根据需要自己开发cni的插件，开源实现里面也有kury（python实现）
虚拟机与容器并行承载同一套应用系统是否可行？
解答1：
负载均衡实现虚拟机应用与容器应用共同支撑业务系统, 这个业务场景没有先例
既然上容器就需要服务治理等一系列微服务框架运维,
vm技术和容器技术对应用来说如果不进行微服务化改造, 是没有任何区分的,
所以你这个场景 是可以实现但 vm 会比 容器快很多, 容器需要多层网络转换
解答2：
我们公司的业务生产环境便是虚拟机与容器并行，容器中运行的是无状态的app及前端静态界面，而中间件及数据库等仍然运行在虚拟机中。
解答3：
完全没有问题，简单的你可以看做两个虚拟数据中心，分别部署一套应用系统，前端负载均衡器实现负载分发。所以部署在容器或者虚拟机对前端是不可见的，只要可访问就行
如何按照网络隔离区域对容器集群规划？
解答：
这个主要看公司对网络安全的要求和成本的平衡。
a 方案每个区都部署一套集群的话，安全性高，但成本高，包括资源成本及维护成本等。
b 方案成本低，也能满足一定的隔离要求
开发测试环境一般更偏向于使用b方案，k8s&#x2F;openshift网络支持多租户，能实现软隔离，及通过给节点作标记Label，定向调度的方式就能够满足隔离的要求。
但是生产环境是需要单独构建集群与开发测试环境隔离的。
如何保障容器镜像的安全防护？
解答1：
我们使用的是集成在harbor中的clair容器镜像静态扫描工具。它会定期同步公共源镜像漏洞数据，并对harbor镜像仓库中的镜像做扫描。如果发现有问题会发出通知，且也支持webhook的方式对接公司统一的告警平台。
解答2：
红帽又一款产品叫QUAY可以做到你提到的所有功能，更详细的信息还请看红帽工程师的博客： http://www.10tiao.com/html/360/201806/2663487906/1.html
容器平台应用容器平台、微服务平台、devops平台在规划建设的时候如何能有序整合起来？
解答1：
容器平台是DevOps中的一部分， 不需要微服务平台，微服务部署于容器平台，在容器平台实现微服务管理和治理能力
解答2：

容器平台与服务治理在以下维度相同，需要从统一化进行管理：1、应用管理；2、服务管理；3、实例管理；4、聚合管理；采用统一权限中心管理，数据权限与操作权限灵活设置，以用户+权限定义产品形态和管理范畴。平台管理（重点在于资源管理），租户管理（重点在于应用管理和服务治理能力）。
在建设DevOps的过程中，需要结合企业自身开发运维的流程，与容器云与微服务治理平台的结合的过程中，更多的考虑整体DevOps过程中需要实现的场景，而不是仅仅考虑CICD。

解答3：
容器平台更多的是规范运维操作以及故障处理，微服务跟多的是做开发规范，DevOps工作的是工作流工具链的梳理，三个各有侧重点。
解答4：
从不同的角度看才有了容器平台、微服务平台、devops平台。它们之间并不是独立的平台。
首先，容器平台是基础设施，微服务平台和devops平台部署在上面。
其次，devops平台建设过程中，需要考虑微服务架构，采用不同的方案在构建devops流水线时也会有所不同。
最后，如果要做持续部署的话，需要考虑公司的安全规范与流程规范。
以下是我们的实践：

分别部署了开发测试容器集群与生产容器集群。
devops工具链与流水线构建只在开发测试环境上，构建与测试完成后，通过人工审核后才允许将镜像同步到生产集群，再单独走部署流水线。
统一微服务架构，及规范。我们互联网应用都采用spring cloud微服务架构，技术统一后devops流水线也方便统一及优化、维护。

容器平台上生产的可靠性及容器平台与devops能否对接？
解答1：
技术上是完全可行的，还需要考虑一些非技术的因素。
由于生产环境的特殊性，公司会有很多复杂的安全规范及流程规范，它们限制devops直接对接生产环境。这个其实与是否在容器平台无关。
安全规范、流程规范，很多公司是一条红线，特别是金融行业，因为多年来正是遵循它们才保证了业务的安全可靠，还有各种认证、审计的需要。
当然技术和规范也都是与时俱进，不断发展的。现在有些公司把流程规范开始融入到持续部署的流程中了，包括临时授权等，也都可以作为流水线中的一个环节。
解答2：
容器平台是devops的一部分，devops不是一个平台
需求提议、项目管理、开发测试、部署交付、运维运营、监控反馈、持续改进，容器平台的定位和价值在这些闭环过程中。
如何更好地实现现有系统的容器化迁移？
解答：
这个真没有，你不能指望平台解决应用的问题
部署流计算框架和机器学习平台在红帽容器平台上有什么要注意的？
解答：
主要是用到GPU的话，务必做好PAAS集群和GPU的配置，其他没有太多需要注意的，很多框架已经在k8s上部署运行了很长时间了。OCP没有额外的要求
Openshift4特性Openshift 4推荐的直接部署物理机还是虚拟机？为物理机部署提供了什么便利？
解答：
Openshift4 部署在物理机和虚拟机都行，但是红帽推荐部署在虚拟化平台上，因为OCP4以后增加了很多基础架构的管理功能，其中就包括针对openshift 节点的自动扩容和缩容。很有用
OpenShift 4对单集群多租户的支持怎么样？
解答：
OpenShift中不仅有用户,还有用户组。OpenShift的账号管理最简单是使用HtPasswd方式，同时OpenShift可以与多种系统账号对接，如OpenLdap，KeyStore，GitLab，GitHub，OpenID等。
OpenShift 4对Serverless的支持情况？
解答：
红帽ocp4 支持knative 啊，不过目前应该是beta版。具体请看 https://github.com/openshift-knative/docs/
正在生产运行的OpenShift3是否可以原地升级到OpenShift4？如果不行，该如何升级？
解答：
不行，未来从OCP3系列到OCP4系列都是迁移，当然红帽会有官方的迁移工具来帮助客户做这个事情，但是也需要准备停机窗口，并且两套集群一个是老得ocp3 集群一个是ocp4集群。
金融行业对安全非常重视，OpenShift4和RHEL8在安全方面改进的体现有哪些？
解答：
openshift一直关注安全 我们的10层安全体系从基础设施层到标准化基础中间件镜像都在层层为客户企业安全进行保障。ocp4的比以前多了coreos作为操作系统选项：CoreOS裁减了传统linux中无用的软件，减少了依赖冲突和attack surface，系统更小、更紧凑、更安全。另外 用来取代传统docker容器的另一种选择其中podman是最重要的组件之一。其主要优势是无需docker守护进程，以普通用户形式启动，是一种更安全的容器管理器。
OpenShift与其它容器平台比较Openshift与Rancher如何选择，是否有可比较性？
解答：
OpenShift有开源版本，是免费的。
它们俩各有各的特点。
OpenShift:

设计了ImageStream，BuildConfig与DeploymentConfig等资源对象，及s2i构建方法，方便了开发者实施Devops。
添加了一个内部镜像仓库。
使用Route资源，为应用提供了一个公共统一的访问入口。类似于Ingress，使用起来比Ingress方便。
提供了一个友好的可视化界面。
对容器有更多的安全策略，更安全
有更高的可靠性。 作为RedHat的企业级容器平台，红帽会对集群做详细的测试，修复bug。
一般版本会落后K8S一个大版本
一般为只管理单个OpenShift集群

Rancher:

具有良好的界面
方便管理多个K8S集群
对网络插件的选择会比OpenShift更加灵活
与K8S版本同步，及时拥有K8S最新的特性

个人认为，单集群管理使用OpenShift，更稳定，更简单，也更安全，而如果是要管理多集群，选择Rancher。不过OpenShift 4起红帽也支持多集群管理，但还不能私有化部署。
两种方案都有不少的企业客户选择，因为都是基于K8S， 功能上都差不多 。不管是构建DevOps流水线，还是生产部署原生应用上。
openshift 与其他容器云厂商产品的异同点有哪些？
解答：
上一个问题中介绍了Openshift与Rancher的差异，可以作为参考。
还有需要补充一点的是，红帽是继google和社区之后k8s最大的贡献者，不少K8S中的特性是来自于OpenShift，比如说RBAC。同时目前很热的operator是coreos的产品，而coreos现在也是红帽的了。
红帽容器化混合云跟其他友商的方案在稳定性、安全性上有哪些不同，是否有更好的表现？
解答：
容器就是linux，红帽的RHEL是最安全的，因此红帽的PAAS平台必然也是最安全的，当然红帽的研发策略一直是做减法，把最重要和最稳定的特性和代码放到自己的企业版，因此在你说的稳定性和安全性上，红帽一定是全球第一，但是特性上不一定，国内创业公司都在做加法，所以特性上都会比红帽PAAS多那么一点
RedhatRHEL8否对6上的命令还能继续兼容，比如服务命令，6迁移到8有什么注意点？
解答1：
rhel8.x 在上线前 一般会等到8.3 第三个维护版本进行上线系统更新
目前在8.0和8.1还有8.2不建议使用
而且从6版本直接升级到8版本,跨度过大业务系统是否支持,都要考虑
只是命令变化不会有太多技术问题,修改下运维脚本没问难度
解答2：
命令会有所改变，关键是8 比6 新太多了，迁移前务必做应用测试，然后再迁移
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>企业级容器云平台建设之功能汇总</title>
    <url>/posts/bf5cf0d71deb.html</url>
    <content><![CDATA[

多集群管理与运维
多区建设（DMZ&#x2F;业务区&#x2F;核心区）
应用商店（模板管理、Operator管理等）
集群监控、告警、日志管理
权限管理，多租户管理&#x2F;账号管理
管理员资源分配
资源列表及管理
应用管理，部署&#x2F;更新&#x2F;下线
应用多集群部署，灰度发布、蓝绿部署
应用网络方案&#x2F;存储&#x2F;运行权限&#x2F;南北流量方案选择
NodePort端口管理
网络隔离策略控制
集群边界路由管理
DNS域名申请管理
自动化运维，集群巡检，集群扩缩容，集群备份
审批流程管理
审计管理
DevOps流水线
容器安全与镜像安全
业务网与管理网，多网络平面
外部负载均衡管理器
镜像仓库，高可用，多区域镜像仓库同步
镜像仓库漏洞扫描、镜像签名
应用日志管理
APM应用性能监控

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>企业级容器云平台建设之资源管理</title>
    <url>/posts/01fb942a14a1.html</url>
    <content><![CDATA[
计算资源管理
多租户权限管理
多集群管理，及集群大小
资源配额与资源限制管理
NodePort端口号管理

网络资源管理
网络业务网、管理网、存储网分离
网卡做bond，提高网络可靠性
网络方案选型
网络策略NetworkPolicy机制
集群边界路由器管理（Ingress&#x2F;Route)
集群DNS域名服务管理

存储资源管理
存储PV选型，动态&#x2F;静态
存储类型，hostpath&#x2F;local&#x2F;NFS&#x2F;Ceph&#x2F;Solidfile等

镜像资源管理
镜像生命周期管理 ，创建&#x2F;查询&#x2F;更新&#x2F;删除
镜像库多租户权限管理
远程复制管理，至少两级镜像库设置，总镜像库&#x2F;子镜像库
镜像库操作审计管理
镜像漏洞安全扫描
镜像签名

补充说明在企业级容器平台建设中，资源管理只是其中的基础部分，另外还需要考虑：监控、告警、日志管理以及运维、安全、审计等方面。
来自：《Kubernetes权威指南：企业级容器云实战》
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>企业级容器平台OpenShift介绍</title>
    <url>/posts/d1705e2516a7.html</url>
    <content><![CDATA[随着移动互联网生态的兴起，传统银行面临跨界竞争，在电子商务支付、消费金融、商业融资的新的业务场景下面临严峻的挑战，数字化转型的需求日益迫切。在这场技术转型的浪潮下，各大银行积极推进科技改革，拥抱云计算，以适应市场不断的变化。随着容器技术的成熟，一个以Kubernetes容器编排引擎为核心的生态圈已经形成，互联网、金融、汽车等各企业都积极投入容器技术的应用中。容器云平台的建设已经被认为是云计算落地的一种快捷的方式。说到容器云平台，就不得不提当前容器调度与编排的事实标准——Kubernetes。Kubernetes通过定义基础资源（容器、存储、网络）的协议接口，为底层基础设施提供了统一的管理方式。研发人员通过声明的方式定义资源编排文件，Kubernetes便通过自身组件自动完成资源的申请与分配，快速实现应用的部署。Kubernetes为容器的管理调度，应用的编排提供了便利，它无所不能，但是在建设企业级容器平台时，我们需要在以Kubernetes为核心的前提下，整合其他组件，例如网络插件、应用访问入口、存储实现、监控、日志、镜像仓库等。如图一所示。OpenShift正是基于Kubernetes实现的一个企业开源容器平台。

OpenShift发展历史OpenShift是由RedHat推出的企业级Kubernetes平台，它是从OKD项目派生的下游容器编排技术。2019 年 9 月 IHS Markit Technology 统计了商业容器软件市场收入的排名，并发布了相关的调查报告。报告中显示红帽份额为 44%，排在第1位，超过了排名第2 - 第5公司的总和。从该报告的结果可以看出，在全球容器市场中，RedHat占有着领先的优势，而RedHat企业级容器产品便是OpenShift。
目前OpenShift最新版本为4.5，生产上使用最多的是3.11版本，也是3系列的最后一个版本。大家都知道OpenShift是基于Kubernetes实现的，但很多人并不清楚OpenShift其实早于Kubernetes诞生。整个OpenShift的发展过程中经历了两次重大的变革，如图二所示。

OpenShift发布于2011年，当时Docker还未出现，容器技术并没有像现在这么普及，也没有形成标准，而是一些大厂专用的技术，各家在容器运行时与容器编排引擎都各成一套，RedHat也不例外。OpenShift在v1与v2版本中一直使用的是RedHat自己开发的容器运行时与容器编排引擎。随着2013年Docker的问世，RedHat立刻意识到了该技术将带来的革命，随即与Docker展开合作，确定了以Docker作为下一代OpenShift容器运行时。在2014年Kubernetes的发布，RedHat经过调研，便很快与Google展开合作，确认了以Kubernetes作为下一代OpenShift的编排引擎。后来的事实证明RedHat做的这两个选择都非常明智。
2015年OpenShift迎来了第一个重大的变革，OpenShift v3发布。RedHat弃用了原先自己的“Gear”和“Cartridge”技术，以Docker、Kubernetes为核心进行全新重构，并且站在开发者的角度，对产品体验进行了全面的升级与优化。OpenShift 3很快得到了开发者的认可，取得了巨大的成功，直到今天使用最多的版本仍然是v3版本。
OpenShift本可以沿着v3的路线不断升级完善，就能够保持自己在容器PaaS领域的优势，然而它的变革并未停止。2018年红帽收购了容器领域的另一巨头CoreOS，容器领域的两只领头羊合二为一，他们将各自积累多年的容器技术进行全面融合，通过Operator实现应用全生命周期的自动化管理，对OpenShift 3版本进行全面的改造，推出了功能更加丰富，更加自动化的4版本，这里不得不佩服红帽的魄力。
OpenShift对Kubernetes的增强OpenShift与Kubernetes的关系类似于Linux与Linux发行版（Rhel、Ubuntu等）的关系。OpenShift以Kubernetes为核心，实现相关的资源协议接口，对其功能进行增强，同时整合其它组件，使其成为企业级容器平台。OpenShift提供稳定的容器、网络、存储等资源协议接口的实现。

以Docker作为默认的容器运行时，对接Kubernetes的容器运行时接口（CRI），当然OpenShift也支持其它的容器运行时，比如CRI-O。
OpenShift默认采用OVS作为容器平台的网络插件，对接Kubernetes的容器网络接口（CNI），实现容器跨主机的网络通信及管理，同时OpenShift也支持其它的网络插件，比如Calico。
存储类型方面，除了Kubernetes默认支持的RBD、NFS、EBS、GlusterFS等，OpenShift支持更多的存储类型，如Local、iSCSI等。OpenShift对原生Kubernetes的安全性及功能进行了增强。
OpenShift最早实现了集群的多租户管理，比如RBAC、Qouta、Namespace等，这些能力在Kubernetes的后期版本中才集成。
为了提高安全性，OpenShift开发了安全上下文约束（SCC），控制容器运行时默认使用最小权限。
为了方便研发人员的持续构建与部署，OpenShift开发了DeploymentConfig及BuildConfig资源，它们在Kubernetes的应用编排资源的基础上添加了更多的控制能力，比如自动触发、部署策略等。
OpenShift实现了Route资源对象，为集群提供了统一的网络入口，方便集群中的应用对外提供服务。受到Route的启发，Kubernetes目前也开发类似功能的实现Ingress。
OpenShift开发了集群内部镜像仓库，并且提供了与应用资源协调调用的能力。OpenShift整合了更多的组件，增强了集群的稳定性。
基于Prometheus、Grafana、AlertManager为集群提供了整体监控与告警的方案，包括集群核心组件的可用性，容器资源使用率等，同时也提供了自定义监控项的扩展能力。
基于EFK组件，为集群组件及应用提供了整体的日志统一管理方案。另外还需要提的是，OpenShift为开发者和集群管理者提供了一个非常容易使用的控制台，通过控制台可以方便完成绝大部分的使用与管理。从上面可以看出，OpenShift的能力对Kubernetes较全面的增强，同时两者又是相辅相成，OpenShift基于Kubernetes实现，也同样反馈与驱动Kubernetes前进。

OpenShift的不足OpenShift提供了一个开箱即用的单容器高可用容器PaaS平台，它可以满足绝大部分企业容器应用部署的需求，为企业能够快速构建自己的云计算服务。但是大型企业容器平台建设需要考虑的情况往往会更复杂。

应用的部署体验够用了吗？OpenShift提供了单平台web控制台，它的可定制化能力较弱，很难对其进行扩展。
多中心、多集群管理有没有？OpenShift更多的是对单集群的管理，缺乏多集群管理的能力。
生产级别的PaaS服务有没有？OpenShift提供了应用商店，更多的并没有达到生产级别的要求，只能在开发测试环境试用。
与企业流程的深度融合有没有？如果需要与企业自身系统进行深度融合，需要对OpenShift控制台进行深度化定制，往往我们还是需要独立开发门户网站。
监控集群及应用是否可以自定制？OpenShift集群提供了整体监控与告警的产品化方案，其监控展示配置是固化的，无法自定义自己的监控展示页面。
日志方案是否足够了？在日志方案上，OpenShift能够满足一般吞吐量的日志收集与管理，随着集群规模的壮大，应用日志并发数增多，OpenShift日志系统会受到性能瓶颈。
集群的网络是否有局限性？OpenShift默认采用ovs作为容器平台的网络插件，它是一种overlay网络，容器间访问需要通过封包解包，这在一定程度上降低了网络性能，同时对于集群内外服务的网络控制也增加了一定的难度。另外各企业有自己的审批流程、开发流程、运维规范、更高的高可用要求、更高的性能要求、更细化的安全要求、自主可控等。而OpenShift产品化程度过高，降低了其自身的灵活性，企业对其定制能力被削弱。所以各大企业即使引入了OpenShift平台，仍然需要在它的基础上建设自己的容器平台门户，并且相关的技术方案也需要独立设计。

OpenShift 4——全新一代OpenShift2019年5月，在完成收购CoreOS的15个月后，RedHat正式发布了OpenShift 4，将其定位为面向混合云的通用操作系统，如图三所示。它虽然依然以Kubernetes为核心，但是整个平台已经脱胎换骨，从功能，架构，到实现都有非常大的改变。其中最大的变化是在底层的操作系统及应用服务的管理方式上。

采用更轻量、更安全、更紧凑的CoreOS操作系统，容器运行时也使用更轻量的CRI-O代替了Docker。CoreOS是一个基于Linux内核的轻量级操作系统，专注于自动化、轻松部署、安全、可靠及可扩缩性，它可以通过配置文件来定义底层操作系统，实现了操作系统的标准化。
全面拥抱Operator，实现应用服务自动部署、自动运维、扩展及故障转移，极大程序简化了应用的管理，同时保证了不同的集群下应用都是完全相同的，无论是内部环境还是公有环境，实现了应用管理的标准化。OpenShift集群的核心组件，运维组件，DevOps工具链等都实现了Operator化。OpenShift 4自身的内功修炼也不止这些：Serverless的支持、服务网格技术的整合、多网络平面的支持、更多持续集成与持续交付功能的拓展等，为应用的全生命周期打造一个完善的生态系统。除了自身的功能外，OpenShift 4与各种云平台的集成方面下足了功夫，支持了AWS、Azure Cloud、VMware vsphere、IBM Cloud等，甚至将会支持与Aliyun的集成。这也解释了为什么它的定位是面向混合云的通用操作系统。虽然当前OpenShift 4还是一个较新的版本，稳定性还需要更多的生产实践进行验证。但是相信在不久的将来OpenShift 4会接过V3的大旗，成为生产上使用最为广泛的容器平台。


总结OpenShift在过去几年的不断创新与探索，敢于自我革命，成就了现在企业级容器平台的江湖地位。虽然OpenShift的产品化降低了它的灵活性，但是它对原生Kubernetes在安全性、稳定性等多个方面进行了增强。在容器平台建设过程中，我们需要结合自身的情况，综合考虑，进行多维度的评估。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>使用ArgoCD实现多集群自动部署</title>
    <url>/posts/5c248e2fc353.html</url>
    <content><![CDATA[什么是Argo CD？Argo CD是用于Kubernetes的声明性GitOps连续交付工具，其优势为：

通过声明式定义应用、配置及环境信息，并且可以通过代码仓库实现版本控制
可通过自动或手动的方式同步应用到配置的期望状态
支持应用在多个环境、多个Kubernetes集群进行统一部署和管理
有UI及CLI多种方式，UI提供应用状态的可观测性，CLI方便与其它持续集成系统对接
支持多种SSO（OIDC，OAuth2，LDAP，SAML 2.0，GitHub，GitLab，Microsoft，LinkedIn）
支持Prometheus监控指标

参考文章https://www.openshift.com/blog/openshift-authentication-integration-with-argocdhttps://www.openshift.com/blog/introduction-to-gitops-with-openshifthttps://argoproj.github.io/argo-cd/
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Vagrant+Ansible一键部署Rancher2-0</title>
    <url>/posts/fcfcc308c797.html</url>
    <content><![CDATA[
Kubernetes的流行程度就不说了，前两天关注了另一个产品Rancher。使用它可以快速部署与接管Kubernetes。而且Rancher的部署非常简单，只需要跑一个容器就完事了。
既然这么简单，那我们就使用Vagrant与Ansible让它更简单。
安装Vagrant + VirtualBox



具体安装不表了，不是本文知识。相信大家多半都使用过，如果没有使用过，赶紧学习起来。

导入Vagrant Box。这里使用的是Centos 7.5

centos75-with-boxadditions.box下载链接: https://pan.baidu.com/s/1U-kJhbY2JirHgw0eP9TBXw 密码: 1a6b
vagrant box add centos/75 centos75-with-boxadditions.box
或者安装vagrant插件
$ vagrant plugin install vagrant-disksize vagrant-ignition vagrant-vbguest

定义Vagrantfile，设置好虚拟机的配置，具体的配置可根据情况调整。

# Vagrantfile# -*- mode: ruby -*-# vi: set ft=ruby :RANCHER_SERVER_IP = &quot;192.168.50.100&quot;RANCHER_AGENT_IP_1 = &quot;192.168.50.101&quot;Vagrant.configure(&quot;2&quot;) do |config|   config.vm.box = &quot;centos/75&quot;  config.vm.define &quot;server&quot; do |server|    server.vm.hostname = &quot;server.rancher.local.com&quot;    server.vm.network &quot;public_network&quot;, ip: RANCHER_SERVER_IP    server.vm.provider &quot;virtualbox&quot; do |v|        v.name = &quot;server&quot;        v.memory = 2048        v.cpus = 1      end  end  config.vm.define &quot;agent1&quot; do |agent|    agent.vm.hostname = &quot;agent1.rancher.local.com&quot;    agent.vm.network &quot;public_network&quot;, ip: RANCHER_AGENT_IP_1    agent.vm.provider &quot;virtualbox&quot; do |v|        v.name = &quot;agent1&quot;        v.memory = 2048        v.cpus = 1      end  end  config.vm.provision &quot;ansible_local&quot; do |ansible|    ansible.playbook = &quot;ansible/playbook.yml&quot;    ansible.groups = &#123;      &quot;servers&quot; =&gt; [&quot;server&quot;],      &quot;agents&quot; =&gt; [&quot;agent1&quot;]    &#125;  endend

编写ansible&#x2F;playbook.yml

这里的过程很简单：

安装docker
设置docker的镜像代理
关闭selinux  
启动docker
部署rancher2.0
这里将完整的代码分享，供参考



Rancher Vagrant：https://gitee.com/xhua/rancher-vagrant.git说明：agent虚拟机只是自动安装了docker及设置了docker代理，在上面部署k8s还需要通过rancher生成对应的docker命令后在上面运行。  
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>利用Openshift中的cronjob-+-zalenium实现网站监控</title>
    <url>/posts/aaf1d512ebcb.html</url>
    <content><![CDATA[背景介绍有一个网站（pf.apps.example.com)，会获取主机的信息，但是主机信息的格式不统一，而且经常性会添加机器，但是信息又是不确定的，所以有时添加新机器会缺失字段，导致网站异常。
有两种解决方法，


优化代码，提高代码的兼容性，不致于导致页面无法显示。



严格规范字段的输入，必须保证字段的完整性。



不管哪种方法，我们都需要在页面出现故障时第一时间知道。这就要求编写代码自动监测网站相关页面的可用性，一旦出现问题，及时发邮件告知，第一时间恢复。
有使用到上篇介绍的zalenium Openshift部署zalenium(容器化的selenium)及Python自动测试
编写监控脚本监控脚本使用selenium控制浏览器，先模拟登录，再访问需要测试的页面，如果页面正常打开则正常，否则发邮件告警。（由于这个网站的每个页面都有id&#x3D;content的一个div，所以通过检测页面中是否存在id&#x3D;content的div来判断页面是否正常）monitor.py
# -*- coding: utf-8 -*-&quot;&quot;&quot;-------------------------------------------------   File Name：     test   Description :   Author :       潘晓华   date：          2018/6/5-------------------------------------------------&quot;&quot;&quot;import unittestfrom selenium import webdriverfrom selenium.webdriver.remote.remote_connection import RemoteConnectionimport smtplibfrom email.mime.text import MIMETextfrom email.header import HeaderTEST_USERNAME = &#x27;test&#x27;TEST_PASSWORD = &#x27;123456&#x27;NOTICE_EMAIL = &#x27;panxiaohua@mail.com&#x27;class SeleniumTestCase(unittest.TestCase):    def setUp(self):        remoteconnection = RemoteConnection(&#x27;http://zalenium.apps.example.com/wd/hub&#x27;,                                            keep_alive=False,                                            resolve_ip=False)        self.driver = webdriver.Remote(command_executor=remoteconnection,                                       desired_capabilities=&#123;                                           &#x27;browserName&#x27;: &quot;chrome&quot;,                                           &#x27;video&#x27;: &#x27;False&#x27;,                                           &#x27;platform&#x27;: &#x27;LINUX&#x27;,                                           &#x27;platformName&#x27;: &#x27;LINUX&#x27;                                       &#125;)        self.driver.implicitly_wait(30)        self.driver.maximize_window()    def test_login_test_case(self):        self.driver.get(&quot;http://pf.apps.example.com&quot;)        username_input = self.driver.find_element_by_id(&#x27;username&#x27;)        password_input = self.driver.find_element_by_id(&#x27;password&#x27;)        login_button = self.driver.find_element_by_id(&#x27;login_btn&#x27;)        username_input.clear()        username_input.send_keys(TEST_USERNAME)        password_input.clear()        password_input.send_keys(TEST_PASSWORD)        login_button.click()        try:            self.driver.find_element_by_id(&#x27;content&#x27;)            self.driver.get(&quot;http://pf.apps.example.com/dashboard/dailyresource&quot;)            self.driver.find_element_by_id(&#x27;content&#x27;)        except Exception, e:            self.notice_by_email()    def notice_by_email(self):        sender = &#x27;panxiaohua@mail.com&#x27;        message = MIMEText(u&#x27;自动平台有故障，请检查&#x27;, &#x27;plain&#x27;, &#x27;utf-8&#x27;)        message[&#x27;From&#x27;] = Header(u&quot;AutoPf&quot;)  # 发送者        message[&#x27;To&#x27;] = Header(&quot;panxiaohua&quot;)  # 接收者        subject = u&#x27;自动化平台有故障告警&#x27;        message[&#x27;Subject&#x27;] = Header(subject)        try:            smtpObj = smtplib.SMTP()            smtpObj.connect(&#x27;mail.com&#x27;, 25)  # 25 为 SMTP 端口号            smtpObj.login(sender, &#x27;123456&#x27;)            smtpObj.sendmail(sender, [NOTICE_EMAIL], message.as_string())            print u&quot;邮件发送成功&quot;        except smtplib.SMTPException:            print u&quot;Error: 无法发送邮件&quot;    def tearDown(self):        self.driver.quit()if __name__ == &#x27;__main__&#x27;:    unittest.main()

制作监控镜像基于python27,安装相关的模块，并将monitor.py导入镜像
# DockerfileFROM centos/python-27-centos7:2.7RUN bash -c &#x27;pip install selenium -i https://pypi.douban.com/simple/&#x27;COPY monitor.py ./monitor.py

创建定时任务通过创建Cronjob每隔一个小时，运行创建的容器，并检测对应网站的状态。
# cronjob.yamlkind: CronJobapiVersion: batch/v1beta1metadata:  name: monitorspec:  jobTemplate:    spec:      template:        spec:          containers:            - name: monitor-autopf              image: harbor.apps.example.com/autopf/monitor:v1              imagePullPolicy: Always              command: [&quot;bash&quot;, &quot;-c&quot;, &quot;python monitor.py&quot;]          restartPolicy: Never  schedule: &quot;0 * * * *&quot;
通过configmap实现通用监控很明显，以上的镜像只能针对该网站的指定两个页面（一个是登录后自动跳转的页面，另一个为“http://pf.apps.example.com/dashboard/dailyresource”），如果有新的监控项或者对其它网站有监控，就必须重新制作对应的镜像。这将会是多麻烦。通过ConfigMap可以将监控代码作为变动项，而保证镜像统一。也就是说将monitor.py（或者有其它依赖的文件）作为configmap资源，挂载到镜像中，最后再通过设置command来执行监控。

创建ConfigMap将monitor.py代码放在configmap中

apiVersion: v1data:  monitor.py: &quot;# -*- coding: utf-8 -*-\r\n\&quot;\&quot;\&quot;\r\n-------------代码内容---------------unittest.main()&quot;kind: ConfigMapmetadata:  creationTimestamp: null  name: monitor

将它挂载到Cronjob中的&#x2F;opt&#x2F;app-root&#x2F;src下

...jobTemplate:    spec:      template:        spec:          containers:          - command:            - bash            - -c            - python monitor.py            image: harbor.apps.example.com/autopf/monitor:v1            imagePullPolicy: Always            name: monitor-autopf            volumeMounts:            - mountPath: /opt/app-root/src              name: monitor-cm          restartPolicy: Never          volumes:          - configMap:              defaultMode: 420              name: monitor            name: monitor-cm  schedule: 0 */6 * * *...
至此便完成了通用监控镜像，如果监控有更新，只需要更改configmap中的monitor.py的代码即可。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>如何为运行OpenShift容器的数字用户设置用户名</title>
    <url>/posts/b3c5c4960a4e.html</url>
    <content><![CDATA[OpenShift容器平台会使用一个不带用户名，数字类型的用户ID（如：10003000）来运行容器，但是有些情况下启动应用的用户必须有用户名，比如访问sftp服务。
**解决思路：**在容器中的&#x2F;etc&#x2F;passwd文件中添加对执行用户的相关配置：:x:::&lt;说明&gt;::&#x2F;sbin&#x2F;nologin。
执行步骤

基础镜像中为&#x2F;etc&#x2F;passwd设置root组用户可写的权限

RUN chmod g=u /etc/passwd

应用启动时为用户设置用户名，要在ENTRYPOINT&#x2F;CMD的脚本中添加如下代码

USER_NAME=$&#123;USER_NAME:-uidname&#125;USER_ID=$(id -u)if ! whoami &amp;&gt; /dev/nullthen    if [ -w /etc/passwd ]    then      echo &quot;$&#123;USER_NAME&#125;:x:$&#123;USER_ID&#125;:0:$&#123;USER_NAME&#125; user:$&#123;HOME&#125;:/sbin/nologin&quot; &gt;&gt; /etc/passwd    fifi
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>如何在Openshift中让Router-Pod独占Router节点</title>
    <url>/posts/084518f28a58.html</url>
    <content><![CDATA[
概念
什么是Router Pod?

Router Pod是Openshift中管理外部流量访问集群服务的重要的入口，它是通过一个haproxy的Pod实现的。由于Router Pod的独特性，几乎所有的流量都过Router中的Pod代理到真正的服务，所以它是一个非常非常重要的服务。

什么是Router节点

因为集群中的Router Pod数量是有限的，外部流量通过负载均衡器到达Router的Pod，所以对于Router Pod必须固定在负载均衡器下的节点上。这些运行Router Pod的节点，我们叫做Router节点。它有两个特点：

运行default&#x2F;router的pod; 
被外部负载均衡器监听

为什么需要让Router Pod独占Router节点上几乎所有的外部访问集群服务的流量都通过Router Pod代理，所以它是非常重要。在正式使用时，需要对它进行保护。让它独占节点，防止其它Pod抢占Router Pod的资源，以确保集群下服务的可用性。
具体实施Router Pod独占绑定Router节点
Router节点上添加label

oc label node router1.it.example.com router=true

Route节点上添加taint

oc adm taint node router2.it.example.com router=true:NoSchedule

Router的DC上添加节点亲和配置

spec:      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: router                operator: In                values:                - &quot;true&quot;

Router的DC上使用污点容忍

spec:      tolerations:      - effect: NoSchedule        key: router        operator: Exists
遇到的问题
最开始的时候并没有使用Node Affinity,而是使用nodeSelector来绑定Pod与Node。但是使用nodeSelector后，部署pod时集群部署pod(pod_deploy)会带上nodeSelector的属性，而并不会带上容忍污点tolerations的属性，这就导致了pod_deploy无法被调度。
解决办法 ：用nodeAffinity替换nodeSelector。pod_deploy也不会带上nodeAffinity的属性，成功被调度

参考文章Pod的调度规则请参考： 玩转Openshift中Pod调度
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>容器平台武林盟主争夺大战</title>
    <url>/posts/deeb75ddf0c6.html</url>
    <content><![CDATA[
容器技术是继虚拟化技术后又一革命性的后台技术，厂商为了争夺容器PaaS的话语权，必将发起一场声势浩大的战争。谁也没想到挑起这场革命性战争的竟然是一家仅3年的创业公司——dotCloud。
2013年前：大战前夕——一片祥和
虚拟化技术已经深入人心，以aws与openstack为主的云平台已经非常成熟。
PaaS理念也得到了普及，cloud foundry成为当时PaaS的标准
cloud foundry吸引了包括百度、京东、华为、IBM 等一大批国内外技术厂商，开启了以开源 PaaS 为核心构建平台层服务能力的变革。“PaaS 的时代就要来了！”


PaaS公司有：Cloud foundry、Heroku、Pivotal、Red Hat
PaaS 项目被大家接纳的一个主要原因，就是它提供了一种名叫“应用托管”的能力。

2013年容器武林大战——一鸣惊人

2013年3月：一家创业公司dotCloud开源了它的产品Docker，解决应用构建、分发与发布的问题，它的最大改进是引入了镜像构建。

Cloud Foundry 的首席产品经理 James Bayer 做了一次详细对比：Docker 使用的还是老技术 Cgroups 和 Namespace ，没有什么特别的新东西，掀不起什么浪。Docker一经发布，便得到的社区的追捧，各大厂商也都相继合作，推出基于Docker的PaaS产品，它的一个小创新，却给迷茫已久的PaaS指明了前进的道路。


2013年6月：Redhat开始了基于Docker的容器平台Openshift的研发

2013年10月：RedHat正式与dotCloud合作

2013年：CoreOS成为Docker项目的贡献者，并在短时间内成为了 Docker 项目中第二重要的力量。

2013年10月：Google发布了自己所用Linux容器系统的开源版本lmctfy（Let Me Container That For You）



面对Docker的强势崛起，Imctfy毫无招架之力，很快关停了该项目，并希望与Docker公司合作共同推进一个中立的容器运行时库作为Docker项目的核心依赖。Docker拒绝了与Google的合作，不久发布了一个容器运行时库 Libcontainer。

2014年容器武林大战——三国鼎立

2014年-2015年：Docker 项目的迅速走红催生出了一个非常繁荣的“Docker 生态”
Docker 生态创业公司们的春天，大量围绕着 Docker 项目的网络、存储、监控、CI&#x2F;CD，甚至 UI 项目纷纷出台，也涌现出了很多 Rancher、Tutum 这样在开源与商业上均取得了巨大成功的创业公司。


2014年6月6日：Google发布了一个名叫 Kubernetes 项目，并早早与RedHat建立合作关系，共同维护推广。
面对着如日中天，而又野心勃勃的Docker，Google终于坐不住了。大家一致认为：Docker作为底层技术，对终端用户影响力终究有限，容器之战最终仍旧会落在PaaS之争上。借助于RedHat在开源软件上的经验与优势，Google强势推出了自己的杀手级产品Kubernetes。


2014年6月10日：Docker 1.0版本发布
2014 年 7 月：Docker 收购 Orchard 掀起了收购的序幕 ，将Fig项目更名为compose
Fig 项目之所以受欢迎，在于它在开发者面前第一次提出了“容器编排”（Container Orchestration）的概念。


2014年10月15日：Azure云和Docker共同举办了Docker全球开发者大会。微软与Docker正式达成合作关系。
2014年12月：Docker公司与CoreOS公司决裂，推出了自己研制的 Rocket（后来叫 rkt）容器。
2014 年12月：Docker公司发布Swarm项目，重新加入PaaS项目战场。三国鼎立形成。
coreos公司的rkt打不开局面，Fleet集群管理项目更是少有人问津，coreos公司完败。RedHat公司作为日期Docker的贡献者也与Docker公司决裂，只剩下Openshfit上一代Paas平台，被Mesos与Swarm碾压。



2015年容器武林大——论持久战
Mesosphere 公司发布了一个名为 Marathon 的项目，它是Mesos的私有PaaS，很快就成为了 Docker Swarm 的一个有力竞争对手。
虽然不能提供像 Swarm 那样的原生 Docker API，Mesos 社区却拥有一个独特的竞争力：超大规模集群的管理经验。


2015年：3月Docker公司收购SocketPlane、Kitematic，10月收购Tutum。Docker在公有市场，企业化的私有市场都有了完善的解决方案
2015年4月8日：Google和CoreOS宣布了一个新项目Tectonic。Google投资了CoreOS $1200万，CoreOS正式投入Google阵营
2015年5月：Docker公司正式进驻中国。
2015 年 6 月 22 日：由 Docker 公司牵头，CoreOS、Google、RedHat 等公司共同宣布，Docker 公司将 Libcontainer 捐出，并改名为 RunC 项目
2015年7月：谷歌与Linux基金会以及众多行业合作伙伴准备共同建立一个云计算基金会（CNCF），Kubernetes1.0发布
2015年7月：Openshift Enterprise V3发布，结合Docker与K8s
2015年：传言微软希望40亿美元收购Docker，以摆脱对K8s的依赖，最终没成

2016年容器武林大战——背水一战
2016年：微软公司也于2016年正式推出Windows容器。
2016年：Docker公司放弃现有的 Swarm 项目，将容器编排和集群管理功能全部内置到 Docker 项目当中。Docker希望利用广大的Docker用户群，实现Swarm的普及，但是适得其反，Docker变得很复杂，稳定性下降。
K8s的战略则恰愉相反，整个社区推行“民主化”，从 API 到容器运行时的每一层，Kubernetes 项目都为开发者暴露出了可以扩展的插件机制，鼓励用户通过代码的方式介入到 Kubernetes 项目的每一个阶段。很快Docker的Swarm败下阵来，丢失了开发者的支持。


这一次容器社区的繁荣，是一次完全以 Kubernetes 项目为核心的“百花争鸣”。
istio&#x2F;Operator&#x2F;Rook等等


Docker的背水一战，并没有挽回局势，败局已定。

2017年容器武林大战——清扫战场
2017年1月：Kubernetes 1.5发布，引入了Container Runtime Initiative（CRI）API，支持可插拔的容器运行时
2017年3月29日：Docker将 Docker 项目的容器运行时部分 Containerd 捐赠给 CNCF 社区
2017年4月：Docker 开源社区版改名为 Moby
2017年8月10日：Openshift 3.6版本
2017年10月：Docker公司在自己的Docker 企业版中内置 Kubernetes 项目

2018年容器武林大战——天下一统
2018 年 1 月 30 日，RedHat 宣布斥资 2.5 亿美元收购 CoreOS。
2018 年 3 月 28 日，Docker 公司的 CTO Solomon Hykes 宣布辞职。

 至此容器武林重归太平，K8S稳坐武林盟主之位。
2019年容器武林大战——最终归属
2019年5月，红帽OpenShift 4发布。
2019年7月，蓝色巨人IBM官方宣布，正式完成对红帽(Red Hat)的收购，耗资340亿美元(约合人民币2340亿元)。
2019年11月，Mirantis今天收购了Docker的企业业务和团队。


Docker作为一家创业公司，通过开源社区的运作取得了巨大的成功之后，雄心勃勃，直面天下武林世家的竞争与围剿，孤身一人对抗整个云计算产业的压力，最后以惨败收场，前景堪忧。但是也因此容器技术在短短几年间深入人心，得到了飞速的发展。
各大门派旗帜
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>对文件目录设置acl权限</title>
    <url>/posts/da2b64a21883.html</url>
    <content><![CDATA[尽管 sudo 可以设置某个用户有特殊权限，确实会增大这个用户的权限。
为了限制这个特殊目录只有特定用户可以读，您可以对特殊用户设置目录访问控制列表 acl。
以 user 用户为例，这样设置之后可以实现只有 user 用户可以对这个目录有读权限，其他普通用户无权限
setfacl -m u:user:r– &#x2F;目录名&#x2F;文件名
如果想要一次设置可以添加 -R 参数 递归设置整个目录，以及整个目录下的所有文件
setfacl -R -m u:user:r– &#x2F;目录名&#x2F;
这样就可以保证某个特殊普通用户对这个目录有可读权限
如果这个目录需要 user 用户可以 cd 进入的话，设置时可以增加大写 X 参数，设置 acl 的时候，大写 X 表示遇到目录给执行权限，遇到文件对象不给执行权限
setfacl -m u:user:r-X &#x2F;目录名&#x2F;setfacl -m u:user:r-X &#x2F;目录名&#x2F;文件名
这样设置，可以让 user 用户能 cd 进入特殊目录
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>应用日志方案</title>
    <url>/posts/10615d84af1c.html</url>
    <content><![CDATA[



原生方式
DaemonSet方式
Sidecar方式



集日志类型
标准输出
标准输出
文件


部署运维
低，原生支持
一般，需维护DaemonSet
较高，需要为每个容器采集日志的


日志分类存储
无法实现
可通过容器、路径映射实现
每个Pod可单独配置，灵活性高


支持集群规模
本地存储无限制，若使用syslog&#x2F;fluentd会有单点限制
取决于配置数
无限制


资源占用
低，docker引擎提供
较低，每个节点运行一个容器
较高，每个Pod运行一个容器


查询便捷性
低，grep原始日志
较高，自定义查询、统计
高，根据业务特点定制


适用场景
测试、POC等非生产环境
日志分类明确的集群
大型、混合型集群


]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>快速使用SonarQube对应用进行静态代码扫描</title>
    <url>/posts/de0ae4c0444d.html</url>
    <content><![CDATA[1、启动SonarQube服务
$ mkdir -p /sonar/&#123;conf,data,extensions&#125;$ docker run -v /sonar/conf:/opt/sonarqube/conf -v /sonar/data:/opt/sonarqube/data -v /sonar/extensions:/opt/sonarqube/extensions -p 9000:9000 xhuaustc/sonarqube:7.9.1
本地目录为~/Downloads/sonar/2、登录SonarQube服务端：http://127.0.0.1:9000/projects，登录用户名：admin，密码：admin3、创建新的项目,获得projectKey与token值4、下载对应版本的sonar scanner工具：SonarScanner下载地址5、请在代码目录中执行扫描工具
/xx/bin/sonar-scanner \  -Dsonar.projectKey=test \  -Dsonar.sources=. \  -Dsonar.host.url=http://127.0.0.1:9000 \  -Dsonar.login=647c09f3fc72c1b935fb06f20941833435690652 \  -Dsonar.exclusions=**/*R*.java，**/*Test.java,**/*(!.java),**/*.js
exclusions排除文件说明：
? 匹配单个字符 ** 匹配0个或多个文件夹 * 匹配0个或多个字符
本地目录为~/Downloads/sonar-scanner-4.2.0.1873-macosx5、登录到SonarQube服务端，查看扫描结果。主要关注issues中的Bug与Vulnerability
说明也可以直接通过docker来扫描应用，参考https://docs.sonarqube.org/latest/analysis/scan/sonarscanner/#例如：
docker run \    --rm \    --user=&quot;$(id -u):$(id -g)&quot; \    -e SONAR_HOST_URL=&quot;http://$&#123;SONARQUBE_URL&#125;&quot;  \    -v &quot;$&#123;YOUR_REPO&#125;:/usr/src&quot; \    sonarsource/sonar-scanner-cli

插件补充1、导出pdf报告插件【适用SonarQube版本 : 5.5–7.5】https://gitee.com/zzulj/sonar-pdf-plugin
参考文档SonarQube中文网站SonarScanner下载地址
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库写测试</title>
    <url>/posts/05180a875e69.html</url>
    <content><![CDATA[先出结果数据库Mysql 5.7数据




NetApp
Ceph
NFS-SSD
NFS-SAS
HOST PATH



写(qtps)
33333
888
20000
3000



延时(us)
1029
40488
1184






NetApp


2018-06-27 17:56:45 ======================== mysql_test ========================2018-06-27 17:56:45 SQL01  exe=5000 fail=0 row=5000 ela=5180 ms avg=1036 us2018-06-27 17:56:45 SQL01    1 ms  exec=    2853, ela=      2530 ms, avg=     886 us, pct= 57, 572018-06-27 17:56:45 SQL01    2 ms  exec=    2122, ela=      2443 ms, avg=    1151 us, pct= 42, 992018-06-27 17:56:45 SQL01    3 ms  exec=      10, ela=        24 ms, avg=    2475 us, pct=  0, 992018-06-27 17:56:45 SQL01    4 ms  exec=       1, ela=         3 ms, avg=    3044 us, pct=  0, 992018-06-27 17:56:45 SQL01    5 ms  exec=       1, ela=         4 ms, avg=    4847 us, pct=  0, 992018-06-27 17:56:45 SQL01    7 ms  exec=       1, ela=         6 ms, avg=    6115 us, pct=  0, 992018-06-27 17:56:45 SQL01    8 ms  exec=       1, ela=         7 ms, avg=    7578 us, pct=  0, 992018-06-27 17:56:45 SQL01    9 ms  exec=       1, ela=         8 ms, avg=    8736 us, pct=  0, 992018-06-27 17:56:45 SQL01   12 ms  exec=       1, ela=        11 ms, avg=   11764 us, pct=  0, 992018-06-27 17:56:45 SQL01   13 ms  exec=       4, ela=        50 ms, avg=   12516 us, pct=  0, 992018-06-27 17:56:45 SQL01   15 ms  exec=       1, ela=        14 ms, avg=   14704 us, pct=  0, 992018-06-27 17:56:45 SQL01   18 ms  exec=       2, ela=        34 ms, avg=   17307 us, pct=  0, 992018-06-27 17:56:45 SQL01   20 ms  exec=       1, ela=        19 ms, avg=   19099 us, pct=  0, 992018-06-27 17:56:45 SQL01   21 ms  exec=       1, ela=        20 ms, avg=   20897 us, pct=  0,1002018-06-27 17:56:45 Total  tran=5000=964/s, qtps=5000=964/s, ela=5181 ms, avg=1036 usSummary: SQL01 exec=200000, rows=200000=100/e, avg=1029 usSummary: exec=33333/s, qtps=33333/s


Ceph


2018-06-27 17:45:33 ======================== mysql_test ========================2018-06-27 17:45:33 SQL01  exe=200 fail=0 row=200 ela=8263 ms avg=41318 us2018-06-27 17:45:33 SQL01   31 ms  exec=       1, ela=        30 ms, avg=   30480 us, pct=  0,  02018-06-27 17:45:33 SQL01   33 ms  exec=       3, ela=        98 ms, avg=   32802 us, pct=  1,  22018-06-27 17:45:33 SQL01   35 ms  exec=     129, ela=      4446 ms, avg=   34468 us, pct= 64, 662018-06-27 17:45:33 SQL01   36 ms  exec=       1, ela=        35 ms, avg=   35763 us, pct=  0, 672018-06-27 17:45:33 SQL01   37 ms  exec=       3, ela=       108 ms, avg=   36208 us, pct=  1, 682018-06-27 17:45:33 SQL01   39 ms  exec=       2, ela=        76 ms, avg=   38462 us, pct=  1, 692018-06-27 17:45:33 SQL01   40 ms  exec=       2, ela=        78 ms, avg=   39477 us, pct=  1, 702018-06-27 17:45:33 SQL01   41 ms  exec=      27, ela=      1088 ms, avg=   40297 us, pct= 13, 842018-06-27 17:45:33 SQL01   45 ms  exec=       1, ela=        44 ms, avg=   44629 us, pct=  0, 842018-06-27 17:45:33 SQL01   46 ms  exec=       2, ela=        91 ms, avg=   45957 us, pct=  1, 852018-06-27 17:45:33 SQL01   47 ms  exec=       1, ela=        46 ms, avg=   46364 us, pct=  0, 862018-06-27 17:45:33 SQL01   52 ms  exec=       4, ela=       206 ms, avg=   51729 us, pct=  2, 882018-06-27 17:45:33 SQL01   53 ms  exec=       2, ela=       104 ms, avg=   52335 us, pct=  1, 892018-06-27 17:45:33 SQL01   54 ms  exec=       1, ela=        53 ms, avg=   53647 us, pct=  0, 892018-06-27 17:45:33 SQL01   58 ms  exec=       1, ela=        57 ms, avg=   57284 us, pct=  0, 902018-06-27 17:45:33 SQL01   60 ms  exec=       1, ela=        59 ms, avg=   59134 us, pct=  0, 902018-06-27 17:45:33 SQL01   62 ms  exec=       1, ela=        61 ms, avg=   61386 us, pct=  0, 912018-06-27 17:45:33 SQL01   64 ms  exec=       2, ela=       126 ms, avg=   63084 us, pct=  1, 922018-06-27 17:45:33 SQL01   69 ms  exec=       3, ela=       206 ms, avg=   68766 us, pct=  1, 932018-06-27 17:45:33 SQL01   75 ms  exec=       1, ela=        74 ms, avg=   74645 us, pct=  0, 942018-06-27 17:45:33 SQL01   76 ms  exec=       1, ela=        75 ms, avg=   75024 us, pct=  0, 942018-06-27 17:45:33 SQL01   77 ms  exec=       2, ela=       152 ms, avg=   76125 us, pct=  1, 952018-06-27 17:45:33 SQL01   84 ms  exec=       1, ela=        83 ms, avg=   83939 us, pct=  0, 962018-06-27 17:45:33 SQL01   87 ms  exec=       2, ela=       172 ms, avg=   86187 us, pct=  1, 972018-06-27 17:45:33 SQL01   92 ms  exec=       1, ela=        91 ms, avg=   91817 us, pct=  0, 972018-06-27 17:45:33 SQL01   98 ms  exec=       1, ela=        97 ms, avg=   97461 us, pct=  0, 982018-06-27 17:45:33 SQL01  109 ms  exec=       1, ela=       108 ms, avg=  108786 us, pct=  0, 982018-06-27 17:45:33 SQL01  110 ms  exec=       1, ela=       109 ms, avg=  109178 us, pct=  0, 992018-06-27 17:45:33 SQL01  115 ms  exec=       1, ela=       114 ms, avg=  114209 us, pct=  0, 992018-06-27 17:45:33 SQL01  163 ms  exec=       1, ela=       162 ms, avg=  162020 us, pct=  0,1002018-06-27 17:45:33 Total  tran=200=24/s, qtps=200=24/s, ela=8264 ms, avg=41320 usSummary: SQL01 exec=8000, rows=8000=100/e, avg=40488 usSummary: exec=888/s, qtps=888/s



NFS


2018-06-27 17:49:03 ======================== mysql_test ========================2018-06-27 17:49:03 SQL01  exe=2000 fail=0 row=2000 ela=2389 ms avg=1194 us2018-06-27 17:49:03 SQL01    1 ms  exec=    1613, ela=      1267 ms, avg=     785 us, pct= 80, 802018-06-27 17:49:03 SQL01    2 ms  exec=     380, ela=       434 ms, avg=    1144 us, pct= 19, 992018-06-27 17:49:03 SQL01    3 ms  exec=       3, ela=         7 ms, avg=    2397 us, pct=  0, 992018-06-27 17:49:03 SQL01    5 ms  exec=       1, ela=         4 ms, avg=    4316 us, pct=  0, 992018-06-27 17:49:03 SQL01   13 ms  exec=       2, ela=        25 ms, avg=   12671 us, pct=  0, 992018-06-27 17:49:03 SQL01  512 ms  exec=       1, ela=       650 ms, avg=  650205 us, pct=  0,1002018-06-27 17:49:03 Total  tran=2000=836/s, qtps=2000=836/s, ela=2389 ms, avg=1194 usSummary: SQL01 exec=80000, rows=80000=100/e, avg=1184 usSummary: exec=20000/s, qtps=20000/s
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>技术中台</title>
    <url>/posts/2192f822e66c.html</url>
    <content><![CDATA[金融机构中台战略之路
在传统金融机构体系中，系统往往由各个项目组独立建设，每个机构中有成百上千套复杂的系统，它们相互调用，形成复杂的网状结构，很难进行管理。随着银行数字化转型的深入，大量业务以互联网的形式向用户开放，对系统高并发、大数据量、强一致性等需求越来越高。与此同时，为了满足用户多样化的需求，银行必须从以业务为中心向以人为中心转变，并做出快速响应。中台建设正是目前解决这些问题的新兴平台型企业架构。
企业中台为业务而生，快速敏捷地响应业务变化，以服务的形式为业务提供支撑，服务接入层以统一的路由适配转发。在整个技术构架上需要考虑可拓展性、敏捷性、轻量化，并注重与前台的交互，灵活地通过服务编排实现应用功能，满足前台需求。因此中台融合分布式、微服务、容器云、DevOps、大数据处理及高可用高性能高并发架构，遵循“高内聚、松耦合”设计原则。业务中台需要微服务、云原生、分布式事务体系支撑，并设计业务模型和微服务边界，最终形成业务单元；数据中台汇聚企业内外割裂的数据，并通过统一治理、建模加工，消除数据孤岛，实现数据资产化，为企业提供客户画像、商品智能推荐、业务实时监控，达到数据驱动业务能力。中台不是微服务，但微服务是当前数字中台建设的最佳实践。中间件服务、容器平台、DevOps、微服务是中台建设的最佳载体，也是技术中台建设的主要内容。
图 技术中台主要内容
技术中台组成
容器平台
DevOps
DevOps平台主要关注与开发效能管理，
微服务治理
中台建设的最大挑战
中台建设只是手段，并不是目标。以用户为中心的持续规模化创新，是中台建设的核心目标。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>某股份制银行：容器云平台建设实践</title>
    <url>/posts/ac86335f2d7c.html</url>
    <content><![CDATA[容器云平台建设行业背景
当前银行业普遍的共识之一是要以金融科技为依托，通过科技创新引领银行的转型升级。云计算、大数据、人工智能成为各银行科技部门重点的投资建设领域。云计算领域的建设主要集中在IaaS和PaaS，目标是降低数据中心成本的同时，为上层应用的创新、快速迭代和稳定运行提供有效支撑。传统的IaaS调度的是虚拟机或者物理机，粒度较大，相对传统的虚拟化技术，在资源使用率、灵活性和弹性方面提升度并不高。依托传统IaaS建设而成的PaaS，也会面临同样的问题。而容器技术恰好可以比较好的解决这些问题，并且在微服务、DevOps、分布式等方面天生具备优势，因此成为数据中心新一代云基础架构的选择。
建设容器云平台的意义
1.让应用真正意义上弹性扩缩容
传统方式下应用和基础环境资源（计算、网络、存储、监控等） 是紧耦合的关系，应用的扩容、缩容意味着基础环境资源的扩容和缩容。基础环境的扩、缩容耗时会非常长，因为涉及到非常多需要人工介入的环境，而且都是串行的，比如创建主机、分配存储、网络接入、操作系统安装、网络访问关系开通、应用部署、监控审计部署、接入负载均衡等等。整个流程走下来通常需要数天到数周的时间。后来我们通过IaaS、虚拟化、自动化工具已经大幅度缩减了基础环境资源扩容的时间，但是整个流程下来仍然需要数个小时到数天，这对于真正需要弹性的应用来说还是不够。
容器云环境下，应用和基础环境资源是解耦的，应用的扩缩容不需要涉及基础环境资源的扩缩容，仅仅需要修改应用部署模板文件中的副本数，然后在容器云平台执行即可。容器云平台会根据副本数来自动创建或者删除副本，使得最终的副本数是部署模板文件中定义的副本数。整个扩容或缩容流程可以在数秒到数十秒内完成。这样当应用面临突发业务量增长，需要紧急扩容的时候，就可以非常快的完成，实现了真正意义上的弹性扩容。
2.为应用微服务化提供有力支撑
应用微服务化是当前应用改造的一个重点方向，因为大家都看到了微服务的好处，就是迭代效率高、资源使用率高(单一微服务可自行扩容)、单一微服务故障 对全局影响有限。但是传统方式下的应用微服务化开发运营是缺乏体系支撑的，成本高昂、便捷性差。比如一个应用由20个微服务组成，每个微服务需要2个副本保持高可用，传统方式下就需要申请20个负载均衡、40个虚拟机来确保隔离性，同时还要为这40个虚拟机分配相应的网络、存储，部署配套的监控审计等，消耗了大量的资源。传统方式下的这套架构没有弹性扩缩容能力，也缺乏自动化的部署管理工具，对运维人员来说，管理的应用从1个变为20个，大大增加了工作量和复杂度，便利性会很差。从应用开发人员的角度看，传统方式下做微服务化改造，随着微服数量的增加，服务之间依赖关系的增加，开发人员会面临很大的挑战。需要部署专门的服务注册发现系统，需要对应用层代码做侵入实现服务的注册发现机制，需要对应用代码做修改以实现服务的探活和依赖性处理。这些服务治理方面的工作牵扯了开发人员很大的精力，使得应用人员无法将精力集中在业务开发本身上，是一种低效率的做法。
容器云环境提供了一套成熟的支撑体系，可以很好的支撑应用的微服务化改造，成本低廉、便捷性好。还是以之前的应用为例，20个微服务中，仅仅对外部提供服务的微服务需要申请负载均衡，内部微服务之间的调用通过service机制即可实现。如果很多微服都需要对外提供服务，也可以通过ingress将所有服务收敛到一个入口上，这样对负载均衡的需求数量就大幅度下降。容器化的微服务都是运行在一个计算机群内，可以共享计算节点，扩容、缩容都不需要申请虚拟机，资源的使用效率可以最高。容器云也为应用的部署运行提供很好的编排工具，可以实现应用变更的完全自动化、滚动升级、一键回滚。对应用开发人员来说，容器云环境可以提供比较完善的可配置化的微服治理框架，包括服务注册发现、服务探活、依赖性处理等，不需要对应用代码做侵入修改，这样可以让应用开发人员将更多精力集中在业务开发本身。
3.让应用实现自动化故障探测、隔离和恢复
传统方式下的应用故障判断、隔离和恢复完全依赖人工介入，耗时很长。比如一旦出现某个应用节点的故障，需要运维人员人工判断是哪一个节点出了问题，然后人工将该节点从负载均衡摘除。随后人工恢复故障节点，再挂到负载均衡下面。这就导致很长的故障窗口期，对业务连续性并不友好.
容器方式下，应用的故障判别、隔离和恢复完全自动化实现，无需人工干预。容器云环境提供一套应用服务的自主探测和处理机制，同时也会检测每一个节点，一旦发现某个应用副本异常，会立即将其从service摘除，之后自动删除故障副本，并在可用的节点上新建新的副本。当探测到新建副本已经可以提供服务后，会自动将新建副本挂载到service下面。这种完全自动化的故障处理恢复机制为应用提供了故障自愈能力，将故障窗口减小到最小。
4.大幅度提升资源使用效率
在没有虚拟机之前，我们使用裸机部署应用，一个裸机部署一个应用，造成了大量的 资源闲置。后来使用虚拟机后，一个裸机上可以虚拟出多个主机，可以部署多个应用，资源使用效率得到了很大的提升。虚拟机之间可以共享CPU，但是无法共享内存和存储，比如一个虚拟机申请了32GB内存和100GB存储，这些资源只能被这个虚拟机独占，无法和其它虚拟机共享。
容器的本质是进程，进程间是可以共享宿主机的CPU、内存、存储和网络的，资源使用效率得到最充分的利用。当然做到这一点的前提是容器能够确保进程运行的基本资源不被抢占，资源层面实现良好的隔离性。同时允许设置资源使用配额上限，避免影响其它应用进程。
容器云平台架构设计
1.总体架构设计
总体架构图如下：
  
自服务管理平台提供8大板块服务，都是按照支持多租户的目标设计实现。其中资源申请板块是租户申请容器资源的入口，包含帐号申请，K8S和镜像库资源申请，日志接入申请。资源变更板块是租户进行资源变更的入口，包括K8S资源扩容和回收，以及帐号权限的修改。集群管理板块为云平台管理员和租户提供集群范围资源的管理，镜像库管理板块提供镜像库和镜像的管理，应用管理板块主要为租户提供K8S namespace内资源的管理，模板管理板块包含K8S资源模板和Helm模板，运维助手提供Pod历史查询以及集群健康检查管理，帐号授权管理板块为云平台管理员提供租户授权管理。
自服务管理平台南向通过K8S API和镜像库API对接多个K8S集群和两个镜像库，实现容器资源的统一纳管。最右边的是行内的运营支撑工具体系，其中统一身份认证为自服务管理平台提供租户帐号的登陆鉴权服务，流程系统(即ITOMS)通过API和自服务管理平台的资源申请板块对接，提供统一的资源申请入口。CMDB和自服务管理平台自身的CMDB交互，提供应用、容器、资源之间的关系视图；DevOps工具链可以从自服务平台获取用户和权限，然后通过K8S API和镜像库API实现应用的自动化流水线发布。ELK日志系统用于存储容器应用的日志，集中监控告警系统接收来自K8S节点和容器应用的监控数据，提供告警推送、置维护、统一监控视图的能力。
2.多集群管理设计
根据银行内网络安全的要求，K8S集群不能通过Overlay网络跨网络隔离区。因此一个K8S集群只能限定在一个网络隔离区内。目前生产和灾备数据中心的每个网络隔离区部署一套或多套K8S集群，所有集群统一由自服务管理平台纳管。同一网络隔离区内，生产和灾备数据中心各部署K8S集群，为应用提供双活容灾部署架构支撑。生产和灾备数据中心分别部署一套镜像库系统，为各自数据中心内的K8S集群提供镜像服务。允许租户跨集群管理自己的容器资源。整体示意图如下：
  
3.多租户管理设计
通过K8S命名空间和镜像库命名空间实现租户资源隔离，一个租户对应于一个或者多个命名空间。云平台管理员可以通过RBAC机制为租户授予相应命名空间的管理权限。租户对授权命名空间内的资源具有管理员权限，但是无法访问非授权命名空间。对于一个租户来说，管理员可以授予他一个K8S集群内一个或多个命名空间的管理权限，也可以授予他多个K8S集群内命名空间的管理权限。整体示意图如下：
  
4.专用和共享计算节点
容器云平台为应用提供两种类型的K8S集群，分别是计算节点共享的K8S集群和计算节点专用的K8S集群。从资源利用率角度，首推共享计算节点的K8S集群。计算节点直接采用物理机，多个应用共享计算节点组成的资源池，资源的弹性和使用效率最高。
如应用需要调整缺省Linux Kernel参数，或者有特殊的敏感的出网络访问关系，或者有很高的安全隔离性要求，可以考虑采用计算节点专用的K8S集群。专用的计算节点考虑资源利用率，主要以虚拟机为主。特殊的应用场景（如GPU）可以使用物理机。通过给计算节点打应用标签的方式，然后在应用部署模板里指定nodeSelector的方式，实现计算节点的独占。
5.存储后端实现
使用Ceph分布式存储作为容器云平台的后端存储，为应用提供持久化的数据存储能力。在生产和灾备数据中心各部署一个Ceph集群，为所属数据中心的K8S集群提供持久化存储后端服务。每个K8S集群创建2个Storage Class。rbd-class提供ReadWriteOnce类型PVC，后台对接的是Ceph RBD；cephfs-class提供ReadWriteMany类型PVC，后台对接的是CephFS。租户可动态申请PVC，仅有创建权限，没有删除权限。整体示意图如下：
  
6.应用监控告警
每一个计算节点上会部署一个监控Agent。应用如需监控，需要在应用部署模板的环境变量里声明监控类型。应用容器启动后，监控Agent会通过容器接口获得容器监控类型环境变量，并自动匹配监控模板(脚本)。监控Agent将监控数据发送到监控服务器。监控服务器根据触发条件判断是否发送告警信息到集中告警平台。在集中告警平台上为每个应用创建虚拟节点，和IP解耦。告警平台收到告警信息后，根据告警数据包含的应用名称字段自动匹配到虚拟节点。虚节点上可设置维护状态，应用变更的时候为了避免告警可以设置虚节点为维护状态，变更完成后可以解除维护状态。示意图如下：
  
目前可监控的应用指标如下：
1.应用容器状态。如果容器状态异常会触发告警;
2.应用Deployment副本数，如果副本数和期望的不一致，会触发告警;
3.应用Statefulset副本数， 如果副本数和期望的不一致，会触发告警;
4.应用Pod状态，如异常，会触发告警;
5.应用容器内部文件系统使用率，如超过80%，会触发告警;
7.应用日志处理
每个计算节点部署一个日志收集代理，该代理面向节点上所有的容器。如应用容器需要监控，就需要在Pod yaml里通过环境变量声明日志路径和kafka topic。容器启动后，日志代理会根据容器环境变量定义的日志路径自动匹配对应的宿主机日志文件路径，并将日志抓取后发送到kafka topic。当前的日志代理以换行符作为分割符，如应用的一条日志里有多行纪录，这条日志会被切分成多个消息来处理，在Kibana上也会呈现多条记录。为了适配这类一条日志有多行纪录的应用，我们也正在设计开发一种可定制化分隔符的日志引擎，可以允许应用在Pod的yaml里声明日志分隔符。
8.应用双活容灾部署架构
生产、灾备中心每个网络区都建设一个K8S集群，都有各自独立的镜像库和后端分布式存储。应用双活要求应用同时运行在生产、灾备中心的两个K8S集群上，前端可以通过负载均衡引流。任意一个数据中心的集群故障不影响应用的可用性。示意图如下：
  
ev6i39ygdkg
应用容器化最佳实践总结
1.镜像和配置分离原则：制作应用镜像时，需要将配置分离出来，这样做可以让应用镜像在不同环境（比如测试和生产）都一致，变得只是配置信息。配置信息可通过环境变量或者加载卷的方式注入容器。在K8S环境下，除了环境变量注入，还可以通过ConfigMap和Serect方式注入配置。ConfigMap和Secret都支持通过卷加载的方式挂载到容器。Secret通常用于保存敏感信息（如密码）.
2.微服务原则：容器环境天生要求微服务化，一个容器只提供一种服务。每个容器原则上只对外提供一个服务监听端口。
3.使用第三方基础镜像制作应用镜像的时候必须包含必要的系统trouleshooting工具，至少包括ps、netstat、ping、curl。否则出现问题的时候会妨碍排错。
4.支持通过NodePort和Ingress对外发布服务。NodePort适用于对外服务较少场景；Ingress适用于对外服务较多，需要统一入口场景。Ingress需要作为应用的的一部分部署在应用命名空间。使用Ingress只需要对外通过一个NodePort暴露服务。
5.NodePort需要向容器平台管理员申请。请仅仅使用分配给项目组的NodePort，禁止使用未经申请的NodePort，否则容易其它项目组产生端口冲突 .
6.如使用StatefulSet部署有状态应用，副本数必须大于等于2，并且在验证了单个Pod失效不影响服务的前提下，才可以生产上线。原因是StatefulSet的Pod在宿主机故障情况下没有自动HA能力，需要人为干预杀死Pod才能触发重建。
7.Deployment&#x2F;StatefulSet&#x2F;Pod的yaml里，必须配置liveness&#x2F;readiness探测，并通过测试才能生产上线。这对于应用的可用性非常重要，请一定重视 。
8.Deployment&#x2F;StatefulSet&#x2F;Pod的yaml里，必须对Container的resources做设置。因为生产环境出于考虑极端情况（一半节点不可用）下的应用高可用。对于独占计算节点的应用，要求应用namespace下所有Pod的request总合不能超过分配总资源(CPU，内存)的50%-1，单个Pod的limit不能超过单个节点资源的60%。
9.对于可以和其它应用共享计算节点(通常是物理节点)的应用，namespace下所有Pod的request总合和limites总合不能超过分配的总资源（比如分配了16C&#x2F;64G，那么request总合&#x2F;limites总合不能超过16C&#x2F;64G）。
10.对于使用独占宿主机节点的应用，Deployment&#x2F;StatefulSet&#x2F;Pod的yaml里，必须配置NodeSelector。生产环境NodeSelector的value值是项目的英文名，测试环境统一是testapp。对于和其它应用共享宿主机节点的应用，可以不配置NodeSelector。
11.对于重要系统，Deployment&#x2F;StatefulSet里，副本(replica)数必须大于2（包含2），禁止为1。这样才能确保服务在单个副本故障的情况下依然可用。对于可靠性要求不高的系统，在资源充足的情况下尽量也保持副本数大于等于2。如资源受限，并且上线前明确说明对可靠性要求不高，可以允许副本数为1 。
12.Pod产生的日志，推荐通过直接写入stdout并配置Kafka Topic的方式，转发到ELK。如果一定要持久化保存，有如下三种方案，但是都要求首先应用层面要做好日志轮循(rotation)，控制好总量大小，因为PVC和HostPath用的宿主机目录通常是无法扩容的。目前仅写入stdout、HostPath的日志，才可以被日志引擎处理发往ELK，HostPath需要挂载到日志目录。HostPath方式受限使用，需要一事一议。写入PVC或者直接写入容器自身的日志将不能被日志引擎抓取。
a)使用StatefulSet方式部署Pod，需要在yaml里声明PVC容量和StorageClass（名字为rbd-class，提供ReadWriteOnce类型的PV），并且通过将日志同时写入stdout，且在yaml里声明stdout日志路径和Kafka Topic的方式，将日志发往ELK。一旦使用PVC，Pod的可用性就会和PVC的可用性关联起来。对于可用性要求很高的系统（A&#x2F;B类系统），如果使用PVC，前提条件是应用实现了灾备双活部署。
b)使用Deployment方式部署Pod，需要在yaml里声明共享型（ReadWriteMany类型）PVC的名字，并且通过将日志同时写入stdout，且在yaml里声明stdout日志路径和Kafka Topic的方式，将日志发往ELK。在多副本情况下，需要应用做好日志文件区分，避免多副本写同一个日志文件。一旦使用PVC，Pod的可用性就会和PVC的可用性关联起来。对于可用性要求很高的系统（A&#x2F;B类系统），如果使用PVC，前提条件是应用实现了灾备双活部署。
c)使用HostPath，将日志写入宿主机的某个目录。这需要应用在多副本的情况下，能够做好日志区分，将所有Pod的日志放到同一个父目录下。如需使用此种方式，请提前联系容器平台管理员创建目录。即使使用HostPath存放日志，可直接通过在yaml里声明日志文件路径和Kafka Topic的方式，将日志发往ELK。使用HostPath存放日志主要的问题是Pod一旦迁移到新的节点，日志写入也会迁移到新的节点，旧节点上的日志文件写入会中断。HostPath仅仅适用于专用计算节点场景，并且需要一事一议。
13.如果两个服务之间有依赖关系，必须在上线前解决启动顺序问题。可以考虑使用K8S的initcontainer机制做探测。
14.对于重要系统，原则上要求应用层面必须实现灾备双活部署，也即应用同时运行在生产、灾备的两个K8S集群上，前端可通过负载均衡引流。任意一个集群的故障不影响应用的可用性
15.生产上线前，请确保在测试环境完成应用HA测试验证，具体的要求是：
a) 杀死任意服务中的单个Pod不影响整体业务
b) 杀死任意服务中的所有Pod，待Pod重启完成后，整体业务服务不受影响
c) 节点故障不影响整体业务
16. 尽可能通过配置prestop或者处理SIGTERM信号，来实现应用容器的优雅停止。缺省情况下，没有配置优雅停止的话，K8S会在grace-period时间（缺省30秒，可在Pod Yaml里调整）到期后，通过SIGKILL杀死Pod内进程。
应用容器化改造案例(某支付类系统)
1.改造背景
支付类系统作为银行的核心系统之一，为了保证可用性和性能，之前都是运行在小型机上，运行成本高昂、可扩展性较差。为了解决这些问题，支付类系统需要进行分布式改造，把应用程序从小型机迁移到X86 PC服务器上，导致服务器的规模从几台扩展为几十台，使得部署环节更加复杂、容易出错。因此希望利用容器平台提供的服务注册发现、动态伸缩以及快速故障检测恢复等能力，降低分布式系统的部署和管理难度。
2.技术实现
如下是某支付类系统容器化后的部署架构，该系统的后端采用容器化方式部署运行。后端也根据微服务的方式，从一个大模块拆分成几个微服务模块，更便于分布式的部署。
  
行内现有的支付类系统大多是有状态的，因为要生成和节点相关的交易流水号。容器化改造时，为了尽可能不影响现有业务逻辑，也需要维持这种有状态的方式。可以利用K8S提供的StatefulSet实现有状态的部署，每个Pod会有固定的名字，比如payapp-01、payapp-02。这样可以根据Pod名字中的索引（01、02等）自动生成交易流水号。
由于现有前置应用和后端应用之间是长连接，只能采用一个Pod一个Service的方式提供服务。每一个Pod都要通过NodePort Service对外提供服务。后端Pod在启动后，会将Pod所在的节点IP地址和自己的NodePort注册到前置应用里，然后由前置应用校验适配后，发起到后端Pod的连接，并一直保持这个连接。为了保持较好的可扩展性，可以预先在前置应用里配置额外的服务端口，这样需要扩展的时候，只需要扩容后端的副本(Pod)数量和Service数量即可。当然，后期如果可以改造为短连接方式，就可以采用1个Service对应多个副本的方式，扩容会更方便，也可省略服务向前置应用的注册环节。
支付类系统是银行的重要系统，必须具备双活容灾能力，具体实现是在生产和同城灾备数据中心的两个K8S集群上分别部署一个多副本的StatefulSet，各副本(Pod)仅和所在数据中心的前置交互。任意数据中心的故障不影响整体业务。
3.效果总结
通过上述容器化改造，达到了如下目的和效果：

支付类应用可以顺利从小型机迁移到X86的虚拟机上。之前只能纵向扩展的问题得到解决，应用得以分布式部署，横向扩展。
应用的弹性扩容能力得到大幅提供，只需要修改部署模板里的副本数即可实现横向扩展。
资源使用效率得到大幅提高，因为做了服务拆分，可以针对模块来匹配资源，扩容所需的资源力度更细，避免了资源的浪费。
应用分布式改造后的部署管理更加简便和高效、可以实现全自动化的部署、升级和回滚。

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>查看Java应用状态</title>
    <url>/posts/191ce310ef24.html</url>
    <content><![CDATA[
jps查看

$ jps16 jar547 Jps

jmap -heap

$ jmap -heap 16

jstat -class pid

$ jstat -class 16
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>混沌工程详细介绍——Netflix持续交付实践探寻</title>
    <url>/posts/98e57b44aefa.html</url>
    <content><![CDATA[本篇来自于本人6月-7月参加的“DevOps案例深度研究”活动Netflix案例研究的第五部分，详细介绍了Netflix的混沌工程。经过一个月的战斗，四个版本的迭代，Netflix战队最后交付了让所有人满意的战果，并获得了全场唯一的案例研究最佳小组奖杯。感谢我们的战友，还有指导老师，姚冬老师和徐磊老师。
  
Netflix实施混沌工程的背景  
08年Netflix决定把它的业务迁移到aws上，从自身运维的角度考虑，它有很多担忧的地方。

很长时间内有两套系统在同时运行，运维的复杂度更高了。
NetFlix的用户量已经达到了1亿，对应用稳定性依赖很高，如果出现故障对用户的影响非常大，甚至是致命的。
它的业务不断复杂，引入微服务架构，对应用的高可用性要求越来越高。
生产环境非常复杂，是多样性的，很难在测试环境中完全模拟生产的状态。

因此Netflix决心探索一种在生产环境验证应用高可用性的一种方法，这就是现在大家所熟知的混沌工程。
混乱工程的发展  

2010年，捣乱猴子诞生
2011年，猴子军团，有了更多场景下的工具集
2012年，开源了捣乱猴子的代码，建立社区，影响了越来越多的公司混沌工程的发展
2014年，NetFlix创建了新的工作角色，混沌工程师，代表了NetFlix将混沌工程融入了公司的运维文化中
2015年，NetFlix与社区正式提出混沌工程原则，从此混沌工程不只是一些工具的集合，而有了一套理论支撑
2016年，NetFlix前员工Kolton Andrus创立了Gremlin，正式将混沌实验工具商用化，可以看到混沌工程影响着越来越多的企业。
2017年，Chaos Monkey 2.0

从一只猴子，发展成庞大的猴子军团，是NetFlix在生产实践中不断探索与不断进化的结果。
NetFlix猴子军团  
NetFlix猴子军团包括很多工具：

Chaos Monkey，捣乱猴子，可以随机关闭生产环境中的实例，确保网站系统能够经受故障的考验，同时不会影响客户的正常使用。
Latency Monkey，延迟猴子，在RESTful服务的调用中引入人为的延时来模拟服务降级，测量上游服务是否会做出恰当响应。通过引入长时间延时，还可以模拟节点甚至整个服务不可用。
Conformity Monkey，一致性猴子，查找不符合最佳实践的实例，并将其关闭。例如，如果某个实例不在自动伸缩组里，那么就该将其关闭，让服务所有者能重新让其正常启动。
Doctor Monkey，医生猴子，查找不健康实例的工具，除了运行在每个实例上的健康检查，还会监控外部健康信号，一旦发现不健康实例就会将其移出服务组。
Janitor Monkey，守卫猴子，查找不再需要的资源，将其回收，这能在一定程度上降低云资源的浪费。
Security Monkey，安全猴子，这是Conformity Monkey的一个扩展，检查系统的安全漏洞，同时也会保证SSL和DRM证书仍然有效。
10-18 Monkey，本地化猴子，进行本地化及国际化的配置检查，确保不同地区、使用不同语言和字符集的用户能正常使用Netflix。
Chaos Gorilla，捣乱大猩猩，Chaos Monkey的升级版，可以模拟整个Amazon Availability Zone故障，以此验证在不影响用户，且无需人工干预的情况下，能够自动进行可用区的重新平衡。
Chaos Kong，捣乱金刚，Chaos Gorilla的升级版，可以模拟整个Region的故障。

这些工具向我们展示了NetFlix的一套方法论，混沌工程并不是一些测试工具的集合，而是一种思想：在生产实践中遇到问题时，利用混沌工程的思想，实施自己的一些混沌实践，引入新的猴子。相信以后会有越来越多的猴子加入到猴子军团。使用 Simian Army 进行混沌工程实验，看起来似乎已经很完美。在生产上做这样不可控的实验，是很危险的。
混沌工程的原则  
引入混沌实践时需要了解混沌工程的五大原则。

建立稳定状态的假设。在做混沌工程实验的时候，首先得确定需要测试的指标已经做了高可用的工作，才能进行验证指标对业务的是否有影响。如果没有做好高可用工作，而引入混沌工程实验的话，对业务而言将会是一声灾难。
多样化现实世界事件不能够凭空想像出一些事件来验证，而是引入那些真实存在的，频繁发生的，且影响重大的事件。对我们而言给这些事件做混沌实验才具有价值。如磁盘故障、网络延时、主机宕机等。
在生产环境运行实验尽量在类生产环境中进行测试，生产环境的多样性是任何其它环境无法比拟的。混沌工程的价值就是保证生产上的业务连续不中断。
持续自动化运行实验实施混沌工程实验一般最开始是人工手动操作，当我们对业务有足够的信心时，要把混沌实验做成持续自动化。在版本升级、不断迭代的过程中，持续不断自动化地做验证，最大程序保证业务的连续性验证。
最小化影响范围做混沌工程的意义就是保证生产上的业务。在我们实施混沌实验时也必须保证对线上业务影响最小。在实施实验时，从小范围开始，不断扩大范围，避开高风险时段，如选择业务量最小的时候实施实验。

混沌工程实践步骤  
有了这些原则，就可以根据业务的真实场景设计混沌工程实验。在真实展开实验时分为两个阶段：准备阶段、执行阶段。

准备阶段：


确认本次实验需要验证的目标。遵循建立稳定状态的假设、多样化现实世界事件的原则。例如：Redis的超时不会对系统影响。代码中已经对Redis超时的情况做了相关的工作，保证业务的可靠。实验只是用来测试验证。
选择实验范围。遵循对线上业务影响最小、尽量与生产环境相近的原则。例如先测试环境验证，生产环境选择最小量用户验证。
确认监控指标。例如：订单成交量、应用请求响应时间、应用响应错误率，做好监控实时查看状态。
团队成员沟通。遵循最小化影响范围。确保团队相关成员了解实施情况，关注业务状态。准备阶段一般只是第一次实验的时候操作，一旦验证好了以后以后，后序重复执行本次工程不需要重新准备，除非对实验过程有变动。


执行阶段：


执行实验。遵循最小化影响范围。执行过程中实时关注指标，如果有异常，随时终止实验。例如，把Redis延时调大，查看监控指标是否有异常。
分析结果。遵循最小化影响范围。根据收集的指标数据确认假设Redis的超时不会对系统影响。如果验证假设不成立，则需要分析代码，确认好原因，再组织下一次的混沌工程实验。
扩大实验范围。遵循最小化影响范围。先小范围测试，再逐步扩大测试范围。
自动化。遵循持续自动化运行实验。当对代码有足够的信心之后，将混沌工程实践做成自动化，让混沌工程实验能够持续保证业务的可用性，获得最大的价值。

Chaos Monkey在K8S集群下的应用  
Kubernetes是容器编排市场的领导者。如何保证K8S集群及运行在上面的应用能够抵御生产中动荡环境的能力和信心呢？正确地使用混沌工程实践是这里面的关键。我做了一些调研，找到了常用的三款在K8S环境下的混沌工程工具。

Kube-monkey：模拟在指定时间随机删除K8s集群中的pod
Powerfulseal：控制K8S中的Pod及Node节点的状态
Gremlin：是一个商业版工具。通过web&#x2F;API&#x2F;CLI的方式对容器、主机，提供数十种攻击方式，方便与其它工具进行集成。

总结：可见混沌工程并不仅仅是在传统运维、云平台发挥着作用，在最新的基础架构设施容器平台中也发挥着它的价值。我相信会有越来越多的企业将意识到混沌工程给系统带来的各种好处，不久的将来，混沌工程实践也许会成为基础设施运维与应用运维不可或缺的一部分。
参考文章Netflix 混沌工程手册Unleash the Chaos MonkeyNetflix的Chaos Monkey的一个实现用于Kubernetes集群获得1.25亿用户以后，Netflix总结了这些系统可用性经验Istio Chaos Engineering: I Meant to Do ThatAWS云上混沌工程实践之启动篇
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>玩转Openshift中Pod调度</title>
    <url>/posts/2aca26a591ca.html</url>
    <content><![CDATA[
大部分情况下，Openshift中的Pod只是容器的载体，通过Deployment、DaemonSet、RC、Job、Cronjob等对象来完成一组Pod的调度与自动控制功能。Pod调度也是由Scheduler组件完成的。

  

Deployment&#x2F;RC：全自动调度

Deployment&#x2F;RC主要是自动部署应用的多个副本，并持续监控，以维持副本的数量。默认是使用系统Master的Scheduler经过一系列算法计算来调度，用户无法干预调度过程与结果。
apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx  labels:    app: nginxspec:  replicas: 1  template:    metadata:      name: nginx      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx        imagePullPolicy: IfNotPresent      restartPolicy: Always  selector:    matchLabels:      app: nginx

NodeSelector：定向调度

通过Node的标签和Pod的nodeSelector属性相匹配，可以达到将pod调度到指定的一些Node上。
oc label nodes &lt;node-name&gt; env=dev
apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx  labels:    app: nginxspec:  replicas: 1  template:    metadata:      name: nginx      labels:        app: nginx    spec:      nodeSelector:        env: &#x27;dev&#x27;       containers:      - name: nginx        image: nginx        imagePullPolicy: IfNotPresent      restartPolicy: Always

NodeAffinity：Node亲和性调度


更具表达力（不仅仅“符合全部”的简单情况）
可以使用软限制、优先采用等限制方式，使得调度在无法满足优先需求的情况下，会退而示其实，继续运行Pod
目前有两种亲和性表达
RequiredDuringSchedulingIgnoreDuringExecution必须满足指定的规则才可以调度Pod到Node上（与nodeSelector类似），为硬限制
PreferredDuringSchedulingIgnoreDuringExecution强调优先满足指定规则，调度器优先选择合适的Node，但不强求，为软限制。多个优先级规则还可以设置权重值，以定义执行的先后顺序


NodeAffinity语法支持的操作符包括In&#x2F;NotIn&#x2F;Exists&#x2F;DoesNotExist&#x2F;Gt&#x2F;Lt
注意事项
同时定义了nodeSelector与nodeAffinity，那必须两个条件都满足，Pod才被调度到指定的Node上
nodeAffinity指定了多个nodeSelectorTerms，那么只需要满足其中一个就能够匹配成功就可以完成调度
nodeSelectorTerms中有多个matchExpressions，则一个节点必须满足所有的matchExpressions才能运行Pod



apiVersion: v1kind: Podmetadata:  name: nginx   labels:    app: nginxspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  restartPolicy: Always  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:          - matchExpressions:              - key: env                operator: In                values:                  - dev      preferredDuringSchedulingIgnoredDuringExecution:        - preference:           weight: 1            matchExpressions:              - key: disk-type                operator: In                values:                  - ssd

PodAffinity：Pod亲和与互斥调度策略


可以根据节点上正在运行的其它Pod的标签来进行限制
必须指定topologyKey：表达节点所属的topology范围
kubernetes.io&#x2F;hostname
failure-domain.beta.kubernetes.io&#x2F;zone
failure-domain.beta.kubernetes.io&#x2F;region


**注意：**PodAffinity规则设置注意事项
除了设置Label Selector和topologyKey，还可以指定namespaces列表来进行限制，namespaces定义与Label Selector和topologyKey同级。默认namespaces设置表示为Pod所在的namespaces，如果namespaces设置为“”则表示所有的namespaces
在所有关联requiredDuringSchedulingIgnoredDuringExecution的matchExpressions全部满足后，才将Pod调度到指定的Node上以下例子要求新的Pod与app=test的Pod为同一个zone,但是不与app=nginx 的Pod为同一个Node



apiVersion: v1kind: Podmetadata:  name: nginx  labels:    app: nginxspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  restartPolicy: Always  affinity:    podAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        - topologyKey: failure-domain.beta.kubernetes.io/zone          labelSelector:            matchExpressions:              - key: app                operator: In                values:                  - test        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:            - podAffinityTerm:                labelSelector:                  matchExpressions:                    - key: app                      operator: In                      values:                        - nginx                topologyKey: kubernetes.io/hostname              weight: 100

Taints与Tolerations（污点与容忍）


Taints与前面的Affinity相反——它让Node拒绝Pod的运行

为node添加一个Taint，效果是NoSchedule(除了NoSchedule还可以取值PreferNoSchedule&#x2F;NoExecute)。意味着除非Pod明确声明可以容忍这个Taint，否则不会被调度到该Node上。如果Pod无法容忍NoExecute的Taint，则上面已经运行的Pod会被驱逐。
oc taint nodes &lt;node-name&gt; key1=value1:NoScheduleoc taint nodes &lt;node-name&gt; key1=value1:NoExecuteoc taint nodes &lt;node-name&gt; key2=value2:NoSchedule
apiVersion: v1kind: Podmetadata:  name: nginx  labels:    app: nginxspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  restartPolicy: Always  tolerations:    - key: &quot;key1&quot;      operator: &quot;Exists&quot;      effect: &quot;NoSchedule&quot;   - key: &quot;key1&quot;     operatior: &quot;Equal&quot;     value: &quot;value1&quot;     effect: &quot;NoExecute&quot;     tolerationSeconds: 3600


DaemonSet：在每个Node上调度一个Pod


管理集群中每个Node上仅运行一份Pod的副本实例。

apiVersion: extensions/v1beta1kind: DaemonSetmetadata:  labels:    component: fluentd  name: logging-fluentd  namespace: loggingspec:  selector:    matchLabels:      component: fluentd  template:    metadata:      labels:        component: fluentd      name: fluentd-elasticsearch    spec:      containers:        image: docker.io/openshift/origin-logging-fluentd:latest        imagePullPolicy: IfNotPresent        name: fluentd-elasticsearch      nodeSelector:        logging-infra-fluentd: &quot;true&quot;

Job：批处理调度


定义批量任务（并行&#x2F;串行启动多个计算进程去处理一批工作项）

按照批处理任务实现方式的不同，可以分为四种模式

Job Template Expansion模式：一个Job对象对应一个待处理的Work item,有几个Work item就产生几个独立的Job，通常适合Work item数量少，每个Work item要处理的数据量比较大的场景。
Queue with Pod Per Work Item模式：采用一个任务队列存放Work item，一个Job对象作为消费者去完成这些Work item，在这种模式下，Job会启动多个Pod，每个Pod对应一个Work item
Queue with Variable Pod Count模式：采用一个任务队列存放Work item，一个Job对象作为消费者去完成这些Work item，Job启动的Pod数量是可变的
Single Job with Static Work Assignment模式：采用静态方式分配任务项


按批量处理的并行问题，Job分为三类

Non-parallel Jobs：通常一个Job启动一个Pod，除非Pod异常才会重启Pod，一量Pod正常结束，则Job结束
Parallel Jobs with a fixed completion count：并行Job会启动多个Pod，需要设定Job的.sepc.completions，当正常结束的Pod数量达到该值后，Job结束。同时.spec.parallelism控制并行度，即同时启动几个Job来处理Work item
Parallel Jobs with a work queue：任务队列方式的并行Job，需要一个独立的Queue，Work item都在一个Queue中存放，不能设置.spec.completions参数。



kind: JobapiVersion: batch/v1metadata:  name: process-itme  labels:    jobgroup: jobexamplespec:  template:    metadata:      name: jobexample      labels:        jobgroup: jobexample    spec:      containers:        - name: c          image: busybox          command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo jobexample &amp;&amp; sleep 5&quot;]      restartPolicy: Never

Cronjob：定时任务


类似Linux Cron的定时任务
定时表达式：Minutes Hours DayofMonth Month DayofWeek Year

kind: CronJobapiVersion: batch/v2alpha1metadata:  name: hellospec:  schedule: &quot;*/1 * * * *&quot;  jobTemplate:    spec:      template:        spec:          containers:            - name: hello              image: busybox              command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo Hello &amp;&amp; sleep 5&quot;]          restartPolicy: OnFailure
参考书目：《Kubernetes权威指南》
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>白化Kubernetes网络</title>
    <url>/posts/37f8f369f5b6.html</url>
    <content><![CDATA[引自：http://www.10tiao.com/html/217/201708/2649694873/1.html

容器的网络是在CaaS集群中无法避免的话题，作为当下最主流的一种容器集群解决方案，Kubernetes对网络进行了合理的抽象，并采用了开放的CNI模型。面对各种容器网络实现，他们有什么不同，应该如何选择？本文将带大家回顾Kubernetes各种主流网络方案的发展历程，并透过现象清本质，用形象的例子展示Weave、Flannel、Calico和Romana等网络解决方案背后的原理。

这次讲一讲容器集群中的网络。其实不同的容器集群解决方案，在网络方面的核心原理都是相似的，只不过这次我们将以Kubernetes为线索，来窥斑见豹的一睹容器网络的发展过程。

我是来自ThoughtWorks的林帆，我们从Docker的0.x版本开始就在对容器的应用场景进行探索，积累了一线的容器运用经验。这次分享会用简洁易懂的方式告诉大家我们对容器网络方面的一些知识归纳。

初入容器集群的人往往会发现，和单节点的容器运用相比，容器的网络和存储是两个让人望而却步的领域。在这些领域里，存在大量的技术名词和术语，就像是一道道拒人于门外的高门槛。

为了便于理解，我们将这些名称简单的分个类别，从简单到复杂，依次递增。今天的话题会涉及的深度大致在这个大池子的中间，希望大家看完以后会对目标线以上的内容不再陌生，目标线以下的内容我们也会依据需要适当的提及。

此外，这个话题会按照Kubernetes的网络发现过程作为时间主线，其中重点介绍CNI标准和它的主流实现。

在早期的Kubernetes中，对网络是没有什么标准的。文档中对网络的描述只有很简单的几句话，其实就是让用户在部署Kubernetes之前，预先准备好容器互联的网络解决方案。Kubernetes只对网络提出设计假设，这三条假设总结起来就是：所有容器都可以和集群里任意其他容器或者主机通信，并且通信双方看到的对方IP地址就是实际的地址（即不经过网络地址转换）。
基于这样的底层网络，Kubernetes设计了Pod - Deployment - Service的经典三层服务访问机制。
既然Kubernetes不提供底层网络实现，在业界就出现了很多企业级的以及开源的第三方解决方案，其中下面这页图中展示了这个时期的主流开源方案。

我们将这些方案依据配置的复杂度，分为“全自动”和“半自动”两类，就像是汽车中的自动挡和手动挡的差别。
“全自动”的解决方案使用起来简单，适用于标准网络环境的容器跨节点通信。
“半自动”的解决方案实际上是对底层协议和内核模块功能的封装，提供了选择十分丰富的网络连接方法，但对使用者的网络原理知识有一定要求。

在Kubernetes的1.1版本中，发生了一件很重要的变化，那就是Kubernetes全面采纳CNI网络标准。
CNI诞生于2015年4月，Kubernetes的1.1版本诞生于2015年9月，之间仅隔5个月。在这个时期，Docker也设计了一套网络标准，称为CNM（即Libnetwork）。Kubernetes采用CNI而非CNM，这背后有很长的一段故事，核心的原因就是CNI对开发者的约束更少，更开放，不依赖于Docker工具，而CNM对Docker有非常强的依赖，无法作为通用的容器网络标准。在Kubernetes的官方博客里有一篇博文详细讨论了个中细节，InfoQ上有一篇该博客的翻译，有兴趣的读者不妨一读。

几乎在Kubernetes宣布采纳CNI以后的1个月，之前提到的“全自动”网络方案就悉数实现了CNI的插件，这也间接说明了CNI的简单。
那么CNI到底有多简单呢？举几个数字。
实现一个CNI插件需要的内容包括一个Json配置文件和一个可执行的文件（脚本或程序），配置文件描述插件的版本、名称、描述等基本信息，可执行文件就是CNI插件本身会在容器需要建立网络和需要销毁容器的时候被调用。
当一个CNI插件被调用时，它能够通过6个环境变量以及1个命令行参数（Json格式文本）来获得需要执行的操作、目标网络Namespace、容器的网卡必要信息，每个CNI插件只需实现两种基本操作：创建网络的ADD操作，和删除网络的DEL操作（以及一个可选的VERSION查看版本操作）。
最新的CNI规范可以通过上图中的链接访问，只有一页网页的内容而已。同时Kuberntes社区也提供了一个利用“docker network”命令实现的简单CNI插件例子，只用了几十行Bash脚本。

那么面对这么多的社区CNI插件，我们怎样选择呢？
直观的说，既然是网络插件，在功能差不多的情况下，我们当然先关心哪个的速度快。
为此我此前专门做过一次对比测试，不过由于使用了公有云的网络环境（云上环境的不同主机之间相对位置不固定），在汇总数据的时候才发现测试结果偏离理论情况过于明显。
这个数据大家且当娱乐，不过对于同一种插件的不同工作模式（比如Flannel的三种模型）之间，由于是使用的相同的虚拟机测试，还是具有一定可参考性。
先抛开测试结果，从理论上说，这些CNI工具的网络速度应该可以分为3个速度等级。
最快的是Romana、Gateway模式的Flannel、BGP模式的Calico。
次一级的是IPIP模式的Calico、Swarm的Overlay网络、VxLan模式的Flannel、Fastpath模式的Weave。
最慢的是UDP模式的Flannel、Sleeve模式的Weave。

这里我也提供出测试容器网络速度的具体方法，以便大家重复这个测试。

要解释这些网络插件和模式速度不同的原因，我们需要先回到这些工具最初要解决的问题上来。那就是跨节点的网络不通。
如果仔细观察，会发现在3种网络速度模式中都有Flannel的身影。因此我们不妨先以Flannel为例来看这些网络工具（和相应的CNI插件）是如何解决网络不通问题的。

跨节点网络不同有几个方面的原因，首先是容器的地址重复。
由于Docker等容器工具只是利用内核的网络Namespace实现了网络隔离，各个节点上的容器是在所属节点上自动分配IP地址的，从全局来看，这种局部地址就像是不同小区里的门牌号，一旦拿到一个更大的范围上看，就可能是会重复的。

为了解决这个问题，Flannel设计了一种全局的网络地址分配机制，即使用Etcd来存储网段和节点之间的关系，然后Flannel配置各个节点上的Docker（或其他容器工具），只在分配到当前节点的网段里选择容器IP地址。
这样就确保了IP地址分配的全局唯一性。

是不是地址不重复网络就可以联通了呢？
这里还有一个问题，是对于不同的主机、以及网络上的路由设备之间，并不知道这些IP容器网段的地址是如何分配的，因此数据包即使被发送到了网络中，也会因为无法进行路由而被丢掉。
虽然地址唯一了，依然无法实现真正的网络通信。

为此，Flannel采用了几种处理方法（也就是Flannel的几种网络模式）。
早期时候用的比较多的一种方式是Overlay网络。
在这种方式下，所有被发送到网络中的数据包会被添加上额外的包头封装。这些包头里通常包含了主机本身的IP地址，因为只有主机的IP地址是原本就可以在网络里路由传播的。
根据不同的封包方式，Flannel提供了UDP和Vxlan两种传输方法。
UDP封包使用了Flannel自定义的一种包头协议，数据是在Linux的用户态进行封包和解包的，因此当数据进入主机后，需要经历两次内核态到用户态的转换。
VxLAN封包采用的是内置在Linux内核里的标准协议，因此虽然它的封包结构比UDP模式复杂，但由于所有数据装、解包过程均在内核中完成，实际的传输速度要比UDP模式快许多。
但比较不幸的是，在Flannel开始流行的时候，大概2014年，主流的Linux系统还是Ubuntu 14.04或者CentOS 6.x，这些发行版的内核比较低，没有包含VxLAN的内核模块。因此多数人在开始接触Flannel时候，都只能使用它的UDP模式，因此Flanned一不小心落得了一个“速度慢”的名声，特别是在之后的Calico出来以后（其实Flannel的Gateway模式与Calico速度相当，甚至理论上还要快一点点，稍后解释）。
这是第一种解决网络无法路由的方法。

第二种思路是，既然在无法进行路由是因为网络中的节点之间没有路由信息，但Flannel是知道这个信息的，能不能把这个信息告诉网络上的节点呢？
这就是Flannel的Host-Gateway模式，在这种模式下，Flannel通过在各个节点上的Agent进程，将容器网络的路由信息刷到主机的路由表上，这样一来所有的主机就都有整个容器网络的路由数据了。
Host-Gateway的方式没有引入像Overlay中的额外装包解包操作，完全是普通的网络路由机制，它的效率与虚拟机直接的通信相差无几。
然而，由于Flannel只能够修改各个主机的路由表，一旦主机直接隔了个其他路由设备，比如三层路由器，这个包就会在路由设备上被丢掉。
这样一来，Host-Gateway的模式就只能用于二层直接可达的网络，由于广播风暴的问题，这种网络通常是比较小规模的，但近年来也出现了一些专门的设备能够构建出大规模的二层网络（就是我们经常听到的“大二层”网络）。
那么其他的CNI插件呢？
由于Flannel几乎是最早的跨网络通信解决方案，其他的方案都可以被看做是Fannel的某种改进版。

比如Weave的工作模式与Flannel是很相似的，它最早只提供了UDP（称为sleeve模式）的网络方式，后来又加上了fastpass方式（基于VxLAN），不过Weave消除了Flannel中用来存储网络地址的额外组件，自己集成了高可用的数据存储功能。

Calico的设计比较新颖，之前提到Flannel的Host-Gateway模式之所以不能跨二层网络，是因为它只能修改主机的路由，Calico把改路由表的做法换成了标准的BGP路由协议。
相当于在每个节点上模拟出一个额外的路由器，由于采用的是标准协议，Calico模拟路由器的路由表信息就可以被传播到网络的其他路由设备中，这样就实现了在三层网络上的高速跨节点网络。
不过在现实中的网络并不总是支持BGP路由的，因此Calico也设计了一种IPIP模式，使用Overlay的方式来传输数据。IPIP的包头非常小，而且也是内置在内核中的，因此它的速度理论上比VxLAN快一点点，但安全性更差。

Cannal将Calico和Flannel做了一下组合，同时支持两者的特性。

Romana只支持与Flannel相同的Host-Gateway模式，但它在网络策略方面做了比较多的增强，通过额外引入的租户概念简化了网络策略所需的IPtables规则数量。

这是几种主流CNI工具的横向对比。

在Kubernetes的1.2版本以后开始引入了一个新的工具，叫做 kubernet，它实现了内置的网络地址分配功能。结合一些云平台上的内网路由表功能，就能够直接执行跨网络通信，相当于把跨网络功能内建到Kubernetes里面了。
这是一个从“只做假设”到“设定标准”到“内置实现”的很大的改变。

在Kubernetes的1.3版本以后，开始加入网络策略相关的支持。并且在1.7版本后结束了Beta阶段，成为正式API的一部分。
值得一说的是，Kubernetes的网络策略采用了比较严格的单向流控制，即假如允许服务A访问服务B，反过来服务B并不一定能访问服务A。这与Docker内置的Network命令实现的隔离有很大不同。

纵向的对比一下主流的容器集群对今天提到的这些网络特性的支持情况和时间点。

Q：Kubernetes的网络策略采用了比较严格的单向流控制，即假如允许服务A访问服务B，反过来服务B并不一定能访问服务A。为什么要设计成严格单向流呢？A：主要是安全性的原因，这是一种更精细的权限控制策略，除了方向，Kuberetes还允许对可访问的端口进行控制。
Q：Open vSwitch有测过么？
A：没有测试，Open vSwitch同样可以配置成Overlay网络或者Macvlan网络等不同的通信模式，速度也会落到相应的档位上。那个测试例子只是为了说明网络速度与采用的通信原理有关，同时引出几种主流的通信模式原理，测试数据是不准确的，建议以在自己的实际环境中测试结果为准。
Q：Calico怎么做网段间的隔离？
A：各种网络工具的网络策略基本上都是基于内核的Iptables模块实现的。比如Calico只是根据用户配置，自动管理各个节点上的Iptables规则。其他有些改进的，比如Romana设计了一种基于“租户”的规则管理机制，可以用更少的Iptables规则实现网络隔离。
Q：如果在Kubernetes里面需要做到平行网络，让每一个Pod获取一个业务IP，除了Bridge+Vlan的方式，还有更好的建议么？
A：这次讲的这些CNI插件都会让每一个Pod获得一个独立业务IP。可以根据实际网络情况和对速度的需求选择。
Q：Calico BGP IPIP NAT三种模式分别怎么配置？原理是怎样的？其中IPIP还有两种模式，区别在哪？
A：在Calico的配置中设置spec.ipip.enabled: ture就会开启IPIP模式，否则默认是纯BGP模式。IPIP的两种模式其实是指纯IPIP（ipip always模式）或者混合IPIP和BGP（ipip cross-subnet），后者指的是“同子网内路由采用BGP，跨子网路由采用IPIP”，主要用于即想在内网获得高速，又想在跨公网时能保持联通的情况。这种模式需要每个节点启动时用–ip参数预先配置节点的子网，并且所有节点版本都在v2.1以上。
Q：能麻烦具体介绍一下kube-proxy这种网络模式的优缺点吗，在测试过程中发现很不稳定，但是又没发现足够的证据。
A：kube-proxy是Kubernetes的一个组件，提供通过Service到Pod的路由，并不是一种网络模式，不同的网络插件都会使用到这个组件。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>研发项目管理实践</title>
    <url>/posts/6b1fc471637d.html</url>
    <content><![CDATA[项目的特征
唯一性、2. 存在风险、3. 临时的、4. 跨部门的团队、5. 渐渐明晰

项目的三约束（QBT）Q：质量、B：资金、T：时间
  

按时、在预算之内、满足性能指标、满足或超出客户的预期完成就是一个成功的项目。
优秀的项目经理在完成一个项目的过程中，让Boss、客户、团队成员都满足。
做项目时要时刻考虑到项目的目的。

项目的来源
合同驱动、2. 市场驱动、3. 变更Change

项目周期  
与不同干系人的沟通干系人指的是对项目成败有利益，对项目有影响的人。

识别干系人。核心团队成员一起来识别。
分析干系人。
对不同的干系人采取不同的策略。

  
影响力来源

信誉、2. 互惠、3. 说服

项目进展
核心团队成员融入到项目里面，能够将项目说清楚
做计划。计划的过程很重要。做计划的过程是把核心团队消化项目的过程，也是责任转移的过程。项目经理要将项目分成模块，细节让负责部分的核心团队成员来做。

团队建设模型（Tuckman模型）
团队建设分为四个阶段：形成阶段、磨合阶段、规范阶段、贡献阶段
四个阶段中间不能跳过
保证核心成员的完整性

项目经理，二件事：管项目，管人。根据对项目与人的管理程度，项目经理分为四种风格类型。
  
在项目的不同阶段，项目经理需要扮演不同类型的角色。

形成阶段。偏向于指挥。告知“方向在哪里”、“为什么要做”、“邀请你加入该项目的职责在哪”、“对现在项目，我很有信心”。
磨合阶段。指导。
规范阶段。支持。
贡献阶段。授权。

项目中的问题沟通人选择
与项目QBT相关，自己出马。Must to have
与项目QBT间接相关，对等出马。Nice to have
团队成员更清楚，核心团队出马。

变更委员会所有变更都欢迎。

变更申请单。变什么、为什么变、不变有什么不好、变更什么好？
领导签字
交给变更委员会甲、乙方确认是否执行变更。Nice to have的不做，只做Must to have的变更。

冲突管理（ARIA模型）
对抗性阶段。谈感受、建立一个安全环境。
共鸣阶段。每个人说出需求。为什么xx那么重要。
创造。不要批评，自由发言，谈方案，头脑风暴。
行动。制定计划，确认时间、工作安排、责任、分工、资源跟踪。
感谢。加大团队凝聚力。

冲突主要分为三种，资源冲突、目标冲突、个性冲突。个性冲突：使用完整的ARIA模型来解决。目标冲突：从共鸣阶段开始。资源冲突：从创造阶段开始。
有意义的点
Need &#x2F; Want多想想提出需求&#x2F;问题的人的需求。话术：为什么你的Want对你那么重要？
Blook定律复杂的项目在中后期不建议加人。

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>网络相关工具简单汇总</title>
    <url>/posts/c641acf49761.html</url>
    <content><![CDATA[lscpi查看Mac地址和网卡带宽[root@localhost] $ yum install pciutils -y[root@localhost] $ lspci -vvv
里面的mac地址格式：46-8d-80-6d-06-07
ip a查看Mac地址与IP[root@localhost] $ yum install iproute[root@localhost] $ ip a
里面的mac地址格式：46:8d:80:6d:06:07
iperf 对集群进行网络流量测试启动服务端
[root@localhost] $ yum install iperf3 -y[root@localhost] $ iperf3 -s
客户端进行测试
[root@localhost] $ yum install iperf3 -y[root@localhost] $ iperf3 -c 10.2.2.2 -P 5
以上默认为作TCP测试，如果要UDP测试，服务端启动与客户端测试都需要加上-u
## 服务端iperf3 -s -u## 客户端iperf3 -c 10.2.2.2 -P 5 -u
qperf对集群进行网络带宽与延时测试启动服务端
[root@localhost] $ yum install qperf -y[root@localhost] $ qperf
客户端进行测试网络延时
[root@localhost] $ yum install qperf -y[root@localhost] $ qperf 10.2.2.2 -t 100 -oo msg_size:8:256K:*2 tcp_lat
客户端进行测试网络带宽
[root@localhost] $ yum install qperf -y[root@localhost] $ qperf 10.2.2.2 -t 100 -oo msg_size:8:256K:*2 tcp_bw
延时与带宽可以一起测试
[root@localhost] $ qperf 10.2.2.2 -t 100 -oo msg_size:8:256K:*2 tcp_bw tcp_lat
查看端口是否连接网线$ ip a | grep &#x27;state UP&#x27;
将会列出所有连接着网线的网卡或者使用ethtool确认
$ ethtool eno1 | grep Speed
将会列出当前网卡的网速，如10000Mb&#x2F;s。如果未连接网线，则会显示为Unknown!
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>通过curl访问OpenShift上部署的Prometheus获取监控数据</title>
    <url>/posts/4f92c76453ff.html</url>
    <content><![CDATA[
Prometheus作为最常用的集群的监控组件，它收集了集群最全的状态信息。那么当我们需要将它与现有的监控告警平台打通，或者根据它开发一个自己的监控展示平台时，就不得不需要获得Prometheus的监控数据了。这时就不得不访问Prometheus的API接口。根据场景的不同有两种方式能够获取到Prometheus的数据

集群外部，通过访问Prometheus UI的链接来获取指标数据
集群内部，进入Prometheus容器中，获取指标数据

1. 集群外部，curl访问Prometheus UI地址由于OpenShift上部署的Prometheus应用对接了OpenShift的用户认证oauth-proxy，所以必须先获取用户的Token后再通过curl访问prometheus服务获取数据，具体操作如下。
# #登录[root@master ~]# oc login -u adminAuthentication required for https://master.example.com:8443 (openshift)Username: adminPassword:Login successful.# #获取Token[root@master ~]# oc whoami -tZhzCkIUKoHpVgen65DIYQodNVdYjguD6Y4AEGiG_Z2E# #获取prometheus数据[root@master ~]# curl -ks -H &#x27;Authorization: Bearer ZhzCkIUKoHpVgen65DIYQodNVdYjguD6Y4AEGiG_Z2E&#x27; \     &#x27;https://prometheus-k8s-openshift-monitoring.apps.example.com/api/v1/query?query=$&#123;QUERY_EXPRESSION&#125;&#x27;
2. 集群内部，进入Prometheus容器获取数据进入Prometheus应用的容器，绕过了OpenShift的认证，直接获取数据，具体操作如下。
[root@master ~]# oc exec -c prometheus prometheus-k8s-0 -- curl -s \  &#x27;http://localhost:9090/api/v1/query?query=$&#123;QUERY_EXPRESSION&#125;&#x27; \| python -m json.tool
其中${QUERY_EXPRESSION}即为Prometheus请求表达式，如：count(kube_pod_container_status_running%7Bcontainer%3D%22etcd%22%7D)来获取正常运行的etcd容器的个数其中%7B为(、%7D为)、%3D为=、%22为&quot;，这些都需要经过HtmlEncode编码。也可以通过Prometheus界面上通过抓包的方式，获取截取具体的QUERY_EXPRESSION表达示。
总结Prometheus API通过HTTP服务的方式向外提供它的监控数据，以上使用curl工具方便地获取监控指标，实际中可以通过各种开发语言进行获取需要的数据。返回数据都为json格式，根据需要可以非常方便地对数据进行格式转化处理。正常情况下，返回的code为2xx，但也会有不正常的请求Code，如下：

404 Bad Request：当参数错误或者缺失时
422 Unprocessable Entity 当表达式无法执行时
503 Service Unavailiable 当请求超时或者被中断时

]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>给非技术人员说清楚容器是什么</title>
    <url>/posts/de13a1f751d3.html</url>
    <content><![CDATA[小明要造一个房子，开始自己买木头、砖头自己造。
出现了一个开发商，他有一个机器，可以在几分钟内造出一个毛坯房，小明只需要按照自己的风格做装修。
巫婆出现，给了小明一个魔法袋，它可以把房子的模型装入袋中，想造房子念个咒语就行。
小明造了很多模板，可以轻松地就能造出很多房子。
来了个商人，告诉小明可以把魔法袋租出去，别人也可以放入模板，也能够造房子，坐收租金。
参考文章https://zhuanlan.zhihu.com/p/53260098
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义Service添加到Openshift中的Prometheus</title>
    <url>/posts/b785857b6330.html</url>
    <content><![CDATA[prometheus.yml中定义了keep, drop, update等状态- job_name: &#x27;kubernetes-service-endpoints&#x27;  tls_config:    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt    # TODO: this should be per target    insecure_skip_verify: true  kubernetes_sd_configs:  - role: endpoints  relabel_configs:    # only scrape infrastructure components    - source_labels: [__meta_kubernetes_namespace]      action: keep      regex: &#x27;default|logging|metrics|kube-.+|openshift|openshift-.+&#x27;    # drop infrastructure components managed by other scrape targets    - source_labels: [__meta_kubernetes_service_name]      action: drop      regex: &#x27;prometheus-node-exporter&#x27;    # only those that have requested scraping    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]      action: keep      regex: true    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]      action: replace      target_label: __scheme__      regex: (https?)    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]      action: replace      target_label: __metrics_path__      regex: (.+)    - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]      action: replace      target_label: __address__      regex: (.+)(?::\d+);(\d+)      replacement: $1:$2    - action: labelmap      regex: __meta_kubernetes_service_label_(.+)    - source_labels: [__meta_kubernetes_namespace]      action: replace      target_label: kubernetes_namespace    - source_labels: [__meta_kubernetes_service_name]      action: replace      target_label: kubernetes_name
创建自定义的Service，添加指定的annotationsprometheus.io/scrape: &quot;true&quot;,prometheus.io/port: &quot;9128&quot;
apiVersion: v1kind: Servicemetadata:  name: ceph-monitor  annotations:    prometheus.io/scrape: &quot;true&quot;    prometheus.io/port: &quot;9128&quot;spec:  ports:    - port: 9128      name: ceph-monitor      protocol: TCP      targetPort: 9128  type: ClusterIP---kind: EndpointsapiVersion: v1metadata:  name: ceph-monitorsubsets:  - addresses:      - ip: 99.248.82.31    ports:      - port: 9128        name: ceph-monitor
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>镜像仓库的镜像清理</title>
    <url>/posts/e37d13447bb1.html</url>
    <content><![CDATA[1. 必备条件镜像仓库打开可删除功能
2. 清理镜像# curl -k -I -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; -I http://localhost:5000/v2/openshift/ocp-router/manifests/v3.11.129获取镜像层的sha256值# curl -X DELETE http://localhost:5000/v2/openshift/ocp-router/manifests/sha256:39ad17c3e10f902d8b098ee5128a87d4293b6d07cbc2d1e52ed9ddf0076e3cf9# #登录到镜像仓库# registry garbage-collect /etc/docker-distribution/registry/config.yml

3. 参考文档私有docker registry的使用–push,pull,search,delete
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>镜像命名规范与Dockerfile编写规范</title>
    <url>/posts/688ce06e973e.html</url>
    <content><![CDATA[镜像命名规范镜像地址：镜像仓库&#x2F;项目名&#x2F;镜像名:标签名

项目名，公共项目使用名称：base，并设置为公开；其它项目使用英文小写项目名，设置为私有。
镜像名中应该包含使用的组件名称及版本信息，使用-连接。
基础镜像命名规则基础镜像命名应按照操作系统大版本：版本号-构建版本的格式命名，例如：base/rehl7:7.6-1
公共镜像全名规则公共镜像命名应按照服务名-中间件-上级中间件：服务版本号-中间件版本号-上级中间件版本号-构建版本的格式命名，中间件名称可以包含大版本号，例如：base/tomcat8-openjdk8:8.5.23-jre8u212-1
应用命名规则使用格式：系统-模块:系统版本-模块版本-构建版本，例如：sys/sys-rs:1.1-1.1-1


同一应用镜像在以上标签的规则下，还可以根据发布流程设置多个标签，以满足部署时版本依赖的管理要求。例如sys/sys-rs:v1代表v1.x的最新版本。

镜像制作规范
关键字使用大写
FROM镜像，指定明确的Tag，不要使用latest
发布向后兼容的镜像可以使用同一个Tag号，否则使用新的Tag号
尽量将命令放在同一个RUN命令下，减少层数
镜像中避免多进程，如果一定要使用，请引入tini命令，用来管理进程

RUN yum install tini -y &amp;&amp; yum clean all -yENTRYPOINT [&#x27;tini&#x27;, &#x27;--&#x27;]

尽量使用exec，使真正应用的进程ID为1
清理临时文件，如yum install后需要执行yum clean all -y
优化Dockerfile命令的顺序，尽量把不变的放在前面
使用WORKDIR指定工作目录，避免绝对路径扩散
使用 set -o pipefail 避免管道错误被忽略

RUN set -o pipefail &amp;&amp; wget -O - https://some.site | wc -l &gt; /number 

优先使用COPY，比ADD更简单明了
始终暴露重要端口
习惯使用环境变量，同时在Dockerfile中为环境变量设置默认值

ENV APP_PORT=8761

避免设置默认密码
镜像中不要安装sshd
使用volume显示设置挂载点，以方便镜像的使用者知道需要如何定义存储卷
支持任一用户运行，对于需要访问的目录文件执行以下命令更新权限

RUN chgrp -R 0 /some/directory &amp;&amp; \    chmod -R g+rwX /some/directory
Dockerfile的最后使用USER指定数字用户
USER 1001

容器内部应用使用Service访问k8s&#x2F;OCP平台的其他服务。
应用基础镜像应该安装公用的依赖库
为镜像设置元数据，例如说明镜像的用途等，UI界面可以根据这些元数据进行定制功能

LABEL io.openshift.tags=&quot;mongodb,mongodb24,nosql&quot; \      io.openshift.min-memory=&quot;8Gi&quot; \      io.openshift.min-cpu&quot;=4&quot; \      io.openshift.non-scalable&quot;=true&quot; \      io.k8s.description=&quot;The MySQL 5.5 Server with master-slave replication support&quot; \      io.openshift.wants=&quot;mongodb,redis&quot;

尽量将应用的日志以标准输出的形式输出，这样可以被容器平台统一收集管理。
镜像中为应用准备好健康检查的探针，方便容器平台对应用进行健康检查。

下图为使用EXEC格式与SHELL格式，执行ENTRYPOINT CMD的区别。
  
docker run 执行多条命令实例
docker run --rm ubuntu sh -c &quot;echo 1 &amp;&amp; echo 2&quot;docker run --rm --entrypoint=&quot;/bin/sh&quot; ubuntu -c &quot;echo 1 &amp;&amp; echo 2&quot;



OpenShift官方文档——镜像构建规范Dockerfile最佳实践ENTRYPOINT 入口点dockerfile最佳实践常用的Dockerfile镜像地址nginx的Dockerfile实例：把日志输出到标准输出
RUN ln -sf /dev/stdout /var/log/nginx/access.log \	&amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.log
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>《大数据时代—生活、工作与思维的大变革》读书笔记</title>
    <url>/posts/d5ac2eba3924.html</url>
    <content><![CDATA[  
信息时代的到来，大数据时代为我们探索事物提供了一个非常不同的思路，并且很明显它将会帮助我们发现更多无法想象的问题及解决方法。这不仅仅是将数据量化，更是思维的一次升级。
引言随着信息时代的发展，数据无处不在，并高速爆炸，同时大数据时代正改变我们的思维方式。大数据告诉我们“是什么”而不是“为什么”。人类一直以来探索这个世界都是建立在因果的基础上，看到事务的发生会去思考内在的原因，并从中发现规律，总结经验。也是在这种不断探索，穷追不舍的坚持下，人类发现了现在的很多理论，可以说人类的现文明是建立在不断追问“为什么”的基础上建立起来的。但是大数据时代改变了这一切。我们不必关心A事物与B事物的关系，但是无法数据显示A发生了某变化，B也会跟着发生变化，那我们就可以得到A变B变的结果，而无需去探索为什么，当我们需要B作某些改变时，我们只要改变A就好了。当发现某中水果对治疗癌症有效果的时候，我们又怎么会为了不清楚之间的关联，而拒绝食用该水果呢？我们不再关心为什么，我们关心的是它是什么。相反思考，恰是大数据发现是“是什么”，而让我们有了更准确的思考方向，去探究“为什么”。
第一部分：大数据时代的思维变革像引言中所说的，大数据时代，我们需要有思维上的变革，关心的是“是什么”，而不是“为什么”。我们不再追寻之间的因果，而关注表现出的相关关系。


要分析与某事物相关的所有数据，而不是依靠分析少量的数据样本



乐于接受数据的纷繁复杂，而不再追求精确性



思维发生了转变，不再探求因果关系，转而关注事物的相关关系。




更多，不是随机样本，而是全体数据

大数据时代使用全量数据能够发现很多意外的关系，而这些关系在以前的抽样分析环境下是无法想像的。慢慢地，我们会完全抛弃样本分析。

更杂，不是精确性，而是混杂性

大数据时代，数据并不统一格式化，同时也可能有错误的地方，网络传输过程中，也可能有延迟，甚至丢失。我们为了获得广泛数据而牺牲精确性，为了高频率放弃精确性，结果可以观察到一些有可能被错过的变化。为了扩大规模，接受适量的数据错误。

更好，不是因果关系，而是相关关系

大数据时代，我们需要让数据发声。相关关系没有绝对，只是一个可能性。建立在相关关系分析法基础上的预测是大数据的核心。通过找出一个关联物并监控它，我们就能预测未来。
第二部分：大数据时代的商业变革
数据化，一切皆可“量化”

数据化就是一种把现象转变为可制表分析的量化形式的过程。

文字变成数据：Google翻译
方位变成数据：预测交通情况
沟通变成数据：微博、twitter、Facebook用户数据
世间万物数据化：我们不会再将世界看作是一边串我们认为或是自然或社会现象的事件，我们会意识到本质上世界是由信息构成的。
价值，“取之不尽，用之不竭“的数据创新



 数据的价值不会随着它的使用而减少，而是可以不断地被处理，产生更多的价值

数据的再利用：对用户搜索关键词分析用户喜好

重组数据：将多个数据集的总和重组在一起，扩大价值

可扩展数据：一开始考虑好数据的可扩展性

数据的折旧值：随着时间的推移，大多数据会失去一部分价值，但潜在价值依然强大

数据废气：用户在线交互的副产品，包括浏览了哪些页面、停留了多久，鼠标光标停留的位置、输入了什么信息等

开放数据：开放数据，挖掘数据的潜在价值

角色定位，数据、技术与思维的三足鼎立


掌握大数据的公司

拥有技术和专业技能的公司

有着大数据思维的公司和个人

全新的数据中间商：从各地方搜集数据进行聚合，提取有用的信息进行利

大数据决定企业竞争力：大数据对中等规模公司帮助不大，超大型的公司占据了规模优势，小公司则具有灵活性。大数据让处于行业两端的公司受益良多，而中等规模的公司要么向两端转换，要么破产


###第三部分：在数据时代的管理变革

风险，让数据主宰一切的隐忧


被“第三只眼”时刻“监视”

隐私被二次利用：目前有法律法规规范数据的用途，但数据更多的价值在于它的二次利用，最终产生了很多创新性的用户

预测与惩罚，不是因为“所做”，而是因为“将做”：如果大数据足够精确，未来我们将失去选择的权利

数据独裁：对数据的盲目崇拜，会导致收集数据的不可靠性

掌控，责任与自由并举的信息管理


个人隐私保护：从个人许可到数据使用者承担责任

个人动机 VS 预测分析：个人有选择自我行为的自由，不就为行为倾向负责

击碎黑盒子，大数据算法师的崛起：监督大数据活动，对数据结果进行审查

反数据垄断大亨


结语大数据时代，是名副其实的“信息社会”，以前我们寻找事物发生原因的想法可能被高估了，很多情况下，弄清“是什么”比寻找“为什么”更加重要。我们尽可能收集数据，不再追求数据的精确性，我们的思维也需要升级。大数据时代，一切皆可数据化，挖掘数据的价值，发现事务发展的规范，为更好的决策做判断依据应对大数据时代，个人信息容易被滥用，我们要建立规范制度来保护我们的数据，规范大数据的使用
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>基于容器技术构建一站式业务支撑平台——实现业务需求快速交付，应用稳定可靠运行</title>
    <url>/posts/406da41e46f6.html</url>
    <content><![CDATA[来自2020年全国职业容器云大赛冠军队二二一队作品。
  
兴业数金 二二一团队成员：潘晓华、方胜、全彬元、徐林、孙佳明
1 建设背景随着互联网的兴起，互联网企业依托互联网，特别是移动互联网为公众提供越来越多方便快捷、稳定高效的服务，对传统业务带来了很大冲击。作为应对，传统行业也在业务上不断创新，带来对IT基础设施和应用架构方面进行转型升级的要求。例如为了支撑电商促销活动对带来的高峰期海量支付请求，某银行很早就对支付渠道相关业务应用进行微服务架构改造，由此带来了容器技术的研究和运用。经多年实践证明，采用容器技术平台很好地支撑了新的业务模式和业务容量。
基于业务发展的需要，和快速进步的金融科技技术，越来越多的传统企业开始思考自身的互联网战略、上云规划等。其中重要内容之一，是希望从技术层面更有效地支持业务创新，如微服务架构、更好的灵活性、扩展性、高可用性、更高效的业务上线效率等，因此跟上云计算技术发展的趋势，建设并推广适合自身的基于容器技术的云平台是关键任务。
2 需求分析2.1 业务需求2.1.1 应用架构****改造需求某银行的客户交互渠道系统存在以下两点架构问题，制约了其快速迭代，影响了用户体验：第一，竖井式系统架构，各模块各自开发和管理基础功能，存在大量重复开发工作；第二，非分布式架构，横向扩展效率低。
为了缩短系统的迭代周期，增强横向扩展能力，该渠道需要运用DDD思想，重构一套使用微服务架构的新系统。本文设计了一套基于容器的一站式业务支撑平台解决方案，用于部署该渠道系统的微服务版本。
2.1.2 应用架构****概要设计基于微服务架构的新系统，将服务端在逻辑上分为业务域和通用服务域，业务域是面向客户的交互界面，主要功能是整合微服务向前端提供统一的交互视图；通用服务域是从各业务领域提炼出来的通用服务的集合，与业务域解耦，只提供单一的服务输出即原子服务，业务域可整合不同的通用域服务单元完成相应的业务逻辑处理，同时通用域还包含与业务无关的后台处理模块。
  
2.2 建设企业级容器平台容器平台作为企业的新一代IT基础设施，不仅需要为基于微服务架构的新业务提供容器化运行和管控平台之外，还必须非常重视安全需求。因此建设容器平台的需求时，需要考虑包括的方面有：
l 管理大规模容器集群能力，包括：提供容器所需的高可用集群、资源池管理、网络通信方案、存储方案、编排调度引擎、微服务运行框架、镜像管理、事件告警、集群监控和日志收集等。
l 为满足安全要求，平台需要考虑应用的高可用性和业务连续性、多租户安全隔离、不同等级业务隔离、防火墙策略、安全漏洞扫描、镜像安全、后台运维的4A纳管、审计日志；如果容器平台还对公网提供访问，那么还需要考虑访问链路加密、安全证书等。
另外，一个重要方面是，容器平台通常是企业IT整个复杂系统中的一部分，因此容器平台还要遵从企业已有IT技术规范和运维要求，例如可能还需要考虑：
l 支持企业自身的应用发布体系、持续集成系统、应用建模规范、高可用管理策略。
l 对接云计算底层资源池（例如IaaS），遵从云计算资源的统一管理和分配。
l 对接或改造容器平台的网络，以满足容器平台中应用与传统虚拟机、物理机中旧业务系统的相互通信，避免或尽可能减少对企业现有网络管理模式的冲击。
l 对接统一身份验证、和整个金融云其它系统采用统一的租户定义、角色定义、资源配额定义等。
l 对接漏洞扫描、集中监控系统、日志分析系统等已有周边系统。
2.3 基于容器平台构建DevOps体系业务市场竞争加剧，业务部门要求业务快速交付，业务系统就要充分复用其它业务应用系统服务：
l 作为业务部门，综合着眼关注业务进度，不关注需求之外的其它交付内容。
l 作为开发部门，期待资源得倒满足，需求明确，交付内容清晰，项目计划到位，资源充足，时间充裕；如何以更有效的方法进行需求调研，更好的架构进行开发，更有效率的测试策略及项目管理方式。
l 云资源交付部门，期待资源容量评估、架构评估、交付内容完整、时间宽裕、安全合规。
l 安全部门，所有各个阶段严格按照网络规范实施交付，并配以持续监测和更新到最新安全机制。
l 运维部门，期待支撑所有交付均考虑运维体系完备性，基于主机和服务两个维度、不同对象目标的运维体系完备；所有运维数据均可以共享互访并使用。
关注在业务场景下，对于容器平台及DevOps要求的需求上，还包括弹性要求、高可用要求、快速交付、需求变化要求等。在 Kubernetes 和容器普及之前，我们通过虚拟机也可以实现DevOps，只是速度相对较慢，因此普及性不高（想象一下通过 x86 虚拟化来实现中间件集群弹性伸缩的效率）。而正是容器的出现，为 DevOps 工具层面的落地提供非常好的承载平台：
l 容器云平台，需要DevOps以标准化和提升IT研发和交付能力。DevOps可以部署在容器云平台上。
l 基于容器化PaaS平台的DevOps，可以使用容器云的资源，譬如DevOps平台的相关技术组件，可以以容器方式部署在容器云上，以支持多pipeline流水线并发编译所需要的弹性资源。
Openshfit 以容器技术和 Kubernetes 为基础，在此之上扩展提供了软件定义网络、软件定义存储、权限管理、企业级镜像仓库、统一入口路由、持续集成流程（ S2I&#x2F;Jenkins ）、统一管理控制台、监控日志等功能，形成覆盖整个软件生命周期的解决方案，实现DevOps落地效率比较高。
3 设计架构3.1 整体设计一站式开发交付运行平台提供持续集成、自动部署、弹性伸缩、高可用、监控告警、微服务治理等特性，解决互联网业务爆发性强，难预测，响应要求高，需求变化快的的特点，为应用提供全生命周期的支撑管理能力，从业务需求管理、应用开发、持续构建、持续部署到应用运维保障。
  
3.1.1 容器平台l 基础设施
IT基础设施为平台提供计算、网络、存储、安全等资源，支持物理机、虚拟化、私有云和公有云等多种环境。
l 容器引擎Docker
平台使用Docker作为容器引擎层。容器技术是一种进程隔离技术，它通过namespace&#x2F;cgroup等技术从内核空间、资源和安全等方面对进程做隔离。因为容器是进程级别的，所以它非常轻量，能够实现秒级启动。另外容器镜像将运行环境与应用一起打包，实现了一处构建，处处部署，为应用的交付与运维管理提供了一种标准的方式。Docker是目前使用最为广泛的容器引擎，已被大规模生产环境验证。
l 容器编排层Openshfit
平台使用Openshfit作为容器编排层，Openshfit基于业界容器编排标准Kubernetes技术，并在安全性上进行了增强，同时为了方便开发者使用容器技术，对应用的构建与部署等集成方面也做了功能的增加，有助于快速构建一个功能全面的容器平台。
作为容器编排引擎，Openshfit具备全面的编排相关的功能：应用编排、应用部署、应用配置、应用升级、负载均衡、弹性伸缩、健康检查、权限管理、容器网络、网络存储等；除此之外，Openshfit还提供了更全面的管理功能，如应用构建、日志中心、资源监控，应用商店等。但是Openshfit自带的运维管理功能有它的局限性，平台将会对其日志中心、资源监控等功能进行全面扩展与增强。
l PaaS服务
平台基于Openshfit之上，采用Operator技术实现的可扩展PaaS服务（包括数据库、消息队列、缓存、反向代理等中间件服务）。Operator为PaaS服务提供构建、扩展、监控、备份等能力。研发与运维人员只需要在平台门户进行提交表单，就能方便快速地构建高成熟度中间件服务。
l 容器平台门户
平台基于Openshfit和Docker容器技术，并与DevOps支撑平台进行集成，提供面向应用全生命周期管理的企业级PaaS云解决方案。提供对平台与应用服配置、部署、运维及管理能力；支持多数据中心、多集群、多租户、多项目等多个维度管理；与DevOps平台集成，实现一体化的持续构建与部署的能力。
3.1.2 DevOps支撑DevOps平台提供了需求管理、CICD流水线、代码配置管理、制品管理、质量管控等功能，提供了从计划到测试的问题持续集成过程，提供了从计划到测试完成的过程持续发布过程，解决了从计划到上线的持续部署过程，覆盖了用户提出价值到用户使用并且监控维护的端到端过程。
通过DevOps平台，实现了在制品的持续流动、持续反馈，进行持续优化，让质量持续提高。
通过DevOps平台，实现了研发数据的度量管理，通过对团队的研发数据进行定量分析，及时发现研发过程中的不足，有助于提高研发团队的效率和质量。
通过AIOPS的接入，通过对日志等数据的持续监控，实现常见问题的自动化运维，保证应用的持续可用。
3.1.3 运维保障l 资源管理
通过开发Openshfit管理门户，实时管理集群资源使用现状，通过记录资源台账（记录计算、存储、网络ip端口资源）的方式，记录并预分配准备上线的系统，保障集群配额充足，及时提出扩容需求。
租户资源：通过ResourceQuotas对租户资源进行限制，以保障租户间不相互影响。
pod资源：通过LimitRange对pod和container资源进行限制。
l 日志管理
集群日志统一由EFK组件收集、管理和展示，通过统一管理，大大提升了日志查找、故障排查的效率，同时Openshfit管理门户也开发了日志的展示和查询功能，实现了在挂历门户对集群故障的初步定位能力。
l 监控告警
除了集群内部的Prometheus，还将集群资源使用情况、集群组件状态、项目pod状态等信息以标准格式输出给集群外部的监控工具，进行统一管理、告警和展示，具备短信邮件等告警提醒等功能。
l 事件管理
容器平台上运行着成千上万个资源，每个资源在生命周期过程中状态不断变化，每一个状态变化过程中都会涉及到多条事件信息，及时获取到这些信息中有问题的部分能够帮助项目最早时间发现问题，这就是事件管理的重要意义。本方案中，将采用EventRouter收集平台所有事件，对于异常事件会在第一时间通过告警平台发出。
l 网络管理
网络管理包括两个部分：传统网络管理、集群内部网络管理。
传统网络管理，主要集群内外网络访问以及不同网络区域之间底层网络控制策略的管理，主要通过网络防火墙策略的维护。
集群内部网络管理，主要通过NetworkPolicy策略进行控制。平台支持图形化配置该策略，为管理员提供良好的操作体验。
l 数据备份
数据备份包括以下两个方面：集群数据备份、应用数据备份，其中集群数据备份包括有资源对象备份、证书与集群配置备份、集群Etcd数据库全量备份。
集群数据备份通过oc、Etcdctl命令客户端及shell命令实现资源对象、证书、配置及Etcd数据库全量数据的备份与恢复。
引入NBU（NetBackup）备份策略，为应用持久化存储做备份。Veritas NetBackup拥有Docker认证，它能为应用存储提供高效、简便、灵活的应用备份解决方案。
l 巡检机制
通过编写健康检查脚本，对集群核心组件的健康状态、核心组件运行过程中的异常日志以及平台的负载状态等按日进行巡检，将高危风险以邮件形式发送给平台运维和研发人员，并及时分析处理，以确保平台稳定运行。
l 集群升级
平台在日常运维中将定期关注集群的最新补丁与漏洞报告，并及时对集群进行升级，保证平台的安全稳定。
3.2 非功能性设计  
3.2.1 高可用性平台高可用性涉及到五个方面：集群高可用、应用高可用、存储高可用、网络高可用以及制品库高可用。
l 集群高可用
集群的高可用主要有两部分：单集群的高可用和多集群提供双活服务两个部分。
单集群的高可用：控制节点是集群最核心的部分，其上面的组件均采用高可用部署：Etcd集群化部署、Master组件服务均为多副本部署。同时Master组件部署在支持自动迁移策略的虚拟机平台，即当底层物理机发生宕机后，其上运行的虚所机会自动漂移到备用节点，并且多个控制节点分散部署，进一步提升集群的可靠性。
双活机制提升集群服务可靠性：在同一机房部署多套集群，集群业务互为备份，同时提供服务。
l 应用高可用
应用高可用主要有两部分：应用设计和应用部署。
应用设计：应用提供对应的健康检查接口，与容器平台的检查机制配合实现应用自动恢复；容器平台支持应用生命周期控制，例如通过HTTP接口健康探测机制可实现故障的容器实例的自动重启。
应用部署：应用采用多集群多副本部署，容器具有轻量、启动快速等特性能够支持快速扩容；平台为应用提供亲和与反亲和等调度策略，应用部署可选择合适的策略以提升应用的可用性；容器平台支持应用滚动升级保证应用升级过程中服务不中断。
l 存储高可用
平台为不同场景提供多种存储方案。分布式存储、本地存储、共享存储等都考虑多副本机制提高存储的高可用性，并且对于重要数据基于NBU提供定时备份。
l 网络高可用
网络高可用主要有两部分：集群底层网络和集群内部网络。
集群底层网络：交换机、路由器等网络设备采用双活部署，主机双网口绑定bond提供可靠的本地网络；集群将数据网、业务网、管理网分离，减小不同网络数据之的干扰。
集群内部网络：集群提供多副本且支持流量分片的Ingress&#x2F;Router节点，并通过高可用负载均衡器承载外部包括TCP、HTTP协议的请求流量。平台提供全面的监控体系，对网络服务的异常进行自动监测与诊断，在满足一定的条件下，实现网络服务自动恢复。
l 制品库高可用
制品库采用单地备份、多地同步的机制。单个制品库的服务端采用多活的方案同时提供服务，底层数据则采用冷备方案对应用镜像进行定期备份。同时平台在各地分别部署制品库，各地制品库通过之前的同步机制实现多地镜像的同步，实现互为备份。
3.2.2 易用性l 可视化管理
平台提供了可视化管理界面，通过界面可以直观地查看集群的状态。同时对于应用开发的整个生命周期都支持可视化，包括需求管理，开发流水线，应用的测试报告，流水线度量指标以及运行状态等。
l 自动化管理
容器平台部分提供自动化运维工具，实现集群自动化部署、自动化升级、自动化备份、定期自动巡检、服务及资源监控告警。DevOps平台部分，自动化流水线实现应用的持续构建，持续部署与持续监控。
l 交互式操作
平台提供表单交互式，实现快速创建PaaS应用以及应用部署。
l 多集群管理
研发人员通过一个入口就可以管理多中心多集群，实现多集群统一管理，同时权限与用户集中化管理，方便容器平台的使用。
l 多系统集成
与监控告警平台、日志平台、DevOps平台等系统集成，实现一站式应用全生命周期的支撑。
l 多租户管理
平台具有丰富的用户与权限管理，支持细粒度的权限控制以满足更多的安全控制的要求。
3.2.3 安全性平台从主机、容器、平台自身、镜像、网络、应用及数据等方面构建全面的容器安全体系，保障平台及应用的安全。
l 主机
主机操作系统安全加固；系统安全补丁管理机制；针对不同安全要求主机划进行分组管理，不同组的应用间物理隔离。
l 容器
容器引擎漏洞管理；容器最小权限原则，禁用root等用户；容器资源限制；容器日志分析与审计；CIS容器配置基准保证。
l 平台
基于RBAC的权限管理；平台升级机制，修复平台漏洞；平台运行日志分析与审计；CIS平台配置基准保证。
l 镜像
制定安全标准镜像作为应用基础镜像；通过镜像扫描机制防范安全漏洞；镜像签名保证平台运行镜像均已被可靠认证。
l 网络
通过底层网络策略，防火墙和ACL，按照安全等级设计不同安全要求的网络区域；集群采用细粒度网络控制机制NetworkPolicy；通过路由分片实现对不同网络安全要求应用进行隔离；通过EgressIP机制实现集群间网络访问控制。
l 应用
DevOps流水线对应用代码与服务进行持续漏洞扫描，进行安全合规审查，保证应用的安全。
l 数据
对于底层存储数据采用硬件加密；对于应用的敏感配置数据采用vault管理的secret资源保存。
3.3 关键模块设计3.3.1 容器平台  
3.3.1.1 服务编排与管理服务编排中，需要考虑资源的统一管理、应用部署及应用弹性伸缩。
资源管理：平台通过对项目配额的管理实现对集群资源的统一分配，能够快速响应开发部门的资源需求；针对不同类型应用提供不同的底层计算节点，如计算型、内存型等，以便进一步提高资源的利用率。
应用部署：应用支持多地多集群部署，通过容器平台可以同时指定应用部署的集群及各自的副本数，容器编排引擎将会自动完成应用的部署。
弹性伸缩：应用通过HPA、VPA资源配置，对应用的负载进行监控实现应用资源限制与副本数配置的扩展与收缩，以支持流量激增的互联网场景。
3.3.1.2 网络网络场景主要考虑以下四种场景：集群内部网络、集群间网络、集群访问外部网络以及外部访问集群间网络。对于金融行业，网络安全非常重要，该方案中充分考虑在任意种场景下的网络安全问题。
集群内部网络：采用OVS-Networkpolicy网络策略实现集群内部服务网络精细化管理，不同项目间的应用默认网络隔离。
集群间网络：平台支持多网络区域下集群的统一管理，不同的网络区域底层通过硬件实现隔离。例如，DMZ区与生产区分别部署Openshfit集群，之间通过硬件防火墙进行隔离。
集群访问外部网络：采用EgressIP机制，为每个Project指定出口IP，该Project下的所有应用都以该IP对外部发送请求，通过集群与外部系统间的防火墙控制网络的访问权限。
外部访问集群：集群服务通过Openshfit Router服务对外提供HTTP&#x2F;TCP服务。并且根据业务类型对Router进行分片，更大程度提升Router服务的性能与扩展性。
3.3.1.3 存储单一的存储方式无法满足复杂的业务场景，方案根据不同的场景内容提供对应的存储介质。
  
场景一：应用内部多容器共享缓存，使用容器自身临时存储。
场景二：应用不同副本间共享存储使用NFS网络存储。
场景三：对于存储IO要求较高，并且支持单节点挂载，使用Ceph RBD分布式存储作为应用持久化存储。
场景四：对于存储IO要求很高，采用本地盘存储方案，将应用与主机通过Node Label进行绑定。
场景五：采用MinIO部署独立的对象存储服务，为容器应用提供相关数据持久化
3.3.1.4 镜像仓库镜像仓库是一个集中存储容器镜像的空间，在企业中建立企业级的镜像仓库有利于集中管理容器镜像，并且利于实现多个环境之间的镜像资源共享。Openshfit组件中提供了镜像仓库，且它与资源对象DeploymentConfig、BuildConfig等能够联动，以快速实现自动构建与自动部署，但是它与项目强关联并且在镜像安全扫描方面有所欠缺，很难将它作为一个企业级通用的镜像仓库。基于上面的考虑，本方案中将引入Harbor作为平台的统一镜像仓库，并通过镜像仓库命名规范与制品库管理规范集中管理所有环境中的镜像。
  
开发测试环境与生产环境网络是隔离的，分别部署独立的Harbor服务。其中项目在开发测试环境的镜像会被分别存放在不同的仓库中分别是DEV&#x2F;SIT&#x2F;UAT&#x2F;PROD，镜像只有经过严格测试达到上线标准后才能推送到PROD仓库中，PROD仓库与其它地区的开发测试环境中的PROD仓库同步，同时这与同地的生产环境中的PROD仓库同步，实现应用镜像的多地分发。
为了提高镜像的下载速度，以加快应用的部署，镜像服务还开启P2P预热，生产中将应用镜像提前分发到P2P网络。
3.3.1.5 监控告警监控告警系统是平台系统运营维护的有利保障，目前云原生生态中较为成熟也是被使用最广的方案是Prometheus套装。通过不同的exporter服务获取相关的指标数据，由Prometheus统一收集，并通过Grafana展示出图表，相关监控项状态与趋势一目了然，下图为Prometheus方案组件架构图。
  
Exporters：面向全方面资源的监控指标，具体有底层节点的相关指标、平台组件状态与性能指标、集群应用容器资源状态指标、自定义应用性能指标。
Prometheus server：通过pull方式从Exporters收集底层设备、平台组件、容器资源、应用自定义所有监控指标。
Grafana：提供对Prometheus采集的监控数据进行可视化展示。
Alertmanager：接入多种告警渠道（邮件、短信、微信等），统一管理多个来源的告警信息，如Prometheus rules策略触发的告警、日志监控触发的告警、自定检查脚本触发的告警，如下图所示为Alertmanager作为统一告警系统架构。
  
为了便于对多个集群集中监控，采用Prometheus Federation架构实现多级监控，将不同集群监控指标统一收集，并将历史监控数据持久化在MinIO S3存储中。
  
3.3.1.6 日志虽然Openshfit组件中提供了基于EFK的日志方案，但这种方案并不能适应所有的业务场景。它有以下不足之处：应用日志只有标准化输出才能被收集；只能有一个日志输出文件，而真实业务场景中会有多个日志文件；Fluentd的性能并不算太好；Fluented直接发送到ES，当日志量大时，很容易发生堵塞，导致日志延时大甚至丢失日志。基于以上这些考虑，本方案中将对日志方案进行性能与功能的增强。
  
采用更轻量及性能更好的Filebeat替换掉Fluentd来收集集群组件日志及应用标准输出的日志，仍然以DaemonSet资源的形式部署。
对于没有标准输出的应用，以sidecar的形式共享日志文件并通过filebeat收集日志文件，将其发送到Kafka中，由logstash转发到ElasticSearch服务，最后由Kibana服务展示。
引入Kafka作为日志缓冲层，提高集群应用日志的吞量。应用的标准输出日志，通过DaemonSet FileBeat服务统一收集并转发到Kafka中，由后端logstash转发到Elasticsearch服务中。
该日志方案不仅满足生产业务中的各种场景，而且能够支持高并发日志量。同时使用的组件均可通过横向扩展来扩大整个系统的吞吐量。
3.3.1.7 容器平台门户为了让架构具有更好的扩展性，云平台设计了四层架构：展示层、业务层、驱动层、数据层，如下图所示。另外为云平台设计了平台控制器，通过它实现容器集群资源对象数据实时缓存到Redis中。
  
l 展示层
负责平台的页面展示，它为用户提供一套可视化，交互式的界面。研发人员通过查看或者填写表单等操作，调用业务层Restful API接口，获取详细的资源与业务数据，并在页面渲染。
l 业务层
负责平台的业务逻辑，为展示层提供Restful API接口。它会对请求的权限做验证，通过调用驱动层获取请求的资源基础信息，并通过Redis获取底层资源的详细信息，最后以JSON格式数据返回给展示层。
l 驱动层
负责与底层集群的交互，它通过指定的证书可直接访问底层集群的Master API Server，进而实现对容器集群资源的管理。同时它通过Restful API为业务层提供所需的服务。
l 数据层
负责保存容器平台的业务数据，包括有项目信息、用户信息、审计信息、权限配置、集群配置信息等。
l 平台控制器
每个集群都会部署单独的平台控制器，并为它绑定获取该集群的所有资源信息的权限，使用watch API监听底层资源的变化，一旦对应集群有新增的资源，或有资源信息发生更改，或资源被删除，平台控制器都会将信息同步在Redis缓存中，从而收集所有集群的资源对象信息，并确保信息一直处于最新状态。
容器平台门户支持多集群管理，集群信息通过平台管理功能保存在数据库中。业务层请求驱动层时会带上被访问的集群名，驱动层通过查询数据库获取集群的访问信息，定位到正确的集群。
3.3.2 DevOps支撑DevOps平台采用禅道做为需求管理工具，同时采用Jenkins作为流水线引擎，其它工具链通过Jenkins进行驱动，实现应用自动高效构建与自动部署。同时Jenkins引入k8s插件，所有节点，包括Master节点与slave节点均部署在容器云平台之上，充分利用容器平台的快速部署能力，实现流水线的高效执行。以下是DevOps支撑能力的整体示意图。
  
3.3.2.1 需求管理引入禅道开源产品进行产品需求管理，并通过禅道中的电子看板实现需求全生命周期的可视化跟踪，完成需求计划管理。通过电子看板中的模块管理实现对需求所属功能模块的划分，让产品人员能够从整体上看到需求的功能划分。通过计划管理实现对需求的统一规划，我们可以定义每个时间节点的目标，根据优先级、工作量等条件把需求规划到每个计划中。
团队成员根据自身情况分析每个需求的工作量大小，并将需求分配到个人，计划开启后，每个人根据需求的实际完成情况修改需求的状态。同时通过与代码库、测试平台的对接，展现需求的代码提交情况、测试情况。
    使用统计报表，图形化展现需求的完成速率、历史情况，帮助项目组进行工作回顾。
3.3.2.2 开发l 代码配置管理
提供组织级的gitlab平台用于研发人员对代码进行配置管理。通过制定配置管理规划能够对提交的代码进行有效管控；通过对代码提交信息的格式约束能够将代码与需求进行关联；通过对代码库的webhook进行配置，代码提交即可触发流水线，对代码进行构建、扫描、测试，能够有效管控代码质量。除此之外，对于关键代码，还进行code review、代码走查等，保证提交的代码质量。
l 代码扫描
提供组织级的静态代码扫描平台SonarQube和Fortify，基于PMD、CheckStyle、FindBugs制订了组织级的代码检查规则。研发人员代码一旦提交到代码库就会触发代码扫描流水线，能够及时发现代码中的Issues和Bugs，可视化技术债务，明确技术债务分布情况、债务点以及改进建议等，从而能够及时得到解决，提高产品质量。
l 制品管理
基于Nexus构建组织级的制品仓库，对制品进行成熟度管理，只有制品满足各项度量指标后，制品才能部署到下一阶段，最终部署生产环境。同时在制品元数据中记录制品的质量数据，确保了部署到生产环境的制品是经过严格测试的、满足质量要求的。
l 单元测试
要求代码的单元测试覆盖率满足一定要求，同时关键代码必须要有单元测试。在实际开发过程中，使用TestNG、Junit等工具进行单元测试案例的编写，并引入Jacoco进行单元测试覆盖率的收集。只有单元测试覆盖率满足一定要求后，代码才能进行打包构建并上传到制品仓库中，进而后期才能进行部署及自动化测试等。
3.3.2.3 测试禅道平台也作为测试管理平台，对测试案例、缺陷进行统一管理，并且与需求进行关联。同时会将测试的结果数据记录在制品的元数据中，确保部署到生产环境的制品必须要符合质量要求。
l 功能测试
采用selenium工具对于需要上线的系统进行功能测试并且生成测试报告，待评审通过以后才能进行上线。通过对系统进行白盒、黑盒、灰盒、边界值等多维度的测试，确保系统能够满足功能需求。
l 自动化测试
采用selenium工具开发实现自动化测试脚本，并作为Jenkins构建任务的一部分。自动化测试主要包含两个方面，一个是UI测试，一个是接口测试。通过CICD流水线将制品成功部署到测试环境以后，自动会触发自动化测试流水线，通过编写好的自动化测试脚本对系统进行测试。
3.3.2.4 部署通过打包构建生成制品以后，制品会根据事先定义好的Dockerfile构建成镜像并上传到镜像仓库中。然后使用容器平台将镜像依次部署到SIT、UAT等环境进行测试。测试通过以后将镜像提交到待发布库中。生成下发流程通过以后，触发容器平台进行生产部署。
3.3.2.5 运营维护通过容器平台，可以很便利的对其中的应用进行日志、性能等多维度的监控，同时提供了预警机制，当机器或应用性能触发阈值以后，会向应用负责人发送短信和邮件提醒。从而应用负责人能够实时掌握应用状态，对出现的一些状况能及时处理。
3.3.2.6 平台度量对各个能力子域的指标进行量化，在流水线的实际执行过程中收集这些指标，同时对收集指标进行分析、打分，得出此次执行的一个量化数据。通过收集每次流水线执行的指标数据，就可以获取到该应用的整个质量的趋势，从而能够更好的指导应用开发。
4 规范指引  
4.1 业务规范4.1.1 业务需求规范l 统一语言
业务需求应当使用统一的语言，所谓统一语言，是指项目研发过程中，产品、研发、测试、运维、管理、运营等角色在交流中，对一些专有名词理解达成统一，以保证大家在沟通中没有信息不一致或信息歧异，提高沟通效率和准确度。
l 业务需求格式
业务需求描述中应明确，清晰，不应使用可能、一般等模棱两可的描述，应当体现出业务功能的角色、活动、价值（效果）。此处不应使用技术语言来描述，要使用用户可以理解的业务语言来描述。通常格式为作为一个&lt;角色&gt;, 我想要&lt;活动&gt;,以便于&lt;商业价值。
举例：作为一个“网站管理员”，我想要“统计每天有多少人访问了我的网站”，以便于“我的赞助商了解我的网站会给他们带来什么收益。”
4.2 应用上云设计规范4.2.1 应用容器化设计规范l 应用中不指定Pod的IP
应用容器化部署后，Pod的替换伴随着IP的变化。若应用指定Pod的IP访问目标应用，一旦目标应用的Pod发生变动，目标应用的IP也会随着变化，目标应用将无法被正常访问。为了保证应用访问服务的稳定性，应用中不直接通过Pod的IP来访问目标应用。可以通过注册中心，获得动态IP的方式来实现服务间的调用，也可以直接使用serviceIP来实现不同应用间的调用。
l 同一Pod内的不同容器间使用127.0.0.1相互访问
同一个Pod可以有多个容器，这些容器共享同一个网络命名空间，它们之间推荐直接使用127.0.0.1来实现互相访问。
l 为应用实例提供健康检查接口
应用需提供健康检测接口，用来配置容器的健康检查策略，从而保证滚动升级过程中的服务可用性。
l 应用容器需要考虑优雅退出
为了保证应用服务的稳定，应用容器需要考虑优雅退出，确保容器退出时关闭所有连接。如果应用程序未处理SIGTERM信号,可以在编排文件中设置preStop Hook,即在关闭容器前等待一段时间，让应用程序完成所有请求。
l 使用CronJob代替crontab服务设置定时任务
不建议在容器中使用crontab服务设置定时任务，推荐通过CronJob资源对象来实现定时任务的功能。
4.2.2 应用部署规范l 应用容器通过使用PV&#x2F;PVC进行持久化数据
容器本身并不带有持久化存储，被销毁时容器中存储的数据也会被清理。容器中如果需要保存数据，需要通过PV（持久化卷）与PVC（持久化卷请求）资源，使用NAS存储，实现数据持久化。
l ConfigMap挂载到容器内部的文件夹必须为空
配置文件可以保存在ConfigMap中，ConfigMap挂载到容器内部的目录会把目录下的原有文件覆盖，所以需要注意挂载的目录尽量为空目录。
l 每个系统申请一个EgressIP，作为访问外部服务的出口IP
为了对系统网络做更精细化的控制，容器云平台每个系统必须申请并绑定一个EgressIP。该系统下的应用Pod都以该IP为出口IP访问外部服务，通过防火墙设置，可以对系统下的应用访问外部服务的网络进行控制。
l 为应用实例设置健康检查
为了更好地监控应用服务的状态，充分发挥容器云平台为应用带来的自愈能力，提高应用的可靠性，应用必须设置readinessProbe与livenessProbe。其中readinessProbe将检测应用容器是否已准备好接受流量，livenessProbe将检测应用容器是否存活。
l 为应用实例设置资源限制（cpu与memory）
为了对资源进行精细化管理与控制，减少底层资源的竞争，也可避免程序因Bug占用过多底层资源，以提高应用的稳定性，容器基础平台的应用部署时必须为每个容器设置资源限制，包括：limits.cpu、limits.memory、requests.cpu、requests.memory。其中limits.cpu、limits.memory为应用运行过程中占用CPU、Memory资源的上限值；requests.cpu、requests.memory为应用运行过程中请求的CPU、Memory资源值。
4.3 应用开发规范4.3.1 数据交互规范l 接口格式
通常每个URI网址代表一种资源接口，网址中不应有动词，尽量使用名词（特殊情况可以使用动词）；网址名称不应大写，若需要分割时使用中杠-不用下杠 _ ；若URI中的名词表示资源集合，使用复数形式。
应当使用url来表达层级，用于按实体关联关系进行对象导航。层级不应过深，复杂场景尽量使用查询参数代替路径中的实体导航。应当将API的版本号放入到URI中。
l HTTP方法
应使用标准的HTTP方法实现对资源的CRUD，包括：
GET：查询（从服务器取出资源一项或多项）；
POST：创建单个新资源。 POST向“资源集合”型uri发起；
PUT：更新单个资源（全量），客户端提供完整的更新后的资源；
DELETE：删除。
其中GET、PUT、DELETE方法应具备幂等性，也就是执行1次和执行N次，对资源状态改变的效果应是等价的。
l HTTP状态码和提示信息
正确设置http状态码，不要自定义http状态码。服务器返回的提示信息应尽量简洁，避免嵌套，采用信息代码（用于日志&#x2F;问题追查）+错误的描述文本（展示给用户）的形式。
l 接口文档
数据交互应形成良好的接口文档，包括但不限于以下内容：
1) HTTP方法类型：也就是我们常写的GET，POST，PUT，DELETE等。
2) url调用方法：从前端调后端的方法地址。
3) 请求参数：包括字段、说明、类型、备注、是否必填。
4) 返回参数：尽量使用JSON，避免使用XML。
l 异常处理
异常应包括业务异常和非业务异常。业务异常由自己的业务代码抛出，表示一个用例的前置条件不满足、业务规则冲突等，比如参数校验不通过、权限校验失败。非业务类异常 表示不在预期内的问题，通常由类库、框架抛出，或由于自己的代码逻辑错误导致，比如数据库连接失败、空指针异常、除0错误等等。
业务异常应返回200的HTTP响应状态码，并返回指定的错误文本提示信息。非业务异常应返回500的HTTP响应状态码，异常信息应进行统一封装，如“服务器端错误，请稍后再试”，非必要情况不应将错误类型展示给用户。
l 安全
对于API的使用应当有身份认证，同时具备一定的安全手段用于预防常见的安全攻击。
4.3.2 应用镜像构建规范l 使用统一的基础镜像
基础镜像涉及应用运行所需要的安全可靠的Linux操作系统。基础镜像由基础架构部门负责维护，定期打安全漏洞的补丁，并基于应用要求构建基础环境镜像，维护到公共镜像仓库。
l 公共依赖层
主要是包括运行时（如JDK）、中间件（如Web容器）、其它公共组件等。公共依赖层镜像由基础架构部门负责维护，定期打安全漏洞的补丁，并基于应用要求构建依赖层镜像，维护到公共镜像仓库。
l 应用层
主要包括程序包（jar包、war包）、程序依赖包（jar lib）等。程序包基于代码基线编译打包，应用层镜像由开发部门负责构建。
l 镜像一次构建，多环境部署
为了保证应用的运行环境保持一致，应用容器镜像应在研发环境下构建，在其它环境下部署进行测试与上线均使用同一个镜像。
l 使用非root用户启动应用容器
为保障容器云平台的安全性，禁止应用容器中使用root用户运行应用。
4.3.3 DevOps指引4.3.3.1 开发流程自动集成1) 项目配置好CI&#x2F;CD流水线后，开发人员提交代码；
2) 自动触发CI&#x2F;CD流水线运行，对项目代码进行代码扫描、单元测试；
3) 获取私服Maven仓库资源（非必须）进行构建；
4) 构建完成后，生成最新的镜像保存到镜像仓库。
下图为开发集成部署流程图。
  
4.3.3.2 开发提测流程1) 开发人员用gitlab给代码打上提测Tag；
2) 开发人员手动在Jenkins上提测，执行提测任务；
3) 自动触发工程指定版本代码的构建，并生成指定版本的镜像；
4) 测试人员在测试环境部署指定版本镜像，进行测试。
下图为开发提测流程图：
  
4.3.3.3 集成测试流程测试人员在测试环境部署指定版本镜像，进行测试。
下图为集成测试流程图：
  
说明：以上流程均可在开发测试PaaS集群操作，如果要上生产部署，需搭为生产环境单独部署另外一套PaaS集群，同时两个集群网络与资源隔离。
4.3.3.4 生产部署流程1) 运维人员从测试环境同步指定版本的镜像；
2) 运维人员在生产环境部署指定版本镜像。
下图为生产部署流程图：
  
4.4 运维规范4.4.1 容器平台运维规范4.4.1.1 定期巡检将Openshfit集群重要组件的健康状态检查封装在shell脚本中，设定定时任务，实现自动化巡检，按时生成巡检报告，以邮件形式发送至平台运维、研发单位。定期巡检包括：master-api、master-controller、Etcd、router、es、prometheus等组件的状态。
4.4.1.2 扩容评估设计Openshfit集群资源预申请台账，结合集群管理门户查询到的集群资源实际使用情况，记录Openshfit集群当前的资源总量和计划上线的系统所需资源，根据台账的资源申请比例，安排计算、存储资源扩容。
4.4.1.3 应急预案根据Openshfit集群特点，制定应急预案：一是记录硬件资源、软件资源、集群逻辑架构、数据备份方案、关联系统、应急联系人等信息；二是记录系统节点及部署模式，便于故障发生时快速查阅节点信息；三是针对平台层11个故障场景（硬件、网络、平台组件）、应用层6个故障（管理门户、sftp等）场景的处置方案，以及故障恢复后的验证方案。
其中平台层故障处置方案主要有以下三个层面：
基础设施故障：物理主机故障、基础网络故障、存储设备故障。
平台服务故障：管理节点集群组件服务异常（Master API、Master Controller、Etcd)，工作节点服务异常（atomic-Openshfit-node、Docker服务），集群间网络异常（sdn、ovs服务）。
运维相关应用故障：监控服务异常（Prometheus服务），日志服务异常（Fluentd、ES服务）。
4.4.2 应用运维规范4.4.2.1 应用发布Openshfit集群将部署文件和镜像文件从制品库拉取至本地，再通过oc命令进行发布。该部署过程按场景编排成多个自动化流程，通过在管理端填写发布相关的参数执行。
4.4.2.2 应用回退Openshfit集群将旧版本的部署文件和镜像文件从制品库拉取至本地，再通过或oc命令进行发布。该回退过程按场景编排成自动化流程，通过在管理端填写版本tag相关的参数执行。
4.4.2.3 应用监控设计每分钟一次的定时任务，将影响应用可用性的关键指标，结合集群各宿主机节点的系统指标数据统一发送至监控平台进行集中管理、展示、和告警。
4.4.2.4 应用伸缩管理根据特定的大流量业务场景需要，平台允许在特定时间区间、在平台可以容纳的容量范围内，对应用进行扩缩容处理，扩缩容命令封装后，编排成自动化运维平台的流程，通过填入扩缩容副本数量等参数执行。
4.4.2.5 日志管理容器应用日志须落盘持久化卷存放，不同容器分不同文件进行存储。日志必须进行合理分级，按时间或大小自动分割，并依据合理的压缩和清理周期管理。
容器应用日志命名合理规范，日志内容要素齐全：时间、日志级别（INFO、WARN、ERROR）、线程名称、交易标识、用户id、日志消息体、异常堆栈等。
容器日志访问须提供两种方式：一是由平台自带的EFK组件统一管理和展示，二是通过应用系统相关虚拟机或物理机节点直接访问。
4.4.3 制品管理规范4.4.3.1 镜像准入管理基础镜像使用RedHat操作系统。
公共镜像原则上仅包括一种中间件软件。若同一公共镜像需要包括大于一种中间件软件，必须采用多次构建方式。
应用镜像必须使用镜像仓库统一提供的基础镜像或公共镜像构建。应用镜像必须符合精简原则，应包括且仅包括针对公共镜像的配置修改、可执行的应用包、依赖包介质和启动命令，不得包括其它与应用无关的文件。应用参数配置文件不应包含在应用镜像内部。
4.4.3.2 镜像命名管理镜像命名使用小写字母，避免下划线，最长64个字符，每段命名间使用“-”连接。
格式如下： -&lt; scope &gt;---
其中：
1) 指项目组或系统，可自行定义，避免使用下划线。
2) 指镜像库的依赖范围，可填选项为一方库、二方库、三方库。
3) 指采用的技术或包类型。每个仓库中应保持同一种类型的二进制文件。
4) 指镜像库的开发成熟度，例如开发、测试和发布阶段。
5) 指镜像库所处的物理地点，例如上海、福州、成都或远程、虚拟等。虚拟仓库不需要指定。
5 分阶段实施规划5.1 方案验证阶段5.1.1 功能测试设计功能测试验证样例，在测试环境按生产1:1搭建容器云平台，详细测试容器云平台各功能点，并将DevOps体系各模块串连起来，完成持续构建与持续部署链路的验证。并设计集成测试验证样例，模拟生产环境发起业务验证，最终形成《集成测试报告》，所有业务场景验证通过。
5.1.2 压力测试为保障基于Openshfit构建的容器平台在实际生产运行中符合非功能性要求，在准生产环境部署Openshfit集群，通过测试工具发起压力测试，测试结果满足业务运行要求。以及测试DevOps系统支持并发构建与部署的最大容量。
5.2 技术推广阶段经过功能测试和压力测试，集群满足上线部署要求，DevOps体系能够完成应用全生命周期的调度后，提交运维单位在生产环境部署集群，部署Openshfit管理门户、配置项目计算资源，对接制品库，从制品库拉取镜像部署应用。通过shell脚本实现监控信息转化，传送至统一监控告警系统，实现分钟级监控集群日常运行状态。
在此阶段中，应不断提升与优化容器平台及DevOps平台，以适应本企业各项目组的实际要求，最终形成最佳实践与标准规范。
5.3 规模化推广阶段经过技术推广，验证了Openshfit集群在生产环境承载业务的能力，在本地生产环境部署了双活集群，前端业务流量通过F5负载均衡至两个集群，实现集群层面的高可用性。后续，本集群可以针对灾备要求高的信息系统，部署异地灾备集群。诸如企业级短信发送平台这类高可用要求高，但对数据查询实时性要求不高的系统，可以部署在异地双活的Openshfit集群，数据库分别部署在两地，每日日结后再将当日数据合并至历史库。根据部署类型，本方案中设计的容器云平台，可以支持本地双活、异地灾备、异地双活的多种集群部署场景。
同时根据企业在上一阶段形成的最佳实践及标准规范，全面推广DevOps体系，项目优先使用DevOps平台全面支撑应用的开发、测试及部署，并建设DevOps体系成熟度评估模型，标准化推广DevOps建设。
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubefwd——Openshift-K8S-本地开发的福音</title>
    <url>/posts/f8319a6131a9.html</url>
    <content><![CDATA[小志：“kubefwd 本地开发的福音啊，本地环境直接连接 svc。”当小志推荐这个工具的时候，我正在 Kubecon 大会的现场，听着 Kubernetes 近一年的各种成就和各种新特性。我无法看到微信另一头小志的表情，也许平静，也许跟我现在一样激动。kubefwd，这是第一次听到这个工具，“本地环境直接连接 svc”，openshift 的 port-fowrward 命令就能做到，没什么也不起的吧。我回复道：“openshift 的 port-forward，再加个自动更新本地 hosts 文件”。小志发来了 kubefwd 请求流向图。

服务 api 与服务 auth 提供的都是 80 端口，但是它们能够同时被本地访问，这是 port-forward 无法做到的！我意识到这个工具很不简单，刚才的想法太草率了。我马上回复：“我刚才理解得不对，它可以做到一个 port 对应多个 service！”此刻的心情其实已经非常激动了，因为意识到 kubefwd 这个工具也许解决了困扰我多天的问题。

近一周一直在脑中徘徊的是徐磊老师介绍微软的 TFS 时的演示：开发人员本地修改代码，可以在开发环境独立的 namespace 下实时查看代码生效的结果，当时我为之震惊。因为之前想的各种流程，终究是需要开发者提交代码才能进行下一步测试的。“创建一个环境，让开发者能够以最方便的形式进行开发，这是最直接地提高效率的方式。”于是我开始寻找一种在 openshift&#x2F;k8s 环境下的开源解决方案，测试了 openshift-connector，虽然它跟 TFS 中的代码同步功能有些类似，但是一直没有测试成功，这个插件的关注者很少。后来找了 redhat 的朋友确认，他告诉我这个项目也许已经停止了。我继续寻找，但终究没有新的收获。

我跟一起参会的小伙伴说：“对我而言，也许这次大会的内容，都不及知道了这个工具。”
晚上回到家，我赶紧做测试。

第一个要验证的就是 kubefwd 对 Openshift 是否支持，毕竟我们开发测试环境，甚至有些项目的生产环境是在 Openshift 上。
第二个要验证的就是 kubefwd 是否支持在 windows 系统上运行，毕竟研发几乎都在 windows 上做开发。

测试结论

kubefwd 对 Openshift 完全支持
kubefwd 在 windows 系统上运行正常

我为什么会如此激动？或者说使用 kubefwd 带来什么样的改变？

开发人员无需在本地模拟一套完整的线上应用环境就能够测试与其他应用的集成效果
本地开发应用也能使用远端集群下的中间件，同时使用的配置与集群中的应用完成一样，无需专门做本地访问的配置管理
开发人员无需提交代码就能与其他应用集成，查看代码生效后的效果
开发环境变成了相对稳定的集成环境，每个开发者本地版本不会互相影响，降低多个应用同时开发的相互干扰

对于 kubefwd 如何使用，看它的帮助说明就够了，非常简单。
Usage:  kubefwd services [flags]Aliases:  services, svcs, svcExamples:  kubefwd svc -n the-project  kubefwd svc -n the-project -l env=dev,component=api  kubefwd svc -n default -l &quot;app in (ws, api)&quot;  kubefwd svc -n default -n the-project  kubefwd svc -n default -d internal.example.com  kubefwd svc -n the-project -x prod-clusterFlags:  -x, --context strings     specify a context to override the current context  -d, --domain string       Append a pseudo domain name to generated host names.      --exitonfailure       Exit(1) on failure. Useful for forcing a container restart.  -h, --help                help for services  -c, --kubeconfig string   absolute path to a kubectl config fil (default &quot;/Users/cjimti/.kube/config&quot;)  -n, --namespace strings   Specify a namespace. Specify multiple namespaces by duplicating this argument.  -l, --selector string     Selector (label query) to filter on; supports &#x27;=&#x27;, &#x27;==&#x27;, and &#x27;!=&#x27; (e.g. -l key1=value1,key2=value2).  -v, --verbose             Verbose output.

项目地址kubefwd https://github.com/txn2/kubefwd
补充小志还分享了个方便端口转发的应用，“oc port-forward 的图形化应用”。地址：https://kube-forwarder.pixelpoint.io/
进一步发现了一个新的工具kt-connect ，一个可以让开发环境访问 K8S 集群下应用的工具，可以直接访问容器。
$ # 检查依赖环境$ ktctl check$ # 1. 安装sshuttle$ sudo pip install sshuttle -i https://pypi.douban.com/simple$ # openshift中，先创建一个project$ oc new-project ktconnect$ oc adm policy add-scc-to-user anyuid -z default #因为默认kt-connect-daemon需要root用户启动$ sudo ktctl -n ktconnect connect11:15AM INF Connect Start At 6759511:15AM INF Client address 192.168.0.13811:15AM INF deploy shadow deployment kt-connect-daemon-yuiaq in namespace ktconnect11:15AM INF pod label: kt=kt-connect-daemon-yuiaq11:15AM INF pod: kt-connect-daemon-yuiaq-665dd6bc55-tzb74 is running,but not ready11:15AM INF pod: kt-connect-daemon-yuiaq-665dd6bc55-tzb74 is running,but not ready11:15AM INF pod: kt-connect-daemon-yuiaq-665dd6bc55-tzb74 is running,but not ready11:15AM INF pod: kt-connect-daemon-yuiaq-665dd6bc55-tzb74 is running,but not ready11:15AM INF Shadow pod: kt-connect-daemon-yuiaq-665dd6bc55-tzb74 is ready.11:15AM INF Fail to get pod cidr from node.Spec.PODCIDR, try to get with pod sampleForwarding from 127.0.0.1:2222 -&gt; 22Forwarding from [::1]:2222 -&gt; 2211:16AM INF port-forward start at pid: 67596Handling connection for 2222Warning: Permanently added &#x27;[127.0.0.1]:2222&#x27; (ECDSA) to the list of known hosts.client: Connected.11:16AM INF vpn(sshuttle) start at pid: 6759711:16AM INF KT proxy start successful
]]></content>
      <tags>
        <tag>openshift</tag>
      </tags>
  </entry>
</search>
